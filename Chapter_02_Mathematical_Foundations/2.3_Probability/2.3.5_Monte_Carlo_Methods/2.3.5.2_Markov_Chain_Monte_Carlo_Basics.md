# 2.3.5.2 Markov Chain Monte Carlo Basics

## Understanding Markov Chain Monte Carlo

Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from probability distributions by constructing a Markov chain that has the desired distribution as its equilibrium distribution. These methods are particularly valuable when direct sampling is difficult or impossible, which is often the case in complex, high-dimensional problems in machine learning and statistics. In this section, we'll explore the fundamentals of MCMC methods, focusing on the basic concepts and algorithms.

## Motivation for MCMC

### The Challenge of High-Dimensional Sampling

Many problems in machine learning involve sampling from complex, high-dimensional distributions:

1. **Bayesian Inference**: Sampling from posterior distributions p(θ|D)
2. **Probabilistic Models**: Sampling from joint distributions over many variables
3. **Optimization**: Exploring complex, multimodal objective functions

Direct sampling methods like rejection sampling and importance sampling often fail in high dimensions due to:

- **Curse of Dimensionality**: The volume of the space grows exponentially with dimension
- **Inefficiency**: Most samples are rejected or have negligible weights
- **Proposal Design**: Finding good proposal distributions becomes increasingly difficult

### The MCMC Solution

MCMC methods address these challenges by:

1. **Sequential Sampling**: Generating each sample based on the previous one
2. **Local Exploration**: Making small, local moves in the parameter space
3. **Asymptotic Guarantees**: Converging to the target distribution regardless of the starting point
4. **Adaptability**: Working with distributions known only up to a normalizing constant

## Markov Chains

### Definition and Properties

A **Markov chain** is a sequence of random variables X₁, X₂, ..., X_n such that the probability of the next state depends only on the current state:

$$P(X_{t+1} = x_{t+1} | X_t = x_t, X_{t-1} = x_{t-1}, ..., X_1 = x_1) = P(X_{t+1} = x_{t+1} | X_t = x_t)$$

Key properties of Markov chains include:

1. **Stationarity**: A distribution π is stationary if X_t ~ π implies X_{t+1} ~ π
2. **Irreducibility**: The chain can reach any state from any other state
3. **Aperiodicity**: The chain doesn't cycle through a fixed sequence of states
4. **Ergodicity**: A chain that is both irreducible and aperiodic will converge to a unique stationary distribution

### Transition Kernels

The **transition kernel** P(x, y) defines the probability of moving from state x to state y:

$$P(x, y) = P(X_{t+1} = y | X_t = x)$$

For a continuous state space, the transition kernel is a conditional probability density function.

### Detailed Balance

A sufficient (but not necessary) condition for a Markov chain to have a stationary distribution π is **detailed balance**:

$$\pi(x) P(x, y) = \pi(y) P(y, x)$$

for all states x and y. This condition ensures that the probability flow between any two states is balanced.

## The Metropolis-Hastings Algorithm

### Algorithm Description

The **Metropolis-Hastings algorithm** is one of the most fundamental MCMC methods. It constructs a Markov chain with a specified stationary distribution π by using a proposal distribution q and an acceptance rule.

The algorithm proceeds as follows:

1. Initialize the chain at a starting point X₀
2. For t = 0, 1, 2, ...:
   a. Sample a candidate point Y from the proposal distribution q(·|X_t)
   b. Compute the acceptance ratio:
      $$\alpha = \min\left(1, \frac{\pi(Y) q(X_t|Y)}{\pi(X_t) q(Y|X_t)}\right)$$
   c. With probability α, set X_{t+1} = Y (accept); otherwise, set X_{t+1} = X_t (reject)

### Properties

The Metropolis-Hastings algorithm has several important properties:

1. **Target Distribution**: The chain converges to the target distribution π regardless of the choice of proposal distribution q
2. **Unnormalized Distributions**: The algorithm only requires π up to a normalizing constant
3. **Acceptance Rate**: The efficiency of the algorithm depends on the acceptance rate, which is influenced by the proposal distribution
4. **Convergence Speed**: The rate of convergence depends on how well the proposal distribution explores the target distribution

### Special Cases

1. **Metropolis Algorithm**: When the proposal distribution is symmetric (q(y|x) = q(x|y)), the acceptance ratio simplifies to:
   $$\alpha = \min\left(1, \frac{\pi(Y)}{\pi(X_t)}\right)$$

2. **Independence Sampler**: When the proposal distribution doesn't depend on the current state (q(y|x) = q(y)), the acceptance ratio becomes:
   $$\alpha = \min\left(1, \frac{\pi(Y) q(X_t)}{\pi(X_t) q(Y)}\right)$$

3. **Random Walk Metropolis**: When the proposal is a random walk (Y = X_t + ε), the acceptance ratio is:
   $$\alpha = \min\left(1, \frac{\pi(Y)}{\pi(X_t)}\right)$$

### Example: Sampling from a Mixture of Gaussians

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def target_pdf(x):
    """Target distribution: mixture of two Gaussians"""
    return 0.3 * stats.norm.pdf(x, -3, 1) + 0.7 * stats.norm.pdf(x, 3, 1)

def metropolis_hastings(n_samples, proposal_std=1.0):
    """Metropolis-Hastings algorithm for sampling from the target distribution"""
    samples = np.zeros(n_samples)
    
    # Initialize
    current = 0.0  # Start at x=0
    
    # Run the Markov chain
    for i in range(n_samples):
        # Propose a new point
        proposal = current + np.random.normal(0, proposal_std)
        
        # Compute acceptance ratio
        ratio = target_pdf(proposal) / target_pdf(current)
        alpha = min(1, ratio)
        
        # Accept or reject
        if np.random.uniform(0, 1) < alpha:
            current = proposal  # Accept
        
        # Store the current sample
        samples[i] = current
    
    return samples

# Generate samples
n_samples = 10000
burn_in = 1000
samples = metropolis_hastings(n_samples + burn_in)
samples = samples[burn_in:]  # Discard burn-in period

# Plot the results
plt.figure(figsize=(12, 6))

# Plot the target distribution
x = np.linspace(-8, 8, 1000)
plt.plot(x, target_pdf(x), 'r-', lw=2, label='Target PDF')

# Plot the histogram of samples
plt.hist(samples, bins=50, density=True, alpha=0.7, label='MCMC Samples')

# Plot the trace of the chain
plt.figure(figsize=(12, 4))
plt.plot(samples[:500], 'b-')
plt.xlabel('Iteration')
plt.ylabel('Sample Value')
plt.title('Trace of the Markov Chain (first 500 samples after burn-in)')
plt.grid(True)

plt.tight_layout()
plt.show()

# Compute acceptance rate
def compute_acceptance_rate(n_samples, proposal_std):
    samples = np.zeros(n_samples)
    current = 0.0
    accepted = 0
    
    for i in range(n_samples):
        proposal = current + np.random.normal(0, proposal_std)
        ratio = target_pdf(proposal) / target_pdf(current)
        alpha = min(1, ratio)
        
        if np.random.uniform(0, 1) < alpha:
            current = proposal
            accepted += 1
        
        samples[i] = current
    
    return accepted / n_samples

# Test different proposal standard deviations
std_values = [0.1, 0.5, 1.0, 2.0, 5.0]
acceptance_rates = [compute_acceptance_rate(5000, std) for std in std_values]

plt.figure(figsize=(10, 6))
plt.plot(std_values, acceptance_rates, 'o-')
plt.xlabel('Proposal Standard Deviation')
plt.ylabel('Acceptance Rate')
plt.title('Effect of Proposal Distribution on Acceptance Rate')
plt.grid(True)
plt.show()
```

## The Gibbs Sampling Algorithm

### Algorithm Description

**Gibbs sampling** is a special case of the Metropolis-Hastings algorithm where the proposal distributions are the conditional distributions of the target distribution. It's particularly useful for multivariate distributions where the conditional distributions are easy to sample from.

The algorithm proceeds as follows:

1. Initialize the chain at a starting point X₀ = (X₀₁, X₀₂, ..., X₀ₙ)
2. For t = 0, 1, 2, ...:
   a. Set X_{t+1} = X_t
   b. For i = 1, 2, ..., n:
      i. Sample X_{t+1,i} from the conditional distribution p(X_i | X_{t+1,1}, ..., X_{t+1,i-1}, X_{t,i+1}, ..., X_{t,n})

### Properties

Gibbs sampling has several important properties:

1. **Acceptance Rate**: All proposals are accepted (acceptance rate = 1)
2. **Conditional Distributions**: Requires the ability to sample from all conditional distributions
3. **Coordinate-wise Updates**: Updates one variable at a time, which can be slow for highly correlated variables
4. **No Tuning**: Doesn't require tuning of proposal distributions

### Example: Sampling from a Bivariate Normal

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def gibbs_sampler_bivariate_normal(n_samples, mu, sigma):
    """Gibbs sampler for a bivariate normal distribution"""
    # Extract parameters
    mu_1, mu_2 = mu
    sigma_1, sigma_2 = np.sqrt(np.diag(sigma))
    rho = sigma[0, 1] / (sigma_1 * sigma_2)
    
    # Initialize samples
    samples = np.zeros((n_samples, 2))
    x2 = 0.0  # Initial value for x2
    
    # Run the Markov chain
    for i in range(n_samples):
        # Sample x1 given x2
        mean_1 = mu_1 + rho * sigma_1 / sigma_2 * (x2 - mu_2)
        sd_1 = sigma_1 * np.sqrt(1 - rho**2)
        x1 = np.random.normal(mean_1, sd_1)
        
        # Sample x2 given x1
        mean_2 = mu_2 + rho * sigma_2 / sigma_1 * (x1 - mu_1)
        sd_2 = sigma_2 * np.sqrt(1 - rho**2)
        x2 = np.random.normal(mean_2, sd_2)
        
        # Store the samples
        samples[i] = [x1, x2]
    
    return samples

# Parameters for bivariate normal
mu = np.array([1.0, 2.0])
sigma = np.array([[1.0, 0.8], [0.8, 1.0]])  # Correlation = 0.8

# Generate samples
n_samples = 5000
burn_in = 1000
samples = gibbs_sampler_bivariate_normal(n_samples + burn_in, mu, sigma)
samples = samples[burn_in:]  # Discard burn-in period

# Plot the results
plt.figure(figsize=(10, 8))

# Scatter plot of samples
plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Gibbs Sampling from Bivariate Normal')
plt.grid(True)

# Plot the trace of the chain
plt.figure(figsize=(12, 8))
plt.subplot(2, 1, 1)
plt.plot(samples[:500, 0], 'b-')
plt.ylabel('X1')
plt.title('Trace of the Markov Chain (first 500 samples after burn-in)')
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(samples[:500, 1], 'r-')
plt.xlabel('Iteration')
plt.ylabel('X2')
plt.grid(True)

plt.tight_layout()
plt.show()

# Compare with direct sampling
direct_samples = np.random.multivariate_normal(mu, sigma, n_samples)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(direct_samples[:, 0], direct_samples[:, 1], alpha=0.5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Direct Sampling')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Gibbs Sampling')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Practical Considerations

### 1. Burn-in Period

The initial samples from an MCMC chain may not be representative of the target distribution. A **burn-in period** (or warm-up period) is often used to discard the initial samples:

$$\{X_{b+1}, X_{b+2}, ..., X_n\}$$

where b is the length of the burn-in period.

### 2. Thinning

To reduce autocorrelation between samples, **thinning** involves keeping only every k-th sample:

$$\{X_b, X_{b+k}, X_{b+2k}, ...\}$$

However, thinning is generally not recommended for reducing variance in Monte Carlo estimates.

### 3. Convergence Diagnostics

Assessing convergence of MCMC chains is crucial. Common diagnostics include:

- **Trace Plots**: Visual inspection of the sample trajectory
- **Autocorrelation Plots**: Measuring correlation between samples at different lags
- **Gelman-Rubin Statistic**: Comparing multiple chains with different starting points
- **Effective Sample Size**: Estimating the equivalent number of independent samples

### 4. Proposal Distribution Tuning

The efficiency of Metropolis-Hastings depends on the proposal distribution:

- **Too Small**: High acceptance rate but slow exploration (small steps)
- **Too Large**: Low acceptance rate and inefficient sampling (most proposals rejected)
- **Optimal**: For random walk Metropolis, an acceptance rate of around 0.234 is often recommended for high-dimensional problems

### Example: Convergence Diagnostics

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def run_metropolis_chain(n_samples, proposal_std, start_point=0.0):
    """Run a Metropolis chain for the mixture of Gaussians target"""
    samples = np.zeros(n_samples)
    current = start_point
    
    for i in range(n_samples):
        proposal = current + np.random.normal(0, proposal_std)
        ratio = target_pdf(proposal) / target_pdf(current)
        alpha = min(1, ratio)
        
        if np.random.uniform(0, 1) < alpha:
            current = proposal
        
        samples[i] = current
    
    return samples

# Run multiple chains with different starting points
n_samples = 5000
burn_in = 1000
n_chains = 3
starting_points = [-5.0, 0.0, 5.0]
chains = [run_metropolis_chain(n_samples + burn_in, 1.0, start) for start in starting_points]
chains = [chain[burn_in:] for chain in chains]  # Discard burn-in

# Plot the traces
plt.figure(figsize=(12, 8))
for i, chain in enumerate(chains):
    plt.plot(chain[:500], label=f'Chain {i+1} (start={starting_points[i]})')
plt.xlabel('Iteration')
plt.ylabel('Sample Value')
plt.title('Trace Plots of Multiple Chains')
plt.legend()
plt.grid(True)
plt.show()

# Compute autocorrelation
def autocorrelation(chain, max_lag=50):
    """Compute autocorrelation for a chain"""
    mean = np.mean(chain)
    var = np.var(chain)
    chain_centered = chain - mean
    
    acf = np.zeros(max_lag + 1)
    for lag in range(max_lag + 1):
        acf[lag] = np.sum(chain_centered[lag:] * chain_centered[:-lag if lag > 0 else None]) / (len(chain) * var)
    
    return acf

# Plot autocorrelation
plt.figure(figsize=(12, 6))
for i, chain in enumerate(chains):
    acf = autocorrelation(chain)
    plt.plot(acf, label=f'Chain {i+1}')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation Function')
plt.legend()
plt.grid(True)
plt.show()

# Compute Gelman-Rubin statistic
def gelman_rubin(chains):
    """Compute Gelman-Rubin statistic for multiple chains"""
    n = chains[0].shape[0]  # Length of each chain
    m = len(chains)  # Number of chains
    
    # Compute chain means
    chain_means = np.array([np.mean(chain) for chain in chains])
    
    # Compute overall mean
    overall_mean = np.mean(chain_means)
    
    # Compute between-chain variance
    B = n * np.sum((chain_means - overall_mean)**2) / (m - 1)
    
    # Compute within-chain variance
    W = np.mean([np.var(chain) for chain in chains])
    
    # Compute variance estimate
    var_estimate = (n - 1) / n * W + B / n
    
    # Compute potential scale reduction factor
    R = np.sqrt(var_estimate / W)
    
    return R

# Compute Gelman-Rubin statistic
R = gelman_rubin(chains)
print(f"Gelman-Rubin statistic: {R:.4f}")
```

## Summary

Markov Chain Monte Carlo methods are powerful techniques for sampling from complex probability distributions:

1. **Basic Principle**: Construct a Markov chain with the target distribution as its stationary distribution
2. **Key Algorithms**:
   - Metropolis-Hastings: General-purpose MCMC with proposal and acceptance step
   - Gibbs Sampling: Special case using conditional distributions as proposals
3. **Properties**:
   - Asymptotic guarantees: Converges to the target distribution
   - Works with unnormalized distributions
   - Sequential sampling approach
4. **Practical Considerations**:
   - Burn-in period to discard initial samples
   - Convergence diagnostics to assess mixing
   - Proposal distribution tuning for efficiency
   - Multiple chains for robustness

In the next section, we'll explore more advanced MCMC methods that address some of the limitations of basic Metropolis-Hastings and Gibbs sampling, particularly for high-dimensional and complex distributions.

## References

1. Robert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods. Springer.
2. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
3. Brooks, S., Gelman, A., Jones, G., & Meng, X. L. (Eds.). (2011). Handbook of Markov Chain Monte Carlo. CRC Press.
4. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
5. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
