# 2.3.5.5 Monte Carlo Integration

## Monte Carlo Integration and Sampling Techniques

Monte Carlo integration is a powerful numerical technique for approximating complex integrals using random sampling. It's particularly valuable in machine learning for computing expectations, marginalizing over latent variables, and evaluating high-dimensional integrals that arise in Bayesian inference and other probabilistic methods. In this section, we'll explore Monte Carlo integration techniques and their applications.

## Basic Monte Carlo Integration

### The Fundamental Principle

The basic idea of Monte Carlo integration is to approximate an integral by averaging function evaluations at randomly sampled points:

$$\int_\Omega f(x) dx \approx \frac{|\Omega|}{n} \sum_{i=1}^n f(X_i)$$

where:
- Ω is the integration domain with volume |Ω|
- X₁, X₂, ..., X_n are independent samples drawn uniformly from Ω
- f is the integrand function

### Error Analysis

The error in Monte Carlo integration decreases as O(1/√n), where n is the number of samples. The standard error is:

$$\sigma_{\hat{I}} = \frac{\sigma_f}{\sqrt{n}}$$

where σ_f is the standard deviation of f(X) when X is sampled uniformly from Ω.

### Example: Computing π

A classic example is estimating π by computing the area of a quarter circle:

```python
import numpy as np
import matplotlib.pyplot as plt

def monte_carlo_pi(n_samples):
    """Estimate π using Monte Carlo integration"""
    # Generate random points in the unit square
    x = np.random.uniform(0, 1, n_samples)
    y = np.random.uniform(0, 1, n_samples)
    
    # Count points inside the quarter circle
    inside = (x**2 + y**2) <= 1
    count = np.sum(inside)
    
    # Estimate π
    pi_estimate = 4 * count / n_samples
    
    # Compute standard error
    std_error = 4 * np.sqrt(pi_estimate * (4 - pi_estimate) / n_samples)
    
    return pi_estimate, std_error, x, y, inside

# Estimate π with different sample sizes
n_samples_list = [100, 1000, 10000, 100000, 1000000]
estimates = []
errors = []

for n_samples in n_samples_list:
    pi_estimate, std_error, _, _, _ = monte_carlo_pi(n_samples)
    estimates.append(pi_estimate)
    errors.append(std_error)
    print(f"n={n_samples}: π ≈ {pi_estimate:.6f} ± {std_error:.6f}, error = {abs(pi_estimate - np.pi):.6f}")

# Plot convergence
plt.figure(figsize=(10, 6))
plt.errorbar(n_samples_list, estimates, yerr=errors, fmt='o-')
plt.axhline(np.pi, color='r', linestyle='--', label='True Value')
plt.xscale('log')
plt.xlabel('Number of Samples')
plt.ylabel('Estimate of π')
plt.title('Monte Carlo Estimation of π')
plt.grid(True)
plt.legend()
plt.show()

# Visualize the estimation for n=10000
n_samples = 10000
pi_estimate, std_error, x, y, inside = monte_carlo_pi(n_samples)

plt.figure(figsize=(8, 8))
plt.scatter(x[inside], y[inside], color='blue', s=1, alpha=0.5, label='Inside')
plt.scatter(x[~inside], y[~inside], color='red', s=1, alpha=0.5, label='Outside')
plt.plot([0, 1, 1, 0, 0], [0, 0, 1, 1, 0], 'k-', lw=2)  # Square
theta = np.linspace(0, np.pi/2, 100)
plt.plot(np.cos(theta), np.sin(theta), 'k-', lw=2)  # Quarter circle
plt.axis('equal')
plt.title(f'Monte Carlo Estimation of π (n={n_samples}): {pi_estimate:.6f} ± {std_error:.6f}')
plt.legend()
plt.grid(True)
plt.show()
```

## Importance Sampling

### Motivation

Basic Monte Carlo integration can be inefficient when the integrand has high variance or is concentrated in small regions. **Importance sampling** addresses this by sampling from a distribution that focuses on the important regions of the integrand.

### The Method

Instead of sampling uniformly, we sample from a proposal distribution q(x) and reweight the samples:

$$\int_\Omega f(x) dx = \int_\Omega \frac{f(x)}{q(x)} q(x) dx \approx \frac{1}{n} \sum_{i=1}^n \frac{f(X_i)}{q(X_i)}$$

where X₁, X₂, ..., X_n are sampled from q(x).

### Optimal Proposal Distribution

The optimal proposal distribution (in terms of minimizing variance) is:

$$q^*(x) \propto |f(x)|$$

In practice, we choose q(x) to be as close as possible to |f(x)| while still being easy to sample from.

### Example: Estimating a Tail Probability

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def direct_mc_tail_prob(n_samples, a):
    """Estimate P(X > a) for X ~ N(0, 1) using direct Monte Carlo"""
    samples = np.random.normal(0, 1, n_samples)
    indicator = samples > a
    estimate = np.mean(indicator)
    std_error = np.sqrt(estimate * (1 - estimate) / n_samples)
    return estimate, std_error

def importance_sampling_tail_prob(n_samples, a):
    """Estimate P(X > a) for X ~ N(0, 1) using importance sampling"""
    # Use an exponential distribution shifted to a as the proposal
    samples = np.random.exponential(1, n_samples) + a
    
    # Compute importance weights
    target_log_pdf = stats.norm.logpdf(samples)
    proposal_log_pdf = np.log(np.exp(-samples + a))  # log of exponential PDF
    log_weights = target_log_pdf - proposal_log_pdf
    weights = np.exp(log_weights)
    
    # Estimate the probability
    estimate = np.mean(weights)
    std_error = np.sqrt(np.var(weights) / n_samples)
    
    return estimate, std_error, samples, weights

# Compare methods for estimating P(X > 5) where X ~ N(0, 1)
a = 5
true_prob = 1 - stats.norm.cdf(a)
print(f"True probability: {true_prob:.10f}")

n_samples = 10000
direct_estimate, direct_std_error = direct_mc_tail_prob(n_samples, a)
print(f"Direct MC: {direct_estimate:.10f} ± {direct_std_error:.10f}")

is_estimate, is_std_error, is_samples, is_weights = importance_sampling_tail_prob(n_samples, a)
print(f"Importance Sampling: {is_estimate:.10f} ± {is_std_error:.10f}")

# Plot the distributions
plt.figure(figsize=(12, 6))

# Plot the target and proposal distributions
x = np.linspace(-1, 10, 1000)
target_pdf = stats.norm.pdf(x)
proposal_pdf = np.exp(-x + a) * (x >= a)

plt.subplot(1, 2, 1)
plt.plot(x, target_pdf, 'b-', label='Target: N(0, 1)')
plt.plot(x, proposal_pdf, 'r-', label='Proposal: Exp(1) + a')
plt.axvline(a, color='k', linestyle='--', label=f'x = {a}')
plt.fill_between(x, target_pdf, where=(x >= a), alpha=0.3, color='b', label='Target Area')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Target vs. Proposal Distribution')
plt.legend()
plt.grid(True)

# Plot the weighted samples
plt.subplot(1, 2, 2)
plt.scatter(is_samples, is_weights, alpha=0.5, s=5)
plt.axhline(1.0, color='k', linestyle='--', label='Weight = 1')
plt.xlabel('Sample Value')
plt.ylabel('Importance Weight')
plt.title('Importance Weights')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# Compare convergence
n_samples_list = [100, 1000, 10000, 100000]
direct_estimates = []
direct_errors = []
is_estimates = []
is_errors = []

for n in n_samples_list:
    direct_estimate, direct_std_error = direct_mc_tail_prob(n, a)
    direct_estimates.append(direct_estimate)
    direct_errors.append(direct_std_error)
    
    is_estimate, is_std_error, _, _ = importance_sampling_tail_prob(n, a)
    is_estimates.append(is_estimate)
    is_errors.append(is_std_error)

plt.figure(figsize=(10, 6))
plt.errorbar(n_samples_list, direct_estimates, yerr=direct_errors, fmt='o-', label='Direct MC')
plt.errorbar(n_samples_list, is_estimates, yerr=is_errors, fmt='s-', label='Importance Sampling')
plt.axhline(true_prob, color='r', linestyle='--', label='True Value')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('Number of Samples')
plt.ylabel('Estimated Probability')
plt.title(f'Estimating P(X > {a}) for X ~ N(0, 1)')
plt.grid(True)
plt.legend()
plt.show()
```

## Control Variates

### Motivation

**Control variates** is a variance reduction technique that leverages known information about a related function to improve the efficiency of Monte Carlo integration.

### The Method

If we want to estimate E[f(X)] and we know E[g(X)] for a correlated function g, we can use:

$$E[f(X)] \approx \frac{1}{n} \sum_{i=1}^n [f(X_i) - c(g(X_i) - E[g(X)])]$$

where c is a coefficient chosen to minimize the variance of the estimator.

### Optimal Coefficient

The optimal value of c is:

$$c^* = \frac{\text{Cov}(f(X), g(X))}{\text{Var}(g(X))}$$

In practice, c is often estimated from the samples.

### Example: Estimating an Integral

```python
import numpy as np
import matplotlib.pyplot as plt

def f(x):
    """Target function"""
    return np.sin(x) * np.exp(-0.1 * x)

def g(x):
    """Control variate function (with known expectation)"""
    return np.sin(x)

# Known expectation of g over [0, π]
E_g = 2 / np.pi

def monte_carlo_estimate(n_samples, use_control_variate=False):
    """Estimate the integral of f over [0, π] using Monte Carlo"""
    # Generate uniform samples in [0, π]
    x = np.random.uniform(0, np.pi, n_samples)
    
    # Evaluate functions
    f_values = f(x)
    
    if use_control_variate:
        g_values = g(x)
        
        # Estimate the optimal coefficient
        cov_fg = np.mean(f_values * g_values) - np.mean(f_values) * np.mean(g_values)
        var_g = np.var(g_values)
        c = cov_fg / var_g
        
        # Apply control variate
        adjusted_values = f_values - c * (g_values - E_g)
        estimate = np.mean(adjusted_values)
        std_error = np.std(adjusted_values, ddof=1) / np.sqrt(n_samples)
    else:
        estimate = np.mean(f_values)
        std_error = np.std(f_values, ddof=1) / np.sqrt(n_samples)
    
    return estimate * np.pi, std_error * np.pi  # Scale by the interval width

# Compute the true value using numerical integration
from scipy import integrate
true_value, _ = integrate.quad(f, 0, np.pi)
print(f"True value: {true_value:.6f}")

# Compare standard MC and control variate MC
n_samples = 10000
mc_estimate, mc_std_error = monte_carlo_estimate(n_samples, use_control_variate=False)
cv_estimate, cv_std_error = monte_carlo_estimate(n_samples, use_control_variate=True)

print(f"Standard MC: {mc_estimate:.6f} ± {mc_std_error:.6f}")
print(f"Control Variate MC: {cv_estimate:.6f} ± {cv_std_error:.6f}")
print(f"Variance reduction factor: {(mc_std_error / cv_std_error)**2:.2f}x")

# Compare convergence
n_samples_list = [100, 1000, 10000, 100000]
mc_estimates = []
mc_errors = []
cv_estimates = []
cv_errors = []

for n in n_samples_list:
    mc_estimate, mc_std_error = monte_carlo_estimate(n, use_control_variate=False)
    mc_estimates.append(mc_estimate)
    mc_errors.append(mc_std_error)
    
    cv_estimate, cv_std_error = monte_carlo_estimate(n, use_control_variate=True)
    cv_estimates.append(cv_estimate)
    cv_errors.append(cv_std_error)

plt.figure(figsize=(10, 6))
plt.errorbar(n_samples_list, mc_estimates, yerr=mc_errors, fmt='o-', label='Standard MC')
plt.errorbar(n_samples_list, cv_estimates, yerr=cv_errors, fmt='s-', label='Control Variate MC')
plt.axhline(true_value, color='r', linestyle='--', label='True Value')
plt.xscale('log')
plt.xlabel('Number of Samples')
plt.ylabel('Estimated Integral')
plt.title('Monte Carlo Integration with Control Variates')
plt.grid(True)
plt.legend()
plt.show()

# Plot the functions
x = np.linspace(0, np.pi, 1000)
plt.figure(figsize=(10, 6))
plt.plot(x, f(x), 'b-', label='f(x) = sin(x) * exp(-0.1x)')
plt.plot(x, g(x), 'r-', label='g(x) = sin(x)')
plt.fill_between(x, f(x), alpha=0.3, color='b')
plt.xlabel('x')
plt.ylabel('Function Value')
plt.title('Target Function and Control Variate')
plt.grid(True)
plt.legend()
plt.show()
```

## Stratified Sampling

### Motivation

**Stratified sampling** improves Monte Carlo integration by dividing the integration domain into non-overlapping regions (strata) and sampling from each stratum independently.

### The Method

1. Divide the domain Ω into K non-overlapping strata: Ω = Ω₁ ∪ Ω₂ ∪ ... ∪ Ω_K
2. Allocate n_k samples to each stratum Ω_k
3. Estimate the integral as:

$$\int_\Omega f(x) dx = \sum_{k=1}^K \int_{\Omega_k} f(x) dx \approx \sum_{k=1}^K |\Omega_k| \frac{1}{n_k} \sum_{i=1}^{n_k} f(X_{ki})$$

where X_{ki} are samples from stratum Ω_k.

### Optimal Allocation

The optimal allocation of samples to strata (to minimize variance) is:

$$n_k \propto |\Omega_k| \sigma_k$$

where σ_k is the standard deviation of f within stratum Ω_k.

### Example: 2D Integration

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def f(x, y):
    """Target function for 2D integration"""
    return np.sin(x) * np.cos(y) * np.exp(-0.1 * (x**2 + y**2))

def monte_carlo_2d(n_samples, stratified=False, n_strata=4):
    """Estimate the integral of f over [0, π]×[0, π] using Monte Carlo"""
    if not stratified:
        # Standard Monte Carlo
        x = np.random.uniform(0, np.pi, n_samples)
        y = np.random.uniform(0, np.pi, n_samples)
        values = f(x, y)
        estimate = np.mean(values) * np.pi**2
        std_error = np.std(values, ddof=1) / np.sqrt(n_samples) * np.pi**2
        return estimate, std_error, x, y
    else:
        # Stratified sampling
        strata_size = np.pi / n_strata
        samples_per_stratum = n_samples // (n_strata**2)
        
        all_x = []
        all_y = []
        all_values = []
        
        for i in range(n_strata):
            for j in range(n_strata):
                # Generate samples within this stratum
                x_min, x_max = i * strata_size, (i + 1) * strata_size
                y_min, y_max = j * strata_size, (j + 1) * strata_size
                
                x = np.random.uniform(x_min, x_max, samples_per_stratum)
                y = np.random.uniform(y_min, y_max, samples_per_stratum)
                
                all_x.extend(x)
                all_y.extend(y)
                all_values.extend(f(x, y))
        
        all_x = np.array(all_x)
        all_y = np.array(all_y)
        all_values = np.array(all_values)
        
        estimate = np.mean(all_values) * np.pi**2
        std_error = np.std(all_values, ddof=1) / np.sqrt(len(all_values)) * np.pi**2
        
        return estimate, std_error, all_x, all_y

# Compute the true value using numerical integration
from scipy import integrate

def integrand(y, x):
    return f(x, y)

true_value, _ = integrate.dblquad(integrand, 0, np.pi, lambda x: 0, lambda x: np.pi)
print(f"True value: {true_value:.6f}")

# Compare standard MC and stratified MC
n_samples = 10000
mc_estimate, mc_std_error, mc_x, mc_y = monte_carlo_2d(n_samples, stratified=False)
strat_estimate, strat_std_error, strat_x, strat_y = monte_carlo_2d(n_samples, stratified=True, n_strata=5)

print(f"Standard MC: {mc_estimate:.6f} ± {mc_std_error:.6f}")
print(f"Stratified MC: {strat_estimate:.6f} ± {strat_std_error:.6f}")
print(f"Variance reduction factor: {(mc_std_error / strat_std_error)**2:.2f}x")

# Visualize the samples
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(mc_x, mc_y, s=1, alpha=0.5)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard Monte Carlo')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.scatter(strat_x, strat_y, s=1, alpha=0.5)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Stratified Monte Carlo')
plt.grid(True)

plt.tight_layout()
plt.show()

# Visualize the function
x = np.linspace(0, np.pi, 100)
y = np.linspace(0, np.pi, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('Target Function for 2D Integration')
plt.colorbar(surf)
plt.show()
```

## Quasi-Monte Carlo Methods

### Motivation

**Quasi-Monte Carlo** methods use deterministic, low-discrepancy sequences instead of random samples to achieve faster convergence rates.

### The Method

Replace random samples with a low-discrepancy sequence {x₁, x₂, ..., x_n} that covers the domain more uniformly:

$$\int_\Omega f(x) dx \approx |\Omega| \frac{1}{n} \sum_{i=1}^n f(x_i)$$

### Convergence Rate

Quasi-Monte Carlo methods can achieve a convergence rate of O((log n)^d/n), which is asymptotically better than the O(1/√n) rate of standard Monte Carlo (especially for low dimensions).

### Common Sequences

1. **Sobol Sequence**: Based on binary fractions
2. **Halton Sequence**: Based on different prime bases for each dimension
3. **Hammersley Sequence**: Similar to Halton but includes a deterministic component

### Example: Quasi-Monte Carlo Integration

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import qmc

def f(x):
    """Target function for integration over [0, 1]"""
    return np.sin(np.pi * x) * np.exp(-x)

def monte_carlo_estimate(n_samples, method='random'):
    """Estimate the integral of f over [0, 1] using different sampling methods"""
    if method == 'random':
        # Standard Monte Carlo
        samples = np.random.uniform(0, 1, n_samples)
    elif method == 'sobol':
        # Sobol sequence
        sampler = qmc.Sobol(d=1, scramble=False)
        samples = sampler.random(n_samples).flatten()
    elif method == 'halton':
        # Halton sequence
        sampler = qmc.Halton(d=1, scramble=False)
        samples = sampler.random(n_samples).flatten()
    else:
        raise ValueError(f"Unknown method: {method}")
    
    values = f(samples)
    estimate = np.mean(values)
    std_error = np.std(values, ddof=1) / np.sqrt(n_samples) if method == 'random' else None
    
    return estimate, std_error, samples

# Compute the true value using numerical integration
from scipy import integrate
true_value, _ = integrate.quad(f, 0, 1)
print(f"True value: {true_value:.10f}")

# Compare different methods
n_samples = 100
methods = ['random', 'sobol', 'halton']
results = {}

for method in methods:
    estimate, std_error, samples = monte_carlo_estimate(n_samples, method)
    results[method] = {'estimate': estimate, 'std_error': std_error, 'samples': samples}
    print(f"{method.capitalize()}: {estimate:.10f}" + 
          (f" ± {std_error:.10f}" if std_error is not None else ""))

# Visualize the samples
plt.figure(figsize=(15, 5))

for i, method in enumerate(methods):
    plt.subplot(1, 3, i+1)
    samples = results[method]['samples']
    plt.scatter(samples, np.zeros_like(samples), s=10, alpha=0.7)
    plt.xlabel('x')
    plt.title(f'{method.capitalize()} Samples (n={n_samples})')
    plt.xlim(0, 1)
    plt.grid(True)

plt.tight_layout()
plt.show()

# Compare convergence
n_samples_list = [10, 100, 1000, 10000]
errors = {method: [] for method in methods}

for n in n_samples_list:
    for method in methods:
        estimate, _, _ = monte_carlo_estimate(n, method)
        error = abs(estimate - true_value)
        errors[method].append(error)

plt.figure(figsize=(10, 6))
for method in methods:
    plt.loglog(n_samples_list, errors[method], 'o-', label=f'{method.capitalize()}')

# Add reference lines
x = np.array(n_samples_list)
plt.loglog(x, 1/np.sqrt(x), 'k--', label='O(1/√n)')
plt.loglog(x, 1/x, 'k:', label='O(1/n)')

plt.xlabel('Number of Samples')
plt.ylabel('Absolute Error')
plt.title('Convergence of Monte Carlo Methods')
plt.grid(True)
plt.legend()
plt.show()
```

## Applications in Machine Learning

### 1. Bayesian Inference

Monte Carlo integration is essential for Bayesian inference, where we need to compute posterior expectations:

$$E[f(\theta)|D] = \int f(\theta) p(\theta|D) d\theta$$

This is often approximated using MCMC samples:

$$E[f(\theta)|D] \approx \frac{1}{n} \sum_{i=1}^n f(\theta^{(i)})$$

where θ⁽ⁱ⁾ are samples from the posterior p(θ|D).

### 2. Variational Inference

In variational inference, Monte Carlo integration is used to estimate the gradient of the ELBO (Evidence Lower Bound):

$$\nabla_\phi \mathcal{L}(\phi) = \nabla_\phi E_{q_\phi(z)}[\log p(x, z) - \log q_\phi(z)]$$

This is often approximated using the reparameterization trick:

$$\nabla_\phi \mathcal{L}(\phi) \approx \frac{1}{n} \sum_{i=1}^n \nabla_\phi [\log p(x, g_\phi(\epsilon^{(i)})) - \log q_\phi(g_\phi(\epsilon^{(i)}))]$$

where ε⁽ⁱ⁾ are samples from a simple distribution and g_φ is a deterministic transformation.

### 3. Reinforcement Learning

Monte Carlo methods are used in reinforcement learning to estimate value functions:

$$V^\pi(s) = E_\pi[G_t | S_t = s] \approx \frac{1}{n} \sum_{i=1}^n G_t^{(i)}$$

where G_t⁽ⁱ⁾ are returns observed from state s following policy π.

### 4. Generative Models

Monte Carlo integration is used in training and evaluating generative models:

- **Variational Autoencoders**: Estimating the ELBO
- **Generative Adversarial Networks**: Approximating intractable expectations
- **Normalizing Flows**: Computing log-likelihoods

### 5. Uncertainty Quantification

Monte Carlo methods provide a way to quantify uncertainty in predictions:

$$p(y|x, D) = \int p(y|x, \theta) p(\theta|D) d\theta \approx \frac{1}{n} \sum_{i=1}^n p(y|x, \theta^{(i)})$$

where θ⁽ⁱ⁾ are samples from the posterior p(θ|D).

## Summary

Monte Carlo integration is a powerful technique for approximating complex integrals in machine learning:

1. **Basic Monte Carlo**: Simple but effective for low-dimensional problems
   - Convergence rate: O(1/√n)
   - Easy to implement and parallelize

2. **Importance Sampling**: Reduces variance by focusing on important regions
   - Requires a good proposal distribution
   - Particularly useful for rare event simulation

3. **Control Variates**: Leverages known information about related functions
   - Can significantly reduce variance
   - Requires finding a correlated control function

4. **Stratified Sampling**: Improves efficiency by sampling from distinct regions
   - Ensures coverage of the entire domain
   - Optimal allocation depends on the function's variability

5. **Quasi-Monte Carlo**: Uses deterministic sequences for faster convergence
   - Convergence rate: O((log n)^d/n)
   - Particularly effective for low to moderate dimensions

These techniques form the foundation for many advanced methods in machine learning, enabling us to handle complex probabilistic models, perform Bayesian inference, and quantify uncertainty in predictions.

## References

1. Owen, A. B. (2013). Monte Carlo theory, methods and examples.
2. Robert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods. Springer.
3. Dick, J., Kuo, F. Y., & Sloan, I. H. (2013). High-dimensional integration: The quasi-Monte Carlo way. Acta Numerica, 22, 133-288.
4. Glasserman, P. (2004). Monte Carlo Methods in Financial Engineering. Springer.
5. Kroese, D. P., Taimre, T., & Botev, Z. I. (2011). Handbook of Monte Carlo Methods. Wiley.
