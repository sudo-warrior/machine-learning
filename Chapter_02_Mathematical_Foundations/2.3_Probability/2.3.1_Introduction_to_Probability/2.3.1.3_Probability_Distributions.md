# 2.3.1.3 Probability Distributions

## Understanding Probability Distributions

Probability distributions are mathematical functions that describe the likelihood of different outcomes in a random experiment. They are fundamental to machine learning, as they allow us to model uncertainty, make predictions, and understand the behavior of random variables. In this section, we'll explore the concept of probability distributions, their types, properties, and applications in machine learning.

## Types of Probability Distributions

Probability distributions can be broadly categorized into two types based on the nature of the random variable they describe:

### 1. Discrete Probability Distributions

Discrete probability distributions describe random variables that can take on a countable number of distinct values.

#### Probability Mass Function (PMF)

For a discrete random variable X, the probability mass function p(x) gives the probability that X equals x:

$$p(x) = P(X = x)$$

**Properties of a valid PMF:**
- Non-negativity: p(x) ≥ 0 for all x
- Normalization: Σ p(x) = 1, summing over all possible values of x
- Range: 0 ≤ p(x) ≤ 1 for all x

**Example: Dice Roll**

For a fair six-sided die, the PMF is:
- p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1/6
- p(x) = 0 for all other values of x

This can be visualized as:

```
  p(x)
   │
   │
1/6│ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐
   │ │ │ │ │ │ │ │ │ │ │ │ │
   │ │ │ │ │ │ │ │ │ │ │ │ │
   └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─── x
     1   2   3   4   5   6
```

#### Cumulative Distribution Function (CDF)

The cumulative distribution function F(x) gives the probability that X is less than or equal to x:

$$F(x) = P(X \leq x) = \sum_{t \leq x} p(t)$$

**Properties of a CDF:**
- Non-decreasing: If x₁ < x₂, then F(x₁) ≤ F(x₂)
- Right-continuous: lim F(x + h) = F(x) as h → 0⁺
- Limits: lim F(x) = 0 as x → -∞, and lim F(x) = 1 as x → ∞

**Example: Dice Roll CDF**

For a fair six-sided die:
- F(0) = 0
- F(1) = 1/6
- F(2) = 2/6
- F(3) = 3/6
- F(4) = 4/6
- F(5) = 5/6
- F(6) = 1
- F(x) = 0 for x < 1
- F(x) = 1 for x > 6
- F(x) = ⌊x⌋/6 for 1 ≤ x ≤ 6, where ⌊x⌋ is the floor function

This can be visualized as:

```
  F(x)
   │                           ┌───────
 1 │                      ┌────┘
   │                 ┌────┘
5/6│            ┌────┘
   │       ┌────┘
4/6│  ┌────┘
   │  │
3/6│  │
   │  │
2/6│  │
   │  │
1/6│  │
   │  │
 0 └──┴───────────────────────────── x
     1   2   3   4   5   6
```

### 2. Continuous Probability Distributions

Continuous probability distributions describe random variables that can take on any value in a continuous range.

#### Probability Density Function (PDF)

For a continuous random variable X, the probability density function f(x) is such that the probability of X falling in an interval [a, b] is:

$$P(a \leq X \leq b) = \int_a^b f(x) dx$$

**Properties of a valid PDF:**
- Non-negativity: f(x) ≥ 0 for all x
- Normalization: ∫ f(x) dx = 1, integrating over the entire domain
- The probability of any single point is zero: P(X = a) = 0 for any a

**Example: Standard Normal Distribution**

The PDF of the standard normal distribution is:

$$f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$$

This can be visualized as:

```
  f(x)
   │
   │      ╱╲
   │     ╱  ╲
   │    ╱    ╲
   │   ╱      ╲
   │  ╱        ╲
   │ ╱          ╲
   │╱            ╲
   └───────────────── x
     -3  0  3
```

#### Cumulative Distribution Function (CDF)

The CDF for a continuous random variable is:

$$F(x) = P(X \leq x) = \int_{-\infty}^x f(t) dt$$

**Properties:**
- Same as for discrete CDFs
- For continuous distributions, F(x) is continuous

**Example: Standard Normal CDF**

The CDF of the standard normal distribution is:

$$F(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{t^2}{2}} dt$$

This doesn't have a closed-form expression and is often denoted as Φ(x).

```
  F(x)
   │                    ┌─────
 1 │                  ╱╱
   │                ╱╱
   │              ╱╱
   │             ╱
0.5│            ╱
   │          ╱╱
   │        ╱╱
   │      ╱╱
 0 └─────┘                 x
     -3  0  3
```

## Common Probability Distributions

### Discrete Distributions

#### 1. Bernoulli Distribution

Models a single binary outcome (success/failure).

- PMF: p(x) = p^x * (1-p)^(1-x) for x ∈ {0, 1}
- Mean: p
- Variance: p(1-p)

**Applications:**
- Modeling binary outcomes (yes/no, success/failure)
- Binary classification in machine learning

#### 2. Binomial Distribution

Models the number of successes in n independent Bernoulli trials.

- PMF: p(x) = (n choose x) * p^x * (1-p)^(n-x) for x ∈ {0, 1, 2, ..., n}
- Mean: np
- Variance: np(1-p)

**Applications:**
- A/B testing
- Quality control
- Modeling count data with fixed number of trials

#### 3. Poisson Distribution

Models the number of events occurring in a fixed interval of time or space.

- PMF: p(x) = (λ^x * e^(-λ)) / x! for x ∈ {0, 1, 2, ...}
- Mean: λ
- Variance: λ

**Applications:**
- Modeling rare events
- Arrival processes
- Count data in machine learning

### Continuous Distributions

#### 1. Uniform Distribution

Models a random variable that is equally likely to take any value in a given range [a, b].

- PDF: f(x) = 1/(b-a) for a ≤ x ≤ b
- Mean: (a+b)/2
- Variance: (b-a)²/12

**Applications:**
- Random number generation
- Initialization in machine learning algorithms
- Prior distributions in Bayesian inference

#### 2. Normal (Gaussian) Distribution

Models a random variable influenced by many small, independent factors.

- PDF: f(x) = (1/(σ√(2π))) * e^(-(x-μ)²/(2σ²))
- Mean: μ
- Variance: σ²

**Applications:**
- Modeling natural phenomena
- Error distributions
- Feature distributions in machine learning
- Central Limit Theorem applications

#### 3. Exponential Distribution

Models the time between events in a Poisson process.

- PDF: f(x) = λe^(-λx) for x ≥ 0
- Mean: 1/λ
- Variance: 1/λ²

**Applications:**
- Survival analysis
- Reliability engineering
- Modeling waiting times

## Properties of Distributions

### 1. Moments

Moments characterize the shape of a distribution:

- **First Moment (Mean)**: E[X] = ∫ x f(x) dx or Σ x p(x)
- **Second Central Moment (Variance)**: Var(X) = E[(X - E[X])²]
- **Third Standardized Moment (Skewness)**: Measures asymmetry
- **Fourth Standardized Moment (Kurtosis)**: Measures "tailedness"

### 2. Quantiles

Quantiles divide a distribution into equal parts:

- **Median**: The 50th percentile (Q₂)
- **Quartiles**: Q₁ (25th percentile), Q₂, Q₃ (75th percentile)
- **Percentiles**: Divide the distribution into 100 equal parts

### 3. Mode

The mode is the value that appears most frequently or has the highest probability density.

### 4. Support

The support of a distribution is the set of values for which the PMF or PDF is non-zero.

## Transformations of Random Variables

If Y = g(X) is a function of a random variable X:

### For Discrete Random Variables

$$P(Y = y) = \sum_{x: g(x) = y} P(X = x)$$

### For Continuous Random Variables

If g is a monotonic function with inverse g⁻¹:

$$f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$

## Multivariate Distributions

Multivariate distributions describe the joint behavior of multiple random variables.

### Joint Distributions

For discrete random variables X and Y:
- Joint PMF: p(x, y) = P(X = x, Y = y)

For continuous random variables X and Y:
- Joint PDF: f(x, y) such that P((X,Y) ∈ A) = ∫∫_A f(x,y) dx dy

### Marginal Distributions

The marginal distribution of X is obtained by summing or integrating over all possible values of Y:

For discrete variables:
$$p_X(x) = \sum_y p(x, y)$$

For continuous variables:
$$f_X(x) = \int f(x, y) dy$$

### Conditional Distributions

The conditional distribution of X given Y = y:

For discrete variables:
$$p(x|y) = \frac{p(x, y)}{p_Y(y)}$$

For continuous variables:
$$f(x|y) = \frac{f(x, y)}{f_Y(y)}$$

## Probability Distributions in Machine Learning

Probability distributions play crucial roles in machine learning:

### 1. Data Modeling

- **Feature Distributions**: Understanding the distribution of features helps in preprocessing and feature engineering
- **Target Variable Modeling**: Classification problems often model the conditional distribution P(Y|X)

### 2. Likelihood Functions

Many learning algorithms maximize the likelihood of the data given the model parameters:

$$L(\theta|X) = P(X|\theta)$$

For independent and identically distributed (i.i.d.) data points:

$$L(\theta|X) = \prod_{i=1}^n P(x_i|\theta)$$

### 3. Prior and Posterior Distributions

In Bayesian machine learning, we use:
- **Prior Distributions**: P(θ) representing our beliefs about parameters before seeing data
- **Posterior Distributions**: P(θ|X) representing updated beliefs after observing data

### 4. Generative Models

Generative models learn the joint distribution P(X, Y) or P(X) to generate new samples:
- Gaussian Mixture Models
- Variational Autoencoders
- Generative Adversarial Networks

### 5. Loss Functions

Many loss functions are derived from probability distributions:
- **Cross-Entropy Loss**: Derived from the KL divergence between the true and predicted distributions
- **Mean Squared Error**: Corresponds to maximum likelihood estimation under a Gaussian noise assumption

## Visualizing Distributions

### Histograms

Histograms approximate the PMF or PDF by counting the frequency of values in bins:

```
  Frequency
     │
     │    ┌─┐
     │    │ │
     │ ┌─┐│ │┌─┐
     │ │ ││ ││ │
     └─┴─┴┴─┴┴─┴─── Value
       1 2 3 4 5
```

### Kernel Density Estimation (KDE)

KDE provides a smooth estimate of the PDF:

```
  Density
     │
     │     ╱╲
     │    ╱  ╲
     │   ╱    ╲
     │  ╱      ╲
     │ ╱        ╲
     └────────────── Value
```

### Box Plots

Box plots show the distribution's quartiles and outliers:

```
       │     ┌───┐     │
       │     │   │     │
  ─────┼─────┤   ├─────┼─────
       │     │   │     │
       │     └───┘     │
       
     Q₁     Q₂(Median)  Q₃
```

### Q-Q Plots

Q-Q plots compare a distribution to a theoretical distribution (often normal):

```
  Sample
  Quantiles
     │       ╱
     │      ╱
     │     ╱
     │    ╱
     │   ╱
     │  ╱
     │ ╱
     │╱
     └─────────── Theoretical Quantiles
```

## Summary

Probability distributions are mathematical functions that describe the likelihood of different outcomes for random variables. They come in two main types:

1. **Discrete Distributions** (with PMFs) for countable outcomes
2. **Continuous Distributions** (with PDFs) for uncountable outcomes

Key properties of distributions include their moments (mean, variance), quantiles, and support. Distributions can be transformed, combined into multivariate distributions, and manipulated through operations like marginalization and conditioning.

In machine learning, probability distributions are used for data modeling, likelihood functions, Bayesian inference, generative modeling, and deriving loss functions. Understanding probability distributions is essential for building probabilistic models, quantifying uncertainty, and making principled predictions.

In the next section, we'll explore how probability theory is specifically applied in machine learning contexts.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
