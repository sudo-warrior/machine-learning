# 2.3.4.3 Maximum Likelihood Estimation

## Understanding Maximum Likelihood Estimation

Maximum Likelihood Estimation (MLE) is a fundamental method for estimating the parameters of a statistical model. It finds the parameter values that maximize the likelihood of observing the given data under the assumed model. MLE serves as the foundation for many machine learning algorithms and provides a principled approach to parameter estimation. In this section, we'll explore the concept of maximum likelihood estimation, its properties, and its applications in machine learning.

## The Likelihood Function

### Definition

The **likelihood function** measures how likely it is to observe the given data under a specific statistical model with particular parameter values.

For a dataset D = {x₁, x₂, ..., x_n} and a model with parameters θ, the likelihood function is:

$$L(\theta|D) = P(D|\theta)$$

For independent and identically distributed (i.i.d.) data points, the likelihood factorizes as:

$$L(\theta|D) = \prod_{i=1}^n P(x_i|\theta)$$

### Log-Likelihood

The **log-likelihood** is the natural logarithm of the likelihood function:

$$\ell(\theta|D) = \log L(\theta|D) = \sum_{i=1}^n \log P(x_i|\theta)$$

Working with the log-likelihood offers several advantages:
- Converts products to sums, which is computationally more stable
- Prevents numerical underflow when dealing with very small probabilities
- Often simplifies the optimization problem
- Preserves the location of the maximum since logarithm is a monotonically increasing function

## Maximum Likelihood Estimation

### Definition

**Maximum Likelihood Estimation (MLE)** finds the parameter values that maximize the likelihood (or log-likelihood) function:

$$\hat{\theta}_{MLE} = \arg\max_{\theta} L(\theta|D) = \arg\max_{\theta} \ell(\theta|D)$$

### Optimization Approaches

Finding the MLE often involves optimization techniques:

#### 1. Analytical Solution

For some models, setting the derivative of the log-likelihood to zero yields a closed-form solution:

$$\frac{\partial \ell(\theta|D)}{\partial \theta} = 0$$

Examples with analytical solutions include:
- Normal distribution (mean and variance)
- Bernoulli and binomial distributions (success probability)
- Poisson distribution (rate parameter)
- Exponential distribution (rate parameter)

#### 2. Numerical Optimization

For more complex models without analytical solutions, numerical optimization methods are used:

- **Gradient Descent**: Update parameters in the direction of the log-likelihood gradient
- **Newton's Method**: Use both first and second derivatives for faster convergence
- **Quasi-Newton Methods**: Approximate the Hessian matrix (e.g., BFGS, L-BFGS)
- **Expectation-Maximization (EM)**: Iteratively estimate parameters with latent variables

## Properties of Maximum Likelihood Estimators

### 1. Consistency

Under certain regularity conditions, MLEs are **consistent**: as the sample size increases, the MLE converges in probability to the true parameter value.

$$\hat{\theta}_{MLE} \xrightarrow{p} \theta_{true} \text{ as } n \rightarrow \infty$$

### 2. Asymptotic Normality

For large samples, MLEs are approximately normally distributed around the true parameter value:

$$\sqrt{n}(\hat{\theta}_{MLE} - \theta_{true}) \xrightarrow{d} N(0, I^{-1}(\theta_{true}))$$

where I(θ) is the Fisher information matrix:

$$I(\theta) = -E\left[\frac{\partial^2 \ell(\theta|X)}{\partial \theta \partial \theta^T}\right]$$

### 3. Asymptotic Efficiency

Among consistent estimators, MLEs achieve the Cramér-Rao lower bound asymptotically, making them asymptotically efficient.

### 4. Invariance

If $\hat{\theta}_{MLE}$ is the MLE of θ, then for any function g, the MLE of g(θ) is g($\hat{\theta}_{MLE}$).

### 5. Lack of Bias

MLEs are not necessarily unbiased for finite samples, although they are often asymptotically unbiased.

## Examples of Maximum Likelihood Estimation

### Example 1: Normal Distribution

For a sample {x₁, x₂, ..., x_n} from a normal distribution N(μ, σ²):

The log-likelihood is:

$$\ell(\mu, \sigma^2|D) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$$

Setting the derivatives to zero:

$$\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) = 0$$

$$\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n (x_i - \mu)^2 = 0$$

The MLEs are:

$$\hat{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^n x_i = \bar{x}$$

$$\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu}_{MLE})^2$$

Note that $\hat{\sigma}^2_{MLE}$ is biased (the unbiased estimator uses n-1 in the denominator).

### Example 2: Bernoulli Distribution

For a sample {x₁, x₂, ..., x_n} from a Bernoulli distribution with parameter p:

The log-likelihood is:

$$\ell(p|D) = \sum_{i=1}^n [x_i \log(p) + (1-x_i) \log(1-p)]$$

Setting the derivative to zero:

$$\frac{\partial \ell}{\partial p} = \sum_{i=1}^n \left[\frac{x_i}{p} - \frac{1-x_i}{1-p}\right] = 0$$

The MLE is:

$$\hat{p}_{MLE} = \frac{1}{n}\sum_{i=1}^n x_i = \bar{x}$$

### Example 3: Exponential Distribution

For a sample {x₁, x₂, ..., x_n} from an exponential distribution with rate parameter λ:

The log-likelihood is:

$$\ell(\lambda|D) = n \log(\lambda) - \lambda \sum_{i=1}^n x_i$$

Setting the derivative to zero:

$$\frac{\partial \ell}{\partial \lambda} = \frac{n}{\lambda} - \sum_{i=1}^n x_i = 0$$

The MLE is:

$$\hat{\lambda}_{MLE} = \frac{n}{\sum_{i=1}^n x_i} = \frac{1}{\bar{x}}$$

## MLE in Machine Learning

### 1. Linear Regression

In linear regression, MLE with Gaussian noise leads to minimizing the mean squared error:

$$\hat{\beta}_{MLE} = \arg\min_{\beta} \sum_{i=1}^n (y_i - \beta^T x_i)^2$$

This has the closed-form solution:

$$\hat{\beta}_{MLE} = (X^T X)^{-1} X^T y$$

### 2. Logistic Regression

In logistic regression, MLE leads to minimizing the cross-entropy loss:

$$\hat{\beta}_{MLE} = \arg\min_{\beta} -\sum_{i=1}^n [y_i \log(p_i) + (1-y_i) \log(1-p_i)]$$

where p_i = σ(β^T x_i) and σ is the sigmoid function.

This typically requires numerical optimization.

### 3. Neural Networks

Neural networks trained with cross-entropy loss (for classification) or mean squared error (for regression) are performing maximum likelihood estimation under certain noise assumptions.

### 4. Naive Bayes

Naive Bayes classifiers estimate class-conditional probabilities using MLE:

$$\hat{p}(x_j|y=c) = \frac{\text{count}(x_j, y=c) + \alpha}{\text{count}(y=c) + \alpha \cdot |\text{values}(x_j)|}$$

where α is a smoothing parameter (α=0 gives the pure MLE).

### 5. Hidden Markov Models

The Baum-Welch algorithm for training Hidden Markov Models is an application of the Expectation-Maximization algorithm, which is a method for finding MLEs with latent variables.

### 6. Gaussian Mixture Models

Gaussian Mixture Models are also trained using the EM algorithm to find the MLE of the mixture weights, means, and covariances.

## Regularized Maximum Likelihood

### Penalized Likelihood

To prevent overfitting, regularization can be added to the log-likelihood:

$$\ell_{penalized}(\theta|D) = \ell(\theta|D) - \lambda \cdot \text{penalty}(\theta)$$

Common penalties include:

- **L1 Regularization (Lasso)**: penalty(θ) = ||θ||₁ = ∑|θᵢ|
  - Promotes sparsity (many parameters exactly zero)
  - Equivalent to MAP estimation with Laplace prior

- **L2 Regularization (Ridge)**: penalty(θ) = ||θ||₂² = ∑θᵢ²
  - Shrinks parameters toward zero
  - Equivalent to MAP estimation with Gaussian prior

- **Elastic Net**: penalty(θ) = α||θ||₁ + (1-α)||θ||₂²
  - Combines L1 and L2 regularization
  - Balances sparsity and parameter shrinkage

### Relationship to MAP Estimation

Regularized MLE is equivalent to Maximum A Posteriori (MAP) estimation with an appropriate prior:

$$\hat{\theta}_{MAP} = \arg\max_{\theta} P(\theta|D) = \arg\max_{\theta} [P(D|\theta) \cdot P(\theta)]$$

$$\log P(\theta|D) = \log P(D|\theta) + \log P(\theta) - \log P(D)$$

Since P(D) doesn't depend on θ:

$$\hat{\theta}_{MAP} = \arg\max_{\theta} [\log P(D|\theta) + \log P(\theta)]$$

This is equivalent to penalized likelihood with penalty(θ) = -log P(θ).

## Limitations of Maximum Likelihood Estimation

### 1. Overfitting

MLE can overfit when:
- The model is complex relative to the amount of data
- The model is misspecified
- The data contains outliers

Solutions include:
- Regularization
- Cross-validation for model selection
- Using simpler models

### 2. Identifiability Issues

MLE assumes that the true parameter can be uniquely identified from the likelihood function. When multiple parameter values yield the same likelihood (non-identifiability), MLE may not have a unique solution.

### 3. Computational Challenges

For complex models:
- The likelihood may have multiple local maxima
- Optimization can be computationally intensive
- Numerical issues can arise (e.g., ill-conditioning)

### 4. Uncertainty Quantification

MLE provides point estimates without directly quantifying parameter uncertainty. For uncertainty quantification, additional methods are needed:
- Confidence intervals based on asymptotic normality
- Bootstrap methods
- Bayesian approaches with posterior distributions

## Practical Implementation

### Example: MLE for Linear Regression

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Generate synthetic data
np.random.seed(42)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
true_sigma = 1.0
y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)

# Define the negative log-likelihood function
def neg_log_likelihood(params):
    intercept, slope, log_sigma = params
    sigma = np.exp(log_sigma)  # Ensure sigma > 0
    
    # Compute the predicted values
    y_pred = intercept + slope * x
    
    # Compute the negative log-likelihood
    nll = n/2 * np.log(2*np.pi) + n * np.log(sigma) + np.sum((y - y_pred)**2) / (2 * sigma**2)
    return nll

# Initial parameter guess
initial_params = [0.0, 0.0, 0.0]  # [intercept, slope, log_sigma]

# Find MLE using numerical optimization
result = minimize(neg_log_likelihood, initial_params, method='BFGS')
mle_intercept, mle_slope, mle_log_sigma = result.x
mle_sigma = np.exp(mle_log_sigma)

print(f"MLE Intercept: {mle_intercept:.4f} (True: {true_intercept})")
print(f"MLE Slope: {mle_slope:.4f} (True: {true_slope})")
print(f"MLE Sigma: {mle_sigma:.4f} (True: {true_sigma})")

# Compare with closed-form solution
X = np.column_stack((np.ones(n), x))
beta_closed = np.linalg.inv(X.T @ X) @ X.T @ y
sigma_closed = np.sqrt(np.sum((y - X @ beta_closed)**2) / n)

print(f"Closed-form Intercept: {beta_closed[0]:.4f}")
print(f"Closed-form Slope: {beta_closed[1]:.4f}")
print(f"Closed-form Sigma: {sigma_closed:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.7, label='Data')
plt.plot(x, true_intercept + true_slope * x, 'r-', label='True Line')
plt.plot(x, mle_intercept + mle_slope * x, 'g--', label='MLE Line')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Maximum Likelihood Estimation for Linear Regression')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Plot the log-likelihood surface
intercept_range = np.linspace(0.5, 1.5, 100)
slope_range = np.linspace(1.5, 2.5, 100)
II, SS = np.meshgrid(intercept_range, slope_range)
LL = np.zeros_like(II)

for i in range(len(intercept_range)):
    for j in range(len(slope_range)):
        LL[j, i] = -neg_log_likelihood([II[j, i], SS[j, i], np.log(mle_sigma)])

plt.figure(figsize=(10, 8))
plt.contourf(II, SS, LL, 50, cmap='viridis')
plt.colorbar(label='Log-Likelihood')
plt.plot(true_intercept, true_slope, 'r*', markersize=10, label='True Parameters')
plt.plot(mle_intercept, mle_slope, 'go', markersize=8, label='MLE Parameters')
plt.xlabel('Intercept')
plt.ylabel('Slope')
plt.title('Log-Likelihood Surface')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

### Example: MLE for Logistic Regression

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.special import expit  # Sigmoid function

# Generate synthetic data
np.random.seed(42)
n = 100
x = np.random.normal(0, 1, n)
true_intercept = 0.5
true_slope = 2.0
p = expit(true_intercept + true_slope * x)
y = np.random.binomial(1, p)

# Define the negative log-likelihood function
def neg_log_likelihood(params):
    intercept, slope = params
    
    # Compute the predicted probabilities
    p_pred = expit(intercept + slope * x)
    
    # Compute the negative log-likelihood
    nll = -np.sum(y * np.log(p_pred) + (1 - y) * np.log(1 - p_pred))
    return nll

# Initial parameter guess
initial_params = [0.0, 0.0]  # [intercept, slope]

# Find MLE using numerical optimization
result = minimize(neg_log_likelihood, initial_params, method='BFGS')
mle_intercept, mle_slope = result.x

print(f"MLE Intercept: {mle_intercept:.4f} (True: {true_intercept})")
print(f"MLE Slope: {mle_slope:.4f} (True: {true_slope})")

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.7, label='Data')

# Plot the true and estimated probability curves
x_sorted = np.sort(x)
p_true = expit(true_intercept + true_slope * x_sorted)
p_mle = expit(mle_intercept + mle_slope * x_sorted)

plt.plot(x_sorted, p_true, 'r-', label='True Probability')
plt.plot(x_sorted, p_mle, 'g--', label='MLE Probability')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Maximum Likelihood Estimation for Logistic Regression')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Plot the log-likelihood surface
intercept_range = np.linspace(-0.5, 1.5, 100)
slope_range = np.linspace(1.0, 3.0, 100)
II, SS = np.meshgrid(intercept_range, slope_range)
LL = np.zeros_like(II)

for i in range(len(intercept_range)):
    for j in range(len(slope_range)):
        LL[j, i] = -neg_log_likelihood([II[j, i], SS[j, i]])

plt.figure(figsize=(10, 8))
plt.contourf(II, SS, LL, 50, cmap='viridis')
plt.colorbar(label='Log-Likelihood')
plt.plot(true_intercept, true_slope, 'r*', markersize=10, label='True Parameters')
plt.plot(mle_intercept, mle_slope, 'go', markersize=8, label='MLE Parameters')
plt.xlabel('Intercept')
plt.ylabel('Slope')
plt.title('Log-Likelihood Surface')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Comparison with Other Estimation Methods

### MLE vs. Method of Moments

**Method of Moments (MoM)** equates sample moments with theoretical moments to estimate parameters:

- **Simplicity**: MoM is often simpler computationally
- **Efficiency**: MLE is generally more efficient (lower variance)
- **Existence**: MoM may exist when MLE doesn't
- **Uniqueness**: MLE may have multiple solutions when MoM has a unique solution

### MLE vs. Bayesian Estimation

**Bayesian Estimation** incorporates prior distributions and computes posterior distributions:

- **Point Estimates**: MLE provides point estimates, Bayesian provides full posterior distributions
- **Prior Information**: Bayesian incorporates prior knowledge, MLE uses only the likelihood
- **Regularization**: Bayesian naturally includes regularization through priors
- **Uncertainty**: Bayesian directly quantifies parameter uncertainty
- **Computation**: MLE is often computationally simpler

### MLE vs. Minimum Distance Estimation

**Minimum Distance Estimation** minimizes a distance measure between empirical and theoretical distributions:

- **Robustness**: Minimum distance methods can be more robust to outliers
- **Efficiency**: MLE is more efficient under correct model specification
- **Flexibility**: Minimum distance methods can be applied to complex models where likelihood is intractable

## Summary

Maximum Likelihood Estimation is a fundamental method for parameter estimation in statistics and machine learning:

1. **Concept**: MLE finds the parameter values that maximize the likelihood of observing the given data under the assumed model.

2. **Likelihood Function**: Measures how likely it is to observe the data given specific parameter values.
   - For i.i.d. data: L(θ|D) = ∏ P(x_i|θ)
   - Log-likelihood: ℓ(θ|D) = ∑ log P(x_i|θ)

3. **Optimization**: MLE involves maximizing the likelihood or log-likelihood.
   - Analytical solutions exist for some models
   - Numerical optimization is used for complex models

4. **Properties**:
   - Consistency: Converges to true parameter as sample size increases
   - Asymptotic normality: Approximately normal for large samples
   - Asymptotic efficiency: Achieves minimum variance asymptotically
   - Invariance: If θ̂ is MLE of θ, then g(θ̂) is MLE of g(θ)

5. **Applications in ML**:
   - Linear and logistic regression
   - Neural networks
   - Naive Bayes
   - Hidden Markov Models
   - Gaussian Mixture Models

6. **Regularization**:
   - Penalized likelihood adds regularization terms
   - Equivalent to MAP estimation with appropriate priors

7. **Limitations**:
   - Overfitting with complex models
   - Identifiability issues
   - Computational challenges
   - Limited uncertainty quantification

Understanding MLE provides a foundation for many machine learning algorithms and a principled approach to parameter estimation under probabilistic models.

## References

1. Casella, G., & Berger, R. L. (2002). Statistical Inference (2nd ed.). Duxbury.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
5. Myung, I. J. (2003). Tutorial on maximum likelihood estimation. Journal of Mathematical Psychology, 47(1), 90-100.
