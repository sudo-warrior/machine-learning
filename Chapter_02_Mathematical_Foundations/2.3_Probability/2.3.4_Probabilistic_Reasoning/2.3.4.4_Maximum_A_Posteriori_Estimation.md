# 2.3.4.4 Maximum A Posteriori Estimation

## Understanding Maximum A Posteriori Estimation

Maximum A Posteriori (MAP) estimation is a Bayesian method for estimating the parameters of a statistical model. Unlike Maximum Likelihood Estimation (MLE), which finds the parameters that maximize the likelihood of the observed data, MAP estimation incorporates prior knowledge about the parameters through a prior distribution. MAP estimation provides a middle ground between pure MLE and full Bayesian inference, offering some of the regularization benefits of Bayesian methods while maintaining the computational simplicity of point estimation. In this section, we'll explore MAP estimation, its properties, and its applications in machine learning.

## Bayesian Framework and MAP Estimation

### Bayes' Theorem

MAP estimation is based on Bayes' theorem, which relates the posterior distribution of parameters given data to the likelihood and prior:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$

where:
- P(θ|D) is the posterior distribution of parameters given data
- P(D|θ) is the likelihood function
- P(θ) is the prior distribution of parameters
- P(D) is the marginal likelihood or evidence

### MAP Estimation

**Maximum A Posteriori (MAP)** estimation finds the parameter values that maximize the posterior probability:

$$\hat{\theta}_{MAP} = \arg\max_{\theta} P(\theta|D)$$

Since P(D) doesn't depend on θ, this is equivalent to:

$$\hat{\theta}_{MAP} = \arg\max_{\theta} P(D|\theta)P(\theta)$$

Taking the logarithm (which preserves the location of the maximum):

$$\hat{\theta}_{MAP} = \arg\max_{\theta} [\log P(D|\theta) + \log P(\theta)]$$

This shows that MAP estimation maximizes the sum of the log-likelihood and the log-prior.

### Comparison with MLE

The relationship between MAP and MLE can be seen clearly:

- **MLE**: $\hat{\theta}_{MLE} = \arg\max_{\theta} \log P(D|\theta)$
- **MAP**: $\hat{\theta}_{MAP} = \arg\max_{\theta} [\log P(D|\theta) + \log P(\theta)]$

The difference is the addition of the log-prior term, which can be viewed as a regularization term that penalizes parameter values with low prior probability.

### Relationship to Regularization

MAP estimation with specific priors corresponds to common regularization techniques:

- **L2 Regularization (Ridge)**: Gaussian prior with zero mean
  - Prior: $P(\theta) \propto \exp(-\lambda ||\theta||_2^2/2)$
  - Log-prior: $\log P(\theta) \propto -\lambda ||\theta||_2^2/2$
  - Penalty term: $\lambda ||\theta||_2^2/2$

- **L1 Regularization (Lasso)**: Laplace prior with zero mean
  - Prior: $P(\theta) \propto \exp(-\lambda ||\theta||_1)$
  - Log-prior: $\log P(\theta) \propto -\lambda ||\theta||_1$
  - Penalty term: $\lambda ||\theta||_1$

- **Elastic Net**: Combination of Gaussian and Laplace priors
  - Prior: $P(\theta) \propto \exp(-\lambda_1 ||\theta||_1 - \lambda_2 ||\theta||_2^2/2)$
  - Log-prior: $\log P(\theta) \propto -\lambda_1 ||\theta||_1 - \lambda_2 ||\theta||_2^2/2$
  - Penalty term: $\lambda_1 ||\theta||_1 + \lambda_2 ||\theta||_2^2/2$

## Common Prior Distributions

### 1. Gaussian (Normal) Prior

The Gaussian prior is commonly used for continuous parameters:

$$P(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\theta-\mu)^2}{2\sigma^2}\right)$$

- **Log-prior**: $\log P(\theta) \propto -\frac{(\theta-\mu)^2}{2\sigma^2}$
- **Effect**: Penalizes parameters far from the mean μ, with strength controlled by variance σ²
- **MAP with Gaussian prior**: Equivalent to ridge regression when applied to linear regression

### 2. Laplace Prior

The Laplace prior promotes sparsity:

$$P(\theta) = \frac{1}{2b} \exp\left(-\frac{|\theta-\mu|}{b}\right)$$

- **Log-prior**: $\log P(\theta) \propto -\frac{|\theta-\mu|}{b}$
- **Effect**: Encourages parameters to be exactly equal to the mean μ (often 0)
- **MAP with Laplace prior**: Equivalent to lasso regression when applied to linear regression

### 3. Beta Prior

The Beta prior is used for parameters constrained to [0, 1], such as probabilities:

$$P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}$$

where B(α, β) is the beta function.

- **Log-prior**: $\log P(\theta) \propto (\alpha-1)\log(\theta) + (\beta-1)\log(1-\theta)$
- **Effect**: Shapes the distribution of probability parameters based on α and β
- **Special cases**: 
  - α = β = 1: Uniform prior
  - α, β > 1: Unimodal, centered at α/(α+β)
  - α, β < 1: U-shaped, favoring values near 0 and 1

### 4. Dirichlet Prior

The Dirichlet prior is a multivariate generalization of the Beta prior, used for probability vectors:

$$P(\theta) = \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1}$$

where θ = (θ₁, θ₂, ..., θ_k) with ∑θᵢ = 1, and B(α) is the multivariate beta function.

- **Log-prior**: $\log P(\theta) \propto \sum_{i=1}^k (\alpha_i-1)\log(\theta_i)$
- **Effect**: Shapes the distribution of probability vectors based on concentration parameters α
- **Applications**: Prior for multinomial parameters, topic models, mixture weights

### 5. Gamma Prior

The Gamma prior is used for positive parameters:

$$P(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{\alpha-1} e^{-\beta\theta}$$

- **Log-prior**: $\log P(\theta) \propto (\alpha-1)\log(\theta) - \beta\theta$
- **Effect**: Shapes the distribution of positive parameters based on shape α and rate β
- **Applications**: Prior for variance parameters, Poisson rate parameters

### 6. Inverse-Gamma Prior

The Inverse-Gamma prior is commonly used for variance parameters:

$$P(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{-\alpha-1} e^{-\beta/\theta}$$

- **Log-prior**: $\log P(\theta) \propto -(\alpha+1)\log(\theta) - \beta/\theta$
- **Effect**: Shapes the distribution of variance parameters
- **Applications**: Prior for variance in normal models

## Computing MAP Estimates

### 1. Analytical Solutions

For some combinations of likelihood and prior, the MAP estimate has a closed-form solution:

#### Example: Normal Likelihood with Normal Prior

For data D = {x₁, x₂, ..., x_n} from N(θ, σ²) with a prior θ ~ N(μ₀, σ₀²):

$$\hat{\theta}_{MAP} = \frac{\frac{n\bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2}}{\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}}$$

This is a weighted average of the MLE (sample mean) and the prior mean, with weights proportional to their precisions.

#### Example: Bernoulli Likelihood with Beta Prior

For data D = {x₁, x₂, ..., x_n} from Bernoulli(θ) with a prior θ ~ Beta(α, β):

$$\hat{\theta}_{MAP} = \frac{\sum_{i=1}^n x_i + \alpha - 1}{n + \alpha + \beta - 2}$$

### 2. Numerical Optimization

For more complex models, numerical optimization methods are used:

- **Gradient Descent**: Update parameters in the direction of the posterior gradient
- **Newton's Method**: Use both first and second derivatives for faster convergence
- **Quasi-Newton Methods**: Approximate the Hessian matrix (e.g., BFGS, L-BFGS)
- **Coordinate Descent**: Optimize one parameter at a time, useful for L1 regularization

### 3. Expectation-Maximization (EM)

For models with latent variables, the EM algorithm can find MAP estimates:

- **E-step**: Compute the expected log-posterior with respect to the latent variables
- **M-step**: Maximize this expected log-posterior with respect to the parameters

## Properties of MAP Estimators

### 1. Regularization Effect

MAP estimation naturally incorporates regularization through the prior:

- **Prevents Overfitting**: Penalizes complex or extreme parameter values
- **Handles Ill-Posed Problems**: Provides unique solutions when MLE is non-unique
- **Improves Generalization**: Often leads to better performance on unseen data

### 2. Asymptotic Behavior

As the sample size increases:

- **Convergence to MLE**: The influence of the prior diminishes, and MAP approaches MLE
- **Consistency**: Under certain conditions, MAP estimators are consistent (converge to true parameters)
- **Asymptotic Normality**: For large samples, MAP estimators are approximately normally distributed

### 3. Invariance

Unlike MLE, MAP estimators are not invariant to reparameterization:

- If $\hat{\theta}_{MAP}$ is the MAP estimate of θ, then g($\hat{\theta}_{MAP}$) is generally not the MAP estimate of g(θ)
- This is because the prior distribution transforms differently than the likelihood under reparameterization

### 4. Relationship to Full Bayesian Inference

MAP provides a point estimate, while full Bayesian inference computes the entire posterior distribution:

- **Computational Efficiency**: MAP is typically much faster than full Bayesian inference
- **Uncertainty Quantification**: MAP doesn't directly quantify parameter uncertainty
- **Decision Making**: For some loss functions, the MAP estimate is the optimal Bayesian decision

## Applications in Machine Learning

### 1. Linear Models

#### Ridge Regression

Ridge regression is equivalent to MAP estimation with a Gaussian prior:

$$\hat{\beta}_{MAP} = \arg\min_{\beta} \left\{ \sum_{i=1}^n (y_i - \beta^T x_i)^2 + \lambda ||\beta||_2^2 \right\}$$

This has the closed-form solution:

$$\hat{\beta}_{MAP} = (X^T X + \lambda I)^{-1} X^T y$$

#### Lasso Regression

Lasso regression is equivalent to MAP estimation with a Laplace prior:

$$\hat{\beta}_{MAP} = \arg\min_{\beta} \left\{ \sum_{i=1}^n (y_i - \beta^T x_i)^2 + \lambda ||\beta||_1 \right\}$$

This typically requires numerical optimization, such as coordinate descent.

### 2. Classification Models

#### Logistic Regression with Regularization

Regularized logistic regression is equivalent to MAP estimation with appropriate priors:

$$\hat{\beta}_{MAP} = \arg\min_{\beta} \left\{ -\sum_{i=1}^n [y_i \log(p_i) + (1-y_i) \log(1-p_i)] + \lambda \text{penalty}(\beta) \right\}$$

where p_i = σ(β^T x_i) and σ is the sigmoid function.

#### Naive Bayes with Smoothing

Naive Bayes with Laplace smoothing is equivalent to MAP estimation with Dirichlet priors:

$$\hat{p}(x_j|y=c) = \frac{\text{count}(x_j, y=c) + \alpha}{\text{count}(y=c) + \alpha \cdot |\text{values}(x_j)|}$$

where α is the smoothing parameter (equivalent to a Dirichlet prior with all parameters equal to α+1).

### 3. Neural Networks

#### Weight Decay

Neural networks with weight decay are performing MAP estimation with Gaussian priors on weights:

$$L(\theta) = \text{DataLoss}(\theta) + \lambda ||\theta||_2^2$$

#### Dropout as Approximate Bayesian Inference

Dropout can be interpreted as approximate Bayesian inference, with MAP estimation as a special case.

### 4. Topic Models

#### Latent Dirichlet Allocation (LDA)

LDA uses Dirichlet priors for topic distributions and word distributions, with MAP estimation often used for inference.

### 5. Gaussian Processes

#### MAP Estimation for Hyperparameters

In Gaussian process regression, MAP estimation is often used for kernel hyperparameters:

$$\hat{\theta}_{MAP} = \arg\max_{\theta} P(y|X, \theta)P(\theta)$$

where θ are the hyperparameters of the kernel function.

## Example: MAP Estimation for Linear Regression

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Generate synthetic data
np.random.seed(42)
n = 20  # Small sample size to highlight the effect of the prior
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
true_sigma = 2.0  # Increased noise to highlight regularization
y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)

# Define the negative log-posterior function (Gaussian prior on slope and intercept)
def neg_log_posterior(params, lambda_reg=1.0, prior_mean=[0, 0], prior_sigma=[10, 10]):
    intercept, slope = params
    
    # Compute the negative log-likelihood (assuming Gaussian noise)
    y_pred = intercept + slope * x
    neg_log_likelihood = np.sum((y - y_pred)**2) / (2 * true_sigma**2)
    
    # Compute the negative log-prior (Gaussian prior)
    neg_log_prior = ((intercept - prior_mean[0])**2 / (2 * prior_sigma[0]**2) + 
                     (slope - prior_mean[1])**2 / (2 * prior_sigma[1]**2))
    
    # Return the negative log-posterior
    return neg_log_likelihood + lambda_reg * neg_log_prior

# Compute MLE (no regularization)
def neg_log_likelihood(params):
    intercept, slope = params
    y_pred = intercept + slope * x
    return np.sum((y - y_pred)**2) / (2 * true_sigma**2)

# Initial parameter guess
initial_params = [0.0, 0.0]  # [intercept, slope]

# Find MLE using numerical optimization
mle_result = minimize(neg_log_likelihood, initial_params, method='BFGS')
mle_intercept, mle_slope = mle_result.x

# Find MAP estimates with different regularization strengths
lambda_values = [0.1, 1.0, 10.0]
map_params = []

for lambda_reg in lambda_values:
    map_result = minimize(lambda p: neg_log_posterior(p, lambda_reg), initial_params, method='BFGS')
    map_params.append(map_result.x)

# Compute closed-form solution for comparison
X = np.column_stack((np.ones(n), x))
beta_mle = np.linalg.inv(X.T @ X) @ X.T @ y

# Ridge regression (MAP with Gaussian prior) closed-form solutions
beta_map = []
for lambda_reg in lambda_values:
    beta = np.linalg.inv(X.T @ X + lambda_reg * np.eye(2)) @ X.T @ y
    beta_map.append(beta)

# Plot the results
plt.figure(figsize=(12, 8))

# Plot the data
plt.scatter(x, y, alpha=0.7, label='Data')
plt.plot(x, true_intercept + true_slope * x, 'k-', label='True Line')

# Plot MLE line
plt.plot(x, mle_intercept + mle_slope * x, 'r--', label='MLE')

# Plot MAP lines with different regularization strengths
colors = ['g', 'b', 'm']
for i, (lambda_reg, params) in enumerate(zip(lambda_values, map_params)):
    intercept, slope = params
    plt.plot(x, intercept + slope * x, f'{colors[i]}-.', 
             label=f'MAP (λ={lambda_reg})')

plt.xlabel('x')
plt.ylabel('y')
plt.title('MAP Estimation for Linear Regression with Different Regularization Strengths')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Print parameter estimates
print(f"True parameters: Intercept={true_intercept}, Slope={true_slope}")
print(f"MLE parameters: Intercept={mle_intercept:.4f}, Slope={mle_slope:.4f}")
for i, lambda_reg in enumerate(lambda_values):
    intercept, slope = map_params[i]
    print(f"MAP parameters (λ={lambda_reg}): Intercept={intercept:.4f}, Slope={slope:.4f}")

# Plot the log-posterior surface for a specific regularization strength
lambda_reg = 1.0
intercept_range = np.linspace(-1, 3, 100)
slope_range = np.linspace(0, 4, 100)
II, SS = np.meshgrid(intercept_range, slope_range)
LP = np.zeros_like(II)

for i in range(len(intercept_range)):
    for j in range(len(slope_range)):
        LP[j, i] = -neg_log_posterior([II[j, i], SS[j, i]], lambda_reg)

plt.figure(figsize=(10, 8))
plt.contourf(II, SS, LP, 50, cmap='viridis')
plt.colorbar(label='Log-Posterior')
plt.plot(true_intercept, true_slope, 'k*', markersize=10, label='True Parameters')
plt.plot(mle_intercept, mle_slope, 'ro', markersize=8, label='MLE Parameters')
plt.plot(map_params[1][0], map_params[1][1], 'go', markersize=8, label=f'MAP Parameters (λ={lambda_reg})')
plt.xlabel('Intercept')
plt.ylabel('Slope')
plt.title(f'Log-Posterior Surface (λ={lambda_reg})')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Example: MAP Estimation for Logistic Regression

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.special import expit  # Sigmoid function

# Generate synthetic data
np.random.seed(42)
n = 50
x = np.random.normal(0, 1, n)
true_intercept = 0.5
true_slope = 2.0
p = expit(true_intercept + true_slope * x)
y = np.random.binomial(1, p)

# Define the negative log-posterior function (Gaussian prior on parameters)
def neg_log_posterior(params, lambda_reg=1.0):
    intercept, slope = params
    
    # Compute the predicted probabilities
    p_pred = expit(intercept + slope * x)
    
    # Compute the negative log-likelihood
    neg_log_likelihood = -np.sum(y * np.log(p_pred) + (1 - y) * np.log(1 - p_pred))
    
    # Compute the negative log-prior (Gaussian prior)
    neg_log_prior = (intercept**2 + slope**2) / 2
    
    # Return the negative log-posterior
    return neg_log_likelihood + lambda_reg * neg_log_prior

# Define the negative log-likelihood function (for MLE)
def neg_log_likelihood(params):
    intercept, slope = params
    p_pred = expit(intercept + slope * x)
    return -np.sum(y * np.log(p_pred) + (1 - y) * np.log(1 - p_pred))

# Initial parameter guess
initial_params = [0.0, 0.0]  # [intercept, slope]

# Find MLE using numerical optimization
mle_result = minimize(neg_log_likelihood, initial_params, method='BFGS')
mle_intercept, mle_slope = mle_result.x

# Find MAP estimates with different regularization strengths
lambda_values = [0.1, 1.0, 10.0]
map_params = []

for lambda_reg in lambda_values:
    map_result = minimize(lambda p: neg_log_posterior(p, lambda_reg), initial_params, method='BFGS')
    map_params.append(map_result.x)

# Plot the results
plt.figure(figsize=(12, 8))

# Plot the data
plt.scatter(x, y, alpha=0.7, label='Data')

# Plot the true, MLE, and MAP probability curves
x_sorted = np.sort(x)
p_true = expit(true_intercept + true_slope * x_sorted)
p_mle = expit(mle_intercept + mle_slope * x_sorted)

plt.plot(x_sorted, p_true, 'k-', label='True Probability')
plt.plot(x_sorted, p_mle, 'r--', label='MLE Probability')

# Plot MAP probability curves with different regularization strengths
colors = ['g', 'b', 'm']
for i, (lambda_reg, params) in enumerate(zip(lambda_values, map_params)):
    intercept, slope = params
    p_map = expit(intercept + slope * x_sorted)
    plt.plot(x_sorted, p_map, f'{colors[i]}-.', 
             label=f'MAP Probability (λ={lambda_reg})')

plt.xlabel('x')
plt.ylabel('Probability / Class Label')
plt.title('MAP Estimation for Logistic Regression with Different Regularization Strengths')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Print parameter estimates
print(f"True parameters: Intercept={true_intercept}, Slope={true_slope}")
print(f"MLE parameters: Intercept={mle_intercept:.4f}, Slope={mle_slope:.4f}")
for i, lambda_reg in enumerate(lambda_values):
    intercept, slope = map_params[i]
    print(f"MAP parameters (λ={lambda_reg}): Intercept={intercept:.4f}, Slope={slope:.4f}")
```

## Practical Considerations

### 1. Prior Selection

Choosing appropriate priors is crucial in MAP estimation:

- **Domain Knowledge**: Incorporate prior knowledge about parameters
- **Regularization Strength**: Control the influence of the prior through hyperparameters
- **Sensitivity Analysis**: Assess how sensitive the MAP estimate is to the choice of prior
- **Empirical Bayes**: Use the data to estimate hyperparameters of the prior

### 2. Computational Efficiency

MAP estimation is typically more computationally efficient than full Bayesian inference:

- **Point Estimate**: Computes a single parameter value rather than a full distribution
- **Optimization**: Uses optimization techniques rather than sampling methods
- **Scalability**: Can be applied to large models where full Bayesian inference is intractable

### 3. Limitations

MAP estimation has several limitations:

- **Point Estimate**: Provides only a point estimate, not a full uncertainty quantification
- **Mode vs. Mean**: The MAP estimate (mode of the posterior) may differ significantly from the posterior mean
- **Non-Invariance**: Not invariant to reparameterization
- **Local Maxima**: May get stuck in local maxima of the posterior

### 4. Hyperparameter Tuning

The regularization strength (equivalent to prior hyperparameters) often needs tuning:

- **Cross-Validation**: Select hyperparameters that minimize validation error
- **Information Criteria**: Use AIC, BIC, or other criteria for model selection
- **Hierarchical Models**: Place priors on hyperparameters themselves

## Comparison with Other Methods

### MAP vs. MLE

- **Regularization**: MAP includes regularization through the prior, MLE doesn't
- **Small Samples**: MAP often performs better with small sample sizes
- **Large Samples**: MAP and MLE converge as sample size increases
- **Computation**: Both involve optimization, with similar computational complexity

### MAP vs. Full Bayesian Inference

- **Uncertainty**: MAP provides a point estimate, Bayesian inference gives a full posterior distribution
- **Computation**: MAP is typically much faster than full Bayesian inference
- **Decision Making**: For some loss functions, the MAP estimate is the optimal Bayesian decision
- **Approximation**: MAP can be viewed as an approximation to full Bayesian inference

### MAP vs. Regularized Optimization

- **Interpretation**: MAP has a probabilistic interpretation, regularized optimization is often ad hoc
- **Hyperparameters**: In MAP, hyperparameters have a clear interpretation as prior parameters
- **Flexibility**: MAP allows for more flexible regularization through different prior distributions

## Summary

Maximum A Posteriori estimation is a powerful method that combines the likelihood of the data with prior knowledge about parameters:

1. **Concept**: MAP finds the parameter values that maximize the posterior probability P(θ|D).
   - Posterior ∝ Likelihood × Prior
   - $\hat{\theta}_{MAP} = \arg\max_{\theta} [P(D|\theta)P(\theta)]$
   - $\hat{\theta}_{MAP} = \arg\max_{\theta} [\log P(D|\theta) + \log P(\theta)]$

2. **Relationship to Regularization**: MAP estimation with specific priors corresponds to common regularization techniques.
   - Gaussian prior → L2 regularization (Ridge)
   - Laplace prior → L1 regularization (Lasso)
   - Combination → Elastic Net

3. **Common Priors**:
   - Gaussian prior for continuous parameters
   - Laplace prior for promoting sparsity
   - Beta prior for probabilities
   - Dirichlet prior for probability vectors
   - Gamma and Inverse-Gamma priors for positive parameters

4. **Computation**:
   - Analytical solutions for some models
   - Numerical optimization for complex models
   - EM algorithm for models with latent variables

5. **Properties**:
   - Regularization effect prevents overfitting
   - Converges to MLE as sample size increases
   - Not invariant to reparameterization
   - Provides a point estimate, not full uncertainty quantification

6. **Applications in ML**:
   - Ridge and Lasso regression
   - Regularized logistic regression
   - Neural networks with weight decay
   - Topic models
   - Gaussian processes

7. **Practical Considerations**:
   - Prior selection is crucial
   - Computationally efficient compared to full Bayesian inference
   - Hyperparameter tuning often needed
   - Limited uncertainty quantification

MAP estimation provides a principled approach to incorporating prior knowledge into parameter estimation, offering a balance between the simplicity of MLE and the full uncertainty quantification of Bayesian inference.

## References

1. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
4. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
5. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
