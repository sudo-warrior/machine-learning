# 2.3.4.2 Prior and Posterior Distributions

## Understanding Prior and Posterior Distributions

Prior and posterior distributions are fundamental concepts in Bayesian statistics and machine learning. They represent our beliefs about unknown parameters or hypotheses before and after observing data. In this section, we'll explore prior and posterior distributions, their properties, and their applications in machine learning.

## Prior Distributions

### Definition

A **prior distribution** represents our initial beliefs about unknown parameters or hypotheses before observing any data. It encapsulates our existing knowledge, assumptions, or uncertainties about the parameters of interest.

For a parameter θ, the prior distribution is denoted as P(θ).

### Types of Prior Distributions

#### 1. Informative Priors

Informative priors incorporate specific knowledge or beliefs about the parameters:

- **Domain Knowledge**: Based on expert opinion or previous studies
- **Previous Data**: Based on results from previous similar experiments
- **Physical Constraints**: Based on known physical limitations or properties

Example: If estimating the probability of a biased coin landing heads, and we believe the coin is likely fair, we might use a Beta(50, 50) prior, which is strongly peaked around 0.5.

#### 2. Weakly Informative Priors

Weakly informative priors provide some regularization without imposing strong beliefs:

- **Regularizing Priors**: Prevent extreme parameter values
- **Default Priors**: Reasonable defaults for common problems
- **Skeptical Priors**: Require substantial evidence to move away from null hypotheses

Example: A Normal(0, 10) prior for a regression coefficient provides some regularization without strongly constraining the parameter.

#### 3. Non-informative Priors

Non-informative priors attempt to represent a state of minimal knowledge:

- **Uniform Priors**: Equal probability across all possible parameter values
- **Jeffreys Priors**: Invariant to reparameterization
- **Reference Priors**: Maximize expected information from the data

Example: A Uniform(0, 1) prior for a probability parameter represents no preference for any value between 0 and 1.

#### 4. Improper Priors

Improper priors are not valid probability distributions (they don't integrate to 1) but can still lead to proper posteriors:

- **Flat Priors**: P(θ) ∝ 1 for θ ∈ (-∞, ∞)
- **Log-Uniform Priors**: P(θ) ∝ 1/θ for θ > 0

Example: A flat prior P(μ) ∝ 1 for a normal mean parameter μ.

### Conjugate Priors

A **conjugate prior** is a prior distribution that, when combined with a specific likelihood function, yields a posterior distribution of the same family. Conjugate priors simplify Bayesian analysis by providing analytical solutions.

Common conjugate prior-likelihood pairs:

| Likelihood | Parameter | Conjugate Prior | Posterior |
|------------|-----------|-----------------|-----------|
| Bernoulli/Binomial | Probability p | Beta | Beta |
| Multinomial | Probability vector p | Dirichlet | Dirichlet |
| Poisson | Rate λ | Gamma | Gamma |
| Normal (known variance) | Mean μ | Normal | Normal |
| Normal (known mean) | Variance σ² | Inverse-Gamma | Inverse-Gamma |
| Normal | Mean μ, variance σ² | Normal-Inverse-Gamma | Normal-Inverse-Gamma |

### Hierarchical Priors

**Hierarchical priors** model dependencies between parameters by placing priors on the parameters of the prior distributions (hyperparameters):

$$P(\theta|\alpha) = \int P(\theta|\eta)P(\eta|\alpha)d\eta$$

where η are intermediate parameters and α are hyperparameters.

Hierarchical priors allow:
- **Partial Pooling**: Sharing information across related parameters
- **Modeling Heterogeneity**: Capturing variations between groups
- **Regularization**: Preventing overfitting in complex models

## Posterior Distributions

### Definition

A **posterior distribution** represents our updated beliefs about unknown parameters or hypotheses after observing data. It combines the prior distribution with the likelihood function using Bayes' theorem.

For a parameter θ and data D, the posterior distribution is:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$

where:
- P(D|θ) is the likelihood function
- P(θ) is the prior distribution
- P(D) is the marginal likelihood or evidence

### Interpretation

The posterior distribution quantifies our uncertainty about parameters after seeing the data:

- **Central Tendency**: The posterior mean, median, or mode provides point estimates
- **Dispersion**: The posterior variance or credible intervals quantify uncertainty
- **Shape**: The full posterior shape captures the distribution of plausible parameter values

### Posterior Summaries

Common ways to summarize posterior distributions include:

#### 1. Point Estimates

- **Posterior Mean**: E[θ|D] = ∫ θ P(θ|D) dθ
- **Posterior Median**: The value θ̃ such that P(θ ≤ θ̃|D) = 0.5
- **Posterior Mode (MAP)**: argmax_θ P(θ|D)

#### 2. Interval Estimates

- **Credible Intervals**: Intervals containing a specified probability mass of the posterior
- **Highest Posterior Density (HPD) Intervals**: The shortest intervals containing a specified probability mass

#### 3. Full Distribution

- **Samples**: Representative draws from the posterior
- **Kernel Density Estimation**: Smooth approximation of the posterior density
- **Analytical Form**: Closed-form expression when available

## Computing Posterior Distributions

### 1. Analytical Solutions

For conjugate prior-likelihood pairs, the posterior has a known analytical form:

Example: Beta-Binomial
- Prior: θ ~ Beta(α, β)
- Likelihood: y ~ Binomial(n, θ) with y successes out of n trials
- Posterior: θ|y ~ Beta(α + y, β + n - y)

### 2. Numerical Integration

For low-dimensional problems without analytical solutions, numerical integration can compute the posterior:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{\int P(D|\theta')P(\theta')d\theta'}$$

Methods include:
- **Quadrature**: For one or two dimensions
- **Monte Carlo Integration**: For higher dimensions

### 3. Markov Chain Monte Carlo (MCMC)

MCMC methods generate samples from the posterior without computing the normalizing constant:

- **Metropolis-Hastings**: Proposes and accepts/rejects samples
- **Gibbs Sampling**: Samples each parameter conditionally on others
- **Hamiltonian Monte Carlo**: Uses gradient information for efficient sampling
- **No-U-Turn Sampler (NUTS)**: Adaptive variant of Hamiltonian Monte Carlo

### 4. Variational Inference

Variational methods approximate the posterior with a simpler distribution Q(θ):

- Minimize the Kullback-Leibler divergence KL(Q||P)
- Typically faster than MCMC but less accurate
- Common variational families include mean-field approximations and Gaussian approximations

## The Role of the Marginal Likelihood

The marginal likelihood or evidence P(D) in Bayes' theorem is:

$$P(D) = \int P(D|\theta)P(\theta)d\theta$$

It serves several important roles:

### 1. Normalization Constant

The marginal likelihood ensures the posterior integrates to 1:

$$\int P(\theta|D)d\theta = \int \frac{P(D|\theta)P(\theta)}{P(D)}d\theta = \frac{1}{P(D)}\int P(D|\theta)P(\theta)d\theta = 1$$

### 2. Model Comparison

The marginal likelihood is used for Bayesian model comparison:

- **Bayes Factors**: Ratio of marginal likelihoods for two models
- **Posterior Model Probabilities**: P(Model|D) ∝ P(D|Model) × P(Model)

### 3. Automatic Occam's Razor

The marginal likelihood naturally penalizes overly complex models:

- Complex models can fit the observed data better (higher likelihood)
- But they spread their prior probability over a larger parameter space
- The marginal likelihood balances fit and complexity

## Prior and Posterior Predictive Distributions

### Prior Predictive Distribution

The **prior predictive distribution** is the distribution of new data before seeing any data:

$$P(D_{new}) = \int P(D_{new}|\theta)P(\theta)d\theta$$

It's useful for:
- **Prior Checking**: Assessing if the prior generates reasonable data
- **Model Criticism**: Comparing different prior specifications
- **Simulation**: Generating synthetic data based on prior beliefs

### Posterior Predictive Distribution

The **posterior predictive distribution** is the distribution of new data after seeing existing data:

$$P(D_{new}|D) = \int P(D_{new}|\theta)P(\theta|D)d\theta$$

It's useful for:
- **Prediction**: Forecasting future observations
- **Model Checking**: Assessing if the model generates data similar to observed data
- **Decision Making**: Accounting for parameter uncertainty in decisions

## Applications in Machine Learning

### 1. Bayesian Linear Regression

In Bayesian linear regression, we place priors on the regression coefficients β and noise variance σ²:

- **Prior**: Often β ~ N(0, Σ_β) and σ² ~ Inverse-Gamma(a, b)
- **Likelihood**: y ~ N(Xβ, σ²I)
- **Posterior**: P(β, σ²|X, y) ∝ P(y|X, β, σ²) × P(β) × P(σ²)

The posterior quantifies uncertainty in the regression coefficients and enables probabilistic predictions.

### 2. Bayesian Neural Networks

Bayesian neural networks place priors on the network weights w:

- **Prior**: Often w ~ N(0, σ²_w I)
- **Likelihood**: Based on the neural network architecture and problem (e.g., Gaussian for regression, categorical for classification)
- **Posterior**: P(w|D) ∝ P(D|w) × P(w)

The posterior over weights allows quantifying prediction uncertainty and avoiding overfitting.

### 3. Gaussian Process Regression

Gaussian process regression places a prior directly on the space of functions:

- **Prior**: f ~ GP(m, k) where m is the mean function and k is the covariance function
- **Likelihood**: y ~ N(f(X), σ²I)
- **Posterior**: f|D ~ GP(m', k') with updated mean and covariance functions

The posterior is another Gaussian process, enabling probabilistic predictions with uncertainty quantification.

### 4. Topic Models

Latent Dirichlet Allocation (LDA) uses Dirichlet priors for topic distributions:

- **Prior**: Topic distributions θ ~ Dirichlet(α) and word distributions φ ~ Dirichlet(β)
- **Likelihood**: Based on the multinomial distribution of words in documents
- **Posterior**: P(θ, φ, z|w) where z are the topic assignments and w are the observed words

The posterior reveals the latent topic structure in the documents.

### 5. Bayesian Optimization

Bayesian optimization uses Gaussian process priors for unknown objective functions:

- **Prior**: f ~ GP(m, k)
- **Likelihood**: y ~ N(f(x), σ²)
- **Posterior**: Updated Gaussian process after observing function evaluations

The posterior guides the search for the optimum by balancing exploration and exploitation.

## Practical Considerations

### 1. Prior Sensitivity Analysis

It's important to assess how sensitive the posterior is to the choice of prior:

- **Multiple Priors**: Compare results with different prior specifications
- **Weakening Priors**: Gradually reduce the informativeness of priors
- **Robustness**: Check if conclusions hold across reasonable prior choices

### 2. Prior Elicitation

Techniques for constructing informative priors from domain knowledge:

- **Expert Elicitation**: Structured methods for extracting expert beliefs
- **Meta-Analysis**: Synthesizing results from previous studies
- **Empirical Bayes**: Using the data to estimate hyperparameters

### 3. Computational Efficiency

Strategies for efficient posterior computation:

- **Conjugate Priors**: Use when possible for analytical solutions
- **Efficient MCMC**: Tuning proposal distributions, using gradient information
- **Approximate Methods**: Variational inference, expectation propagation
- **Subsampling**: Working with data subsets for large datasets

### 4. Posterior Diagnostics

Methods for checking the quality of posterior approximations:

- **MCMC Diagnostics**: Trace plots, autocorrelation, effective sample size, R-hat statistic
- **Variational Diagnostics**: ELBO convergence, comparing with MCMC results
- **Sensitivity Analysis**: Checking robustness to algorithmic choices

## Example: Beta-Binomial Model

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Prior parameters
alpha_prior = 2
beta_prior = 2

# Data
n_trials = 20
n_successes = 15

# Posterior parameters
alpha_posterior = alpha_prior + n_successes
beta_posterior = beta_prior + n_trials - n_successes

# Plot prior, likelihood, and posterior
theta = np.linspace(0, 1, 1000)
prior = stats.beta.pdf(theta, alpha_prior, beta_prior)
likelihood = stats.binom.pmf(n_successes, n_trials, theta) / stats.binom.pmf(n_successes, n_trials, n_successes/n_trials)  # Normalized likelihood
posterior = stats.beta.pdf(theta, alpha_posterior, beta_posterior)

plt.figure(figsize=(12, 8))

plt.plot(theta, prior, 'b--', label=f'Prior: Beta({alpha_prior}, {beta_prior})')
plt.plot(theta, likelihood, 'r-.', label=f'Likelihood: Bin({n_successes}|{n_trials}, θ)')
plt.plot(theta, posterior, 'g-', label=f'Posterior: Beta({alpha_posterior}, {beta_posterior})')

# Add vertical lines for point estimates
plt.axvline(alpha_prior / (alpha_prior + beta_prior), color='b', linestyle=':', label='Prior Mean')
plt.axvline(n_successes / n_trials, color='r', linestyle=':', label='MLE')
plt.axvline(alpha_posterior / (alpha_posterior + beta_posterior), color='g', linestyle=':', label='Posterior Mean')

plt.xlabel('θ (Probability of Success)')
plt.ylabel('Density')
plt.title('Beta-Binomial Conjugate Model')
plt.legend()
plt.grid(alpha=0.3)

# Add 95% credible interval
ci_low, ci_high = stats.beta.ppf([0.025, 0.975], alpha_posterior, beta_posterior)
plt.fill_between(theta, 0, posterior, where=(theta >= ci_low) & (theta <= ci_high), 
                 color='g', alpha=0.2, label='95% Credible Interval')

plt.annotate(f'95% CI: [{ci_low:.3f}, {ci_high:.3f}]', 
             xy=(0.5, 0.5), xycoords='axes fraction',
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))

plt.show()

# Prior and posterior predictive distributions
x_new = np.arange(0, n_trials + 1)

# Prior predictive
prior_pred = np.zeros_like(x_new, dtype=float)
for i, x in enumerate(x_new):
    prior_pred[i] = stats.beta.expect(lambda p: stats.binom.pmf(x, n_trials, p),
                                      args=(alpha_prior, beta_prior))

# Posterior predictive
post_pred = np.zeros_like(x_new, dtype=float)
for i, x in enumerate(x_new):
    post_pred[i] = stats.beta.expect(lambda p: stats.binom.pmf(x, n_trials, p),
                                     args=(alpha_posterior, beta_posterior))

plt.figure(figsize=(12, 6))
plt.bar(x_new - 0.2, prior_pred, width=0.4, alpha=0.7, label='Prior Predictive')
plt.bar(x_new + 0.2, post_pred, width=0.4, alpha=0.7, label='Posterior Predictive')
plt.axvline(n_successes, color='r', linestyle='--', label='Observed Successes')

plt.xlabel('Number of Successes in 20 Trials')
plt.ylabel('Probability')
plt.title('Prior and Posterior Predictive Distributions')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Example: Bayesian Linear Regression

```python
import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm

# Generate synthetic data
np.random.seed(42)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
true_sigma = 1.0
y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)

# Prior specifications
intercept_prior_mean = 0
intercept_prior_sd = 10
slope_prior_mean = 0
slope_prior_sd = 10
sigma_prior_alpha = 1
sigma_prior_beta = 1

# Bayesian linear regression with PyMC3
with pm.Model() as model:
    # Priors
    intercept = pm.Normal('intercept', mu=intercept_prior_mean, sigma=intercept_prior_sd)
    slope = pm.Normal('slope', mu=slope_prior_mean, sigma=slope_prior_sd)
    sigma = pm.HalfCauchy('sigma', beta=sigma_prior_beta)
    
    # Likelihood
    likelihood = pm.Normal('likelihood', mu=intercept + slope * x, sigma=sigma, observed=y)
    
    # Prior predictive sampling
    prior_samples = pm.sample_prior_predictive(100)
    
    # Inference
    trace = pm.sample(2000, tune=1000, return_inferencedata=True)
    
    # Posterior predictive sampling
    posterior_samples = pm.sample_posterior_predictive(trace)

# Plot prior predictive distribution
plt.figure(figsize=(12, 6))
plt.scatter(x, y, alpha=0.7, label='Data')

# Plot prior predictive samples
for i in range(20):
    prior_intercept = prior_samples['intercept'][i]
    prior_slope = prior_samples['slope'][i]
    plt.plot(x, prior_intercept + prior_slope * x, 'r-', alpha=0.1)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Prior Predictive Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Plot posterior distribution
plt.figure(figsize=(12, 6))
plt.scatter(x, y, alpha=0.7, label='Data')
plt.plot(x, true_intercept + true_slope * x, 'r', label='True Line')

# Plot posterior samples
post = trace.posterior
for i in range(20):
    idx = np.random.randint(len(post.intercept))
    chain = np.random.randint(len(post.intercept.chain))
    intercept_sample = post.intercept[chain, idx].item()
    slope_sample = post.slope[chain, idx].item()
    plt.plot(x, intercept_sample + slope_sample * x, 'g', alpha=0.3)

# Posterior mean
intercept_mean = post.intercept.mean().item()
slope_mean = post.slope.mean().item()
plt.plot(x, intercept_mean + slope_mean * x, 'b', label='Posterior Mean')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Posterior Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Plot posterior predictive distribution
plt.figure(figsize=(12, 6))
plt.scatter(x, y, alpha=0.7, label='Data')

# Plot posterior predictive samples
y_pred = posterior_samples['likelihood']
for i in range(20):
    plt.plot(x, y_pred[i], 'g-', alpha=0.1)

# Plot 90% prediction interval
y_pred_mean = y_pred.mean(axis=0)
y_pred_5 = np.percentile(y_pred, 5, axis=0)
y_pred_95 = np.percentile(y_pred, 95, axis=0)

plt.plot(x, y_pred_mean, 'b-', label='Predictive Mean')
plt.fill_between(x, y_pred_5, y_pred_95, color='b', alpha=0.2, label='90% Prediction Interval')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Posterior Predictive Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Summary

Prior and posterior distributions are fundamental concepts in Bayesian statistics and machine learning:

1. **Prior Distributions**:
   - Represent initial beliefs about parameters before seeing data
   - Can be informative, weakly informative, non-informative, or improper
   - Conjugate priors simplify computation by yielding posteriors in the same family
   - Hierarchical priors model dependencies between parameters

2. **Posterior Distributions**:
   - Represent updated beliefs about parameters after seeing data
   - Combine prior distributions with likelihood functions using Bayes' theorem
   - Can be summarized using point estimates, interval estimates, or the full distribution
   - Quantify uncertainty about parameters and enable probabilistic predictions

3. **Computing Posteriors**:
   - Analytical solutions for conjugate prior-likelihood pairs
   - Numerical integration for low-dimensional problems
   - MCMC methods for sampling from complex posteriors
   - Variational inference for approximate but faster computation

4. **Marginal Likelihood**:
   - Normalizes the posterior distribution
   - Enables model comparison through Bayes factors
   - Implements automatic Occam's razor to balance fit and complexity

5. **Predictive Distributions**:
   - Prior predictive distribution for checking priors and generating synthetic data
   - Posterior predictive distribution for making predictions and checking model fit

6. **Applications in ML**:
   - Bayesian linear regression
   - Bayesian neural networks
   - Gaussian process regression
   - Topic models
   - Bayesian optimization

Understanding prior and posterior distributions is essential for applying Bayesian methods in machine learning, enabling principled uncertainty quantification, regularization, and decision-making under uncertainty.

## References

1. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
2. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
3. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
4. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
5. Kruschke, J. K. (2014). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (2nd ed.). Academic Press.
