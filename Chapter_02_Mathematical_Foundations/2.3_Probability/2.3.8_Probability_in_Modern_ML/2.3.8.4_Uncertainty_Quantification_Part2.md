# 2.3.8.4 Uncertainty Quantification in Machine Learning - Part 2

## Advanced Methods for Uncertainty Quantification

Building on the foundations covered in Part 1, this section explores more advanced methods for uncertainty quantification in modern machine learning models. We'll focus on ensemble methods, Bayesian neural networks, and techniques for handling uncertainty in deep learning.

## Ensemble Methods

Ensemble methods combine multiple models to improve predictive performance and provide uncertainty estimates.

### Random Forests and Uncertainty

Random Forests naturally quantify uncertainty through the variance of predictions across trees:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = X.flatten()

# Sort for better visualization
idx = np.argsort(X)
X = X[idx]
y = y[idx]

# Reshape X for sklearn
X_reshaped = X.reshape(-1, 1)

# Fit Random Forest model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_reshaped, y)

# Generate predictions
X_test = np.linspace(X.min() - 2, X.max() + 2, 1000).reshape(-1, 1)
y_pred = rf.predict(X_test)

# Get predictions from individual trees
tree_preds = np.array([tree.predict(X_test) for tree in rf.estimators_])

# Calculate mean and standard deviation across trees
y_mean = np.mean(tree_preds, axis=0)
y_std = np.std(tree_preds, axis=0)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, y_mean, color='red', label='Random Forest mean')
plt.fill_between(X_test.flatten(), 
                 y_mean - 2 * y_std, 
                 y_mean + 2 * y_std, 
                 color='red', alpha=0.2, label='±2σ (Tree variance)')

# Highlight extrapolation regions
plt.axvspan(X.min() - 2, X.min(), alpha=0.2, color='yellow', label='Extrapolation region')
plt.axvspan(X.max(), X.max() + 2, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Random Forest Uncertainty Estimation')
plt.legend()
plt.grid(True)
plt.show()

# Plot histogram of tree predictions for specific points
plt.figure(figsize=(12, 5))

# Point in interpolation region
interp_idx = 500  # Middle of the range
plt.subplot(1, 2, 1)
plt.hist(tree_preds[:, interp_idx], bins=20, alpha=0.7)
plt.axvline(y_mean[interp_idx], color='red', linestyle='--', label='Mean prediction')
plt.xlabel('Prediction')
plt.ylabel('Frequency')
plt.title(f'Tree Predictions (Interpolation, x={X_test[interp_idx, 0]:.2f})')
plt.legend()

# Point in extrapolation region
extrap_idx = 950  # Near the end of the range
plt.subplot(1, 2, 2)
plt.hist(tree_preds[:, extrap_idx], bins=20, alpha=0.7)
plt.axvline(y_mean[extrap_idx], color='red', linestyle='--', label='Mean prediction')
plt.xlabel('Prediction')
plt.ylabel('Frequency')
plt.title(f'Tree Predictions (Extrapolation, x={X_test[extrap_idx, 0]:.2f})')
plt.legend()

plt.tight_layout()
plt.show()
```

### Deep Ensembles

**Deep Ensembles** train multiple neural networks with different random initializations:

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Generate synthetic data with heteroscedastic noise
np.random.seed(42)
n_samples = 100
X = np.sort(np.random.uniform(-3, 3, n_samples))

# True function
def f(x):
    return x**3 - 2*x + 1

# Noise level increases with |x|
def noise_level(x):
    return 0.2 + 0.3 * np.abs(x)

# Generate noisy observations
y_true = f(X)
noise = np.array([np.random.normal(0, noise_level(x)) for x in X])
y_noisy = y_true + noise

# Reshape X for TensorFlow
X_reshaped = X.reshape(-1, 1)

# Function to create and train a model
def create_and_train_model(X, y):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(1,)),
        Dense(64, activation='relu'),
        Dense(2)  # Output mean and log variance
    ])
    
    # Custom loss function for heteroscedastic regression
    def heteroscedastic_loss(y_true, y_pred):
        mean = y_pred[:, 0]
        log_var = y_pred[:, 1]
        precision = tf.exp(-log_var)
        return tf.reduce_mean(precision * (y_true - mean)**2 + log_var)
    
    model.compile(optimizer=Adam(0.01), loss=heteroscedastic_loss)
    model.fit(X, y, epochs=200, verbose=0)
    return model

# Train ensemble of models
n_models = 5
ensemble = [create_and_train_model(X_reshaped, y_noisy) for _ in range(n_models)]

# Generate predictions
X_test = np.linspace(-4, 4, 1000).reshape(-1, 1)
predictions = np.array([model.predict(X_test) for model in ensemble])

# Extract means and variances
means = predictions[:, :, 0]
log_vars = predictions[:, :, 1]
vars = np.exp(log_vars)

# Calculate ensemble statistics
ensemble_mean = np.mean(means, axis=0)
aleatoric_uncertainty = np.mean(vars, axis=0)  # Average of individual variances
epistemic_uncertainty = np.var(means, axis=0)  # Variance of means
total_uncertainty = aleatoric_uncertainty + epistemic_uncertainty

# Plot the results
plt.figure(figsize=(12, 8))

# Plot data and predictions
plt.subplot(2, 1, 1)
plt.scatter(X, y_noisy, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, f(X_test), 'k--', label='True function')
plt.plot(X_test, ensemble_mean, color='red', label='Ensemble mean')

# Plot individual model predictions
for i in range(n_models):
    plt.plot(X_test, means[i], alpha=0.3, color='gray')

plt.fill_between(X_test.flatten(), 
                 ensemble_mean - 2 * np.sqrt(total_uncertainty), 
                 ensemble_mean + 2 * np.sqrt(total_uncertainty), 
                 color='red', alpha=0.2, label='Total uncertainty (±2σ)')

# Highlight extrapolation regions
plt.axvspan(-4, -3, alpha=0.2, color='yellow', label='Extrapolation region')
plt.axvspan(3, 4, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Deep Ensemble Predictions')
plt.legend()
plt.grid(True)

# Plot uncertainty decomposition
plt.subplot(2, 1, 2)
plt.plot(X_test, aleatoric_uncertainty, 'b-', label='Aleatoric uncertainty')
plt.plot(X_test, epistemic_uncertainty, 'g-', label='Epistemic uncertainty')
plt.plot(X_test, total_uncertainty, 'r-', label='Total uncertainty')

# Highlight extrapolation regions
plt.axvspan(-4, -3, alpha=0.2, color='yellow')
plt.axvspan(3, 4, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('Variance')
plt.title('Uncertainty Decomposition')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Quantile Regression Forests

**Quantile Regression Forests** directly estimate the conditional distribution of the target:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Generate synthetic data with heteroscedastic noise
np.random.seed(42)
n_samples = 200
X = np.sort(np.random.uniform(-3, 3, n_samples))

# True function
def f(x):
    return x**3 - 2*x + 1

# Noise level increases with |x|
def noise_level(x):
    return 0.2 + 0.3 * np.abs(x)

# Generate noisy observations
y_true = f(X)
noise = np.array([np.random.normal(0, noise_level(x)) for x in X])
y_noisy = y_true + noise

# Reshape X for sklearn
X_reshaped = X.reshape(-1, 1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_noisy, test_size=0.2, random_state=42)

# Fit Random Forest model
rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)
rf.fit(X_train, y_train)

# Function to compute quantiles from Random Forest
def quantile_forest(rf, X, quantiles):
    # Get predictions from all trees
    tree_preds = np.array([tree.predict(X) for tree in rf.estimators_])
    
    # Compute quantiles
    return np.quantile(tree_preds, quantiles, axis=0)

# Generate predictions
X_pred = np.linspace(-4, 4, 1000).reshape(-1, 1)
y_pred_median = rf.predict(X_pred)

# Compute quantiles
quantiles = [0.05, 0.25, 0.75, 0.95]
y_pred_quantiles = quantile_forest(rf, X_pred, quantiles)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y_noisy, color='blue', alpha=0.5, label='Observations')
plt.plot(X_pred, f(X_pred), 'k--', label='True function')
plt.plot(X_pred, y_pred_median, color='red', label='RF median prediction')

# Plot quantiles
plt.fill_between(X_pred.flatten(), 
                 y_pred_quantiles[0], 
                 y_pred_quantiles[3], 
                 color='red', alpha=0.1, label='90% prediction interval')
plt.fill_between(X_pred.flatten(), 
                 y_pred_quantiles[1], 
                 y_pred_quantiles[2], 
                 color='red', alpha=0.2, label='50% prediction interval')

# Highlight extrapolation regions
plt.axvspan(-4, -3, alpha=0.2, color='yellow', label='Extrapolation region')
plt.axvspan(3, 4, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Quantile Regression Forest')
plt.legend()
plt.grid(True)
plt.show()
```

## Bayesian Neural Networks

Bayesian Neural Networks (BNNs) place prior distributions over network weights and compute posterior distributions.

### Monte Carlo Dropout

**Monte Carlo Dropout** is a simple approximation to Bayesian inference in neural networks:

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Generate synthetic data
np.random.seed(42)
n_samples = 100
X = np.sort(np.random.uniform(-3, 3, n_samples))
y = np.sin(X) + 0.3 * np.random.randn(n_samples)

# Reshape X for TensorFlow
X_reshaped = X.reshape(-1, 1)

# Build model with dropout
model = Sequential([
    Dense(64, activation='relu', input_shape=(1,)),
    Dropout(0.5),  # Dropout layer
    Dense(64, activation='relu'),
    Dropout(0.5),  # Dropout layer
    Dense(1)
])

# Compile and train
model.compile(optimizer=Adam(0.01), loss='mse')
model.fit(X_reshaped, y, epochs=500, verbose=0)

# Function to perform MC Dropout inference
def mc_dropout_predict(model, X, n_samples=100):
    predictions = np.zeros((n_samples, len(X)))
    
    for i in range(n_samples):
        predictions[i, :] = model(X, training=True).numpy().flatten()
    
    return predictions

# Generate predictions
X_test = np.linspace(-4, 4, 1000).reshape(-1, 1)
predictions = mc_dropout_predict(model, X_test)

# Calculate statistics
mean = np.mean(predictions, axis=0)
std = np.std(predictions, axis=0)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, mean, color='red', label='Predictive mean')
plt.fill_between(X_test.flatten(), 
                 mean - 2 * std, 
                 mean + 2 * std, 
                 color='red', alpha=0.2, label='±2σ (Epistemic uncertainty)')

# Highlight extrapolation regions
plt.axvspan(-4, -3, alpha=0.2, color='yellow', label='Extrapolation region')
plt.axvspan(3, 4, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Monte Carlo Dropout')
plt.legend()
plt.grid(True)
plt.show()

# Plot predictive distributions at specific points
plt.figure(figsize=(12, 5))

# Point in interpolation region
interp_idx = 500  # Middle of the range
plt.subplot(1, 2, 1)
plt.hist(predictions[:, interp_idx], bins=20, alpha=0.7)
plt.axvline(mean[interp_idx], color='red', linestyle='--', label='Mean prediction')
plt.xlabel('Prediction')
plt.ylabel('Frequency')
plt.title(f'MC Dropout Predictions (Interpolation, x={X_test[interp_idx, 0]:.2f})')
plt.legend()

# Point in extrapolation region
extrap_idx = 950  # Near the end of the range
plt.subplot(1, 2, 2)
plt.hist(predictions[:, extrap_idx], bins=20, alpha=0.7)
plt.axvline(mean[extrap_idx], color='red', linestyle='--', label='Mean prediction')
plt.xlabel('Prediction')
plt.ylabel('Frequency')
plt.title(f'MC Dropout Predictions (Extrapolation, x={X_test[extrap_idx, 0]:.2f})')
plt.legend()

plt.tight_layout()
plt.show()
```

### Variational Inference for BNNs

**Variational Inference** approximates the posterior distribution over weights:

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_probability as tfp

tfd = tfp.distributions
tfpl = tfp.layers

# Generate synthetic data
np.random.seed(42)
n_samples = 100
X = np.sort(np.random.uniform(-3, 3, n_samples))
y = np.sin(X) + 0.3 * np.random.randn(n_samples)

# Reshape X for TensorFlow
X_reshaped = X.reshape(-1, 1)
y_reshaped = y.reshape(-1, 1)

# Function to build a Bayesian neural network
def build_bnn(input_shape):
    # Prior distribution for weights
    prior = tfd.Normal(loc=0., scale=1.)
    
    # Posterior distribution for weights
    posterior = lambda kernel_size, bias_size, dtype: tfpl.VariationalGaussianProcess(
        kernel_provider=tfpl.LinearKernelProvider(),
        event_shape=[1],
        inducing_index_points=np.linspace(-4, 4, 10).reshape(-1, 1).astype(np.float32),
        variational_inducing_observations_distribution=tfd.Normal(
            loc=tf.zeros([10, 1], dtype=tf.float32),
            scale=tf.ones([10, 1], dtype=tf.float32)),
        jitter=1e-3,
        convert_to_tensor_fn=tfd.Distribution.sample)
    
    # Build the model
    model = tf.keras.Sequential([
        tfpl.DenseVariational(64, activation='relu',
                             input_shape=input_shape,
                             make_prior_fn=lambda *args, **kwargs: prior,
                             make_posterior_fn=lambda *args, **kwargs: 
                                 tfd.Independent(tfd.Normal(loc=tf.Variable(tf.random.normal([np.prod(input_shape), 64])),
                                                          scale=tf.nn.softplus(tf.Variable(tf.random.normal([np.prod(input_shape), 64])))),
                                                num_dims=1)),
        tfpl.DenseVariational(1,
                             make_prior_fn=lambda *args, **kwargs: prior,
                             make_posterior_fn=lambda *args, **kwargs: 
                                 tfd.Independent(tfd.Normal(loc=tf.Variable(tf.random.normal([64, 1])),
                                                          scale=tf.nn.softplus(tf.Variable(tf.random.normal([64, 1])))),
                                                num_dims=1))
    ])
    
    return model

# Build and compile the model
model = build_bnn((1,))

# Define the negative log likelihood loss
def negative_log_likelihood(y_true, y_pred):
    return -y_pred.log_prob(y_true)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
             loss=negative_log_likelihood)

# Train the model
model.fit(X_reshaped, y_reshaped, epochs=1000, verbose=0)

# Generate predictions
X_test = np.linspace(-4, 4, 1000).reshape(-1, 1)
predictions = np.array([model(X_test).sample() for _ in range(100)])

# Calculate statistics
mean = np.mean(predictions, axis=0)
std = np.std(predictions, axis=0)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, mean, color='red', label='Predictive mean')
plt.fill_between(X_test.flatten(), 
                 mean - 2 * std, 
                 mean + 2 * std, 
                 color='red', alpha=0.2, label='±2σ (Epistemic uncertainty)')

# Highlight extrapolation regions
plt.axvspan(-4, -3, alpha=0.2, color='yellow', label='Extrapolation region')
plt.axvspan(3, 4, alpha=0.2, color='yellow')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Bayesian Neural Network with Variational Inference')
plt.legend()
plt.grid(True)
plt.show()
```

## Calibration of Uncertainty Estimates

Uncertainty estimates should be well-calibrated, meaning that confidence levels match empirical frequencies.

### Evaluating Calibration

**Calibration curves** (reliability diagrams) assess how well predicted probabilities match empirical frequencies:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import calibration_curve

# Generate synthetic classification data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get predicted probabilities
y_prob = rf.predict_proba(X_test)[:, 1]

# Compute calibration curve
prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)

# Plot calibration curve
plt.figure(figsize=(10, 6))
plt.plot(prob_pred, prob_true, 's-', label='Random Forest')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('Calibration Curve (Reliability Diagram)')
plt.legend()
plt.grid(True)
plt.show()

# Compute Brier score
from sklearn.metrics import brier_score_loss
brier_score = brier_score_loss(y_test, y_prob)
print(f"Brier score: {brier_score:.4f}")
```

### Calibration Methods

**Platt Scaling** and **Isotonic Regression** can improve calibration:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import brier_score_loss

# Generate synthetic classification data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Calibrate with Platt scaling
platt = CalibratedClassifierCV(rf, method='sigmoid', cv='prefit')
platt.fit(X_test, y_test)

# Calibrate with isotonic regression
isotonic = CalibratedClassifierCV(rf, method='isotonic', cv='prefit')
isotonic.fit(X_test, y_test)

# Get predicted probabilities
y_prob_rf = rf.predict_proba(X_test)[:, 1]
y_prob_platt = platt.predict_proba(X_test)[:, 1]
y_prob_isotonic = isotonic.predict_proba(X_test)[:, 1]

# Compute calibration curves
prob_true_rf, prob_pred_rf = calibration_curve(y_test, y_prob_rf, n_bins=10)
prob_true_platt, prob_pred_platt = calibration_curve(y_test, y_prob_platt, n_bins=10)
prob_true_isotonic, prob_pred_isotonic = calibration_curve(y_test, y_prob_isotonic, n_bins=10)

# Compute Brier scores
brier_rf = brier_score_loss(y_test, y_prob_rf)
brier_platt = brier_score_loss(y_test, y_prob_platt)
brier_isotonic = brier_score_loss(y_test, y_prob_isotonic)

# Plot calibration curves
plt.figure(figsize=(10, 6))
plt.plot(prob_pred_rf, prob_true_rf, 's-', label=f'Random Forest (Brier: {brier_rf:.4f})')
plt.plot(prob_pred_platt, prob_true_platt, 's-', label=f'Platt Scaling (Brier: {brier_platt:.4f})')
plt.plot(prob_pred_isotonic, prob_true_isotonic, 's-', label=f'Isotonic Regression (Brier: {brier_isotonic:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('Calibration Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Plot histograms of predicted probabilities
plt.figure(figsize=(12, 5))

plt.subplot(1, 3, 1)
plt.hist(y_prob_rf, bins=20, range=(0, 1), alpha=0.7)
plt.title('Random Forest')
plt.xlabel('Predicted probability')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(y_prob_platt, bins=20, range=(0, 1), alpha=0.7)
plt.title('Platt Scaling')
plt.xlabel('Predicted probability')

plt.subplot(1, 3, 3)
plt.hist(y_prob_isotonic, bins=20, range=(0, 1), alpha=0.7)
plt.title('Isotonic Regression')
plt.xlabel('Predicted probability')

plt.tight_layout()
plt.show()
```

## Summary

Advanced methods for uncertainty quantification provide powerful tools for modern machine learning:

1. **Ensemble Methods**:
   - Random Forests: Natural uncertainty quantification through tree variance
   - Deep Ensembles: Multiple neural networks with different initializations
   - Quantile Regression Forests: Direct estimation of conditional distributions

2. **Bayesian Neural Networks**:
   - Monte Carlo Dropout: Simple approximation to Bayesian inference
   - Variational Inference: Approximating the posterior over weights

3. **Calibration Methods**:
   - Evaluating Calibration: Reliability diagrams and Brier score
   - Platt Scaling: Logistic regression on model outputs
   - Isotonic Regression: Non-parametric calibration

These methods enable more reliable uncertainty estimates in complex machine learning models, which is crucial for decision-making in critical applications.

In Part 3, we will explore applications of uncertainty quantification in various domains and discuss recent advances in the field.

## References

1. Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in Neural Information Processing Systems, 30.
2. Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (pp. 1050-1059).
3. Meinshausen, N. (2006). Quantile regression forests. Journal of Machine Learning Research, 7, 983-999.
4. Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine Learning (pp. 625-632).
