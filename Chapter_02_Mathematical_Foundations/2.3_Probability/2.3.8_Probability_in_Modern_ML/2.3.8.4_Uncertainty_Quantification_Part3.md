# 2.3.8.4 Uncertainty Quantification in Machine Learning - Part 3

## Applications of Uncertainty Quantification

Uncertainty quantification (UQ) has numerous practical applications across various domains. This section explores how UQ is applied in different fields and the impact it has on decision-making processes.

## Active Learning

Active learning uses uncertainty to select the most informative samples for labeling, reducing annotation costs.

### Uncertainty Sampling

**Uncertainty sampling** selects points with the highest predictive uncertainty:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, 
                          n_redundant=0, n_clusters_per_class=1, random_state=42)

# Split into labeled and unlabeled pools
n_initial = 10
indices = np.arange(len(X))
np.random.shuffle(indices)
labeled_indices = indices[:n_initial]
unlabeled_indices = indices[n_initial:]

X_labeled = X[labeled_indices]
y_labeled = y[labeled_indices]
X_unlabeled = X[unlabeled_indices]
y_unlabeled = y[unlabeled_indices]

# Function to get uncertainty estimates from Random Forest
def get_uncertainty(model, X):
    # Probability of the most confident class
    probs = model.predict_proba(X)
    max_probs = np.max(probs, axis=1)
    return 1 - max_probs  # Higher value = more uncertain

# Active learning loop
n_queries = 50
accuracies = []
random_accuracies = []

# Initial model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_labeled, y_labeled)

# Test set (separate from both labeled and unlabeled pools)
X_test, y_test = make_classification(n_samples=500, n_features=2, n_informative=2, 
                                    n_redundant=0, n_clusters_per_class=1, random_state=43)

# Initial accuracy
accuracies.append(accuracy_score(y_test, model.predict(X_test)))

# Random selection model
random_model = RandomForestClassifier(n_estimators=100, random_state=42)
random_model.fit(X_labeled, y_labeled)
random_accuracies.append(accuracy_score(y_test, random_model.predict(X_test)))

# Active learning iterations
for i in range(n_queries):
    # Get uncertainty estimates
    uncertainties = get_uncertainty(model, X_unlabeled)
    
    # Select the most uncertain point
    idx = np.argmax(uncertainties)
    query_idx = unlabeled_indices[idx]
    
    # Add to labeled pool
    X_labeled = np.vstack([X_labeled, X_unlabeled[idx:idx+1]])
    y_labeled = np.append(y_labeled, y_unlabeled[idx])
    
    # Remove from unlabeled pool
    unlabeled_indices = np.delete(unlabeled_indices, idx)
    X_unlabeled = np.delete(X_unlabeled, idx, axis=0)
    y_unlabeled = np.delete(y_unlabeled, idx)
    
    # Retrain model
    model.fit(X_labeled, y_labeled)
    
    # Evaluate
    accuracies.append(accuracy_score(y_test, model.predict(X_test)))
    
    # Random selection (for comparison)
    random_idx = np.random.randint(len(unlabeled_indices))
    random_query_idx = unlabeled_indices[random_idx]
    
    # Add to random labeled pool
    random_X_labeled = np.vstack([X_labeled[:-1], X_unlabeled[random_idx:random_idx+1]])
    random_y_labeled = np.append(y_labeled[:-1], y_unlabeled[random_idx])
    
    # Retrain random model
    random_model.fit(random_X_labeled, random_y_labeled)
    
    # Evaluate random model
    random_accuracies.append(accuracy_score(y_test, random_model.predict(X_test)))

# Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(range(n_initial, n_initial + n_queries + 1), accuracies, 'b-', label='Uncertainty sampling')
plt.plot(range(n_initial, n_initial + n_queries + 1), random_accuracies, 'r--', label='Random sampling')
plt.xlabel('Number of labeled samples')
plt.ylabel('Accuracy')
plt.title('Active Learning: Uncertainty Sampling vs Random Sampling')
plt.legend()
plt.grid(True)
plt.show()

# Visualize the final labeled points
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.5, label='Unlabeled')
plt.scatter(X_labeled[:, 0], X_labeled[:, 1], c=y_labeled, cmap='viridis', edgecolors='k', label='Labeled')
plt.title(f'Active Learning: {len(X_labeled)} Labeled Points')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Class')
plt.legend()
plt.grid(True)
plt.show()
```

## Anomaly Detection

Uncertainty quantification helps identify anomalous or out-of-distribution samples.

### Out-of-Distribution Detection

**Out-of-distribution (OOD) detection** identifies samples that differ from the training distribution:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_moons, make_blobs
from sklearn.model_selection import train_test_split

# Generate in-distribution data (two moons)
np.random.seed(42)
X_in, y_in = make_moons(n_samples=1000, noise=0.1, random_state=42)

# Generate out-of-distribution data (blobs)
X_out, _ = make_blobs(n_samples=500, centers=[[3, 3], [-3, -3]], cluster_std=0.5, random_state=42)

# Split in-distribution data
X_train, X_test, y_train, y_test = train_test_split(X_in, y_in, test_size=0.2, random_state=42)

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Function to compute predictive entropy
def predictive_entropy(model, X):
    probs = model.predict_proba(X)
    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)
    return entropy

# Compute entropy for in-distribution and out-of-distribution data
entropy_in = predictive_entropy(rf, X_test)
entropy_out = predictive_entropy(rf, X_out)

# Plot the data and entropy
plt.figure(figsize=(15, 5))

# Plot the data
plt.subplot(1, 3, 1)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.5, label='In-distribution (train)')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', marker='x', alpha=0.5, label='In-distribution (test)')
plt.scatter(X_out[:, 0], X_out[:, 1], c='red', marker='o', alpha=0.5, label='Out-of-distribution')
plt.title('Data Distribution')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)

# Plot entropy
plt.subplot(1, 3, 2)
plt.scatter(X_test[:, 0], X_test[:, 1], c=entropy_in, cmap='plasma', marker='x', alpha=0.5, label='In-distribution')
plt.scatter(X_out[:, 0], X_out[:, 1], c=entropy_out, cmap='plasma', marker='o', alpha=0.5, label='Out-of-distribution')
plt.colorbar(label='Predictive Entropy')
plt.title('Predictive Entropy')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)

# Plot entropy histograms
plt.subplot(1, 3, 3)
plt.hist(entropy_in, bins=30, alpha=0.5, label='In-distribution')
plt.hist(entropy_out, bins=30, alpha=0.5, label='Out-of-distribution')
plt.xlabel('Predictive Entropy')
plt.ylabel('Frequency')
plt.title('Entropy Distribution')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Evaluate OOD detection performance
from sklearn.metrics import roc_curve, auc

# Use entropy as the OOD score
y_true = np.concatenate([np.zeros_like(entropy_in), np.ones_like(entropy_out)])
y_score = np.concatenate([entropy_in, entropy_out])

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_true, y_score)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for OOD Detection')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
```

## Decision-Making Under Uncertainty

Uncertainty quantification enables better decision-making by accounting for prediction confidence.

### Cost-Sensitive Decision-Making

**Cost-sensitive decision-making** considers both predictions and their uncertainty:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, 
                          n_redundant=0, weights=[0.9, 0.1], random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get predicted probabilities
y_prob = rf.predict_proba(X_test)[:, 1]

# Define cost matrix (cost of each prediction outcome)
# [TN, FP]
# [FN, TP]
cost_matrix = np.array([
    [0, 10],    # Cost of TN=0, FP=10
    [100, 0]    # Cost of FN=100, TP=0
])

# Function to compute expected cost
def expected_cost(prob, cost_matrix, threshold):
    # Make predictions based on threshold
    pred = (prob >= threshold).astype(int)
    
    # Compute confusion matrix
    cm = confusion_matrix(y_test, pred)
    
    # Compute total cost
    total_cost = np.sum(cm * cost_matrix)
    
    return total_cost, cm

# Compute costs for different thresholds
thresholds = np.linspace(0, 1, 100)
costs = []
for threshold in thresholds:
    cost, _ = expected_cost(y_prob, cost_matrix, threshold)
    costs.append(cost)

# Find optimal threshold
optimal_idx = np.argmin(costs)
optimal_threshold = thresholds[optimal_idx]
optimal_cost, optimal_cm = expected_cost(y_prob, cost_matrix, optimal_threshold)

# Default threshold (0.5)
default_cost, default_cm = expected_cost(y_prob, cost_matrix, 0.5)

# Plot cost vs threshold
plt.figure(figsize=(10, 6))
plt.plot(thresholds, costs, 'b-')
plt.axvline(optimal_threshold, color='r', linestyle='--', 
           label=f'Optimal threshold = {optimal_threshold:.2f}')
plt.axvline(0.5, color='g', linestyle='--', 
           label='Default threshold = 0.5')
plt.xlabel('Threshold')
plt.ylabel('Expected Cost')
plt.title('Cost-Sensitive Decision-Making')
plt.legend()
plt.grid(True)
plt.show()

# Print results
print(f"Default threshold (0.5):")
print(f"Confusion Matrix:\n{default_cm}")
print(f"Total Cost: {default_cost}")
print(f"Accuracy: {accuracy_score(y_test, (y_prob >= 0.5).astype(int)):.4f}")
print("\n")
print(f"Optimal threshold ({optimal_threshold:.2f}):")
print(f"Confusion Matrix:\n{optimal_cm}")
print(f"Total Cost: {optimal_cost}")
print(f"Accuracy: {accuracy_score(y_test, (y_prob >= optimal_threshold).astype(int)):.4f}")

# Visualize decision boundaries
plt.figure(figsize=(15, 5))

# Create a meshgrid for visualization
h = 0.02  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z_prob = rf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z_prob = Z_prob.reshape(xx.shape)

# Default threshold
plt.subplot(1, 3, 1)
Z_default = (Z_prob >= 0.5).astype(int)
plt.contourf(xx, yy, Z_default, alpha=0.3, cmap='viridis')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolors='k')
plt.title(f'Default Threshold (0.5)\nCost: {default_cost}')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True)

# Optimal threshold
plt.subplot(1, 3, 2)
Z_optimal = (Z_prob >= optimal_threshold).astype(int)
plt.contourf(xx, yy, Z_optimal, alpha=0.3, cmap='viridis')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolors='k')
plt.title(f'Optimal Threshold ({optimal_threshold:.2f})\nCost: {optimal_cost}')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True)

# Probability map
plt.subplot(1, 3, 3)
plt.contourf(xx, yy, Z_prob, alpha=0.8, cmap='viridis')
plt.colorbar(label='Probability of Class 1')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolors='k')
plt.title('Probability Map')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Summary

Uncertainty quantification has numerous practical applications:

1. **Active Learning**:
   - Uncertainty sampling selects the most informative points for labeling
   - Reduces annotation costs and improves model performance

2. **Anomaly Detection**:
   - Out-of-distribution detection identifies samples that differ from the training distribution
   - Helps prevent model failures on unusual inputs

3. **Decision-Making Under Uncertainty**:
   - Cost-sensitive decision-making considers both predictions and their uncertainty
   - Optimizes decision thresholds based on the costs of different outcomes

These applications demonstrate how uncertainty quantification can improve the reliability and effectiveness of machine learning systems in real-world scenarios.

## References

1. Settles, B. (2009). Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences.
2. Hendrycks, D., & Gimpel, K. (2017). A baseline for detecting misclassified and out-of-distribution examples in neural networks. International Conference on Learning Representations.
3. Elkan, C. (2001). The foundations of cost-sensitive learning. In International Joint Conference on Artificial Intelligence (Vol. 17, No. 1, pp. 973-978).
