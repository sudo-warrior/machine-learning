# 2.3.8.2 Probabilistic Graphical Models in Modern ML

## Introduction to Probabilistic Graphical Models

Probabilistic Graphical Models (PGMs) provide a powerful framework for representing complex probability distributions through graphs. They combine probability theory with graph theory to efficiently encode dependencies among random variables. In modern machine learning, PGMs serve as both modeling tools and theoretical foundations for many algorithms. This section explores key types of PGMs and their applications in contemporary machine learning.

## Bayesian Networks

### Structure and Representation

**Bayesian Networks** (BNs), also known as directed graphical models, represent joint probability distributions using directed acyclic graphs (DAGs):

- **Nodes**: Random variables
- **Edges**: Direct dependencies between variables
- **Conditional Probability Tables/Distributions**: Quantify relationships

The joint distribution factorizes according to the graph structure:

$$p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^n p(x_i | \text{parents}(x_i))$$

### Example: Simple Bayesian Network

```python
import numpy as np
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination

# Define the structure of the Bayesian network
model = BayesianNetwork([('Rain', 'Sprinkler'), ('Rain', 'Wet_Grass'), 
                         ('Sprinkler', 'Wet_Grass')])

# Define the CPDs
cpd_rain = TabularCPD(variable='Rain', variable_card=2, 
                      values=[[0.8], [0.2]])  # P(Rain=False)=0.8, P(Rain=True)=0.2

cpd_sprinkler = TabularCPD(variable='Sprinkler', variable_card=2,
                           values=[[0.6, 0.2], [0.4, 0.8]],
                           evidence=['Rain'], evidence_card=[2])
# P(Sprinkler=False|Rain=False)=0.6, P(Sprinkler=False|Rain=True)=0.2

cpd_wet_grass = TabularCPD(variable='Wet_Grass', variable_card=2,
                           values=[[1.0, 0.1, 0.1, 0.01], [0.0, 0.9, 0.9, 0.99]],
                           evidence=['Sprinkler', 'Rain'], evidence_card=[2, 2])
# P(Wet_Grass=False|Sprinkler=False,Rain=False)=1.0, etc.

# Add CPDs to the model
model.add_cpds(cpd_rain, cpd_sprinkler, cpd_wet_grass)

# Check if the model is valid
print(f"Model is valid: {model.check_model()}")

# Perform inference
inference = VariableElimination(model)

# Query: P(Rain|Wet_Grass=True)
result = inference.query(variables=['Rain'], evidence={'Wet_Grass': 1})
print("\nP(Rain|Wet_Grass=True):")
print(result)

# Query: P(Sprinkler|Wet_Grass=True, Rain=False)
result = inference.query(variables=['Sprinkler'], evidence={'Wet_Grass': 1, 'Rain': 0})
print("\nP(Sprinkler|Wet_Grass=True, Rain=False):")
print(result)
```

### Learning Bayesian Networks

Learning Bayesian networks involves two tasks:

1. **Structure Learning**: Determining the graph structure
   - Constraint-based methods (e.g., PC algorithm)
   - Score-based methods (e.g., hill climbing with BIC score)
   - Hybrid methods

2. **Parameter Learning**: Estimating the conditional probabilities
   - Maximum Likelihood Estimation (MLE)
   - Bayesian estimation with priors

```python
from pgmpy.estimators import HillClimbSearch, BicScore
from pgmpy.sampling import BayesianModelSampling

# Generate synthetic data from our model
sampler = BayesianModelSampling(model)
data = sampler.forward_sample(size=1000)

# Learn structure from data
hc = HillClimbSearch(data)
bic = BicScore(data)
best_model = hc.estimate(scoring_method=bic)

# Print the learned edges
print("\nLearned model edges:")
print(best_model.edges())

# Compare with the original model
print("\nOriginal model edges:")
print(model.edges())
```

## Markov Random Fields

### Structure and Representation

**Markov Random Fields** (MRFs), also known as undirected graphical models, represent distributions using undirected graphs:

- **Nodes**: Random variables
- **Edges**: Mutual dependencies between variables
- **Potential Functions**: Define compatibility between variable assignments

The joint distribution is defined through potential functions over maximal cliques:

$$p(x_1, x_2, \ldots, x_n) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \phi_c(x_c)$$

where $\mathcal{C}$ is the set of maximal cliques and $Z$ is the normalization constant.

### Example: Image Denoising with MRF

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import convolve2d

# Generate a simple binary image
np.random.seed(42)
true_image = np.zeros((20, 20))
true_image[5:15, 5:15] = 1  # Square in the middle

# Add noise
noise_level = 0.2
noisy_image = true_image.copy()
flip_mask = np.random.random(true_image.shape) < noise_level
noisy_image[flip_mask] = 1 - noisy_image[flip_mask]

# Define MRF energy function (negative log probability)
def energy(image, noisy_image, beta=1.0, lambda_=1.0):
    # Data term: penalty for deviating from noisy image
    data_term = lambda_ * np.sum((image - noisy_image)**2)
    
    # Smoothness term: penalty for neighboring pixels being different
    kernel = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])
    neighbors = convolve2d(image, kernel, mode='same', boundary='wrap')
    smoothness_term = -beta * np.sum(image * neighbors)
    
    return data_term + smoothness_term

# Gibbs sampling for MRF inference
def gibbs_sampling(noisy_image, n_iterations=1000, beta=1.0, lambda_=1.0):
    image = noisy_image.copy()
    h, w = image.shape
    
    for _ in range(n_iterations):
        # Random pixel
        i, j = np.random.randint(0, h), np.random.randint(0, w)
        
        # Compute energy for both possible values
        image[i, j] = 0
        energy_0 = energy(image, noisy_image, beta, lambda_)
        
        image[i, j] = 1
        energy_1 = energy(image, noisy_image, beta, lambda_)
        
        # Sample according to relative probabilities
        p1 = np.exp(-energy_1) / (np.exp(-energy_0) + np.exp(-energy_1))
        image[i, j] = np.random.random() < p1
    
    return image

# Denoise the image
denoised_image = gibbs_sampling(noisy_image, n_iterations=10000, beta=2.0, lambda_=0.5)

# Plot the results
plt.figure(figsize=(15, 5))
plt.subplot(131)
plt.imshow(true_image, cmap='gray')
plt.title('Original Image')
plt.axis('off')

plt.subplot(132)
plt.imshow(noisy_image, cmap='gray')
plt.title(f'Noisy Image (Noise Level: {noise_level})')
plt.axis('off')

plt.subplot(133)
plt.imshow(denoised_image, cmap='gray')
plt.title('Denoised Image (MRF)')
plt.axis('off')

plt.tight_layout()
plt.show()
```

### Conditional Random Fields

**Conditional Random Fields** (CRFs) are discriminative models that directly model the conditional distribution p(y|x):

$$p(y|x) = \frac{1}{Z(x)} \prod_{c \in \mathcal{C}} \phi_c(y_c, x)$$

CRFs are widely used in sequence labeling tasks like named entity recognition and part-of-speech tagging.

## Factor Graphs

### Structure and Representation

**Factor Graphs** provide a unified representation for both directed and undirected models:

- **Variable Nodes**: Random variables
- **Factor Nodes**: Functions over subsets of variables
- **Edges**: Connect variables to the factors they participate in

The joint distribution is defined as:

$$p(x_1, x_2, \ldots, x_n) = \frac{1}{Z} \prod_{j=1}^m f_j(X_j)$$

where $f_j$ are factors and $X_j$ are the variables connected to factor $j$.

### Message Passing Algorithms

Factor graphs enable efficient inference through message passing algorithms:

- **Sum-Product Algorithm**: Computes marginal distributions
- **Max-Product Algorithm**: Finds the most likely configuration

## Deep Generative Models as PGMs

### Variational Autoencoders

**Variational Autoencoders** (VAEs) can be viewed as directed graphical models with neural network parameterizations:

- **Latent Variables**: z ~ p(z)
- **Observed Variables**: x ~ p(x|z)
- **Inference Network**: q(z|x) approximates the posterior p(z|x)

```python
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

# Load and preprocess MNIST data
(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# Network parameters
original_dim = 784  # MNIST image size
intermediate_dim = 256  # Hidden layer size
latent_dim = 2  # Latent space dimension

# Define encoder
inputs = Input(shape=(original_dim,))
h = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)

# Sampling function
def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

# Sample from latent space
z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

# Define decoder
decoder_h = Dense(intermediate_dim, activation='relu')
decoder_mean = Dense(original_dim, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)

# Define VAE model
vae = Model(inputs, x_decoded_mean)

# Define loss function (ELBO)
reconstruction_loss = original_dim * tf.keras.losses.binary_crossentropy(inputs, x_decoded_mean)
kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)

# Compile and train
vae.compile(optimizer='adam')
vae.fit(x_train, epochs=10, batch_size=128, validation_data=(x_test, None), verbose=1)

# Define encoder and decoder models for later use
encoder = Model(inputs, [z_mean, z_log_var, z])
decoder_input = Input(shape=(latent_dim,))
_h_decoded = decoder_h(decoder_input)
_x_decoded_mean = decoder_mean(_h_decoded)
decoder = Model(decoder_input, _x_decoded_mean)

# Display a 2D plot of the digit classes in the latent space
z_mean, _, _ = encoder.predict(x_test, batch_size=128)
plt.figure(figsize=(12, 10))
plt.scatter(z_mean[:, 0], z_mean[:, 1], c=np.argmax(y_test, axis=1))
plt.colorbar()
plt.xlabel('z[0]')
plt.ylabel('z[1]')
plt.title('Latent Space')
plt.show()

# Display a grid of generated digits
n = 15  # Figure with 15x15 digits
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))
# Linearly spaced coordinates on the unit square were transformed
# through the inverse CDF (ppf) of the Gaussian to produce values
# of the latent variables z, since the prior of the latent space
# is Gaussian
grid_x = norm.ppf(np.linspace(0.05, 0.95, n))
grid_y = norm.ppf(np.linspace(0.05, 0.95, n))

for i, yi in enumerate(grid_y):
    for j, xi in enumerate(grid_x):
        z_sample = np.array([[xi, yi]])
        x_decoded = decoder.predict(z_sample)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure, cmap='Greys_r')
plt.title('Generated Digits')
plt.axis('off')
plt.show()
```

### Normalizing Flows

**Normalizing Flows** transform a simple base distribution into a complex target distribution through a sequence of invertible transformations:

$$p(x) = p(z) \left| \det \frac{\partial f^{-1}}{\partial x} \right|$$

where $z = f^{-1}(x)$ and $f$ is an invertible function.

### Energy-Based Models

**Energy-Based Models** (EBMs) define a probability distribution through an energy function:

$$p(x) = \frac{e^{-E(x)}}{Z}$$

Modern EBMs often use neural networks to parameterize the energy function.

## Applications in Modern Machine Learning

### Computer Vision

PGMs are used in various computer vision tasks:

- **Image Segmentation**: CRFs for refining segmentation boundaries
- **Object Detection**: Structured prediction with graphical models
- **Scene Understanding**: Modeling relationships between objects

```python
# Example: CRF for image segmentation refinement (pseudocode)
def refine_segmentation_with_crf(image, initial_segmentation):
    # Convert initial segmentation to unary potentials
    unary_potentials = convert_to_unary(initial_segmentation)
    
    # Define pairwise potentials based on image features
    pairwise_potentials = create_pairwise_gaussian(image) + create_pairwise_bilateral(image)
    
    # Create CRF model
    crf = DenseCRF(image.shape[0] * image.shape[1], num_classes)
    crf.setUnaryEnergy(unary_potentials)
    crf.addPairwiseEnergy(pairwise_potentials)
    
    # Perform inference
    refined_segmentation = crf.inference(num_iterations)
    
    return refined_segmentation
```

### Natural Language Processing

PGMs provide structured models for language:

- **Part-of-Speech Tagging**: Linear-chain CRFs
- **Named Entity Recognition**: Semi-Markov CRFs
- **Semantic Parsing**: Probabilistic context-free grammars

### Healthcare and Bioinformatics

PGMs model complex biological systems:

- **Disease Diagnosis**: Bayesian networks for medical diagnosis
- **Genetic Analysis**: Modeling gene interactions
- **Drug Discovery**: Predicting molecular properties

### Reinforcement Learning

PGMs enhance reinforcement learning algorithms:

- **Partially Observable MDPs**: Modeling uncertainty in state
- **Hierarchical RL**: Structured policies with graphical models
- **Model-Based RL**: Learning environment dynamics

## Challenges and Future Directions

### Scalability

Traditional PGM inference algorithms struggle with high-dimensional data:

- **Approximate Inference**: Variational methods, belief propagation
- **Amortized Inference**: Learning to perform inference
- **Neural Approximations**: Using neural networks to approximate inference

### Integration with Deep Learning

Combining PGMs with deep learning is an active research area:

- **Structured Prediction**: Adding structure to neural network outputs
- **Interpretability**: Using PGMs to explain neural network decisions
- **Hybrid Models**: Combining the strengths of both approaches

### Causal Inference

Moving from correlation to causation:

- **Causal Discovery**: Learning causal structure from data
- **Counterfactual Reasoning**: "What if" scenarios
- **Interventions**: Predicting effects of actions

## Summary

Probabilistic Graphical Models provide a powerful framework for structured probabilistic modeling in machine learning:

1. **Types of PGMs**:
   - Bayesian Networks: Directed graphs representing factorized joint distributions
   - Markov Random Fields: Undirected graphs with potential functions
   - Factor Graphs: Unified representation with explicit factor nodes

2. **Deep Generative Models as PGMs**:
   - Variational Autoencoders: Directed models with neural parameterizations
   - Normalizing Flows: Invertible transformations of simple distributions
   - Energy-Based Models: Defining distributions through energy functions

3. **Applications**:
   - Computer Vision: Image segmentation, object detection
   - Natural Language Processing: Sequence labeling, parsing
   - Healthcare: Disease diagnosis, genetic analysis
   - Reinforcement Learning: Structured policies, environment modeling

4. **Challenges and Future Directions**:
   - Scalability: Handling high-dimensional data
   - Integration with Deep Learning: Combining structure and flexibility
   - Causal Inference: Moving beyond correlation

PGMs continue to evolve, providing structured probabilistic foundations for modern machine learning while incorporating advances in deep learning and causal reasoning.

## References

1. Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.
2. Wainwright, M. J., & Jordan, M. I. (2008). Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends in Machine Learning, 1(1-2), 1-305.
3. Kingma, D. P., & Welling, M. (2013). Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114.
4. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., ... & Torr, P. H. (2015). Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1529-1537).
