# 2.3.8.5 Causal Inference in Machine Learning - Part 3

## Causal Machine Learning

Building on the foundations and methods covered in Parts 1 and 2, this section explores the integration of causal inference with machine learning. Causal machine learning combines the predictive power of modern ML algorithms with causal reasoning to enable more robust and interpretable models.

## Heterogeneous Treatment Effects

While average treatment effects provide a single summary statistic, treatment effects often vary across individuals.

### Conditional Average Treatment Effects

**Conditional Average Treatment Effects (CATE)** estimate how treatment effects vary with covariates:

$$\tau(x) = E[Y(1) - Y(0) | X = x]$$

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Generate synthetic data with heterogeneous treatment effects
np.random.seed(42)
n = 1000

# Covariates
age = np.random.normal(40, 10, n)
severity = np.random.normal(5, 2, n)

# Treatment (randomized)
treatment = np.random.binomial(1, 0.5, n)

# Treatment effect (varies by age and severity)
effect = 10 - 0.1 * age - severity

# Potential outcomes
y0 = 70 + 0.1 * age - 2 * severity + np.random.normal(0, 5, n)  # Outcome if not treated
y1 = y0 + effect + np.random.normal(0, 5, n)  # Outcome if treated

# Observed outcome
y_obs = treatment * y1 + (1 - treatment) * y0

# True CATE
true_cate = effect

# Create a DataFrame
data = pd.DataFrame({
    'Age': age,
    'Severity': severity,
    'Treatment': treatment,
    'Observed Outcome': y_obs,
    'True CATE': true_cate
})

# Estimate CATE using the S-learner approach
# Train a single model with treatment as a feature
X = np.column_stack([age, severity, treatment])
s_model = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)
s_model.fit(X, y_obs)

# Predict potential outcomes
X1 = np.column_stack([age, severity, np.ones(n)])
X0 = np.column_stack([age, severity, np.zeros(n)])
y1_pred_s = s_model.predict(X1)
y0_pred_s = s_model.predict(X0)
cate_s = y1_pred_s - y0_pred_s

# Estimate CATE using the T-learner approach
# Train separate models for treated and control groups
t_model1 = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)
t_model0 = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)

X_t = np.column_stack([age[treatment == 1], severity[treatment == 1]])
X_c = np.column_stack([age[treatment == 0], severity[treatment == 0]])

t_model1.fit(X_t, y_obs[treatment == 1])
t_model0.fit(X_c, y_obs[treatment == 0])

# Predict potential outcomes
X_all = np.column_stack([age, severity])
y1_pred_t = t_model1.predict(X_all)
y0_pred_t = t_model0.predict(X_all)
cate_t = y1_pred_t - y0_pred_t

# Plot true vs. estimated CATE
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(true_cate, cate_s, alpha=0.3)
plt.plot([-10, 10], [-10, 10], 'r--')
plt.xlabel('True CATE')
plt.ylabel('Estimated CATE (S-learner)')
plt.title('S-learner CATE Estimates')
plt.grid(True)

plt.subplot(1, 3, 2)
plt.scatter(true_cate, cate_t, alpha=0.3)
plt.plot([-10, 10], [-10, 10], 'r--')
plt.xlabel('True CATE')
plt.ylabel('Estimated CATE (T-learner)')
plt.title('T-learner CATE Estimates')
plt.grid(True)

plt.subplot(1, 3, 3)
plt.scatter(age, severity, c=true_cate, cmap='viridis', alpha=0.5)
plt.colorbar(label='True CATE')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('True CATE by Covariates')
plt.grid(True)

plt.tight_layout()
plt.show()

# Plot CATE as a function of covariates
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(age, true_cate, alpha=0.3, label='True')
plt.scatter(age, cate_t, alpha=0.3, label='Estimated')
plt.xlabel('Age')
plt.ylabel('CATE')
plt.title('CATE vs. Age')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 2)
plt.scatter(severity, true_cate, alpha=0.3, label='True')
plt.scatter(severity, cate_t, alpha=0.3, label='Estimated')
plt.xlabel('Severity')
plt.ylabel('CATE')
plt.title('CATE vs. Severity')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 3)
# Create a grid for visualization
age_grid = np.linspace(20, 60, 50)
severity_grid = np.linspace(1, 9, 50)
age_mesh, severity_mesh = np.meshgrid(age_grid, severity_grid)
X_grid = np.column_stack([age_mesh.ravel(), severity_mesh.ravel()])

# Predict CATE on the grid
y1_grid = t_model1.predict(X_grid)
y0_grid = t_model0.predict(X_grid)
cate_grid = y1_grid - y0_grid
cate_grid = cate_grid.reshape(age_mesh.shape)

# Plot CATE heatmap
plt.contourf(age_mesh, severity_mesh, cate_grid, cmap='viridis', alpha=0.8)
plt.colorbar(label='Estimated CATE')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('CATE Heatmap')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Meta-Learners for CATE Estimation

Several meta-learning approaches have been developed for CATE estimation:

- **S-learner**: Single model with treatment as a feature
- **T-learner**: Separate models for treated and control groups
- **X-learner**: Combines treatment effect estimates from both groups
- **R-learner**: Directly optimizes for treatment effect estimation

## Causal Forests

**Causal Forests** adapt random forests for causal inference:

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Generate synthetic data with heterogeneous treatment effects
np.random.seed(42)
n = 1000

# Covariates
age = np.random.normal(40, 10, n)
severity = np.random.normal(5, 2, n)

# Treatment (randomized)
treatment = np.random.binomial(1, 0.5, n)

# Treatment effect (varies by age and severity)
effect = 10 - 0.1 * age - severity

# Potential outcomes
y0 = 70 + 0.1 * age - 2 * severity + np.random.normal(0, 5, n)  # Outcome if not treated
y1 = y0 + effect + np.random.normal(0, 5, n)  # Outcome if treated

# Observed outcome
y_obs = treatment * y1 + (1 - treatment) * y0

# True CATE
true_cate = effect

# Create a DataFrame
data = pd.DataFrame({
    'Age': age,
    'Severity': severity,
    'Treatment': treatment,
    'Observed Outcome': y_obs,
    'True CATE': true_cate
})

# Split data
X = np.column_stack([age, severity])
train_idx, test_idx = train_test_split(np.arange(n), test_size=0.3, random_state=42)

X_train = X[train_idx]
treatment_train = treatment[train_idx]
y_train = y_obs[train_idx]
true_cate_train = true_cate[train_idx]

X_test = X[test_idx]
treatment_test = treatment[test_idx]
y_test = y_obs[test_idx]
true_cate_test = true_cate[test_idx]

# Implement a simplified causal forest
def causal_forest(X, treatment, y, n_trees=100, min_samples_leaf=5, max_features=None):
    n_samples, n_features = X.shape
    
    # Initialize forest
    forest = []
    
    for i in range(n_trees):
        # Bootstrap sample
        bootstrap_idx = np.random.choice(n_samples, n_samples, replace=True)
        X_boot = X[bootstrap_idx]
        treatment_boot = treatment[bootstrap_idx]
        y_boot = y[bootstrap_idx]
        
        # Train a tree on the bootstrap sample
        tree = RandomForestRegressor(n_estimators=1, min_samples_leaf=min_samples_leaf, 
                                    max_features=max_features, random_state=i)
        tree.fit(X_boot, y_boot)
        
        # Store the tree and the bootstrap indices
        forest.append((tree, bootstrap_idx))
    
    return forest

def predict_causal_forest(forest, X):
    n_samples = X.shape[0]
    n_trees = len(forest)
    
    # Initialize predictions
    cate_pred = np.zeros(n_samples)
    
    for tree, _ in forest:
        # Predict treatment effect
        X1 = np.column_stack([X, np.ones(n_samples)])
        X0 = np.column_stack([X, np.zeros(n_samples)])
        
        y1_pred = tree.predict(X1)
        y0_pred = tree.predict(X0)
        
        cate_pred += (y1_pred - y0_pred)
    
    return cate_pred / n_trees

# Train a causal forest
X_train_with_treatment = np.column_stack([X_train, treatment_train])
forest = causal_forest(X_train_with_treatment, treatment_train, y_train, n_trees=100)

# Predict CATE
cate_pred = predict_causal_forest(forest, X_test)

# Evaluate CATE estimation
mse = np.mean((cate_pred - true_cate_test) ** 2)
print(f"Mean Squared Error for CATE estimation: {mse:.4f}")

# Plot true vs. predicted CATE
plt.figure(figsize=(10, 6))
plt.scatter(true_cate_test, cate_pred, alpha=0.3)
plt.plot([-10, 10], [-10, 10], 'r--')
plt.xlabel('True CATE')
plt.ylabel('Predicted CATE')
plt.title('Causal Forest: True vs. Predicted CATE')
plt.grid(True)
plt.show()

# Plot CATE by covariates
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(X_test[:, 0], true_cate_test, alpha=0.3, label='True')
plt.scatter(X_test[:, 0], cate_pred, alpha=0.3, label='Predicted')
plt.xlabel('Age')
plt.ylabel('CATE')
plt.title('CATE vs. Age')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 2)
plt.scatter(X_test[:, 1], true_cate_test, alpha=0.3, label='True')
plt.scatter(X_test[:, 1], cate_pred, alpha=0.3, label='Predicted')
plt.xlabel('Severity')
plt.ylabel('CATE')
plt.title('CATE vs. Severity')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 3)
plt.scatter(X_test[:, 0], X_test[:, 1], c=cate_pred, cmap='viridis', alpha=0.5)
plt.colorbar(label='Predicted CATE')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('Predicted CATE by Covariates')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Causal Representation Learning

**Causal Representation Learning** aims to discover causal variables from high-dimensional data:

- **Disentangled Representations**: Separate causal factors of variation
- **Invariant Representations**: Features that remain stable across environments
- **Counterfactual Representations**: Enable counterfactual reasoning

## Applications of Causal Machine Learning

### Personalized Treatment Assignment

Causal machine learning enables personalized treatment decisions:

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Use the CATE estimates from the previous example
# Optimal treatment assignment: treat if CATE > 0
optimal_treatment = (true_cate_test > 0).astype(int)
predicted_treatment = (cate_pred > 0).astype(int)

# Evaluate treatment policies
def evaluate_policy(treatment_policy, true_cate):
    # Expected outcome under the policy
    return np.mean(treatment_policy * true_cate)

# Compare policies
random_policy = np.random.binomial(1, 0.5, len(true_cate_test))
always_treat = np.ones_like(true_cate_test)
never_treat = np.zeros_like(true_cate_test)

random_value = evaluate_policy(random_policy, true_cate_test)
always_treat_value = evaluate_policy(always_treat, true_cate_test)
never_treat_value = evaluate_policy(never_treat, true_cate_test)
optimal_value = evaluate_policy(optimal_treatment, true_cate_test)
predicted_value = evaluate_policy(predicted_treatment, true_cate_test)

# Plot policy comparison
policies = ['Random', 'Always Treat', 'Never Treat', 'Optimal', 'Predicted']
values = [random_value, always_treat_value, never_treat_value, optimal_value, predicted_value]

plt.figure(figsize=(10, 6))
plt.bar(policies, values)
plt.ylabel('Expected Outcome')
plt.title('Comparison of Treatment Policies')
plt.grid(True, axis='y')
plt.show()

# Plot treatment decisions
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(X_test[:, 0], X_test[:, 1], c=optimal_treatment, cmap='viridis', alpha=0.5)
plt.colorbar(label='Optimal Treatment')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('Optimal Treatment Decisions')
plt.grid(True)

plt.subplot(1, 3, 2)
plt.scatter(X_test[:, 0], X_test[:, 1], c=predicted_treatment, cmap='viridis', alpha=0.5)
plt.colorbar(label='Predicted Treatment')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('Predicted Treatment Decisions')
plt.grid(True)

plt.subplot(1, 3, 3)
agreement = (optimal_treatment == predicted_treatment).astype(int)
plt.scatter(X_test[:, 0], X_test[:, 1], c=agreement, cmap='RdYlGn', alpha=0.5)
plt.colorbar(label='Agreement')
plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('Agreement Between Optimal and Predicted')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Counterfactual Explanations

Causal machine learning enables counterfactual explanations for model predictions:

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Generate synthetic data for a classification problem
np.random.seed(42)
n = 1000

# Covariates
age = np.random.normal(40, 10, n)
severity = np.random.normal(5, 2, n)

# Outcome (binary classification)
logit = -3 + 0.05 * age + 0.5 * severity
prob = 1 / (1 + np.exp(-logit))
outcome = np.random.binomial(1, prob)

# Create a DataFrame
data = pd.DataFrame({
    'Age': age,
    'Severity': severity,
    'Outcome': outcome
})

# Train a classifier
X = np.column_stack([age, severity])
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X, outcome)

# Generate counterfactual explanations
def generate_counterfactual(model, x, desired_outcome, feature_ranges, step_size=0.1, max_iter=100):
    """
    Generate a counterfactual explanation for a given instance.
    
    Parameters:
    -----------
    model : sklearn model
        The trained model
    x : array-like
        The instance to explain
    desired_outcome : int
        The desired outcome (0 or 1)
    feature_ranges : list of tuples
        The allowed ranges for each feature (min, max)
    step_size : float
        The step size for the gradient
    max_iter : int
        Maximum number of iterations
        
    Returns:
    --------
    counterfactual : array-like
        The counterfactual instance
    """
    x_cf = x.copy()
    
    for i in range(max_iter):
        # Get current prediction
        pred = model.predict([x_cf])[0]
        
        # Check if desired outcome is achieved
        if pred == desired_outcome:
            return x_cf
        
        # Get feature importances
        importances = model.feature_importances_
        
        # Update the most important feature
        most_important = np.argmax(importances)
        
        # Determine direction of change
        proba = model.predict_proba([x_cf])[0]
        direction = 1 if proba[desired_outcome] < 0.5 else -1
        
        # Update the feature
        x_cf[most_important] += direction * step_size * importances[most_important]
        
        # Ensure the feature is within the allowed range
        x_cf[most_important] = max(feature_ranges[most_important][0], 
                                  min(feature_ranges[most_important][1], x_cf[most_important]))
    
    return x_cf

# Select an instance to explain
instance_idx = 0
x = X[instance_idx]
y = outcome[instance_idx]
print(f"Instance: Age={x[0]:.1f}, Severity={x[1]:.1f}, Outcome={y}")

# Generate counterfactual
feature_ranges = [(20, 60), (1, 9)]  # Allowed ranges for age and severity
desired_outcome = 1 - y  # Opposite of the actual outcome
counterfactual = generate_counterfactual(clf, x, desired_outcome, feature_ranges)

print(f"Counterfactual: Age={counterfactual[0]:.1f}, Severity={counterfactual[1]:.1f}, Predicted Outcome={clf.predict([counterfactual])[0]}")

# Visualize the decision boundary and counterfactual
plt.figure(figsize=(10, 6))

# Create a meshgrid for visualization
h = 0.1  # step size in the mesh
x_min, x_max = 20, 60
y_min, y_max = 1, 9
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdBu')
plt.colorbar(label='Probability of Outcome=1')

# Plot the original instance and counterfactual
plt.scatter(x[0], x[1], c='black', s=100, marker='o', label='Original')
plt.scatter(counterfactual[0], counterfactual[1], c='black', s=100, marker='x', label='Counterfactual')
plt.arrow(x[0], x[1], counterfactual[0] - x[0], counterfactual[1] - x[1], 
         color='black', width=0.01, head_width=0.3, head_length=0.3)

plt.xlabel('Age')
plt.ylabel('Severity')
plt.title('Counterfactual Explanation')
plt.legend()
plt.grid(True)
plt.show()
```

### Causal Feature Selection

Causal feature selection identifies features that have a causal effect on the outcome:

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import mutual_info_regression

# Generate synthetic data with causal and non-causal features
np.random.seed(42)
n = 1000

# Causal features
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(0, 1, n)

# Non-causal features (correlated with causal features)
x3 = 0.7 * x1 + 0.3 * np.random.normal(0, 1, n)
x4 = 0.6 * x2 + 0.4 * np.random.normal(0, 1, n)

# Pure noise features
x5 = np.random.normal(0, 1, n)
x6 = np.random.normal(0, 1, n)

# Outcome (only depends on x1 and x2)
y = 2 * x1 - 1 * x2 + np.random.normal(0, 1, n)

# Create a DataFrame
data = pd.DataFrame({
    'X1': x1,
    'X2': x2,
    'X3': x3,
    'X4': x4,
    'X5': x5,
    'X6': x6,
    'Y': y
})

# Feature selection methods
# 1. Correlation
correlations = [np.corrcoef(data['X1'], data['Y'])[0, 1],
               np.corrcoef(data['X2'], data['Y'])[0, 1],
               np.corrcoef(data['X3'], data['Y'])[0, 1],
               np.corrcoef(data['X4'], data['Y'])[0, 1],
               np.corrcoef(data['X5'], data['Y'])[0, 1],
               np.corrcoef(data['X6'], data['Y'])[0, 1]]

# 2. Mutual Information
X = data[['X1', 'X2', 'X3', 'X4', 'X5', 'X6']].values
mutual_info = mutual_info_regression(X, data['Y'])

# 3. Feature Importance from Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, data['Y'])
importances = rf.feature_importances_

# 4. Causal Feature Selection (simplified)
# We'll use a simple approach: train models with different feature subsets
# and evaluate their performance on interventional data
def evaluate_causal_importance(X, y, feature_idx):
    # Train a model without the feature
    X_without = np.delete(X, feature_idx, axis=1)
    model_without = RandomForestRegressor(n_estimators=100, random_state=42)
    model_without.fit(X_without, y)
    
    # Generate interventional data: randomize the feature
    X_interv = X.copy()
    X_interv[:, feature_idx] = np.random.permutation(X[:, feature_idx])
    
    # Predict on interventional data
    y_pred_without = model_without.predict(np.delete(X_interv, feature_idx, axis=1))
    
    # Train a model with the feature
    model_with = RandomForestRegressor(n_estimators=100, random_state=42)
    model_with.fit(X, y)
    
    # Predict on interventional data
    y_pred_with = model_with.predict(X_interv)
    
    # Compare performance
    mse_without = np.mean((y - y_pred_without) ** 2)
    mse_with = np.mean((y - y_pred_with) ** 2)
    
    # Return the difference in MSE
    return mse_without - mse_with

causal_importances = [evaluate_causal_importance(X, data['Y'], i) for i in range(6)]

# Plot feature importance from different methods
plt.figure(figsize=(15, 5))

plt.subplot(1, 4, 1)
plt.bar(['X1', 'X2', 'X3', 'X4', 'X5', 'X6'], np.abs(correlations))
plt.ylabel('Absolute Correlation')
plt.title('Correlation-Based Importance')
plt.grid(True, axis='y')

plt.subplot(1, 4, 2)
plt.bar(['X1', 'X2', 'X3', 'X4', 'X5', 'X6'], mutual_info)
plt.ylabel('Mutual Information')
plt.title('Mutual Information Importance')
plt.grid(True, axis='y')

plt.subplot(1, 4, 3)
plt.bar(['X1', 'X2', 'X3', 'X4', 'X5', 'X6'], importances)
plt.ylabel('Feature Importance')
plt.title('Random Forest Importance')
plt.grid(True, axis='y')

plt.subplot(1, 4, 4)
plt.bar(['X1', 'X2', 'X3', 'X4', 'X5', 'X6'], causal_importances)
plt.ylabel('Causal Importance')
plt.title('Causal Feature Importance')
plt.grid(True, axis='y')

plt.tight_layout()
plt.show()

# Print true causal relationships
print("True causal relationships:")
print("X1 -> Y (coefficient: 2)")
print("X2 -> Y (coefficient: -1)")
print("X3, X4, X5, X6 have no direct causal effect on Y")
```

## Summary

Causal machine learning integrates causal inference with modern ML techniques:

1. **Heterogeneous Treatment Effects**:
   - Conditional Average Treatment Effects (CATE) estimate how treatment effects vary with covariates
   - Meta-learners like S-learner, T-learner, X-learner, and R-learner provide frameworks for CATE estimation

2. **Causal Forests**:
   - Adapt random forests for causal inference
   - Estimate treatment effects at the individual level
   - Provide uncertainty quantification for treatment effects

3. **Causal Representation Learning**:
   - Discovers causal variables from high-dimensional data
   - Aims for disentangled, invariant, and counterfactual representations

4. **Applications**:
   - Personalized Treatment Assignment: Optimizes treatment decisions based on individual characteristics
   - Counterfactual Explanations: Provides actionable insights for model predictions
   - Causal Feature Selection: Identifies features with causal effects on outcomes

Causal machine learning addresses key limitations of standard ML approaches by focusing on causal relationships rather than just correlations, enabling more robust, interpretable, and actionable models.

## References

1. Athey, S., & Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.
2. Wager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.
3. Künzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the National Academy of Sciences, 116(10), 4156-4165.
4. Bengio, Y., Deleu, T., Rahaman, N., Ke, R., Lachapelle, S., Bilaniuk, O., ... & Pal, C. (2019). A meta-transfer objective for learning to disentangle causal mechanisms. arXiv preprint arXiv:1901.10912.
