# 2.3.8.4 Uncertainty Quantification in Machine Learning - Part 1

## Introduction to Uncertainty Quantification

Uncertainty quantification (UQ) is the science of identifying, quantifying, and reducing uncertainties in computational and real-world systems. In machine learning, UQ is crucial for building reliable models that know when they don't know. This section explores the foundations of uncertainty quantification in machine learning, focusing on types of uncertainty and basic methods for quantifying them.

## Types of Uncertainty

### Aleatoric Uncertainty

**Aleatoric uncertainty** (also called statistical uncertainty) represents the inherent randomness in the data or process being modeled:

- **Homoscedastic**: Constant across all inputs
- **Heteroscedastic**: Varies with the input

Aleatoric uncertainty cannot be reduced by collecting more data, as it represents irreducible noise.

### Epistemic Uncertainty

**Epistemic uncertainty** (also called systematic uncertainty) represents uncertainty due to limited knowledge or data:

- **Model uncertainty**: Uncertainty about model parameters or structure
- **Approximation uncertainty**: Limitations in the model's ability to represent the true function
- **Extrapolation uncertainty**: Uncertainty when predicting far from training data

Unlike aleatoric uncertainty, epistemic uncertainty can be reduced by collecting more data or improving the model.

### Example: Visualizing Different Types of Uncertainty

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

# Generate synthetic data with heteroscedastic noise
np.random.seed(42)
n_samples = 100
X = np.sort(np.random.uniform(0, 10, n_samples))[:, np.newaxis]

# True function
def f(x):
    return np.sin(x) * np.exp(-0.1 * x)

# Noise level increases with x
def noise_level(x):
    return 0.05 + 0.3 * (x / 10)

# Generate noisy observations
y_true = f(X)
noise = np.array([np.random.normal(0, noise_level(x)) for x in X])
y_noisy = y_true + noise

# Fit a Gaussian Process with a noise kernel (for aleatoric uncertainty)
kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)
gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)
gp.fit(X, y_noisy)

# Generate predictions on a fine grid
X_test = np.linspace(0, 12, 1000)[:, np.newaxis]
y_pred, sigma = gp.predict(X_test, return_std=True)

# Fit another GP with less data to show epistemic uncertainty
X_sparse = X[::5]  # Take every 5th point
y_sparse = y_noisy[::5]
gp_sparse = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)
gp_sparse.fit(X_sparse, y_sparse)
y_pred_sparse, sigma_sparse = gp_sparse.predict(X_test, return_std=True)

# Plot the results
plt.figure(figsize=(12, 8))

# Plot for aleatoric uncertainty (full data)
plt.subplot(2, 1, 1)
plt.scatter(X, y_noisy, color='red', alpha=0.5, label='Observations')
plt.plot(X_test, y_pred, color='blue', label='Predicted mean')
plt.fill_between(X_test.ravel(), 
                 y_pred - 2 * sigma, 
                 y_pred + 2 * sigma, 
                 color='blue', alpha=0.2, label='Aleatoric uncertainty (±2σ)')
plt.plot(X_test, f(X_test), 'k--', label='True function')
plt.title('Aleatoric Uncertainty (Full Data)')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)

# Plot for epistemic uncertainty (sparse data)
plt.subplot(2, 1, 2)
plt.scatter(X_sparse, y_sparse, color='red', alpha=0.5, label='Sparse observations')
plt.plot(X_test, y_pred_sparse, color='green', label='Predicted mean (sparse)')
plt.fill_between(X_test.ravel(), 
                 y_pred_sparse - 2 * sigma_sparse, 
                 y_pred_sparse + 2 * sigma_sparse, 
                 color='green', alpha=0.2, label='Total uncertainty (±2σ)')
plt.plot(X_test, f(X_test), 'k--', label='True function')
plt.axvspan(10, 12, alpha=0.2, color='yellow', label='Extrapolation region')
plt.title('Epistemic + Aleatoric Uncertainty (Sparse Data)')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Basic Methods for Uncertainty Quantification

### Bayesian Methods

Bayesian methods provide a principled framework for uncertainty quantification by treating model parameters as random variables:

1. **Prior Distribution**: Initial belief about parameters p(θ)
2. **Likelihood**: Probability of data given parameters p(D|θ)
3. **Posterior Distribution**: Updated belief after observing data p(θ|D)
4. **Predictive Distribution**: Predictions that account for parameter uncertainty p(y|x,D)

#### Bayesian Linear Regression

```python
import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm

# Generate synthetic data
np.random.seed(42)
n_samples = 50
X = np.sort(np.random.uniform(0, 10, n_samples))
y = 2 * X + 1 + np.random.normal(0, 2, n_samples)

# Reshape X for PyMC3
X = X[:, np.newaxis]

# Define and fit Bayesian linear regression model
with pm.Model() as linear_model:
    # Priors
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Expected value of outcome
    mu = intercept + slope * X.flatten()
    
    # Likelihood (sampling distribution) of observations
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace = pm.sample(2000, tune=1000, return_inferencedata=True)

# Generate predictions with uncertainty
X_test = np.linspace(0, 12, 100)[:, np.newaxis]
with linear_model:
    pm_pred = pm.sample_posterior_predictive(trace, var_names=['y_obs'], 
                                            predictions=True,
                                            samples=1000)

# Extract predictions
y_pred = np.zeros((1000, 100))
for i in range(1000):
    y_pred[i] = intercept.distribution.random() + slope.distribution.random() * X_test.flatten()

# Calculate mean and credible intervals
y_mean = np.mean(y_pred, axis=0)
y_lower = np.percentile(y_pred, 2.5, axis=0)
y_upper = np.percentile(y_pred, 97.5, axis=0)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, y_mean, color='red', label='Posterior mean')
plt.fill_between(X_test.flatten(), y_lower, y_upper, color='red', alpha=0.2, label='95% credible interval')
plt.axvspan(10, 12, alpha=0.2, color='yellow', label='Extrapolation region')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Bayesian Linear Regression with Uncertainty')
plt.legend()
plt.grid(True)
plt.show()

# Plot posterior distributions
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
pm.plot_posterior(trace, var_names=['intercept'], ax=plt.gca())
plt.title('Posterior Distribution of Intercept')

plt.subplot(1, 2, 2)
pm.plot_posterior(trace, var_names=['slope'], ax=plt.gca())
plt.title('Posterior Distribution of Slope')

plt.tight_layout()
plt.show()
```

### Frequentist Methods

Frequentist approaches to uncertainty quantification focus on the sampling distribution of estimators:

1. **Confidence Intervals**: Ranges that contain the true parameter with a specified probability
2. **Prediction Intervals**: Ranges that contain future observations with a specified probability
3. **Bootstrap**: Resampling technique to estimate sampling distributions

#### Confidence and Prediction Intervals in Linear Regression

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from scipy import stats

# Generate synthetic data
np.random.seed(42)
n_samples = 50
X = np.sort(np.random.uniform(0, 10, n_samples))
y = 2 * X + 1 + np.random.normal(0, 2, n_samples)

# Reshape X for sklearn
X = X[:, np.newaxis]

# Fit linear regression model
model = LinearRegression()
model.fit(X, y)

# Generate predictions
X_test = np.linspace(0, 12, 100)[:, np.newaxis]
y_pred = model.predict(X_test)

# Calculate confidence intervals
n = X.shape[0]
p = X.shape[1]
y_mean = np.mean(y)
dof = n - p - 1
t_critical = stats.t.ppf(0.975, dof)  # 95% confidence interval

# Calculate standard errors
X_mean = np.mean(X)
ss_x = np.sum((X - X_mean) ** 2)
se_slope = np.sqrt(mean_squared_error(y, model.predict(X)) / ss_x / n)
se_intercept = se_slope * np.sqrt(np.sum(X ** 2) / n)

# Calculate confidence intervals for the regression line
se_y_hat = np.sqrt(mean_squared_error(y, model.predict(X)) * (1/n + (X_test - X_mean)**2 / ss_x))
ci_lower = y_pred - t_critical * se_y_hat
ci_upper = y_pred + t_critical * se_y_hat

# Calculate prediction intervals
pi_lower = y_pred - t_critical * np.sqrt(mean_squared_error(y, model.predict(X)) * (1 + 1/n + (X_test - X_mean)**2 / ss_x))
pi_upper = y_pred + t_critical * np.sqrt(mean_squared_error(y, model.predict(X)) * (1 + 1/n + (X_test - X_mean)**2 / ss_x))

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, y_pred, color='red', label='Regression line')
plt.fill_between(X_test.flatten(), ci_lower.flatten(), ci_upper.flatten(), color='red', alpha=0.2, label='95% confidence interval')
plt.fill_between(X_test.flatten(), pi_lower.flatten(), pi_upper.flatten(), color='green', alpha=0.1, label='95% prediction interval')
plt.axvspan(10, 12, alpha=0.2, color='yellow', label='Extrapolation region')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression with Confidence and Prediction Intervals')
plt.legend()
plt.grid(True)
plt.show()
```

### Bootstrap Methods

Bootstrap methods provide a flexible approach to uncertainty quantification by resampling the data:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.utils import resample

# Generate synthetic data
np.random.seed(42)
n_samples = 50
X = np.sort(np.random.uniform(0, 10, n_samples))
y = 2 * X + 1 + np.random.normal(0, 2, n_samples)

# Reshape X for sklearn
X = X[:, np.newaxis]

# Fit linear regression model
model = LinearRegression()
model.fit(X, y)

# Generate predictions
X_test = np.linspace(0, 12, 100)[:, np.newaxis]
y_pred = model.predict(X_test)

# Bootstrap to estimate uncertainty
n_bootstrap = 1000
bootstrap_predictions = np.zeros((n_bootstrap, len(X_test)))

for i in range(n_bootstrap):
    # Resample with replacement
    indices = np.random.randint(0, len(X), len(X))
    X_boot = X[indices]
    y_boot = y[indices]
    
    # Fit model on bootstrap sample
    model_boot = LinearRegression()
    model_boot.fit(X_boot, y_boot)
    
    # Make predictions
    bootstrap_predictions[i] = model_boot.predict(X_test).flatten()

# Calculate mean and confidence intervals
y_mean = np.mean(bootstrap_predictions, axis=0)
y_lower = np.percentile(bootstrap_predictions, 2.5, axis=0)
y_upper = np.percentile(bootstrap_predictions, 97.5, axis=0)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.5, label='Observations')
plt.plot(X_test, y_mean, color='red', label='Bootstrap mean')
plt.fill_between(X_test.flatten(), y_lower, y_upper, color='red', alpha=0.2, label='95% bootstrap interval')
plt.axvspan(10, 12, alpha=0.2, color='yellow', label='Extrapolation region')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Bootstrap Uncertainty Estimation')
plt.legend()
plt.grid(True)
plt.show()

# Plot bootstrap distribution of parameters
slopes = np.zeros(n_bootstrap)
intercepts = np.zeros(n_bootstrap)

for i in range(n_bootstrap):
    # Resample with replacement
    indices = np.random.randint(0, len(X), len(X))
    X_boot = X[indices]
    y_boot = y[indices]
    
    # Fit model on bootstrap sample
    model_boot = LinearRegression()
    model_boot.fit(X_boot, y_boot)
    
    # Store parameters
    slopes[i] = model_boot.coef_[0]
    intercepts[i] = model_boot.intercept_

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(intercepts, bins=30, alpha=0.7)
plt.axvline(model.intercept_, color='red', linestyle='--', label='Original estimate')
plt.xlabel('Intercept')
plt.ylabel('Frequency')
plt.title('Bootstrap Distribution of Intercept')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(slopes, bins=30, alpha=0.7)
plt.axvline(model.coef_[0], color='red', linestyle='--', label='Original estimate')
plt.xlabel('Slope')
plt.ylabel('Frequency')
plt.title('Bootstrap Distribution of Slope')
plt.legend()

plt.tight_layout()
plt.show()
```

## Summary

Uncertainty quantification is essential for building reliable machine learning models:

1. **Types of Uncertainty**:
   - Aleatoric: Inherent randomness in the data (irreducible)
   - Epistemic: Uncertainty due to limited knowledge (reducible)

2. **Bayesian Methods**:
   - Treat parameters as random variables
   - Provide posterior distributions over parameters
   - Enable predictive distributions that account for parameter uncertainty

3. **Frequentist Methods**:
   - Confidence intervals for parameters
   - Prediction intervals for future observations
   - Based on sampling distributions of estimators

4. **Bootstrap Methods**:
   - Flexible, non-parametric approach
   - Estimate uncertainty through resampling
   - Applicable to complex models and statistics

In Part 2, we will explore more advanced methods for uncertainty quantification in modern machine learning, including ensemble methods, Bayesian neural networks, and calibration techniques.

## References

1. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
2. Efron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. CRC Press.
3. Hullermeier, E., & Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods. Machine Learning, 110(3), 457-506.
4. Kendall, A., & Gal, Y. (2017). What uncertainties do we need in Bayesian deep learning for computer vision? Advances in Neural Information Processing Systems, 30.
