# 2.3.6.5 Advanced Applications of Probabilistic Programming

## Advanced Applications in Machine Learning

Beyond the basic applications covered previously, probabilistic programming enables sophisticated modeling approaches that address complex machine learning challenges. In this section, we'll explore advanced applications of probabilistic programming, focusing on cutting-edge techniques and their implementations.

## Bayesian Deep Learning

### Variational Autoencoders (VAEs)

VAEs combine deep learning with probabilistic modeling to create powerful generative models:

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pyro
import pyro.distributions as dist
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam

# Define a VAE model in Pyro
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super().__init__()
        # Encoder
        self.encoder_fc1 = nn.Linear(input_dim, hidden_dim)
        self.encoder_fc2_mean = nn.Linear(hidden_dim, latent_dim)
        self.encoder_fc2_std = nn.Linear(hidden_dim, latent_dim)
        
        # Decoder
        self.decoder_fc1 = nn.Linear(latent_dim, hidden_dim)
        self.decoder_fc2 = nn.Linear(hidden_dim, input_dim)
        
        # Parameters
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
    
    def encode(self, x):
        h = F.relu(self.encoder_fc1(x))
        mean = self.encoder_fc2_mean(h)
        std = F.softplus(self.encoder_fc2_std(h))
        return mean, std
    
    def decode(self, z):
        h = F.relu(self.decoder_fc1(z))
        return torch.sigmoid(self.decoder_fc2(h))
    
    def model(self, x=None):
        # Prior
        prior_loc = torch.zeros(self.latent_dim)
        prior_scale = torch.ones(self.latent_dim)
        
        # Sample from prior (if x is None) or approximate posterior (if x is given)
        with pyro.plate("data", x.shape[0] if x is not None else 1):
            z_loc = torch.zeros(self.latent_dim)
            z_scale = torch.ones(self.latent_dim)
            
            # Sample latent variables
            z = pyro.sample("latent", dist.Normal(z_loc, z_scale).to_event(1))
            
            # Decode
            loc = self.decode(z)
            
            # Observation
            pyro.sample("obs", dist.Bernoulli(loc).to_event(1), obs=x)
    
    def guide(self, x):
        # Encode
        z_loc, z_scale = self.encode(x)
        
        # Sample latent variables from approximate posterior
        with pyro.plate("data", x.shape[0]):
            pyro.sample("latent", dist.Normal(z_loc, z_scale).to_event(1))
    
    def reconstruct(self, x):
        # Encode
        z_loc, z_scale = self.encode(x)
        
        # Sample from approximate posterior
        z = dist.Normal(z_loc, z_scale).sample()
        
        # Decode
        return self.decode(z)

# Example usage with MNIST
def train_vae(train_loader, test_loader, input_dim=784, hidden_dim=400, latent_dim=20, num_epochs=10):
    # Initialize model
    vae = VAE(input_dim, hidden_dim, latent_dim)
    
    # Setup optimizer
    optimizer = Adam({"lr": 1e-3})
    
    # Setup inference algorithm
    svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())
    
    # Training loop
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_idx, (data, _) in enumerate(train_loader):
            # Reshape data
            data = data.view(-1, input_dim)
            
            # ELBO loss
            loss = svi.step(data)
            total_loss += loss
        
        # Print progress
        avg_loss = total_loss / len(train_loader.dataset)
        print(f"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f}")
    
    return vae
```

### Bayesian Convolutional Neural Networks

Incorporating uncertainty in CNNs through Bayesian methods:

```python
import numpy as np
import pymc as pm
import theano.tensor as tt
import theano

# Define a simple Bayesian CNN model
def bayesian_cnn(X, y, input_shape, n_classes):
    with pm.Model() as model:
        # Reshape input for convolution
        X_reshaped = X.reshape((-1, *input_shape))
        
        # Priors for first convolutional layer
        filters1 = pm.Normal('filters1', mu=0, sigma=0.1, shape=(16, input_shape[0], 3, 3))
        bias1 = pm.Normal('bias1', mu=0, sigma=0.1, shape=16)
        
        # First convolutional layer
        conv1 = tt.nnet.conv2d(X_reshaped, filters1, border_mode='half')
        conv1 = conv1 + bias1.dimshuffle('x', 0, 'x', 'x')
        act1 = tt.nnet.relu(conv1)
        pool1 = tt.signal.pool.pool_2d(act1, (2, 2), ignore_border=True)
        
        # Priors for second convolutional layer
        filters2 = pm.Normal('filters2', mu=0, sigma=0.1, shape=(32, 16, 3, 3))
        bias2 = pm.Normal('bias2', mu=0, sigma=0.1, shape=32)
        
        # Second convolutional layer
        conv2 = tt.nnet.conv2d(pool1, filters2, border_mode='half')
        conv2 = conv2 + bias2.dimshuffle('x', 0, 'x', 'x')
        act2 = tt.nnet.relu(conv2)
        pool2 = tt.signal.pool.pool_2d(act2, (2, 2), ignore_border=True)
        
        # Flatten
        flat = pool2.flatten(2)
        
        # Priors for fully connected layer
        n_flat = flat.shape[1].eval({X: np.zeros((1, *X.shape[1:]), dtype=np.float32)})
        weights_fc = pm.Normal('weights_fc', mu=0, sigma=0.1, shape=(n_flat, n_classes))
        bias_fc = pm.Normal('bias_fc', mu=0, sigma=0.1, shape=n_classes)
        
        # Fully connected layer
        fc = tt.dot(flat, weights_fc) + bias_fc
        
        # Softmax output
        p = tt.nnet.softmax(fc)
        
        # Likelihood
        y_obs = pm.Categorical('y_obs', p=p, observed=y)
        
        return model
```

## Causal Inference

### Bayesian Causal Models

Inferring causal relationships from observational data:

```python
import numpy as np
import pymc as pm

# Generate synthetic data with a causal structure: X -> Z <- Y
np.random.seed(42)
n = 1000

# Exogenous variables
X = np.random.normal(0, 1, n)
Y = np.random.normal(0, 1, n)

# Endogenous variable with causal parents X and Y
Z = 0.5 * X + 0.5 * Y + np.random.normal(0, 0.5, n)

# Define the causal model
with pm.Model() as causal_model:
    # Priors for causal effects
    beta_x = pm.Normal('beta_x', mu=0, sigma=1)
    beta_y = pm.Normal('beta_y', mu=0, sigma=1)
    
    # Expected value of Z given its parents
    mu_z = beta_x * X + beta_y * Y
    
    # Likelihood
    sigma_z = pm.HalfNormal('sigma_z', sigma=1)
    z_obs = pm.Normal('z_obs', mu=mu_z, sigma=sigma_z, observed=Z)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

### Counterfactual Reasoning

Estimating the effects of interventions:

```python
import numpy as np
import pymc as pm

# Generate synthetic data with treatment effect
np.random.seed(42)
n = 1000

# Confounding variable
age = np.random.normal(50, 10, n)

# Treatment assignment (influenced by age)
p_treatment = 1 / (1 + np.exp(-(age - 50) / 10))
treatment = np.random.binomial(1, p_treatment)

# Outcome (influenced by both age and treatment)
true_effect = 5.0  # True treatment effect
outcome = 0.1 * age + true_effect * treatment + np.random.normal(0, 5, n)

# Define the causal model
with pm.Model() as treatment_model:
    # Priors
    alpha = pm.Normal('alpha', mu=0, sigma=10)  # Intercept
    beta_age = pm.Normal('beta_age', mu=0, sigma=1)  # Age effect
    beta_treatment = pm.Normal('beta_treatment', mu=0, sigma=10)  # Treatment effect
    
    # Expected outcome
    mu = alpha + beta_age * age + beta_treatment * treatment
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=5)
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=outcome)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
    
    # Counterfactual: What if everyone received treatment?
    counterfactual_treated = alpha + beta_age * age + beta_treatment
    
    # Counterfactual: What if no one received treatment?
    counterfactual_control = alpha + beta_age * age
    
    # Average treatment effect
    ate = pm.Deterministic('ate', beta_treatment)
```

## Probabilistic Time Series Models

### Bayesian Structural Time Series

Flexible time series modeling with trend, seasonality, and regression components:

```python
import numpy as np
import pymc as pm

# Generate synthetic time series data
np.random.seed(42)
n = 200
time = np.arange(n)

# Trend component
trend = 0.05 * time

# Seasonal component (weekly pattern)
period = 7
seasonal = 5 * np.sin(2 * np.pi * time / period)

# Regression component with external regressors
X = np.column_stack([
    np.random.normal(0, 1, n),  # Random regressor
    np.random.binomial(1, 0.3, n)  # Binary regressor
])
beta_true = np.array([2.0, -3.0])
regression = X @ beta_true

# Combine components and add noise
y = trend + seasonal + regression + np.random.normal(0, 1, n)

# Define the Bayesian structural time series model
with pm.Model() as bsts_model:
    # Trend component
    trend_level = pm.GaussianRandomWalk('trend_level', sigma=0.1, shape=n)
    
    # Seasonal component
    seasonal_init = pm.Normal('seasonal_init', mu=0, sigma=5, shape=period-1)
    seasonal_full = pm.Deterministic('seasonal_full', 
                                    tt.concatenate([seasonal_init, 
                                                   -tt.sum(seasonal_init, keepdims=True)]))
    seasonal = pm.GaussianRandomWalk('seasonal', 
                                    mu=tt.tile(seasonal_full, n // period + 1)[:n], 
                                    sigma=0.1, shape=n)
    
    # Regression component
    beta = pm.Normal('beta', mu=0, sigma=5, shape=X.shape[1])
    regression = pm.Deterministic('regression', tt.dot(X, beta))
    
    # Combine components
    mu = trend_level + seasonal + regression
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=5)
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

### Bayesian Dynamic Linear Models

State space models with time-varying parameters:

```python
import numpy as np
import pymc as pm

# Generate synthetic data from a dynamic linear model
np.random.seed(42)
n = 100

# Time-varying coefficient
beta_true = np.zeros(n)
beta_true[0] = 1.0
for t in range(1, n):
    beta_true[t] = 0.9 * beta_true[t-1] + np.random.normal(0, 0.1)

# Regressor
X = np.random.normal(0, 1, n)

# Observations
y = beta_true * X + np.random.normal(0, 0.5, n)

# Define the dynamic linear model
with pm.Model() as dlm_model:
    # Prior for initial state
    beta_0 = pm.Normal('beta_0', mu=0, sigma=1)
    
    # State transition variance
    sigma_state = pm.HalfNormal('sigma_state', sigma=1)
    
    # Time-varying coefficient
    beta = pm.GaussianRandomWalk('beta', mu=beta_0, sigma=sigma_state, shape=n)
    
    # Expected value
    mu = beta * X
    
    # Observation variance
    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma_obs, observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

## Bayesian Nonparametrics

### Dirichlet Process Mixture Models

Clustering with an unknown number of components:

```python
import numpy as np
import pymc as pm

# Generate synthetic data from a mixture
np.random.seed(42)
n = 200
true_means = np.array([-5.0, 0.0, 5.0])
true_sigmas = np.array([1.0, 1.5, 0.8])
true_weights = np.array([0.3, 0.5, 0.2])

# Generate component assignments
z_true = np.random.choice(3, size=n, p=true_weights)
# Generate observations
y = true_means[z_true] + np.random.normal(0, true_sigmas[z_true])

# Define the Dirichlet process mixture model
with pm.Model() as dp_model:
    # Concentration parameter
    alpha = pm.Gamma('alpha', alpha=1, beta=1)
    
    # Base distribution for component means
    mu_dp = pm.Normal('mu_dp', mu=0, sigma=10)
    tau_dp = pm.Gamma('tau_dp', alpha=1, beta=1)
    
    # Dirichlet process mixture
    dp = pm.DirichletProcess('dp', alpha=alpha, base_dist=pm.Normal.dist(mu=mu_dp, tau=tau_dp), shape=n)
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=5)
    y_obs = pm.Normal('y_obs', mu=dp, sigma=sigma, observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

### Gaussian Process Latent Variable Models

Nonlinear dimensionality reduction with Gaussian processes:

```python
import numpy as np
import pymc as pm

# Generate synthetic high-dimensional data with a low-dimensional structure
np.random.seed(42)
n = 100
latent_dim = 2
observed_dim = 10

# Generate latent variables
X_latent_true = np.random.normal(0, 1, (n, latent_dim))

# Generate high-dimensional observations through a nonlinear mapping
Y = np.zeros((n, observed_dim))
for d in range(observed_dim):
    # Nonlinear function of latent variables
    Y[:, d] = np.sin(X_latent_true[:, 0]) + np.cos(X_latent_true[:, 1]) + \
              0.1 * np.random.normal(0, 1, n)

# Define the Gaussian process latent variable model
with pm.Model() as gplvm_model:
    # Prior for latent variables
    X_latent = pm.Normal('X_latent', mu=0, sigma=1, shape=(n, latent_dim))
    
    # GP hyperparameters
    length_scale = pm.Gamma('length_scale', alpha=2, beta=1)
    eta = pm.HalfNormal('eta', sigma=2)
    
    # Covariance function
    cov_func = eta**2 * pm.gp.cov.ExpQuad(latent_dim, length_scale)
    
    # GP prior for each output dimension
    for d in range(observed_dim):
        gp = pm.gp.Marginal(cov_func=cov_func)
        y_d = gp.marginal_likelihood(f'y_{d}', X=X_latent, y=Y[:, d])
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

## Probabilistic Programming for Reinforcement Learning

### Bayesian Policy Gradient

Incorporating parameter uncertainty in policy gradient methods:

```python
import numpy as np
import pymc as pm
import gym

# Define a simple policy network
def policy_network(state, weights):
    # Simple linear policy: action = weights * state
    return np.dot(weights, state)

# Generate synthetic trajectories
np.random.seed(42)
n_trajectories = 20
n_steps = 10
state_dim = 4
action_dim = 1

# True policy weights
true_weights = np.array([0.5, -0.3, 0.2, -0.1])

# Generate trajectories
trajectories = []
for _ in range(n_trajectories):
    states = np.random.normal(0, 1, (n_steps, state_dim))
    actions = np.array([policy_network(s, true_weights) + np.random.normal(0, 0.1) 
                        for s in states])
    rewards = np.array([np.sum(s**2) + a**2 + np.random.normal(0, 0.1) 
                        for s, a in zip(states, actions)])
    trajectories.append((states, actions, rewards))

# Define the Bayesian policy gradient model
with pm.Model() as bpg_model:
    # Prior for policy weights
    weights = pm.Normal('weights', mu=0, sigma=1, shape=state_dim)
    
    # Likelihood for each trajectory
    for traj_idx, (states, actions, rewards) in enumerate(trajectories):
        # Expected actions under the policy
        expected_actions = pm.math.dot(states, weights)
        
        # Action likelihood
        sigma_action = pm.HalfNormal(f'sigma_action_{traj_idx}', sigma=1)
        pm.Normal(f'actions_{traj_idx}', mu=expected_actions, sigma=sigma_action, 
                 observed=actions)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

### Bayesian Model-Based Reinforcement Learning

Learning probabilistic transition models for planning:

```python
import numpy as np
import pymc as pm
import gym

# Generate synthetic transition data from a simple environment
np.random.seed(42)
n_samples = 100
state_dim = 2
action_dim = 1

# True transition dynamics: s' = A*s + B*a + noise
A_true = np.array([[0.9, 0.1], [0.1, 0.9]])
B_true = np.array([[0.5], [0.5]])

# Generate data
states = np.random.normal(0, 1, (n_samples, state_dim))
actions = np.random.normal(0, 1, (n_samples, action_dim))
next_states = np.zeros((n_samples, state_dim))

for i in range(n_samples):
    next_states[i] = A_true @ states[i] + B_true @ actions[i] + np.random.normal(0, 0.1, state_dim)

# Define the Bayesian transition model
with pm.Model() as transition_model:
    # Priors for transition matrices
    A = pm.Normal('A', mu=0, sigma=1, shape=(state_dim, state_dim))
    B = pm.Normal('B', mu=0, sigma=1, shape=(state_dim, action_dim))
    
    # Expected next states
    mu = pm.math.dot(states, A.T) + pm.math.dot(actions, B.T)
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=1, shape=state_dim)
    for d in range(state_dim):
        pm.Normal(f'next_state_{d}', mu=mu[:, d], sigma=sigma[d], observed=next_states[:, d])
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

## Probabilistic Programming for Anomaly Detection

### Bayesian Changepoint Detection

Identifying changes in time series data:

```python
import numpy as np
import pymc as pm

# Generate synthetic data with changepoints
np.random.seed(42)
n = 300
true_changepoints = [100, 200]
y = np.concatenate([
    np.random.normal(0, 1, true_changepoints[0]),
    np.random.normal(3, 1.5, true_changepoints[1] - true_changepoints[0]),
    np.random.normal(-2, 0.8, n - true_changepoints[1])
])

# Define the Bayesian changepoint model
with pm.Model() as changepoint_model:
    # Prior for number of changepoints
    max_changepoints = 5
    changepoint_indicators = pm.Bernoulli('changepoint_indicators', p=0.1, shape=n-1)
    
    # Constrain to at most max_changepoints
    pm.Potential('max_changepoints_constraint', 
                -1e10 * pm.math.switch(pm.math.sum(changepoint_indicators) > max_changepoints, 1, 0))
    
    # Segment parameters
    means = pm.Normal('means', mu=0, sigma=5, shape=max_changepoints+1)
    sigmas = pm.HalfNormal('sigmas', sigma=5, shape=max_changepoints+1)
    
    # Compute segment assignments
    segment_idx = pm.math.cumsum(changepoint_indicators)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=means[segment_idx], sigma=sigmas[segment_idx], observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

### Bayesian Outlier Detection

Identifying anomalous observations:

```python
import numpy as np
import pymc as pm

# Generate synthetic data with outliers
np.random.seed(42)
n = 100
true_mean = 0
true_sigma = 1
outlier_prob = 0.1
outlier_scale = 5

# Generate regular observations and outliers
is_outlier = np.random.binomial(1, outlier_prob, n)
y = true_mean + true_sigma * np.random.normal(0, 1, n) + is_outlier * outlier_scale * np.random.normal(0, 1, n)

# Define the Bayesian outlier model
with pm.Model() as outlier_model:
    # Regular component parameters
    mu = pm.Normal('mu', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Outlier component parameters
    outlier_sigma = pm.HalfNormal('outlier_sigma', sigma=10)
    
    # Mixing probability
    p_outlier = pm.Beta('p_outlier', alpha=1, beta=10)
    
    # Latent outlier indicators
    z = pm.Bernoulli('z', p=p_outlier, shape=n)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, 
                     sigma=pm.math.switch(z, outlier_sigma, sigma), 
                     observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000)
```

## Summary

Advanced applications of probabilistic programming demonstrate its power and flexibility:

1. **Bayesian Deep Learning**:
   - Variational autoencoders for generative modeling
   - Bayesian CNNs for uncertainty in computer vision

2. **Causal Inference**:
   - Bayesian causal models for inferring causal relationships
   - Counterfactual reasoning for estimating intervention effects

3. **Probabilistic Time Series Models**:
   - Bayesian structural time series for flexible decomposition
   - Dynamic linear models for time-varying parameters

4. **Bayesian Nonparametrics**:
   - Dirichlet process mixtures for clustering with unknown components
   - Gaussian process latent variable models for nonlinear dimensionality reduction

5. **Reinforcement Learning**:
   - Bayesian policy gradient for parameter uncertainty
   - Model-based RL with probabilistic transition models

6. **Anomaly Detection**:
   - Changepoint detection for identifying regime shifts
   - Outlier detection for identifying anomalous observations

These advanced applications showcase how probabilistic programming enables sophisticated modeling approaches that combine the flexibility of programming with the rigor of Bayesian inference.

## References

1. Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518), 859-877.
2. Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). Cambridge University Press.
3. Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. Nature, 521(7553), 452-459.
4. Teh, Y. W. (2010). Dirichlet process. Encyclopedia of Machine Learning, 280-287.
5. Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2), 1-142.
