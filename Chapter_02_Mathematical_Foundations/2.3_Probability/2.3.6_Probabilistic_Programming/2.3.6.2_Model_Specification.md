# 2.3.6.2 Model Specification in Probabilistic Programming

## Model Specification Techniques

A key strength of probabilistic programming is the ability to express complex probabilistic models using familiar programming constructs. In this section, we'll explore different approaches to model specification in probabilistic programming languages (PPLs), focusing on common patterns and techniques.

## Directed Graphical Models

### Basic Structure

Many PPLs use a syntax that corresponds to directed graphical models (Bayesian networks), where variables depend on their parents in the graph:

```python
# Pseudocode for a simple Bayesian network
def simple_model():
    # Prior distributions
    mu = normal(0, 1)
    sigma = half_normal(1)
    
    # Likelihood
    x = normal(mu, sigma)
    
    return mu, sigma, x
```

This code defines a simple hierarchical model where:
- `mu` and `sigma` are drawn from prior distributions
- `x` is drawn from a normal distribution parameterized by `mu` and `sigma`

### Plate Notation

For repeated observations or parameters, PPLs often provide constructs similar to plate notation in graphical models:

```python
# Pseudocode for a model with repeated observations
def repeated_observations_model(n_obs):
    # Prior distributions
    mu = normal(0, 1)
    sigma = half_normal(1)
    
    # Likelihood for multiple observations
    x = []
    for i in range(n_obs):
        x.append(normal(mu, sigma))
    
    return mu, sigma, x
```

Some PPLs provide vectorized operations to express this more concisely:

```python
# Pseudocode with vectorized operations
def vectorized_model(n_obs):
    # Prior distributions
    mu = normal(0, 1)
    sigma = half_normal(1)
    
    # Vectorized likelihood
    x = normal(mu, sigma, size=n_obs)
    
    return mu, sigma, x
```

### Hierarchical Models

Hierarchical (multilevel) models are naturally expressed in PPLs:

```python
# Pseudocode for a hierarchical model
def hierarchical_model(data, groups):
    # Hyperpriors
    mu_global = normal(0, 10)
    sigma_global = half_normal(5)
    
    # Group-level parameters
    group_effects = []
    for g in range(groups):
        effect = normal(mu_global, sigma_global)
        group_effects.append(effect)
    
    # Data likelihood
    for i, (x, g) in enumerate(data):
        y[i] ~ normal(group_effects[g], 1.0)
    
    return mu_global, sigma_global, group_effects
```

This model captures variation at both the global and group levels, a common pattern in many applications.

## Generative Models

### Forward Sampling

PPLs naturally express generative models, which define a joint distribution over observed and latent variables through a sampling process:

```python
# Pseudocode for a mixture model
def mixture_model(n_samples, K):
    # Mixture weights
    weights = dirichlet([1.0] * K)
    
    # Component parameters
    means = []
    for k in range(K):
        means.append(normal(0, 10))
    
    # Data generation
    x = []
    z = []
    for i in range(n_samples):
        # Latent cluster assignment
        z_i = categorical(weights)
        z.append(z_i)
        # Observation
        x_i = normal(means[z_i], 1.0)
        x.append(x_i)
    
    return weights, means, z, x
```

This model:
1. Generates mixture weights from a Dirichlet distribution
2. Samples component means from a normal distribution
3. For each data point, samples a cluster assignment and then an observation

### Conditioning

Generative models become inference models when we condition on observed data:

```python
# Pseudocode for inference in a mixture model
def mixture_inference(data, K):
    # Mixture weights
    weights = dirichlet([1.0] * K)
    
    # Component parameters
    means = []
    for k in range(K):
        means.append(normal(0, 10))
    
    # Data likelihood
    z = []
    for i, x_i in enumerate(data):
        # Latent cluster assignment
        z_i = categorical(weights)
        z.append(z_i)
        # Observation (conditioning)
        observe(normal(means[z_i], 1.0), x_i)
    
    return weights, means, z
```

The `observe` statement conditions the model on the observed data, enabling inference of the latent variables.

## Stochastic Functions

### Function Composition

PPLs often support stochastic functions, which enable more complex models through composition:

```python
# Pseudocode for a stochastic function
def random_walk(n_steps, step_size):
    position = 0.0
    positions = [position]
    
    for t in range(n_steps):
        # Random step
        step = normal(0, step_size)
        position += step
        positions.append(position)
    
    return positions

# Using the stochastic function in a model
def random_walk_model(observed_positions):
    # Prior on step size
    step_size = half_normal(1.0)
    
    # Generate a random walk
    positions = random_walk(len(observed_positions) - 1, step_size)
    
    # Condition on observations
    for t, pos in enumerate(observed_positions):
        observe(normal(positions[t], 0.1), pos)
    
    return step_size
```

This approach allows building complex models from simpler components.

### Recursive Models

Some PPLs support recursive stochastic functions, enabling models like stochastic grammars or branching processes:

```python
# Pseudocode for a branching process
def branching_process(depth, branching_prob):
    if depth == 0:
        return 1
    
    # Decide whether to branch
    if bernoulli(branching_prob):
        # Generate left and right branches
        left = branching_process(depth - 1, branching_prob)
        right = branching_process(depth - 1, branching_prob)
        return left + right
    else:
        return 1

# Using the recursive model
def tree_model(observed_size):
    # Prior on branching probability
    branching_prob = beta(2, 2)
    
    # Generate a random tree
    size = branching_process(10, branching_prob)
    
    # Condition on observed size
    observe(poisson(size), observed_size)
    
    return branching_prob
```

Recursive models can capture complex hierarchical structures like trees, graphs, and sequences.

## Practical Examples

### Linear Regression in PyMC

Here's a concrete example of a Bayesian linear regression model in PyMC:

```python
import numpy as np
import pymc as pm

# Generate synthetic data
np.random.seed(42)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
true_sigma = 1.0
y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)

# Define the model
with pm.Model() as linear_model:
    # Priors
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Expected value of outcome
    mu = intercept + slope * x
    
    # Likelihood (sampling distribution) of observations
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000, return_inferencedata=True)
```

This model:
1. Defines prior distributions for the intercept, slope, and noise standard deviation
2. Specifies the linear relationship between predictors and outcomes
3. Conditions on the observed data through the `observed` parameter

### Hierarchical Model in Stan

Here's a hierarchical model implemented in Stan:

```stan
// Hierarchical model for school effects
data {
  int<lower=0> J;          // number of schools
  real y[J];               // estimated treatment effects
  real<lower=0> sigma[J];  // standard error of effect estimates
}
parameters {
  real mu;                 // population mean
  real<lower=0> tau;       // population standard deviation
  vector[J] eta;           // standardized school effects
}
transformed parameters {
  vector[J] theta = mu + tau * eta;  // school effects
}
model {
  mu ~ normal(0, 5);       // prior on population mean
  tau ~ cauchy(0, 5);      // prior on population standard deviation
  eta ~ normal(0, 1);      // prior on standardized effects
  y ~ normal(theta, sigma); // likelihood
}
```

This Stan model:
1. Defines data, parameters, and transformed parameters blocks
2. Specifies priors for the population parameters and standardized effects
3. Defines the likelihood of the observed data given the parameters

### Mixture Model in Pyro

Here's a Gaussian mixture model implemented in Pyro:

```python
import torch
import pyro
import pyro.distributions as dist

def mixture_model(data, K):
    # Mixture weights
    weights = pyro.sample('weights', dist.Dirichlet(torch.ones(K)))
    
    # Component means
    means = []
    for k in range(K):
        mean_k = pyro.sample(f'mean_{k}', dist.Normal(0, 10))
        means.append(mean_k)
    
    # Data likelihood
    with pyro.plate('data', len(data)):
        # Latent cluster assignments
        z = pyro.sample('z', dist.Categorical(weights), infer={'enumerate': 'parallel'})
        
        # Observations
        pyro.sample('obs', dist.Normal(means[z], 1.0), obs=data)
    
    return weights, means
```

This Pyro model:
1. Samples mixture weights from a Dirichlet distribution
2. Samples component means from normal distributions
3. Uses a plate to handle multiple observations efficiently
4. Conditions on the observed data through the `obs` parameter

## Model Composition and Reuse

### Building Blocks

PPLs encourage building complex models from reusable components:

```python
# Pseudocode for model components
def normal_mixture(weights, means, std):
    # Sample component
    k = categorical(weights)
    # Sample from component
    return normal(means[k], std)

def hierarchical_prior(n_groups, mu_prior, sigma_prior):
    # Global parameters
    mu = sample_from(mu_prior)
    sigma = sample_from(sigma_prior)
    
    # Group parameters
    group_params = []
    for g in range(n_groups):
        param_g = normal(mu, sigma)
        group_params.append(param_g)
    
    return mu, sigma, group_params

# Combining components
def complex_model(data, groups, K):
    # Hierarchical prior for mixture weights
    _, _, group_weights = hierarchical_prior(
        n_groups=len(groups),
        mu_prior=dirichlet([1.0] * K),
        sigma_prior=gamma(1.0, 1.0)
    )
    
    # Component means
    means = []
    for k in range(K):
        means.append(normal(0, 10))
    
    # Data likelihood
    for i, (x, g) in enumerate(data):
        # Use mixture model for each observation
        observe(normal_mixture(group_weights[g], means, 1.0), x)
    
    return group_weights, means
```

This approach promotes modularity and code reuse.

### Model Libraries

Many PPLs provide libraries of pre-defined models:

```python
# Pseudocode for using model libraries
from model_library import linear_regression, gaussian_process, hmm

# Linear regression
linear_model = linear_regression(x, y, priors={'slope': normal(0, 10)})

# Gaussian process
gp_model = gaussian_process(x, y, kernel='rbf', priors={'length_scale': gamma(2, 2)})

# Hidden Markov model
hmm_model = hmm(observations, n_states=3, emission='gaussian')
```

These libraries simplify common modeling tasks while allowing customization.

## Summary

Model specification in probabilistic programming offers a flexible and expressive framework for defining complex probabilistic models:

1. **Directed Graphical Models**: Natural expression of Bayesian networks with variables depending on their parents
2. **Generative Models**: Forward sampling and conditioning to define joint distributions
3. **Stochastic Functions**: Composition and recursion for building complex models
4. **Practical Examples**: Concrete implementations in PyMC, Stan, and Pyro
5. **Model Composition**: Building complex models from reusable components

These techniques enable the specification of a wide range of models, from simple regression to complex hierarchical and generative models, making probabilistic programming a powerful tool for Bayesian modeling and inference.

## References

1. Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., ... & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1).
2. Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2, e55.
3. Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., ... & Goodman, N. D. (2019). Pyro: Deep universal probabilistic programming. Journal of Machine Learning Research, 20(1), 973-978.
4. van de Meent, J. W., Paige, B., Yang, H., & Wood, F. (2018). An introduction to probabilistic programming. arXiv preprint arXiv:1809.10756.
