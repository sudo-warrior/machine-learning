# 2.3.6.4 Basic Applications of Probabilistic Programming

## Applying Probabilistic Programming in Machine Learning

Probabilistic programming languages (PPLs) provide powerful tools for implementing Bayesian models in various machine learning applications. In this section, we'll explore some fundamental applications of probabilistic programming, focusing on common use cases and practical examples.

## Bayesian Regression Models

### Linear Regression

Bayesian linear regression extends traditional linear regression by placing prior distributions on the parameters and computing full posterior distributions rather than point estimates:

```python
import numpy as np
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
true_sigma = 1.0
y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)

# Define the model
with pm.Model() as linear_model:
    # Priors
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Expected value of outcome
    mu = intercept + slope * x
    
    # Likelihood (sampling distribution) of observations
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace = pm.sample(1000, tune=1000, return_inferencedata=True)

# Plot the results
az.plot_trace(trace)
plt.tight_layout()
plt.show()

# Plot the regression line with uncertainty
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.7, label='Data')

# Plot a subset of posterior samples
post = trace.posterior
for i in range(0, 100, 10):
    idx = np.random.randint(len(post.intercept))
    chain = np.random.randint(len(post.intercept.chain))
    intercept_sample = post.intercept[chain, idx].item()
    slope_sample = post.slope[chain, idx].item()
    plt.plot(x, intercept_sample + slope_sample * x, 'g-', alpha=0.1)

# Plot the posterior mean
intercept_mean = post.intercept.mean().item()
slope_mean = post.slope.mean().item()
plt.plot(x, intercept_mean + slope_mean * x, 'b--', label='Posterior Mean')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Bayesian Linear Regression')
plt.legend()
plt.grid(True)
plt.show()
```

### Robust Regression

Bayesian models can easily incorporate robust error distributions to handle outliers:

```python
import numpy as np
import pymc as pm

# Generate synthetic data with outliers
np.random.seed(42)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 1.0
true_slope = 2.0
y = true_intercept + true_slope * x + np.random.normal(0, 1.0, n)
# Add outliers
y[np.random.choice(n, 5, replace=False)] += np.random.normal(0, 10, 5)

# Define the robust regression model
with pm.Model() as robust_model:
    # Priors
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)
    nu = pm.Gamma('nu', alpha=2, beta=0.1)  # Degrees of freedom
    
    # Expected value of outcome
    mu = intercept + slope * x
    
    # Student's t likelihood for robustness
    y_obs = pm.StudentT('y_obs', nu=nu, mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace_robust = pm.sample(1000, tune=1000)
```

### Hierarchical Regression

Hierarchical (multilevel) models capture group-level variation:

```python
import numpy as np
import pymc as pm

# Generate synthetic hierarchical data
np.random.seed(42)
n_groups = 8
n_per_group = 20
group_intercepts = np.random.normal(1.0, 0.5, n_groups)
x = np.random.uniform(0, 10, n_groups * n_per_group)
group_idx = np.repeat(np.arange(n_groups), n_per_group)
y = group_intercepts[group_idx] + 2.0 * x + np.random.normal(0, 1.0, n_groups * n_per_group)

# Define the hierarchical model
with pm.Model() as hierarchical_model:
    # Hyperpriors
    mu_intercept = pm.Normal('mu_intercept', mu=0, sigma=10)
    sigma_intercept = pm.HalfNormal('sigma_intercept', sigma=5)
    
    # Group-level intercepts
    group_intercept = pm.Normal('group_intercept', mu=mu_intercept, 
                               sigma=sigma_intercept, shape=n_groups)
    
    # Global slope
    slope = pm.Normal('slope', mu=0, sigma=10)
    
    # Error term
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Expected value
    mu = group_intercept[group_idx] + slope * x
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace_hierarchical = pm.sample(1000, tune=1000)
```

## Classification Models

### Logistic Regression

Bayesian logistic regression for binary classification:

```python
import numpy as np
import pymc as pm

# Generate synthetic classification data
np.random.seed(42)
n = 100
x = np.random.normal(0, 1, n)
true_intercept = 0.5
true_slope = 2.0
p = 1 / (1 + np.exp(-(true_intercept + true_slope * x)))
y = np.random.binomial(1, p)

# Define the logistic regression model
with pm.Model() as logistic_model:
    # Priors
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    
    # Linear predictor
    logit_p = intercept + slope * x
    
    # Likelihood
    y_obs = pm.Bernoulli('y_obs', logit_p=logit_p, observed=y)
    
    # Inference
    trace_logistic = pm.sample(1000, tune=1000)
```

### Multinomial Logistic Regression

Extending to multi-class classification:

```python
import numpy as np
import pymc as pm

# Generate synthetic multi-class data
np.random.seed(42)
n = 300
n_classes = 3
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(0, 1, n)
X = np.column_stack([x1, x2])

# True coefficients (including intercept)
true_betas = np.array([
    [0.0, 1.0, 1.0],   # Class 0
    [2.0, -1.0, 0.5],  # Class 1
    [-1.0, 0.0, -1.5]  # Class 2
])

# Compute class probabilities
logits = np.dot(np.column_stack([np.ones(n), X]), true_betas.T)
p = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)

# Generate class labels
y = np.zeros(n, dtype=int)
for i in range(n):
    y[i] = np.random.choice(n_classes, p=p[i])

# Define the multinomial logistic regression model
with pm.Model() as multinomial_model:
    # Priors for coefficients (including intercept)
    betas = pm.Normal('betas', mu=0, sigma=5, shape=(3, n_classes))
    
    # Linear predictor
    logits = pm.math.dot(np.column_stack([np.ones(n), X]), betas)
    
    # Likelihood
    y_obs = pm.Categorical('y_obs', logits=logits, observed=y)
    
    # Inference
    trace_multinomial = pm.sample(1000, tune=1000)
```

## Time Series Models

### Autoregressive (AR) Models

Bayesian autoregressive models for time series:

```python
import numpy as np
import pymc as pm

# Generate synthetic AR(1) data
np.random.seed(42)
n = 200
true_alpha = 0.8
true_sigma = 1.0
y = np.zeros(n)
y[0] = np.random.normal(0, true_sigma / np.sqrt(1 - true_alpha**2))
for t in range(1, n):
    y[t] = true_alpha * y[t-1] + np.random.normal(0, true_sigma)

# Define the AR(1) model
with pm.Model() as ar1_model:
    # Priors
    alpha = pm.Uniform('alpha', lower=-1, upper=1)  # Stationarity constraint
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Expected value
    mu = np.zeros(n)
    mu[1:] = alpha * y[:-1]
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace_ar = pm.sample(1000, tune=1000)
```

### State Space Models

Hidden Markov Models (HMMs) for latent state inference:

```python
import numpy as np
import pymc as pm

# Generate synthetic HMM data
np.random.seed(42)
n = 100
# True transition matrix
true_trans_prob = np.array([
    [0.8, 0.2],
    [0.3, 0.7]
])
# True emission parameters
true_means = np.array([-2.0, 2.0])
true_sigma = 1.0

# Generate hidden states
z = np.zeros(n, dtype=int)
z[0] = np.random.binomial(1, 0.5)
for t in range(1, n):
    z[t] = np.random.binomial(1, true_trans_prob[z[t-1], 1])

# Generate observations
y = true_means[z] + np.random.normal(0, true_sigma, n)

# Define the HMM model
with pm.Model() as hmm_model:
    # Priors for transition probabilities
    alpha = pm.Dirichlet('alpha', a=np.ones(2), shape=(2, 2))
    
    # Priors for emission parameters
    means = pm.Normal('means', mu=0, sigma=10, shape=2)
    sigma = pm.HalfNormal('sigma', sigma=5)
    
    # Hidden state sequence
    z = pm.DiscreteMarkov('z', trans=alpha, shape=n)
    
    # Observations
    y_obs = pm.Normal('y_obs', mu=means[z], sigma=sigma, observed=y)
    
    # Inference (using NUTS for continuous parameters and Metropolis for discrete)
    trace_hmm = pm.sample(1000, tune=1000)
```

## Mixture Models

### Gaussian Mixture Model

Clustering with Gaussian mixture models:

```python
import numpy as np
import pymc as pm

# Generate synthetic mixture data
np.random.seed(42)
n = 300
true_means = np.array([-5.0, 0.0, 5.0])
true_sigmas = np.array([1.0, 1.5, 0.8])
true_weights = np.array([0.3, 0.5, 0.2])

# Generate component assignments
z_true = np.random.choice(3, size=n, p=true_weights)
# Generate observations
y = true_means[z_true] + np.random.normal(0, true_sigmas[z_true])

# Define the Gaussian mixture model
with pm.Model() as gmm_model:
    # Priors for mixture weights
    weights = pm.Dirichlet('weights', a=np.ones(3))
    
    # Priors for component parameters
    means = pm.Normal('means', mu=0, sigma=10, shape=3)
    sigmas = pm.HalfNormal('sigmas', sigma=5, shape=3)
    
    # Latent cluster assignments
    z = pm.Categorical('z', p=weights, shape=n)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=means[z], sigma=sigmas[z], observed=y)
    
    # Inference
    trace_gmm = pm.sample(1000, tune=1000)
```

## Gaussian Processes

### Regression with Gaussian Processes

Nonparametric regression using Gaussian processes:

```python
import numpy as np
import pymc as pm

# Generate synthetic data
np.random.seed(42)
n = 30
X = np.sort(np.random.uniform(0, 10, n))[:, None]
y = np.sin(X[:, 0]) + 0.1 * np.random.randn(n)

# Define the Gaussian process model
with pm.Model() as gp_model:
    # Priors for GP hyperparameters
    length_scale = pm.Gamma('length_scale', alpha=2, beta=0.5)
    eta = pm.HalfNormal('eta', sigma=2)
    
    # Specify the covariance function
    cov_func = eta**2 * pm.gp.cov.ExpQuad(1, length_scale)
    
    # Specify the GP
    gp = pm.gp.Marginal(cov_func=cov_func)
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=1)
    y_obs = gp.marginal_likelihood('y_obs', X=X, y=y, noise=sigma)
    
    # Inference
    trace_gp = pm.sample(1000, tune=1000)
    
    # Predictions
    X_new = np.linspace(0, 10, 100)[:, None]
    mu, var = gp.predict(X_new, point=trace_gp[-1], diag=True)
```

## Bayesian Neural Networks

### Simple Bayesian Neural Network

Placing priors on neural network weights:

```python
import numpy as np
import pymc as pm
import theano.tensor as tt

# Generate synthetic data
np.random.seed(42)
n = 100
X = np.random.uniform(-3, 3, size=(n, 1))
y = np.sin(X[:, 0]) + 0.1 * np.random.randn(n)

# Define the Bayesian neural network model
with pm.Model() as bnn_model:
    # Priors for weights and biases
    n_hidden = 5
    
    # Input to hidden layer
    w1 = pm.Normal('w1', mu=0, sigma=1, shape=(1, n_hidden))
    b1 = pm.Normal('b1', mu=0, sigma=1, shape=n_hidden)
    
    # Hidden to output layer
    w2 = pm.Normal('w2', mu=0, sigma=1, shape=n_hidden)
    b2 = pm.Normal('b2', mu=0, sigma=1)
    
    # Forward pass
    a1 = pm.math.tanh(pm.math.dot(X, w1) + b1)
    mu = pm.math.dot(a1, w2) + b2
    
    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=1)
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Inference
    trace_bnn = pm.sample(1000, tune=1000)
```

## Summary

Probabilistic programming enables a wide range of Bayesian machine learning applications:

1. **Regression Models**:
   - Linear regression with full uncertainty quantification
   - Robust regression for handling outliers
   - Hierarchical regression for grouped data

2. **Classification Models**:
   - Logistic regression for binary classification
   - Multinomial logistic regression for multi-class problems

3. **Time Series Models**:
   - Autoregressive models for temporal dependencies
   - State space models for latent dynamics

4. **Mixture Models**:
   - Gaussian mixture models for clustering
   - Flexible density estimation

5. **Gaussian Processes**:
   - Nonparametric regression with uncertainty
   - Flexible function approximation

6. **Bayesian Neural Networks**:
   - Neural networks with prior distributions on weights
   - Uncertainty quantification in deep learning

These applications demonstrate the versatility of probabilistic programming for implementing Bayesian models across various machine learning tasks.

## References

1. Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2, e55.
2. Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.
3. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
4. Neal, R. M. (2012). Bayesian Learning for Neural Networks. Springer Science & Business Media.
