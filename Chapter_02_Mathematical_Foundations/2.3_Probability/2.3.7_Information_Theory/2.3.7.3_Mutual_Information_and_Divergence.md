# 2.3.7.3 Mutual Information and Divergence Measures

## Mutual Information and Divergence Measures

This section explores mutual information, which quantifies the shared information between random variables, and divergence measures, which quantify the difference between probability distributions.

## Mutual Information

### Definition of Mutual Information

**Mutual information** I(X; Y) measures the amount of information shared between two random variables X and Y:

$$I(X; Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}$$

### Alternative Expressions

Mutual information can be expressed in terms of entropy:

$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)$$

These expressions highlight different interpretations of mutual information:
- The reduction in uncertainty about X after observing Y
- The reduction in uncertainty about Y after observing X
- The total information in X and Y minus their joint information

### Properties of Mutual Information

1. **Non-negativity**: $I(X; Y) \geq 0$, with equality if and only if X and Y are independent
2. **Symmetry**: $I(X; Y) = I(Y; X)$
3. **Upper Bound**: $I(X; Y) \leq \min(H(X), H(Y))$, with equality if one variable determines the other

### Example: Computing Mutual Information

```python
import numpy as np

# Define a joint probability distribution
# X: Weather (Sunny, Rainy)
# Y: Activity (Beach, Movie, Hiking)
joint_probs = np.array([
    [0.3, 0.1, 0.1],  # Sunny: Beach, Movie, Hiking
    [0.05, 0.35, 0.1]  # Rainy: Beach, Movie, Hiking
])

# Compute marginal probabilities
p_x = joint_probs.sum(axis=1)  # Probability of weather
p_y = joint_probs.sum(axis=0)  # Probability of activity

# Compute entropy of X
h_x = -np.sum(p_x * np.log2(p_x))
print(f"H(X) = {h_x:.4f} bits")

# Compute entropy of Y
h_y = -np.sum(p_y * np.log2(p_y))
print(f"H(Y) = {h_y:.4f} bits")

# Compute joint entropy
h_xy = -np.sum(joint_probs * np.log2(joint_probs + 1e-10))
print(f"H(X,Y) = {h_xy:.4f} bits")

# Compute conditional entropy H(Y|X)
p_y_given_x = joint_probs / p_x[:, np.newaxis]
h_y_given_x = 0
for i in range(len(p_x)):
    h_y_given_x -= p_x[i] * np.sum(p_y_given_x[i] * np.log2(p_y_given_x[i] + 1e-10))
print(f"H(Y|X) = {h_y_given_x:.4f} bits")

# Compute mutual information
mutual_info = 0
for i in range(len(p_x)):
    for j in range(len(p_y)):
        if joint_probs[i, j] > 0:
            mutual_info += joint_probs[i, j] * np.log2(joint_probs[i, j] / (p_x[i] * p_y[j]))

print(f"I(X; Y) = {mutual_info:.4f} bits")

# Verify using entropy formulas
i_xy_from_entropy = h_x + h_y - h_xy
print(f"I(X; Y) from entropy = {i_xy_from_entropy:.4f} bits")

i_xy_from_conditional = h_y - h_y_given_x
print(f"I(X; Y) from conditional entropy = {i_xy_from_conditional:.4f} bits")
```

### Normalized Mutual Information

To compare mutual information across different variable pairs, we can normalize it:

$$NMI(X; Y) = \frac{I(X; Y)}{\sqrt{H(X) \cdot H(Y)}}$$

This gives a value between 0 (no mutual information) and 1 (perfect correlation).

```python
# Compute normalized mutual information
nmi = mutual_info / np.sqrt(h_x * h_y)
print(f"Normalized Mutual Information = {nmi:.4f}")
```

## Kullback-Leibler Divergence

### Definition of KL Divergence

The **Kullback-Leibler (KL) divergence** measures the difference between two probability distributions P and Q:

$$D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

For continuous distributions:

$$D_{KL}(P || Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx$$

### Interpretation of KL Divergence

KL divergence can be interpreted as:
1. The information lost when Q is used to approximate P
2. The relative entropy of P with respect to Q
3. The inefficiency of assuming distribution Q when the true distribution is P

### Properties of KL Divergence

1. **Non-negativity**: $D_{KL}(P || Q) \geq 0$, with equality if and only if P = Q almost everywhere
2. **Asymmetry**: Generally, $D_{KL}(P || Q) \neq D_{KL}(Q || P)$
3. **Not a metric**: KL divergence doesn't satisfy the triangle inequality and isn't symmetric

### Example: Computing KL Divergence

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Define two normal distributions
mu1, sigma1 = 0, 1  # Distribution P
mu2, sigma2 = 1, 1.5  # Distribution Q

# Create points for evaluation
x = np.linspace(-5, 5, 1000)
p = stats.norm.pdf(x, mu1, sigma1)
q = stats.norm.pdf(x, mu2, sigma2)

# Compute KL divergence (approximation using discrete points)
kl_divergence = np.sum(p * np.log(p / q + 1e-10)) * (x[1] - x[0])
print(f"KL(P || Q) ≈ {kl_divergence:.4f}")

# Compute the reverse KL divergence
reverse_kl = np.sum(q * np.log(q / p + 1e-10)) * (x[1] - x[0])
print(f"KL(Q || P) ≈ {reverse_kl:.4f}")

# Plot the distributions
plt.figure(figsize=(10, 6))
plt.plot(x, p, 'b-', label='P ~ N(0, 1)')
plt.plot(x, q, 'r-', label='Q ~ N(1, 1.5)')
plt.fill_between(x, p, q, where=(p > q), color='blue', alpha=0.3, label='P > Q')
plt.fill_between(x, p, q, where=(p <= q), color='red', alpha=0.3, label='P <= Q')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('KL Divergence Between Normal Distributions')
plt.legend()
plt.grid(True)
plt.show()
```

### Relationship to Mutual Information

Mutual information can be expressed as a KL divergence:

$$I(X; Y) = D_{KL}(p(x, y) || p(x)p(y))$$

This shows that mutual information measures how far the joint distribution is from the product of marginals (independence).

## Cross-Entropy

### Definition of Cross-Entropy

The **cross-entropy** between two probability distributions P and Q is:

$$H(P, Q) = -\sum_{x} P(x) \log Q(x)$$

### Relationship to KL Divergence

Cross-entropy is related to KL divergence and entropy:

$$H(P, Q) = H(P) + D_{KL}(P || Q)$$

where H(P) is the entropy of P.

### Applications in Machine Learning

Cross-entropy is commonly used as a loss function in classification tasks:

$$L = -\sum_{i=1}^{n} y_i \log \hat{y}_i$$

where:
- $y_i$ is the true probability (often 0 or 1 for one-hot encoded labels)
- $\hat{y}_i$ is the predicted probability

### Example: Cross-Entropy Loss

```python
import numpy as np

# True class probabilities (one-hot encoded)
y_true = np.array([0, 1, 0])

# Predicted class probabilities (from a model)
y_pred1 = np.array([0.1, 0.8, 0.1])  # Good prediction
y_pred2 = np.array([0.3, 0.3, 0.4])  # Poor prediction

# Compute cross-entropy loss
def cross_entropy(y_true, y_pred):
    # Add small epsilon to avoid log(0)
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)
    return -np.sum(y_true * np.log(y_pred))

loss1 = cross_entropy(y_true, y_pred1)
loss2 = cross_entropy(y_true, y_pred2)

print(f"Cross-entropy loss (good prediction): {loss1:.4f}")
print(f"Cross-entropy loss (poor prediction): {loss2:.4f}")
```

## Jensen-Shannon Divergence

### Definition of JS Divergence

The **Jensen-Shannon divergence** is a symmetric measure of similarity between two probability distributions:

$$JSD(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M)$$

where $M = \frac{1}{2}(P + Q)$ is the average of the two distributions.

### Properties of JS Divergence

1. **Symmetry**: $JSD(P || Q) = JSD(Q || P)$
2. **Bounded**: $0 \leq JSD(P || Q) \leq 1$ (when using log base 2)
3. **Metric**: The square root of JSD is a metric

### Example: Computing JS Divergence

```python
import numpy as np
from scipy import stats

# Define two normal distributions
mu1, sigma1 = 0, 1  # Distribution P
mu2, sigma2 = 2, 1  # Distribution Q

# Create points for evaluation
x = np.linspace(-5, 5, 1000)
p = stats.norm.pdf(x, mu1, sigma1)
q = stats.norm.pdf(x, mu2, sigma2)

# Compute the mixture distribution M
m = 0.5 * (p + q)

# Compute KL divergences
kl_p_m = np.sum(p * np.log(p / m + 1e-10)) * (x[1] - x[0])
kl_q_m = np.sum(q * np.log(q / m + 1e-10)) * (x[1] - x[0])

# Compute JS divergence
js_divergence = 0.5 * kl_p_m + 0.5 * kl_q_m
print(f"JS(P || Q) ≈ {js_divergence:.4f}")
```

## Applications in Machine Learning

### 1. Feature Selection

Mutual information is widely used for feature selection:

```python
from sklearn.feature_selection import mutual_info_classif
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer

# Load data
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# Compute mutual information between features and target
mi_scores = mutual_info_classif(X, y)

# Create a DataFrame to display feature names and scores
feature_scores = pd.DataFrame({'Feature': cancer.feature_names, 'MI Score': mi_scores})
feature_scores = feature_scores.sort_values('MI Score', ascending=False)

print("Top 5 features by mutual information:")
print(feature_scores.head(5))
```

### 2. Neural Network Training

Cross-entropy loss is fundamental in training neural networks for classification:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load and preprocess data
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# One-hot encode the labels
encoder = OneHotEncoder(sparse=False)
y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))
y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))

# Create a simple neural network
model = Sequential([
    Dense(10, activation='relu', input_shape=(4,)),
    Dense(3, activation='softmax')
])

# Compile with cross-entropy loss
model.compile(optimizer='adam', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train_one_hot, 
                    epochs=50, 
                    batch_size=8,
                    validation_data=(X_test, y_test_one_hot),
                    verbose=0)

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)
print(f"Test accuracy: {accuracy:.4f}")
print(f"Test loss (cross-entropy): {loss:.4f}")
```

### 3. Clustering Evaluation

Mutual information is used to evaluate clustering quality:

```python
from sklearn.metrics.cluster import mutual_info_score, normalized_mutual_info_score
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

# Load data
iris = load_iris()
X, true_labels = iris.data, iris.target

# Perform clustering
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X)

# Evaluate clustering using mutual information
mi = mutual_info_score(true_labels, cluster_labels)
nmi = normalized_mutual_info_score(true_labels, cluster_labels)

print(f"Mutual Information: {mi:.4f}")
print(f"Normalized Mutual Information: {nmi:.4f}")
```

### 4. Generative Adversarial Networks (GANs)

JS divergence is implicitly minimized in the original GAN formulation:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# Simple GAN for generating 1D data
def build_generator():
    model = Sequential([
        Dense(16, activation='relu', input_dim=1),
        Dense(32, activation='relu'),
        Dense(1, activation='tanh')  # Output in [-1, 1] range
    ])
    return model

def build_discriminator():
    model = Sequential([
        Dense(32, activation='relu', input_dim=1),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')  # Output probability
    ])
    return model

# Build and compile the discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', 
                      optimizer=Adam(0.001),
                      metrics=['accuracy'])

# Build the generator
generator = build_generator()

# Build the GAN
discriminator.trainable = False
gan = Sequential([generator, discriminator])
gan.compile(loss='binary_crossentropy', optimizer=Adam(0.001))

# Training parameters
batch_size = 128
epochs = 1000
sample_interval = 200

# Target distribution: mixture of two Gaussians
def generate_real_samples(n):
    # 50% from N(-2, 0.5), 50% from N(2, 0.5)
    samples = np.zeros(n)
    for i in range(n):
        if np.random.random() < 0.5:
            samples[i] = np.random.normal(-2, 0.5)
        else:
            samples[i] = np.random.normal(2, 0.5)
    return samples.reshape(-1, 1), np.ones((n, 1))

# Generate random noise as input to the generator
def generate_latent_points(n):
    return np.random.normal(0, 1, (n, 1))

# Use the generator to generate fake samples
def generate_fake_samples(n):
    latent_points = generate_latent_points(n)
    X = generator.predict(latent_points)
    return X, np.zeros((n, 1))

# Simple training loop (just a few iterations for demonstration)
for epoch in range(5):
    # Train discriminator
    X_real, y_real = generate_real_samples(batch_size//2)
    X_fake, y_fake = generate_fake_samples(batch_size//2)
    X = np.vstack((X_real, X_fake))
    y = np.vstack((y_real, y_fake))
    d_loss, _ = discriminator.train_on_batch(X, y)
    
    # Train generator
    X_latent = generate_latent_points(batch_size)
    y_gan = np.ones((batch_size, 1))  # We want the generator to fool the discriminator
    g_loss = gan.train_on_batch(X_latent, y_gan)
    
    print(f"Epoch {epoch+1}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}")
```

## Summary

Mutual information and divergence measures are powerful tools for quantifying relationships between random variables and distributions:

1. **Mutual Information** I(X; Y): Measures the information shared between X and Y
   - Can be expressed as H(X) - H(X|Y) or H(X) + H(Y) - H(X, Y)
   - Equals zero if and only if X and Y are independent

2. **KL Divergence** D_KL(P || Q): Measures the difference between distributions P and Q
   - Asymmetric: D_KL(P || Q) ≠ D_KL(Q || P)
   - Equals zero if and only if P = Q

3. **Cross-Entropy** H(P, Q): Combines entropy and KL divergence
   - Commonly used as a loss function in classification
   - Equals H(P) + D_KL(P || Q)

4. **Jensen-Shannon Divergence**: A symmetric measure of distribution similarity
   - Bounded between 0 and 1
   - Square root is a metric

These concepts are fundamental in machine learning for:
- Feature selection
- Neural network training
- Clustering evaluation
- Generative modeling

## References

1. Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory (2nd ed.). Wiley-Interscience.
2. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
