# 2.3.7.2 Joint and Conditional Entropy

## Joint and Conditional Entropy

Building on the concept of entropy for a single random variable, we can extend information theory to multiple random variables. This section explores joint entropy, conditional entropy, and their relationships.

## Joint Entropy

### Definition of Joint Entropy

For two random variables X and Y with joint distribution p(x, y), the **joint entropy** H(X, Y) is defined as:

$$H(X, Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)$$

This measures the total uncertainty in the joint distribution of X and Y.

### Properties of Joint Entropy

1. **Non-negativity**: $H(X, Y) \geq 0$
2. **Upper Bound**: $H(X, Y) \leq H(X) + H(Y)$, with equality if and only if X and Y are independent
3. **Symmetry**: $H(X, Y) = H(Y, X)$

### Example: Computing Joint Entropy

```python
import numpy as np

# Define a joint probability distribution
# X: Weather (Sunny, Rainy)
# Y: Activity (Beach, Movie, Hiking)
joint_probs = np.array([
    [0.3, 0.1, 0.1],  # Sunny: Beach, Movie, Hiking
    [0.05, 0.35, 0.1]  # Rainy: Beach, Movie, Hiking
])

# Compute marginal probabilities
p_x = joint_probs.sum(axis=1)  # Probability of weather
p_y = joint_probs.sum(axis=0)  # Probability of activity

# Compute entropy of X
h_x = -np.sum(p_x * np.log2(p_x))
print(f"H(X) = {h_x:.4f} bits")

# Compute entropy of Y
h_y = -np.sum(p_y * np.log2(p_y))
print(f"H(Y) = {h_y:.4f} bits")

# Compute joint entropy
h_xy = -np.sum(joint_probs * np.log2(joint_probs + 1e-10))
print(f"H(X,Y) = {h_xy:.4f} bits")

# Check if H(X,Y) ≤ H(X) + H(Y)
print(f"H(X) + H(Y) = {h_x + h_y:.4f} bits")
print(f"Is H(X,Y) ≤ H(X) + H(Y)? {h_xy <= h_x + h_y}")
```

## Conditional Entropy

### Definition of Conditional Entropy

The **conditional entropy** H(Y|X) measures the remaining uncertainty about Y after observing X:

$$H(Y|X) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y|x)$$

Alternatively, it can be expressed as:

$$H(Y|X) = -\sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x)$$

### Properties of Conditional Entropy

1. **Non-negativity**: $H(Y|X) \geq 0$
2. **Reduction of Uncertainty**: $H(Y|X) \leq H(Y)$, with equality if and only if X and Y are independent
3. **Zero Conditional Entropy**: $H(Y|X) = 0$ if and only if Y is a deterministic function of X

### Example: Computing Conditional Entropy

```python
import numpy as np

# Using the joint distribution from the previous example
# Compute conditional probabilities
p_y_given_x = joint_probs / p_x[:, np.newaxis]

# Compute conditional entropy H(Y|X)
h_y_given_x = 0
for i in range(len(p_x)):
    h_y_given_x -= p_x[i] * np.sum(p_y_given_x[i] * np.log2(p_y_given_x[i] + 1e-10))
print(f"H(Y|X) = {h_y_given_x:.4f} bits")

# Check if H(Y|X) ≤ H(Y)
print(f"Is H(Y|X) ≤ H(Y)? {h_y_given_x <= h_y}")

# Compute conditional entropy H(X|Y)
p_x_given_y = joint_probs / p_y
p_x_given_y = p_x_given_y.T  # Transpose to match dimensions

h_x_given_y = 0
for j in range(len(p_y)):
    h_x_given_y -= p_y[j] * np.sum(p_x_given_y[j] * np.log2(p_x_given_y[j] + 1e-10))
print(f"H(X|Y) = {h_x_given_y:.4f} bits")
```

## Chain Rule of Entropy

### Definition of the Chain Rule

The joint entropy can be decomposed using the chain rule:

$$H(X, Y) = H(X) + H(Y|X)$$

This extends to multiple random variables:

$$H(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n H(X_i | X_1, X_2, \ldots, X_{i-1})$$

### Interpretation of the Chain Rule

The chain rule shows that the total uncertainty in a set of random variables can be broken down into:
1. The uncertainty in the first variable
2. The remaining uncertainty in the second variable after observing the first
3. And so on for all variables

### Example: Verifying the Chain Rule

```python
import numpy as np

# Using the previously computed entropies
print(f"H(X) + H(Y|X) = {h_x + h_y_given_x:.4f} bits")
print(f"H(X,Y) = {h_xy:.4f} bits")
print(f"Are they equal? {np.isclose(h_x + h_y_given_x, h_xy)}")

# Alternative decomposition
print(f"H(Y) + H(X|Y) = {h_y + h_x_given_y:.4f} bits")
print(f"Are they equal to H(X,Y)? {np.isclose(h_y + h_x_given_y, h_xy)}")
```

## Conditional Entropy and Decision Trees

### Information Gain

In decision trees, **information gain** measures the reduction in entropy achieved by splitting on a particular feature:

$$\text{IG}(Y, X) = H(Y) - H(Y|X)$$

where Y is the target variable and X is the feature.

### Example: Information Gain in Decision Trees

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Function to compute entropy
def entropy(y):
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

# Function to compute conditional entropy
def conditional_entropy(X_feature, y):
    # Discretize the feature into bins for simplicity
    X_binned = np.digitize(X_feature, bins=np.linspace(min(X_feature), max(X_feature), 5))
    
    # Compute conditional entropy
    cond_entropy = 0
    for bin_val in np.unique(X_binned):
        bin_indices = X_binned == bin_val
        bin_prob = np.sum(bin_indices) / len(X_feature)
        bin_entropy = entropy(y[bin_indices])
        cond_entropy += bin_prob * bin_entropy
    
    return cond_entropy

# Compute information gain for each feature
target_entropy = entropy(y)
information_gains = []

for i in range(X.shape[1]):
    cond_entropy = conditional_entropy(X[:, i], y)
    ig = target_entropy - cond_entropy
    information_gains.append(ig)
    print(f"Feature {iris.feature_names[i]}: Information Gain = {ig:.4f}")

# Compare with feature importances from a decision tree
dt = DecisionTreeClassifier(criterion='entropy')
dt.fit(X, y)
print("\nDecision Tree Feature Importances:")
for i, importance in enumerate(dt.feature_importances_):
    print(f"Feature {iris.feature_names[i]}: Importance = {importance:.4f}")
```

## Applications in Machine Learning

### 1. Feature Selection

Conditional entropy helps identify features that reduce uncertainty about the target:

```python
from sklearn.feature_selection import mutual_info_classif
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer

# Load data
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# Compute mutual information (which is related to conditional entropy)
mi_scores = mutual_info_classif(X, y)

# Create a DataFrame to display feature names and scores
feature_scores = pd.DataFrame({'Feature': cancer.feature_names, 'MI Score': mi_scores})
feature_scores = feature_scores.sort_values('MI Score', ascending=False)

print("Top 5 features by mutual information:")
print(feature_scores.head(5))
```

### 2. Clustering Evaluation

Conditional entropy can evaluate how well cluster assignments match true labels:

```python
from sklearn.metrics.cluster import normalized_mutual_info_score
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris

# Load data
iris = load_iris()
X = iris.data
true_labels = iris.target

# Perform clustering
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X)

# Evaluate using normalized mutual information
# (related to conditional entropy)
nmi = normalized_mutual_info_score(true_labels, cluster_labels)
print(f"Normalized Mutual Information: {nmi:.4f}")
```

### 3. Bayesian Networks

Conditional entropy helps in learning the structure of Bayesian networks:

```python
import numpy as np
import pandas as pd
from pgmpy.estimators import HillClimbSearch
from pgmpy.estimators import BicScore
from pgmpy.models import BayesianNetwork

# Generate synthetic data for a simple Bayesian network
np.random.seed(42)
n_samples = 1000

# A -> B -> C
a = np.random.binomial(1, 0.5, n_samples)
b = np.random.binomial(1, 0.3 + 0.5 * a, n_samples)
c = np.random.binomial(1, 0.2 + 0.6 * b, n_samples)

# Create DataFrame
data = pd.DataFrame({'A': a, 'B': b, 'C': c})

# Learn structure using Hill Climb Search and BIC score
hc = HillClimbSearch(data)
bic = BicScore(data)
best_model = hc.estimate(scoring_method=bic)

# Print the edges of the learned model
print("Learned edges:")
print(best_model.edges())

# Create and fit the Bayesian Network
model = BayesianNetwork(best_model.edges())
model.fit(data)

# Print the CPDs (Conditional Probability Distributions)
for cpd in model.get_cpds():
    print(f"\nCPD of {cpd.variable}:")
    print(cpd)
```

## Summary

Joint and conditional entropy extend the concept of entropy to multiple random variables:

1. **Joint Entropy** H(X, Y): Measures the total uncertainty in the joint distribution of X and Y
2. **Conditional Entropy** H(Y|X): Measures the remaining uncertainty about Y after observing X
3. **Chain Rule**: H(X, Y) = H(X) + H(Y|X), decomposing joint entropy into marginal and conditional components

These concepts are fundamental in machine learning for:
- Feature selection and information gain in decision trees
- Evaluating clustering algorithms
- Learning graphical models like Bayesian networks
- Understanding the relationships between variables in data

## References

1. Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory (2nd ed.). Wiley-Interscience.
2. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
3. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
