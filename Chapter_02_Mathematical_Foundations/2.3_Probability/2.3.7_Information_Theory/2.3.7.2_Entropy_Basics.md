# 2.3.7.1 Entropy Basics

## Introduction to Information Theory

Information theory is a mathematical framework for quantifying information, uncertainty, and randomness. Developed by Claude Shannon in the 1940s, it has become fundamental to many fields, including machine learning, data compression, cryptography, and communications. In this section, we'll explore the core concepts of entropy and information content.

## Entropy: Measuring Uncertainty

### Definition of Entropy

**Entropy** is the central concept in information theory, measuring the average amount of uncertainty or surprise in a random variable. For a discrete random variable X with probability mass function p(x), the entropy H(X) is defined as:

$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$$

where:
- $\mathcal{X}$ is the set of all possible values of X
- $\log$ is typically the logarithm with base 2 (giving entropy in bits), though natural logarithm (base e) is also common (giving entropy in nats)
- By convention, $0 \log 0 = 0$

### Interpretation of Entropy

Entropy can be interpreted in several ways:

1. **Uncertainty Measure**: Higher entropy indicates greater uncertainty about the outcome of a random variable
2. **Information Content**: The average number of bits needed to encode outcomes of the random variable
3. **Surprise**: The average surprise when observing outcomes (where surprise is measured as $-\log p(x)$)

### Properties of Entropy

1. **Non-negativity**: $H(X) \geq 0$, with equality if and only if X is deterministic
2. **Upper Bound**: For a discrete random variable with n possible values, $H(X) \leq \log(n)$, with equality if and only if X is uniformly distributed
3. **Concavity**: Entropy is a concave function of the probability distribution

### Example: Binary Entropy

For a binary random variable with probability p of one outcome and 1-p of the other:

$$H(p) = -p \log p - (1-p) \log (1-p)$$

This function, known as the binary entropy function, reaches its maximum value of 1 bit when p = 0.5 (maximum uncertainty) and its minimum value of 0 bits when p = 0 or p = 1 (no uncertainty).

```python
import numpy as np
import matplotlib.pyplot as plt

def binary_entropy(p):
    """Compute the binary entropy function."""
    if p == 0 or p == 1:
        return 0
    return -p * np.log2(p) - (1-p) * np.log2(1-p)

# Plot the binary entropy function
p_values = np.linspace(0.001, 0.999, 1000)
entropy_values = [binary_entropy(p) for p in p_values]

plt.figure(figsize=(10, 6))
plt.plot(p_values, entropy_values)
plt.xlabel('Probability p')
plt.ylabel('Entropy H(p) (bits)')
plt.title('Binary Entropy Function')
plt.grid(True)
plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)
plt.axvline(x=0.5, color='r', linestyle='--', alpha=0.3)
plt.show()
```

### Entropy for Continuous Random Variables

For a continuous random variable X with probability density function f(x), the **differential entropy** h(X) is defined as:

$$h(X) = -\int_{-\infty}^{\infty} f(x) \log f(x) dx$$

Unlike discrete entropy, differential entropy can be negative and doesn't have the same direct interpretation in terms of bits.

## Information Content and Self-Information

### Self-Information

The **self-information** or **surprisal** of an outcome x is defined as:

$$I(x) = -\log p(x)$$

This measures the "surprise" associated with observing x: rare events (low probability) have high information content, while common events (high probability) have low information content.

### Relationship to Entropy

Entropy is the expected value of self-information:

$$H(X) = E[I(X)] = E[-\log p(X)] = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$$

### Example: Information Content of Events

```python
import numpy as np

# Define a probability distribution
outcomes = ['A', 'B', 'C', 'D']
probabilities = [0.5, 0.25, 0.125, 0.125]

# Compute information content for each outcome
information = [-np.log2(p) for p in probabilities]

# Display results
for outcome, prob, info in zip(outcomes, probabilities, information):
    print(f"Outcome: {outcome}, Probability: {prob}, Information: {info:.2f} bits")

# Compute entropy
entropy = sum(p * i for p, i in zip(probabilities, information))
print(f"Entropy of distribution: {entropy:.2f} bits")
```

## Applications in Machine Learning

### 1. Decision Trees

Entropy is used in decision trees to measure the impurity of a set of examples and to calculate information gain when selecting the best feature to split on:

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load data
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a decision tree using entropy criterion
dt = DecisionTreeClassifier(criterion='entropy', max_depth=3)
dt.fit(X_train, y_train)

# Evaluate
accuracy = dt.score(X_test, y_test)
print(f"Decision tree accuracy: {accuracy:.4f}")
```

### 2. Feature Selection

Entropy-based measures like information gain can be used to select the most informative features:

```python
from sklearn.feature_selection import mutual_info_classif
import numpy as np

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(100, 5)  # 5 features
y = (X[:, 0] > 0.5).astype(int)  # Only first feature is relevant

# Compute mutual information between each feature and the target
mi_scores = mutual_info_classif(X, y)
print("Mutual information scores:")
for i, score in enumerate(mi_scores):
    print(f"Feature {i}: {score:.4f}")
```

### 3. Neural Network Training

Cross-entropy, which is derived from entropy, is commonly used as a loss function in classification tasks:

```python
import numpy as np

# True class probabilities (one-hot encoded)
y_true = np.array([0, 1, 0])

# Predicted class probabilities (from a model)
y_pred1 = np.array([0.1, 0.8, 0.1])  # Good prediction
y_pred2 = np.array([0.3, 0.3, 0.4])  # Poor prediction

# Compute cross-entropy loss
def cross_entropy(y_true, y_pred):
    # Add small epsilon to avoid log(0)
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)
    return -np.sum(y_true * np.log(y_pred))

loss1 = cross_entropy(y_true, y_pred1)
loss2 = cross_entropy(y_true, y_pred2)

print(f"Cross-entropy loss (good prediction): {loss1:.4f}")
print(f"Cross-entropy loss (poor prediction): {loss2:.4f}")
```

## Summary

Entropy is a fundamental concept in information theory that measures uncertainty in probability distributions:

1. **Definition**: $H(X) = -\sum_{x} p(x) \log p(x)$
2. **Interpretations**: Uncertainty measure, average information content, average surprise
3. **Properties**: Non-negative, upper-bounded by log(n), concave
4. **Self-Information**: $I(x) = -\log p(x)$ measures the surprise of individual outcomes
5. **Applications**: Decision trees, feature selection, neural network loss functions

Understanding entropy provides insights into the theoretical foundations of many machine learning algorithms and helps in designing more effective models.

## References

1. Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379-423.
2. Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory (2nd ed.). Wiley-Interscience.
3. MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
