# 2.3.3.3 Conditional Distributions

## Understanding Conditional Distributions

Conditional distributions are a fundamental concept in probability theory and machine learning. They describe the probability distribution of a random variable (or set of variables) when we have information about the values of other related random variables. In this section, we'll explore conditional distributions, their properties, and their applications in machine learning.

## Definition and Basic Concepts

### Conditional Probability Distribution

A **conditional probability distribution** specifies the probability distribution of a random variable given that another random variable takes a specific value.

For discrete random variables X and Y, the conditional probability mass function (PMF) of X given Y = y is:

$$p(x|y) = \frac{p(x, y)}{p_Y(y)}$$

provided that p_Y(y) > 0.

For continuous random variables X and Y, the conditional probability density function (PDF) of X given Y = y is:

$$f(x|y) = \frac{f(x, y)}{f_Y(y)}$$

provided that f_Y(y) > 0.

### Interpretation

The conditional distribution represents our updated beliefs about one variable after observing another variable. It answers questions like:
- What is the probability that X = x, given that we know Y = y?
- How does the distribution of X change when we learn the value of Y?

## Computing Conditional Distributions

### Discrete Case

For discrete random variables, computing the conditional distribution involves dividing the joint probability by the marginal probability of the conditioning variable.

#### Example: Dice Sum and First Die

Consider two fair dice, where X is the value of the first die and Y is the sum of both dice.

To find the conditional PMF of X given Y = 8:

$$p(x|Y=8) = \frac{p(x, Y=8)}{p_Y(8)}$$

We know:
- p(x, Y=8) = 1/36 if x is in {2, 3, 4, 5, 6} and Y - x = 8 - x is in {1, 2, 3, 4, 5, 6}
- p(x, Y=8) = 0 otherwise
- p_Y(8) = 5/36 (there are 5 ways to get a sum of 8)

So:
- p(X=2|Y=8) = (1/36)/(5/36) = 1/5
- p(X=3|Y=8) = (1/36)/(5/36) = 1/5
- p(X=4|Y=8) = (1/36)/(5/36) = 1/5
- p(X=5|Y=8) = (1/36)/(5/36) = 1/5
- p(X=6|Y=8) = (1/36)/(5/36) = 1/5

The conditional distribution is uniform over {2, 3, 4, 5, 6}.

### Continuous Case

For continuous random variables, computing the conditional distribution also involves dividing the joint density by the marginal density of the conditioning variable.

#### Example: Bivariate Normal Distribution

For a bivariate normal distribution with PDF:

$$f(x, y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma_Y^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}\right]\right)$$

The conditional distribution of X given Y = y is:

$$f(x|y) = \frac{1}{\sqrt{2\pi}\sigma_X\sqrt{1-\rho^2}} \exp\left(-\frac{(x - \mu_X - \rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y))^2}{2\sigma_X^2(1-\rho^2)}\right)$$

This is a normal distribution with:
- Mean: $\mu_{X|Y} = \mu_X + \rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y)$
- Variance: $\sigma_{X|Y}^2 = \sigma_X^2(1-\rho^2)$

## Visualizing Conditional Distributions

### Conditional Histograms

For discrete or binned data, we can visualize conditional distributions using histograms for different values of the conditioning variable:

```
    Y=1: ┌─┐ ┌─┐ ┌─┐
          │ │ │ │ │ │
          └─┘ └─┘ └─┘
             X

    Y=2: ┌─┐ ┌─┐ ┌─┐
          │ │ │ │ │ │
          └─┘ └─┘ └─┘
             X

    Y=3: ┌─┐ ┌─┐ ┌─┐
          │ │ │ │ │ │
          └─┘ └─┘ └─┘
             X
```

### Conditional Density Plots

For continuous data, we can visualize conditional distributions using density plots for different values of the conditioning variable:

```
    f(x|y)
       │
       │    y=1
       │     ╱╲
       │    ╱  ╲
       │   ╱    ╲
       │  ╱ y=2  ╲
       │ ╱   ╱╲   ╲
       │╱   ╱  ╲   ╲
       └────────────── x
```

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Generate bivariate normal data
np.random.seed(42)
n = 1000
mu = [0, 0]
sigma = [[1, 0.8], [0.8, 1]]  # Strong correlation
data = np.random.multivariate_normal(mu, sigma, size=n)

# Create a figure with conditional distributions
plt.figure(figsize=(12, 8))

# Scatter plot with conditional distributions
plt.scatter(data[:, 0], data[:, 1], alpha=0.3)

# Add conditional distributions for different y values
y_values = [-1.5, -0.5, 0.5, 1.5]
colors = ['red', 'green', 'blue', 'purple']
x_range = np.linspace(-3, 3, 1000)

for i, y_val in enumerate(y_values):
    # Compute conditional mean and variance
    cond_mean = mu[0] + sigma[0][1]/sigma[1][1] * (y_val - mu[1])
    cond_var = sigma[0][0] - sigma[0][1]**2/sigma[1][1]
    
    # Plot conditional distribution
    cond_pdf = stats.norm.pdf(x_range, loc=cond_mean, scale=np.sqrt(cond_var))
    
    # Scale the PDF for visualization
    scaled_pdf = cond_pdf * 0.5
    
    # Plot the conditional distribution at the given y value
    plt.plot(x_range, y_val + scaled_pdf, color=colors[i], 
             label=f'f(x|y={y_val})')
    
    # Add a horizontal line at the y value
    plt.axhline(y=y_val, color=colors[i], linestyle='--', alpha=0.3)

plt.xlabel('X')
plt.ylabel('Y')
plt.title('Bivariate Normal Distribution with Conditional Distributions')
plt.legend()
plt.grid(alpha=0.3)
plt.axis('equal')
plt.show()
```

## Properties of Conditional Distributions

### Relationship to Joint and Marginal Distributions

The conditional, joint, and marginal distributions are related by:

$$p(x, y) = p(x|y) \cdot p_Y(y) = p(y|x) \cdot p_X(x)$$

This relationship leads to Bayes' theorem:

$$p(x|y) = \frac{p(y|x) \cdot p_X(x)}{p_Y(y)}$$

### Conditional Expectation

The **conditional expectation** of X given Y = y, denoted E[X|Y = y], is the expected value of X under the conditional distribution:

For discrete random variables:
$$E[X|Y = y] = \sum_x x \cdot p(x|y)$$

For continuous random variables:
$$E[X|Y = y] = \int_{-\infty}^{\infty} x \cdot f(x|y) \, dx$$

### Law of Total Expectation

The law of total expectation (or law of iterated expectation) states:

$$E[X] = E[E[X|Y]]$$

That is, the expected value of X equals the expected value of the conditional expectation of X given Y.

### Conditional Variance

The **conditional variance** of X given Y = y, denoted Var(X|Y = y), is:

$$\text{Var}(X|Y = y) = E[(X - E[X|Y = y])^2|Y = y]$$

### Law of Total Variance

The law of total variance states:

$$\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])$$

This decomposes the total variance into:
1. The expected value of the conditional variance (within-group variance)
2. The variance of the conditional expectation (between-group variance)

### Independence and Conditional Distributions

If X and Y are independent, then:
- p(x|y) = p_X(x) for all x, y (discrete case)
- f(x|y) = f_X(x) for all x, y (continuous case)

That is, the conditional distribution of X given Y = y is the same as the marginal distribution of X.

## Conditional Distributions in Higher Dimensions

For more than two random variables, we can condition on multiple variables.

### Three Variables Example

For random variables X, Y, and Z:

- Conditional distribution of X given Y = y: p(x|y) or f(x|y)
- Conditional distribution of X given Y = y and Z = z: p(x|y, z) or f(x|y, z)
- Conditional distribution of (X, Y) given Z = z: p(x, y|z) or f(x, y|z)

### Conditional Independence

Random variables X and Y are **conditionally independent** given Z if:

$$p(x, y|z) = p(x|z) \cdot p(y|z)$$

for all x, y, z where p(z) > 0.

Equivalently, X and Y are conditionally independent given Z if:

$$p(x|y, z) = p(x|z)$$

for all x, y, z where p(y, z) > 0.

## Applications in Machine Learning

### 1. Supervised Learning

Conditional distributions are at the heart of supervised learning:
- **Classification**: Modeling p(class|features)
- **Regression**: Modeling the conditional distribution of the target given features
- **Prediction Intervals**: Using the conditional distribution to quantify uncertainty

### 2. Probabilistic Graphical Models

Conditional distributions are fundamental to graphical models:
- **Bayesian Networks**: Factorize joint distributions using conditional distributions
- **Markov Random Fields**: Define joint distributions using conditional distributions
- **Conditional Random Fields**: Directly model conditional distributions

### 3. Generative Models

Conditional distributions enable powerful generative models:
- **Conditional Generative Adversarial Networks (cGANs)**: Generate samples conditioned on specific inputs
- **Conditional Variational Autoencoders (cVAEs)**: Learn conditional distributions of data
- **Diffusion Models**: Generate samples by iteratively denoising from a conditional distribution

### 4. Bayesian Inference

Conditional distributions are central to Bayesian inference:
- **Posterior Distribution**: p(parameters|data) is a conditional distribution
- **Conditional Posterior**: p(parameters₁|parameters₂, data) for hierarchical models
- **Predictive Distribution**: p(new_data|data) is a conditional distribution

### 5. Causal Inference

Conditional distributions help in causal reasoning:
- **Adjustment Formula**: E[Y|do(X=x)] = ∑_z E[Y|X=x, Z=z]P(Z=z)
- **Propensity Score**: Conditioning on the probability of treatment
- **Instrumental Variables**: Using conditional distributions to estimate causal effects

## Computational Methods for Conditional Distributions

### Exact Methods

For simple cases, conditional distributions can be computed exactly:
- **Direct Computation**: Using the definition p(x|y) = p(x, y)/p_Y(y)
- **Conjugate Priors**: In Bayesian inference, using conjugate priors to get analytical posteriors
- **Variable Elimination**: Efficiently computing conditionals in graphical models

### Approximate Methods

For complex models, approximate methods are needed:
- **Conditional Monte Carlo**: Sampling from conditional distributions
- **Gibbs Sampling**: Iteratively sampling from conditional distributions
- **Variational Inference**: Approximating intractable conditionals with simpler distributions
- **Conditional Density Estimation**: Learning conditional distributions from data

### Example: Sampling from a Conditional Distribution

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Define a bivariate normal distribution
mu = [0, 0]
sigma = [[1, 0.8], [0.8, 1]]

# Function to sample from the conditional distribution X|Y=y
def sample_conditional(y_val, n_samples=1000):
    # Compute conditional mean and variance
    cond_mean = mu[0] + sigma[0][1]/sigma[1][1] * (y_val - mu[1])
    cond_var = sigma[0][0] - sigma[0][1]**2/sigma[1][1]
    
    # Sample from the conditional distribution
    samples = np.random.normal(cond_mean, np.sqrt(cond_var), n_samples)
    return samples

# Generate samples from different conditional distributions
y_values = [-2, -1, 0, 1, 2]
plt.figure(figsize=(12, 8))

for y_val in y_values:
    samples = sample_conditional(y_val)
    plt.hist(samples, bins=30, alpha=0.5, density=True, 
             label=f'X|Y={y_val}')

plt.xlabel('X')
plt.ylabel('Conditional Density')
plt.title('Samples from Conditional Distributions X|Y=y')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Challenges and Considerations

### 1. Conditioning on Rare Events

When conditioning on rare events (low probability y values):
- Estimation becomes challenging due to limited data
- Numerical issues can arise in computation
- Importance sampling or other specialized techniques may be needed

### 2. Conditional Density Estimation

Estimating conditional distributions from data is challenging:
- **Parametric Approaches**: Assume a specific form (e.g., conditional normal)
- **Non-parametric Approaches**: Kernel conditional density estimation
- **Semi-parametric Approaches**: Mixture models, copulas
- **Neural Approaches**: Neural conditional density estimators

### 3. Curse of Dimensionality

As the number of conditioning variables increases:
- Data becomes sparse in the conditioning space
- More samples are needed for accurate estimation
- Dimensionality reduction or structured models become necessary

### 4. Causality vs. Conditioning

Conditioning and causal intervention are different:
- p(y|x) ≠ p(y|do(x)) in general
- Conditioning observes x naturally occurring
- Intervention sets x regardless of its natural causes
- Causal inference requires additional assumptions beyond conditional distributions

## Summary

Conditional distributions are a fundamental concept in probability theory and machine learning:

1. **Definition**: Conditional distributions specify the probability distribution of a random variable given the values of other random variables.

2. **Computation**: For discrete variables, divide the joint probability by the marginal probability of the conditioning variable; for continuous variables, divide the joint density by the marginal density.

3. **Properties**: Conditional distributions are related to joint and marginal distributions through Bayes' theorem and lead to important concepts like conditional expectation and variance.

4. **Higher Dimensions**: Conditioning extends naturally to higher dimensions, leading to concepts like conditional independence.

5. **Applications**: Conditional distributions are crucial in supervised learning, probabilistic graphical models, generative models, Bayesian inference, and causal inference.

6. **Computational Methods**: Range from exact methods like direct computation to approximate methods like conditional Monte Carlo and variational inference.

Understanding conditional distributions is essential for modeling dependencies between variables, making predictions, and reasoning under uncertainty in machine learning.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). Cambridge University Press.
