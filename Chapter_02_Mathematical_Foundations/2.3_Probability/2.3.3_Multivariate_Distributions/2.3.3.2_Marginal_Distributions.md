# 2.3.3.2 Marginal Distributions

## Understanding Marginal Distributions

Marginal distributions are a fundamental concept in probability theory and statistics, particularly when working with multiple random variables. They allow us to focus on the distribution of a single variable (or a subset of variables) from a joint distribution, "marginalizing out" the other variables. In this section, we'll explore marginal distributions, their properties, and their applications in machine learning.

## Definition and Basic Concepts

### Marginal Distribution

A **marginal distribution** is the probability distribution of a subset of random variables from a larger set of jointly distributed random variables, obtained by integrating or summing out the other variables.

For discrete random variables X and Y with joint probability mass function (PMF) p(x, y), the marginal PMF of X is:

$$p_X(x) = \sum_y p(x, y)$$

For continuous random variables X and Y with joint probability density function (PDF) f(x, y), the marginal PDF of X is:

$$f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy$$

### Interpretation

The marginal distribution of a variable represents its probability distribution when we don't know or don't care about the values of the other variables. It answers questions like:
- What is the probability that X = x, regardless of what Y is?
- What is the distribution of X alone, ignoring Y?

## Computing Marginal Distributions

### Discrete Case

For discrete random variables, computing the marginal distribution involves summing the joint probabilities over all possible values of the variables being marginalized out.

#### Example: Dice Sum and First Die

Consider two fair dice, where X is the value of the first die and Y is the sum of both dice.

The joint PMF p(x, y) is:
- p(x, y) = 1/6 if y - x is in {1, 2, 3, 4, 5, 6}
- p(x, y) = 0 otherwise

To find the marginal PMF of X:

$$p_X(x) = \sum_y p(x, y) = \sum_{y=x+1}^{x+6} \frac{1}{6} = \frac{6}{6} = 1 \text{ for } x \in \{1, 2, 3, 4, 5, 6\}$$

So p_X(x) = 1/6 for x ∈ {1, 2, 3, 4, 5, 6}, which is the PMF of a fair die.

To find the marginal PMF of Y:

$$p_Y(y) = \sum_x p(x, y) = \sum_{x=\max(1, y-6)}^{\min(6, y-1)} \frac{1}{6}$$

This gives:
- p_Y(2) = 1/36 (only when x=1, y=2)
- p_Y(3) = 2/36 (when x=1, y=3 or x=2, y=3)
- ...
- p_Y(7) = 6/36 = 1/6 (when x=1, y=7 or x=2, y=7 or ... or x=6, y=7)
- ...
- p_Y(12) = 1/36 (only when x=6, y=12)

### Continuous Case

For continuous random variables, computing the marginal distribution involves integrating the joint density function over the variables being marginalized out.

#### Example: Bivariate Normal Distribution

For a bivariate normal distribution with PDF:

$$f(x, y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma_Y^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}\right]\right)$$

The marginal distribution of X is:

$$f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy = \frac{1}{\sqrt{2\pi}\sigma_X} \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}\right)$$

This is a normal distribution with mean μ_X and variance σ_X².

Similarly, the marginal distribution of Y is a normal distribution with mean μ_Y and variance σ_Y².

## Visualizing Marginal Distributions

### Joint and Marginal Histograms

For discrete or binned data, we can visualize the joint and marginal distributions using histograms:

```
    ┌───────────────┐ ┌───────┐
    │               │ │       │
    │    Joint      │ │   Y   │
    │  Histogram    │ │ Margin│
    │               │ │       │
    └───────────────┘ └───────┘
    ┌───────────────┐
    │      X        │
    │    Margin     │
    └───────────────┘
```

### Scatter Plots with Marginal Distributions

For continuous data, we can create scatter plots with marginal distributions on the axes:

```
    ┌───────────────┐ ┌───────┐
    │               │ │       │
    │    Scatter    │ │   Y   │
    │     Plot      │ │ Margin│
    │               │ │       │
    └───────────────┘ └───────┘
    ┌───────────────┐
    │      X        │
    │    Margin     │
    └───────────────┘
```

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Generate bivariate normal data
np.random.seed(42)
n = 1000
mu = [0, 0]
sigma = [[1, 0.5], [0.5, 1]]
data = np.random.multivariate_normal(mu, sigma, size=n)

# Create a figure with a scatter plot and marginal distributions
fig = plt.figure(figsize=(10, 10))
gs = fig.add_gridspec(2, 2, width_ratios=(7, 2), height_ratios=(2, 7),
                      left=0.1, right=0.9, bottom=0.1, top=0.9,
                      wspace=0.05, hspace=0.05)

# Scatter plot
ax_scatter = fig.add_subplot(gs[1, 0])
ax_scatter.scatter(data[:, 0], data[:, 1], alpha=0.5)
ax_scatter.set_xlabel('X')
ax_scatter.set_ylabel('Y')

# X marginal distribution
ax_x = fig.add_subplot(gs[0, 0], sharex=ax_scatter)
sns.histplot(data[:, 0], ax=ax_x, kde=True)
ax_x.set_ylabel('Density')
ax_x.tick_params(labelbottom=False)

# Y marginal distribution
ax_y = fig.add_subplot(gs[1, 1], sharey=ax_scatter)
sns.histplot(data[:, 1], ax=ax_y, kde=True, orientation='horizontal')
ax_y.set_xlabel('Density')
ax_y.tick_params(labelleft=False)

plt.suptitle('Bivariate Normal Distribution with Marginals')
plt.show()
```

## Properties of Marginal Distributions

### Relationship to Joint Distribution

The joint distribution completely determines the marginal distributions, but the reverse is not true. Different joint distributions can have the same marginals.

### Moments

The moments of marginal distributions are related to the joint moments:

- **Mean**: E[X] is the same whether computed from the joint or marginal distribution
- **Variance**: Var(X) is the same whether computed from the joint or marginal distribution
- **Higher Moments**: Similarly preserved

### Independence and Marginal Distributions

If X and Y are independent, then:
- p(x, y) = p_X(x) · p_Y(y) for all x, y (discrete case)
- f(x, y) = f_X(x) · f_Y(y) for all x, y (continuous case)

However, knowing only the marginal distributions does not tell us whether the variables are independent.

## Marginal Distributions in Higher Dimensions

For more than two random variables, we can marginalize over multiple variables.

### Three Variables Example

For discrete random variables X, Y, and Z with joint PMF p(x, y, z):

- Marginal of X: p_X(x) = ∑_y ∑_z p(x, y, z)
- Marginal of X and Y: p_{X,Y}(x, y) = ∑_z p(x, y, z)

For continuous random variables X, Y, and Z with joint PDF f(x, y, z):

- Marginal of X: f_X(x) = ∫∫ f(x, y, z) dy dz
- Marginal of X and Y: f_{X,Y}(x, y) = ∫ f(x, y, z) dz

### General Case

For a set of random variables X₁, X₂, ..., X_n with joint distribution p or f, the marginal distribution of a subset X_i, X_j, ... is obtained by summing or integrating over all other variables.

## Applications in Machine Learning

### 1. Probabilistic Graphical Models

Marginal distributions are crucial in probabilistic graphical models:
- **Inference**: Computing marginal probabilities of variables of interest
- **Learning**: Estimating model parameters from marginal observations
- **Prediction**: Making predictions based on marginal distributions

### 2. Latent Variable Models

Marginal distributions are used in latent variable models:
- **Mixture Models**: The observed data follows a marginal distribution obtained by "mixing" distributions conditioned on latent variables
- **Variational Autoencoders**: The marginal likelihood of the data is approximated using variational methods
- **Hidden Markov Models**: The marginal distribution of observed variables is computed by summing out hidden states

### 3. Bayesian Inference

Marginal distributions play a key role in Bayesian inference:
- **Marginal Likelihood**: p(data) = ∫ p(data|θ)p(θ) dθ, used for model comparison
- **Posterior Predictive Distribution**: p(new_data|data) = ∫ p(new_data|θ)p(θ|data) dθ
- **Marginalization of Nuisance Parameters**: Integrating out parameters not of direct interest

### 4. Feature Selection and Dimensionality Reduction

Marginal distributions inform feature selection:
- **Mutual Information**: Comparing joint and product of marginals to measure dependence
- **Principal Component Analysis**: Projecting data onto directions of maximum marginal variance
- **Independent Component Analysis**: Finding directions that make the marginals as independent as possible

### 5. Missing Data Handling

Marginal distributions help with missing data:
- **Marginalization**: Integrating out missing variables
- **Imputation**: Using conditional distributions derived from joint and marginal distributions
- **EM Algorithm**: Alternating between estimating missing values and updating parameters

## Computational Methods for Marginal Distributions

### Exact Methods

For simple cases, marginal distributions can be computed exactly:
- **Direct Summation/Integration**: For small discrete spaces or analytically tractable integrals
- **Variable Elimination**: Efficiently computing marginals in graphical models by eliminating variables in an optimal order
- **Belief Propagation**: Message-passing algorithm for computing marginals in tree-structured graphical models

### Approximate Methods

For complex models, approximate methods are needed:
- **Monte Carlo Sampling**: Estimating marginals by drawing samples from the joint distribution
- **Variational Inference**: Approximating intractable marginals with simpler distributions
- **Loopy Belief Propagation**: Applying belief propagation to graphs with cycles
- **Markov Chain Monte Carlo**: Sampling-based methods for high-dimensional distributions

### Example: Marginalizing with Sampling

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Define a joint distribution (bivariate normal)
mu = [0, 0]
sigma = [[1, 0.5], [0.5, 1]]

# Generate samples from the joint distribution
np.random.seed(42)
samples = np.random.multivariate_normal(mu, sigma, size=10000)

# Estimate marginal distribution of X using samples
x_samples = samples[:, 0]
plt.figure(figsize=(10, 6))
plt.hist(x_samples, bins=50, density=True, alpha=0.7, label='Sampled Marginal')

# Compare with the true marginal distribution (normal with mean 0, variance 1)
x = np.linspace(-4, 4, 1000)
plt.plot(x, stats.norm.pdf(x, loc=0, scale=1), 'r-', lw=2, label='True Marginal')

plt.xlabel('X')
plt.ylabel('Probability Density')
plt.title('Marginal Distribution of X Estimated from Samples')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Challenges and Considerations

### 1. Computational Complexity

Computing marginal distributions can be computationally intensive:
- For discrete variables with k possible values each, marginalizing over m variables requires summing k^m terms
- For continuous variables, high-dimensional integrals are often intractable

### 2. Numerical Stability

Numerical issues can arise when computing marginals:
- Underflow/overflow when multiplying many small/large probabilities
- Precision loss in floating-point arithmetic
- Solutions include working in log space and using stable algorithms

### 3. Curse of Dimensionality

As the number of dimensions increases:
- The volume of the space increases exponentially
- More samples are needed for accurate estimation
- Approximation methods become necessary

### 4. Model Misspecification

If the joint distribution is misspecified, the marginal distributions will also be incorrect:
- Robust estimation methods can help
- Model validation is crucial
- Sensitivity analysis can assess the impact of assumptions

## Summary

Marginal distributions are a fundamental concept in probability theory and machine learning:

1. **Definition**: Marginal distributions are obtained by summing or integrating the joint distribution over the variables not of interest.

2. **Computation**: For discrete variables, sum over the variables to marginalize; for continuous variables, integrate over them.

3. **Properties**: Marginals preserve moments but don't fully determine the joint distribution.

4. **Higher Dimensions**: Marginalization extends naturally to higher dimensions by summing or integrating over multiple variables.

5. **Applications**: Marginal distributions are crucial in probabilistic graphical models, latent variable models, Bayesian inference, feature selection, and missing data handling.

6. **Computational Methods**: Range from exact methods like direct summation/integration to approximate methods like sampling and variational inference.

Understanding marginal distributions is essential for working with multivariate data, making inferences about individual variables, and building probabilistic models in machine learning.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.
