# 2.3.2.4.2 Continuous Distributions (Part 1)

## Common Continuous Probability Distributions

Continuous probability distributions model random variables that can take any value within a continuous range. These distributions are fundamental in machine learning for modeling real-valued data such as heights, weights, times, and many other measurements. In this section, we'll explore the most important continuous probability distributions, their properties, and applications in machine learning.

## Uniform Distribution

### Definition

The **uniform distribution** models a random variable that is equally likely to take any value within a given interval [a, b].

A random variable X follows a uniform distribution on [a, b], denoted as X ~ Uniform(a, b), if its probability density is constant over the interval.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{1}{b-a} & \text{for } a \leq x \leq b \\
0 & \text{otherwise}
\end{cases}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \begin{cases}
0 & \text{for } x < a \\
\frac{x-a}{b-a} & \text{for } a \leq x \leq b \\
1 & \text{for } x > b
\end{cases}$$

### Properties

- **Mean**: E[X] = (a + b) / 2
- **Variance**: Var(X) = (b - a)² / 12
- **Moment Generating Function**: M_X(t) = (e^{tb} - e^{ta}) / (t(b-a)) for t ≠ 0
- **Entropy**: H(X) = ln(b - a)

### Visualization

```
  f(x)
   │
   │
1/(b-a)│ ┌───────────────┐
   │ │               │
   │ │               │
   └─┴───────────────┴─── x
     a               b
```

### Applications in Machine Learning

1. **Random Initialization**: Initializing weights and parameters
2. **Noise Generation**: Adding uniform noise for regularization
3. **Quantization Error**: Modeling rounding errors
4. **Bayesian Priors**: Non-informative priors for parameters
5. **Exploration**: Uniform exploration in reinforcement learning

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Uniform distribution on [0, 5]
a, b = 0, 5
x = np.linspace(-1, 6, 1000)
pdf = stats.uniform.pdf(x, loc=a, scale=b-a)
cdf = stats.uniform.cdf(x, loc=a, scale=b-a)

# Plot PDF
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(x, pdf)
plt.fill_between(x, pdf, where=((x >= a) & (x <= b)), alpha=0.3)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title(f'Uniform PDF on [{a}, {b}]')
plt.grid(alpha=0.3)

# Plot CDF
plt.subplot(1, 2, 2)
plt.plot(x, cdf)
plt.xlabel('x')
plt.ylabel('Cumulative Probability')
plt.title(f'Uniform CDF on [{a}, {b}]')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Expected value and variance
print(f"Expected value: {(a + b) / 2}")
print(f"Variance: {(b - a)**2 / 12}")
```

## Normal (Gaussian) Distribution

### Definition

The **normal distribution** (also called Gaussian distribution) is one of the most important probability distributions in statistics and machine learning. It models phenomena influenced by many small, independent random factors.

A random variable X follows a normal distribution with parameters μ (mean) and σ² (variance), denoted as X ~ N(μ, σ²), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \text{ for } x \in \mathbb{R}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{x-\mu}{\sigma\sqrt{2}}\right)\right]$$

where erf is the error function.

### Properties

- **Mean**: E[X] = μ
- **Variance**: Var(X) = σ²
- **Moment Generating Function**: M_X(t) = exp(μt + σ²t²/2)
- **Symmetry**: The distribution is symmetric around the mean
- **Standard Normal**: When μ = 0 and σ = 1, we have the standard normal distribution, often denoted as Z ~ N(0, 1)
- **Linear Transformation**: If X ~ N(μ, σ²), then aX + b ~ N(aμ + b, a²σ²)
- **Sum of Normals**: If X ~ N(μ₁, σ₁²) and Y ~ N(μ₂, σ₂²) are independent, then X + Y ~ N(μ₁ + μ₂, σ₁² + σ₂²)

### Visualization

```
  f(x)
   │
   │      ╱╲
   │     ╱  ╲
   │    ╱    ╲
   │   ╱      ╲
   │  ╱        ╲
   │ ╱          ╲
   │╱            ╲
   └───────────────── x
     μ-3σ  μ  μ+3σ
```

### The 68-95-99.7 Rule

For a normal distribution:
- About 68% of values lie within 1 standard deviation of the mean
- About 95% of values lie within 2 standard deviations of the mean
- About 99.7% of values lie within 3 standard deviations of the mean

### Applications in Machine Learning

1. **Feature Distributions**: Modeling naturally occurring measurements
2. **Error Terms**: Modeling noise and measurement errors
3. **Bayesian Inference**: Prior and posterior distributions
4. **Neural Networks**: Weight initialization and output distributions
5. **Central Limit Theorem**: Approximating distributions of sums of random variables
6. **Gaussian Processes**: Modeling functions with uncertainty

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Normal distribution with μ = 0, σ = 1
mu, sigma = 0, 1
x = np.linspace(-4, 4, 1000)
pdf = stats.norm.pdf(x, loc=mu, scale=sigma)
cdf = stats.norm.cdf(x, loc=mu, scale=sigma)

# Plot PDF
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(x, pdf)
plt.fill_between(x, pdf, alpha=0.3)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title(f'Normal PDF (μ={mu}, σ={sigma})')
plt.grid(alpha=0.3)

# Plot CDF
plt.subplot(1, 2, 2)
plt.plot(x, cdf)
plt.xlabel('x')
plt.ylabel('Cumulative Probability')
plt.title(f'Normal CDF (μ={mu}, σ={sigma})')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Expected value and variance
print(f"Expected value: {mu}")
print(f"Variance: {sigma**2}")

# Illustrate the 68-95-99.7 rule
x_fill_1 = np.linspace(-1, 1, 1000)
x_fill_2 = np.linspace(-2, 2, 1000)
x_fill_3 = np.linspace(-3, 3, 1000)

plt.figure(figsize=(10, 6))
plt.plot(x, pdf, 'k-', lw=2)
plt.fill_between(x_fill_1, stats.norm.pdf(x_fill_1), color='blue', alpha=0.3, label='68%')
plt.fill_between(x_fill_2, stats.norm.pdf(x_fill_2), color='green', alpha=0.3, label='95%')
plt.fill_between(x_fill_3, stats.norm.pdf(x_fill_3), color='red', alpha=0.3, label='99.7%')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('The 68-95-99.7 Rule for Normal Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Exponential Distribution

### Definition

The **exponential distribution** models the time between events in a Poisson process, where events occur continuously and independently at a constant average rate.

A random variable X follows an exponential distribution with parameter λ (lambda), denoted as X ~ Exp(λ), if it represents the time until the next event in a Poisson process with rate λ.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\lambda e^{-\lambda x} & \text{for } x \geq 0 \\
0 & \text{for } x < 0
\end{cases}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \begin{cases}
1 - e^{-\lambda x} & \text{for } x \geq 0 \\
0 & \text{for } x < 0
\end{cases}$$

### Properties

- **Mean**: E[X] = 1/λ
- **Variance**: Var(X) = 1/λ²
- **Moment Generating Function**: M_X(t) = λ/(λ-t) for t < λ
- **Memoryless Property**: P(X > s+t | X > s) = P(X > t)
- **Relationship to Poisson**: If events occur according to a Poisson process with rate λ, then the time between consecutive events follows Exp(λ)

### Visualization

```
  f(x)
   │
 λ │\
   │ \
   │  \
   │   \
   │    \
   │     \
   │      \
   └───────\─────── x
     0      \
```

### Applications in Machine Learning

1. **Survival Analysis**: Modeling time until an event (failure, death, etc.)
2. **Reliability Engineering**: Component lifetime modeling
3. **Queueing Theory**: Service time and inter-arrival time modeling
4. **Reinforcement Learning**: Modeling time between state transitions
5. **Bayesian Inference**: Prior distributions for rate parameters

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Exponential distribution with λ = 0.5
lam = 0.5
x = np.linspace(0, 10, 1000)
pdf = stats.expon.pdf(x, scale=1/lam)  # scipy uses scale = 1/λ
cdf = stats.expon.cdf(x, scale=1/lam)

# Plot PDF
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(x, pdf)
plt.fill_between(x, pdf, alpha=0.3)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title(f'Exponential PDF (λ={lam})')
plt.grid(alpha=0.3)

# Plot CDF
plt.subplot(1, 2, 2)
plt.plot(x, cdf)
plt.xlabel('x')
plt.ylabel('Cumulative Probability')
plt.title(f'Exponential CDF (λ={lam})')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Expected value and variance
print(f"Expected value: {1/lam}")
print(f"Variance: {1/lam**2}")

# Illustrate the memoryless property
s = 2  # Given that X > 2
t_values = np.linspace(0, 5, 100)
p_x_gt_t = np.exp(-lam * t_values)  # P(X > t)
p_x_gt_s_plus_t_given_x_gt_s = np.exp(-lam * t_values)  # P(X > s+t | X > s) = P(X > t)

plt.figure(figsize=(8, 6))
plt.plot(t_values, p_x_gt_t, 'b-', label='P(X > t)')
plt.plot(t_values, p_x_gt_s_plus_t_given_x_gt_s, 'r--', label='P(X > s+t | X > s)')
plt.xlabel('t')
plt.ylabel('Probability')
plt.title('Memoryless Property of Exponential Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Gamma Distribution

### Definition

The **gamma distribution** is a flexible distribution for modeling positive continuous random variables. It generalizes the exponential distribution and is related to the chi-squared distribution.

A random variable X follows a gamma distribution with shape parameter α (alpha) and rate parameter β (beta), denoted as X ~ Gamma(α, β), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} & \text{for } x > 0 \\
0 & \text{for } x \leq 0
\end{cases}$$

where Γ(α) is the gamma function.

Note: Some sources use a scale parameter θ = 1/β instead of the rate parameter β.

### Cumulative Distribution Function (CDF)

The CDF doesn't have a simple closed form but can be expressed in terms of the incomplete gamma function:

$$F(x) = \frac{\gamma(\alpha, \beta x)}{\Gamma(\alpha)} \text{ for } x \geq 0$$

where γ(α, x) is the lower incomplete gamma function.

### Properties

- **Mean**: E[X] = α/β
- **Variance**: Var(X) = α/β²
- **Moment Generating Function**: M_X(t) = (1 - t/β)^(-α) for t < β
- **Special Cases**:
  - When α = 1, the gamma distribution reduces to the exponential distribution with rate β
  - When α = n/2 and β = 1/2, the gamma distribution is equivalent to the chi-squared distribution with n degrees of freedom
- **Sum of Gammas**: If X ~ Gamma(α₁, β) and Y ~ Gamma(α₂, β) are independent, then X + Y ~ Gamma(α₁ + α₂, β)

### Visualization

```
  f(x)
   │
   │     ╱╲
   │    ╱  ╲
   │   ╱    ╲
   │  ╱      ╲
   │ ╱        ╲
   │╱          ╲
   └─────────────── x
     0
```

The shape varies significantly with different parameters:
- α < 1: The PDF is infinite at x = 0 and decreases monotonically
- α = 1: The PDF starts at β at x = 0 (exponential distribution)
- α > 1: The PDF starts at 0, rises to a mode at (α-1)/β, then decreases

### Applications in Machine Learning

1. **Bayesian Inference**: Prior distributions for precision parameters
2. **Reliability Analysis**: Modeling time to failure with wear-out
3. **Queueing Theory**: Service time distributions
4. **Rainfall Modeling**: Amount of precipitation
5. **Income Modeling**: Distribution of income and wealth
6. **Reinforcement Learning**: Modeling reward distributions

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Gamma distributions with different shape parameters
x = np.linspace(0, 10, 1000)
alphas = [0.5, 1.0, 2.0, 5.0]
beta = 1.0

plt.figure(figsize=(10, 6))
for alpha in alphas:
    pdf = stats.gamma.pdf(x, a=alpha, scale=1/beta)  # scipy uses scale = 1/β
    plt.plot(x, pdf, label=f'α={alpha}, β={beta}')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Gamma Distribution PDF for Different Shape Parameters')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance for α=2, β=1
alpha, beta = 2.0, 1.0
print(f"Expected value: {alpha/beta}")
print(f"Variance: {alpha/beta**2}")

# Relationship to exponential distribution
x = np.linspace(0, 5, 1000)
pdf_gamma = stats.gamma.pdf(x, a=1.0, scale=1/beta)  # Gamma with α=1, β=1
pdf_expon = stats.expon.pdf(x, scale=1/beta)  # Exponential with λ=1

plt.figure(figsize=(8, 6))
plt.plot(x, pdf_gamma, 'b-', label='Gamma(α=1, β=1)')
plt.plot(x, pdf_expon, 'r--', label='Exponential(λ=1)')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Gamma Distribution with α=1 is Equivalent to Exponential Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Beta Distribution

### Definition

The **beta distribution** is a flexible distribution for modeling random variables constrained to the interval [0, 1], such as probabilities, proportions, and rates.

A random variable X follows a beta distribution with shape parameters α (alpha) and β (beta), denoted as X ~ Beta(α, β), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)} & \text{for } 0 \leq x \leq 1 \\
0 & \text{otherwise}
\end{cases}$$

where B(α, β) is the beta function, defined as:

$$B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$$

### Cumulative Distribution Function (CDF)

The CDF is given by the regularized incomplete beta function:

$$F(x) = I_x(\alpha, \beta) \text{ for } 0 \leq x \leq 1$$

where I_x(α, β) is the regularized incomplete beta function.

### Properties

- **Mean**: E[X] = α/(α+β)
- **Variance**: Var(X) = αβ/((α+β)²(α+β+1))
- **Mode**: (α-1)/(α+β-2) for α, β > 1
- **Special Cases**:
  - Beta(1, 1) is the uniform distribution on [0, 1]
  - Beta(α, β) with α = β is symmetric around 0.5
  - As α, β → ∞ with α/(α+β) fixed, the beta distribution approaches a normal distribution
- **Conjugate Prior**: The beta distribution is the conjugate prior for the Bernoulli, binomial, and geometric distributions

### Visualization

```
  f(x)
   │
   │     ╱╲
   │    ╱  ╲
   │   ╱    ╲
   │  ╱      ╲
   │ ╱        ╲
   │╱          ╲
   └─────────────── x
     0         1
```

The shape varies significantly with different parameters:
- α, β < 1: U-shaped, with peaks at 0 and 1
- α, β = 1: Uniform distribution
- α, β > 1: Unimodal
- α = β: Symmetric around 0.5
- α > β: Skewed to the right
- α < β: Skewed to the left

### Applications in Machine Learning

1. **Bayesian Inference**: Prior and posterior distributions for probabilities
2. **A/B Testing**: Modeling conversion rates
3. **Reinforcement Learning**: Modeling success probabilities
4. **Bounded Variables**: Any variable constrained to [0, 1]
5. **Mixture Models**: Component weights
6. **Bayesian Neural Networks**: Modeling uncertainty in predictions

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Beta distributions with different parameters
x = np.linspace(0, 1, 1000)
params = [(0.5, 0.5), (1, 1), (2, 2), (5, 1), (1, 3), (2, 5)]

plt.figure(figsize=(12, 8))
for i, (alpha, beta) in enumerate(params):
    pdf = stats.beta.pdf(x, alpha, beta)
    plt.subplot(2, 3, i+1)
    plt.plot(x, pdf)
    plt.fill_between(x, pdf, alpha=0.3)
    plt.xlabel('x')
    plt.ylabel('Probability Density')
    plt.title(f'Beta(α={alpha}, β={beta})')
    plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Expected value and variance for α=2, β=5
alpha, beta = 2, 5
mean = alpha / (alpha + beta)
var = (alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1))
print(f"Expected value: {mean}")
print(f"Variance: {var}")

# Bayesian updating example
# Prior: Beta(1, 1) (uniform)
# Data: 7 successes, 3 failures
# Posterior: Beta(1+7, 1+3) = Beta(8, 4)

x = np.linspace(0, 1, 1000)
prior = stats.beta.pdf(x, 1, 1)
posterior = stats.beta.pdf(x, 8, 4)

plt.figure(figsize=(10, 6))
plt.plot(x, prior, 'b--', label='Prior: Beta(1, 1)')
plt.plot(x, posterior, 'r-', label='Posterior: Beta(8, 4)')
plt.xlabel('Probability of Success')
plt.ylabel('Probability Density')
plt.title('Bayesian Updating with Beta Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Summary (Part 1)

Continuous probability distributions are essential tools in machine learning for modeling real-valued data. In this first part, we've explored four fundamental continuous distributions:

1. **Uniform Distribution**: Models random variables equally likely to take any value in an interval
   - Applications: Random initialization, noise generation, non-informative priors

2. **Normal (Gaussian) Distribution**: Models phenomena influenced by many small, independent factors
   - Applications: Natural measurements, error terms, Bayesian inference, neural networks

3. **Exponential Distribution**: Models the time between events in a Poisson process
   - Applications: Survival analysis, reliability engineering, queueing theory

4. **Gamma Distribution**: A flexible distribution for positive continuous random variables
   - Applications: Bayesian inference, reliability analysis, service times

5. **Beta Distribution**: Models random variables constrained to the interval [0, 1]
   - Applications: Probabilities, proportions, Bayesian inference, A/B testing

Understanding these distributions and their properties is crucial for:
- Choosing appropriate models for different types of data
- Performing Bayesian inference
- Quantifying uncertainty in predictions
- Designing effective machine learning algorithms

In the next part, we'll explore additional continuous distributions important in machine learning, including the lognormal, Student's t, chi-squared, and multivariate normal distributions.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
