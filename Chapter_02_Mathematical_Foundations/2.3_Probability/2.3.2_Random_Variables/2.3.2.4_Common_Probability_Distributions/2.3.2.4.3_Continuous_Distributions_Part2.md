# 2.3.2.4.3 Continuous Distributions (Part 2)

## More Continuous Probability Distributions

In this section, we continue our exploration of important continuous probability distributions used in machine learning. We'll cover several additional distributions that are particularly relevant for modeling various phenomena and for statistical inference in machine learning applications.

## Lognormal Distribution

### Definition

The **lognormal distribution** models random variables whose logarithm follows a normal distribution. It's useful for modeling quantities that are the product of many small independent factors.

A random variable X follows a lognormal distribution with parameters μ and σ², denoted as X ~ LogNormal(μ, σ²), if ln(X) ~ N(μ, σ²).

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}} & \text{for } x > 0 \\
0 & \text{for } x \leq 0
\end{cases}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \begin{cases}
\frac{1}{2} + \frac{1}{2}\text{erf}\left(\frac{\ln x - \mu}{\sigma\sqrt{2}}\right) & \text{for } x > 0 \\
0 & \text{for } x \leq 0
\end{cases}$$

where erf is the error function.

### Properties

- **Mean**: E[X] = e^{μ + σ²/2}
- **Variance**: Var(X) = (e^{σ²} - 1)e^{2μ + σ²}
- **Mode**: e^{μ - σ²}
- **Median**: e^μ
- **Relationship to Normal**: If X ~ LogNormal(μ, σ²), then ln(X) ~ N(μ, σ²)
- **Multiplicative Property**: If X ~ LogNormal(μ₁, σ₁²) and Y ~ LogNormal(μ₂, σ₂²) are independent, then XY ~ LogNormal(μ₁ + μ₂, σ₁² + σ₂²)

### Visualization

```
  f(x)
   │
   │    ╱╲
   │   ╱  ╲
   │  ╱    ╲
   │ ╱      ╲
   │╱        ╲
   └──────────────── x
     0
```

The lognormal distribution is always right-skewed, with the degree of skewness increasing with σ.

### Applications in Machine Learning

1. **Income and Wealth Modeling**: Distribution of incomes, asset prices
2. **Survival Analysis**: Time-to-event data with multiplicative effects
3. **Biological Growth**: Population sizes, organism growth
4. **Network Effects**: Quantities affected by multiplicative processes
5. **Finance**: Stock prices, returns over long periods
6. **Bayesian Neural Networks**: Modeling uncertainty in positive parameters

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Lognormal distributions with different parameters
x = np.linspace(0, 10, 1000)
params = [(0, 0.25), (0, 0.5), (0, 1), (1, 0.5)]

plt.figure(figsize=(10, 6))
for mu, sigma in params:
    pdf = stats.lognorm.pdf(x, s=sigma, scale=np.exp(mu))  # scipy parameterization
    plt.plot(x, pdf, label=f'μ={mu}, σ={sigma}')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Lognormal Distribution PDF')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
mu, sigma = 0, 0.5
mean = np.exp(mu + sigma**2/2)
var = (np.exp(sigma**2) - 1) * np.exp(2*mu + sigma**2)
print(f"Expected value: {mean}")
print(f"Variance: {var}")

# Relationship to normal distribution
np.random.seed(42)
lognormal_samples = np.random.lognormal(mean=mu, sigma=sigma, size=1000)
log_samples = np.log(lognormal_samples)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(lognormal_samples, bins=30, density=True, alpha=0.7)
x_ln = np.linspace(0, max(lognormal_samples), 1000)
plt.plot(x_ln, stats.lognorm.pdf(x_ln, s=sigma, scale=np.exp(mu)), 'r-', lw=2)
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Lognormal Distribution')

plt.subplot(1, 2, 2)
plt.hist(log_samples, bins=30, density=True, alpha=0.7)
x_n = np.linspace(min(log_samples), max(log_samples), 1000)
plt.plot(x_n, stats.norm.pdf(x_n, loc=mu, scale=sigma), 'r-', lw=2)
plt.xlabel('ln(x)')
plt.ylabel('Density')
plt.title('Normal Distribution of ln(X)')

plt.tight_layout()
plt.show()
```

## Student's t-Distribution

### Definition

The **Student's t-distribution** (or simply t-distribution) models the distribution of the sample mean when the population variance is unknown and estimated from the data. It's particularly important for statistical inference with small sample sizes.

A random variable X follows a t-distribution with ν (nu) degrees of freedom, denoted as X ~ t(ν), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}} \text{ for } x \in \mathbb{R}$$

where Γ is the gamma function.

### Cumulative Distribution Function (CDF)

The CDF doesn't have a simple closed form but can be expressed in terms of the incomplete beta function.

### Properties

- **Mean**: E[X] = 0 for ν > 1 (undefined for ν ≤ 1)
- **Variance**: Var(X) = ν/(ν-2) for ν > 2 (undefined for ν ≤ 2)
- **Symmetry**: The distribution is symmetric around 0
- **Limiting Behavior**: As ν → ∞, the t-distribution approaches the standard normal distribution
- **Heavy Tails**: The t-distribution has heavier tails than the normal distribution, especially for small ν

### Visualization

```
  f(x)
   │
   │      ╱╲
   │     ╱  ╲
   │    ╱    ╲
   │   ╱      ╲
   │  ╱        ╲
   │ ╱          ╲
   └───────────────── x
     -3    0    3
```

The t-distribution looks similar to the standard normal distribution but has heavier tails. As ν increases, it approaches the standard normal distribution.

### Applications in Machine Learning

1. **Statistical Inference**: Confidence intervals and hypothesis tests
2. **Robust Regression**: Less sensitive to outliers than normal distribution
3. **Bayesian Inference**: Prior distributions for regression coefficients
4. **Small Sample Sizes**: More appropriate than normal when sample size is small
5. **Bayesian Neural Networks**: Modeling uncertainty in parameters
6. **Robust Modeling**: Handling data with outliers

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# t-distributions with different degrees of freedom
x = np.linspace(-5, 5, 1000)
dfs = [1, 2, 5, 30, np.inf]  # np.inf represents the normal distribution

plt.figure(figsize=(10, 6))
for df in dfs:
    if df == np.inf:
        pdf = stats.norm.pdf(x)
        label = 'Normal'
    else:
        pdf = stats.t.pdf(x, df)
        label = f't(ν={df})'
    plt.plot(x, pdf, label=label)

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('t-Distribution PDF for Different Degrees of Freedom')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
df = 5
# Mean is 0 for df > 1
var = df / (df - 2) if df > 2 else np.inf
print(f"Expected value: 0")
print(f"Variance: {var}")

# Compare tails of t and normal distributions
x_tail = np.linspace(2, 5, 1000)
pdf_t = stats.t.pdf(x_tail, df=3)
pdf_norm = stats.norm.pdf(x_tail)

plt.figure(figsize=(8, 6))
plt.semilogy(x_tail, pdf_t, 'r-', label='t(ν=3)')
plt.semilogy(x_tail, pdf_norm, 'b--', label='Normal')
plt.xlabel('x')
plt.ylabel('Probability Density (log scale)')
plt.title('Comparing Tails of t and Normal Distributions')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Chi-Squared Distribution

### Definition

The **chi-squared distribution** models the sum of squares of independent standard normal random variables. It's particularly important for hypothesis testing and confidence interval construction.

A random variable X follows a chi-squared distribution with k degrees of freedom, denoted as X ~ χ²(k), if X = Z₁² + Z₂² + ... + Z_k², where Z₁, Z₂, ..., Z_k are independent standard normal random variables.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1} e^{-x/2} & \text{for } x > 0 \\
0 & \text{for } x \leq 0
\end{cases}$$

where Γ is the gamma function.

### Cumulative Distribution Function (CDF)

The CDF can be expressed in terms of the incomplete gamma function:

$$F(x) = \frac{\gamma(k/2, x/2)}{\Gamma(k/2)} \text{ for } x \geq 0$$

where γ(s, x) is the lower incomplete gamma function.

### Properties

- **Mean**: E[X] = k
- **Variance**: Var(X) = 2k
- **Relationship to Gamma**: X ~ χ²(k) is equivalent to X ~ Gamma(k/2, 1/2)
- **Sum of Chi-Squared**: If X ~ χ²(k₁) and Y ~ χ²(k₂) are independent, then X + Y ~ χ²(k₁ + k₂)
- **Limiting Behavior**: For large k, the distribution approaches a normal distribution with mean k and variance 2k

### Visualization

```
  f(x)
   │
   │     ╱╲
   │    ╱  ╲
   │   ╱    ╲
   │  ╱      ╲
   │ ╱        ╲
   │╱          ╲
   └─────────────── x
     0
```

The shape varies with the degrees of freedom:
- k = 1: The PDF is infinite at x = 0 and decreases monotonically
- k = 2: The PDF starts at 0.5 at x = 0 and decreases monotonically
- k > 2: The PDF starts at 0, rises to a mode at k-2, then decreases

### Applications in Machine Learning

1. **Hypothesis Testing**: Goodness-of-fit tests, independence tests
2. **Feature Selection**: Chi-squared test for feature importance
3. **Confidence Intervals**: For variance estimation
4. **Model Evaluation**: Likelihood ratio tests
5. **Principal Component Analysis**: Testing significance of components
6. **Mahalanobis Distance**: For outlier detection and classification

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Chi-squared distributions with different degrees of freedom
x = np.linspace(0, 20, 1000)
dfs = [1, 2, 3, 5, 10]

plt.figure(figsize=(10, 6))
for df in dfs:
    pdf = stats.chi2.pdf(x, df)
    plt.plot(x, pdf, label=f'k={df}')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Chi-Squared Distribution PDF')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
k = 5
print(f"Expected value: {k}")
print(f"Variance: {2*k}")

# Relationship to gamma distribution
x = np.linspace(0, 15, 1000)
pdf_chi2 = stats.chi2.pdf(x, k)
pdf_gamma = stats.gamma.pdf(x, a=k/2, scale=2)  # Gamma(k/2, 2)

plt.figure(figsize=(8, 6))
plt.plot(x, pdf_chi2, 'r-', label=f'χ²({k})')
plt.plot(x, pdf_gamma, 'b--', label=f'Gamma({k/2}, 2)')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Chi-Squared and Equivalent Gamma Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## F-Distribution

### Definition

The **F-distribution** models the ratio of two chi-squared random variables, each divided by its degrees of freedom. It's particularly important for analysis of variance (ANOVA) and comparing variances.

A random variable X follows an F-distribution with d₁ and d₂ degrees of freedom, denoted as X ~ F(d₁, d₂), if X = (U/d₁)/(V/d₂), where U ~ χ²(d₁) and V ~ χ²(d₂) are independent chi-squared random variables.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{\sqrt{\frac{(d_1 x)^{d_1} \cdot d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x \cdot \text{B}(\frac{d_1}{2}, \frac{d_2}{2})} & \text{for } x > 0 \\
0 & \text{for } x \leq 0
\end{cases}$$

where B is the beta function.

### Properties

- **Mean**: E[X] = d₂/(d₂-2) for d₂ > 2 (undefined for d₂ ≤ 2)
- **Variance**: Var(X) = 2d₂²(d₁+d₂-2)/(d₁(d₂-2)²(d₂-4)) for d₂ > 4 (undefined for d₂ ≤ 4)
- **Relationship to Beta**: If X ~ F(d₁, d₂), then d₁X/(d₁X + d₂) ~ Beta(d₁/2, d₂/2)
- **Reciprocal Property**: If X ~ F(d₁, d₂), then 1/X ~ F(d₂, d₁)

### Applications in Machine Learning

1. **Analysis of Variance (ANOVA)**: Comparing means of multiple groups
2. **Feature Selection**: F-tests for feature importance
3. **Model Comparison**: Testing nested models
4. **Variance Component Analysis**: In mixed-effects models
5. **Regression Analysis**: Testing significance of regression coefficients

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# F-distributions with different degrees of freedom
x = np.linspace(0, 5, 1000)
params = [(1, 1), (2, 1), (5, 2), (10, 5), (100, 100)]

plt.figure(figsize=(10, 6))
for d1, d2 in params:
    pdf = stats.f.pdf(x, d1, d2)
    plt.plot(x, pdf, label=f'F({d1}, {d2})')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('F-Distribution PDF')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
d1, d2 = 10, 5
mean = d2 / (d2 - 2) if d2 > 2 else np.inf
var = 2 * d2**2 * (d1 + d2 - 2) / (d1 * (d2 - 2)**2 * (d2 - 4)) if d2 > 4 else np.inf
print(f"Expected value: {mean}")
print(f"Variance: {var}")
```

## Cauchy Distribution

### Definition

The **Cauchy distribution** (also known as the Lorentz distribution) is a continuous distribution with extremely heavy tails. It's notable for not having a defined mean or variance.

A random variable X follows a Cauchy distribution with location parameter x₀ and scale parameter γ, denoted as X ~ Cauchy(x₀, γ), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \frac{1}{\pi\gamma\left[1 + \left(\frac{x-x_0}{\gamma}\right)^2\right]} \text{ for } x \in \mathbb{R}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{x-x_0}{\gamma}\right)$$

### Properties

- **Median and Mode**: x₀
- **No Moments**: The mean, variance, and higher moments are undefined
- **Stability**: The sum of independent Cauchy random variables is also Cauchy
- **Heavy Tails**: The tails decay as 1/x², much slower than the exponential decay of the normal distribution
- **Special Case**: The standard Cauchy distribution (x₀ = 0, γ = 1) is the t-distribution with 1 degree of freedom

### Visualization

```
  f(x)
   │
   │      ╱╲
   │     ╱  ╲
   │    ╱    ╲
   │   ╱      ╲
   │  ╱        ╲
   │ ╱          ╲
   └───────────────── x
     x₀-3γ x₀ x₀+3γ
```

The Cauchy distribution looks similar to the normal distribution but has much heavier tails.

### Applications in Machine Learning

1. **Robust Statistics**: Models for data with extreme outliers
2. **Physics Models**: Resonance phenomena, spectral lines
3. **Financial Modeling**: Asset returns with extreme events
4. **Bayesian Inference**: Prior distributions for location parameters
5. **Robust Regression**: Cauchy-based loss functions

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Cauchy distributions with different parameters
x = np.linspace(-10, 10, 1000)
params = [(0, 0.5), (0, 1), (0, 2), (2, 1)]

plt.figure(figsize=(10, 6))
for x0, gamma in params:
    pdf = stats.cauchy.pdf(x, loc=x0, scale=gamma)
    plt.plot(x, pdf, label=f'x₀={x0}, γ={gamma}')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Cauchy Distribution PDF')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Compare tails of Cauchy and normal distributions
x_tail = np.linspace(3, 10, 1000)
pdf_cauchy = stats.cauchy.pdf(x_tail, loc=0, scale=1)
pdf_norm = stats.norm.pdf(x_tail)

plt.figure(figsize=(8, 6))
plt.semilogy(x_tail, pdf_cauchy, 'r-', label='Cauchy(0, 1)')
plt.semilogy(x_tail, pdf_norm, 'b--', label='Normal(0, 1)')
plt.xlabel('x')
plt.ylabel('Probability Density (log scale)')
plt.title('Comparing Tails of Cauchy and Normal Distributions')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Demonstrate the lack of a well-defined mean
np.random.seed(42)
n_samples = 1000
cauchy_samples = stats.cauchy.rvs(loc=0, scale=1, size=n_samples)
sample_means = np.cumsum(cauchy_samples) / np.arange(1, n_samples + 1)

plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, n_samples + 1), sample_means)
plt.xlabel('Number of Samples')
plt.ylabel('Sample Mean')
plt.title('Sample Mean of Cauchy Distribution Does Not Converge')
plt.grid(alpha=0.3)
plt.show()
```

## Weibull Distribution

### Definition

The **Weibull distribution** is a flexible distribution for modeling the time until failure or the lifetime of objects. It's widely used in reliability engineering and survival analysis.

A random variable X follows a Weibull distribution with shape parameter k and scale parameter λ, denoted as X ~ Weibull(k, λ), if its probability density function is given by the formula below.

### Probability Density Function (PDF)

$$f(x) = \begin{cases}
\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}e^{-(x/\lambda)^k} & \text{for } x \geq 0 \\
0 & \text{for } x < 0
\end{cases}$$

### Cumulative Distribution Function (CDF)

$$F(x) = \begin{cases}
1 - e^{-(x/\lambda)^k} & \text{for } x \geq 0 \\
0 & \text{for } x < 0
\end{cases}$$

### Properties

- **Mean**: E[X] = λΓ(1 + 1/k)
- **Variance**: Var(X) = λ²[Γ(1 + 2/k) - (Γ(1 + 1/k))²]
- **Special Cases**:
  - k = 1: The Weibull distribution reduces to the exponential distribution with rate 1/λ
  - k = 2: The Weibull distribution becomes the Rayleigh distribution
  - k = 3.4: The Weibull distribution approximates the normal distribution
- **Hazard Rate**: The hazard rate (failure rate) is proportional to x^(k-1)

### Visualization

```
  f(x)
   │
   │     ╱╲
   │    ╱  ╲
   │   ╱    ╲
   │  ╱      ╲
   │ ╱        ╲
   │╱          ╲
   └─────────────── x
     0
```

The shape varies significantly with the shape parameter k:
- k < 1: The PDF is infinite at x = 0 and decreases monotonically (decreasing failure rate)
- k = 1: The PDF starts at 1/λ at x = 0 and decreases monotonically (constant failure rate)
- k > 1: The PDF starts at 0, rises to a mode, then decreases (increasing failure rate)

### Applications in Machine Learning

1. **Survival Analysis**: Modeling time until an event occurs
2. **Reliability Engineering**: Component lifetime modeling
3. **Extreme Value Theory**: Modeling maxima of distributions
4. **Natural Phenomena**: Wind speed, particle size distributions
5. **Bayesian Inference**: Prior distributions for scale parameters

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Weibull distributions with different shape parameters
x = np.linspace(0, 3, 1000)
ks = [0.5, 1.0, 1.5, 2.0, 3.5]
lam = 1.0

plt.figure(figsize=(10, 6))
for k in ks:
    pdf = stats.weibull_min.pdf(x, k, scale=lam)  # scipy parameterization
    plt.plot(x, pdf, label=f'k={k}, λ={lam}')

plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Weibull Distribution PDF')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
k, lam = 2.0, 1.0
mean = lam * np.math.gamma(1 + 1/k)
var = lam**2 * (np.math.gamma(1 + 2/k) - np.math.gamma(1 + 1/k)**2)
print(f"Expected value: {mean}")
print(f"Variance: {var}")

# Relationship to exponential distribution
x = np.linspace(0, 3, 1000)
pdf_weibull = stats.weibull_min.pdf(x, 1.0, scale=lam)
pdf_expon = stats.expon.pdf(x, scale=lam)

plt.figure(figsize=(8, 6))
plt.plot(x, pdf_weibull, 'r-', label=f'Weibull(k=1, λ={lam})')
plt.plot(x, pdf_expon, 'b--', label=f'Exponential(λ={lam})')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Weibull with k=1 is Equivalent to Exponential Distribution')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

## Summary (Part 2)

Continuous probability distributions are essential tools in machine learning for modeling various phenomena. In this second part, we've explored several additional important continuous distributions:

1. **Lognormal Distribution**: Models random variables whose logarithm follows a normal distribution
   - Applications: Income modeling, biological growth, finance

2. **Student's t-Distribution**: Models the distribution of the sample mean with unknown population variance
   - Applications: Statistical inference, robust regression, Bayesian neural networks

3. **Chi-Squared Distribution**: Models the sum of squares of independent standard normal random variables
   - Applications: Hypothesis testing, feature selection, model evaluation

4. **F-Distribution**: Models the ratio of two chi-squared random variables
   - Applications: ANOVA, feature selection, model comparison

5. **Cauchy Distribution**: A heavy-tailed distribution with undefined mean and variance
   - Applications: Robust statistics, physics models, financial modeling

6. **Weibull Distribution**: A flexible distribution for modeling lifetimes and failure times
   - Applications: Survival analysis, reliability engineering, extreme value theory

Understanding these distributions and their properties is crucial for:
- Choosing appropriate models for different types of data
- Performing statistical inference and hypothesis testing
- Quantifying uncertainty in predictions
- Building robust machine learning models

Together with the distributions covered in Part 1, these form a comprehensive toolkit for probabilistic modeling in machine learning.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
