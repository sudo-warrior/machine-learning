# 2.3.2.4.1 Discrete Distributions

## Common Discrete Probability Distributions

Discrete probability distributions model random variables that can take on a countable number of distinct values. These distributions are essential in machine learning for modeling count data, categorical outcomes, and many other discrete phenomena. In this section, we'll explore the most common discrete probability distributions, their properties, and applications in machine learning.

## Bernoulli Distribution

### Definition

The **Bernoulli distribution** is the simplest discrete probability distribution, modeling a single binary outcome (success/failure).

A random variable X follows a Bernoulli distribution with parameter p if:
- X = 1 with probability p (success)
- X = 0 with probability 1-p (failure)

### Probability Mass Function (PMF)

$$p(x) = p^x (1-p)^{1-x} \text{ for } x \in \{0, 1\}$$

### Properties

- **Mean (Expected Value)**: E[X] = p
- **Variance**: Var(X) = p(1-p)
- **Moment Generating Function**: M_X(t) = (1-p) + pe^t

### Visualization

```
  p(x)
   │
1-p│ ┌─┐
   │ │ │
   │ │ │
   │ │ │         ┌─┐
 p │ │ │         │ │
   │ │ │         │ │
   └─┴─┴─────────┴─┴─── x
     0           1
```

### Applications in Machine Learning

1. **Binary Classification**: Modeling the probability of class membership
2. **Bernoulli Trials**: Basis for more complex distributions
3. **Binary Outcomes**: Click/no-click, purchase/no-purchase, etc.
4. **Stochastic Neurons**: In neural networks with binary activations

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Bernoulli distribution with p = 0.3
p = 0.3
x = np.array([0, 1])
pmf = stats.bernoulli.pmf(x, p)

plt.figure(figsize=(8, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks([0, 1])
plt.xlabel('x')
plt.ylabel('Probability')
plt.title(f'Bernoulli Distribution (p={p})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
print(f"Expected value: {p}")
print(f"Variance: {p * (1-p)}")
```

## Binomial Distribution

### Definition

The **binomial distribution** models the number of successes in n independent Bernoulli trials, each with success probability p.

A random variable X follows a binomial distribution with parameters n and p if it represents the number of successes in n independent trials, where each trial has a success probability p.

### Probability Mass Function (PMF)

$$p(x) = \binom{n}{x} p^x (1-p)^{n-x} \text{ for } x \in \{0, 1, 2, ..., n\}$$

where $\binom{n}{x} = \frac{n!}{x!(n-x)!}$ is the binomial coefficient.

### Properties

- **Mean**: E[X] = np
- **Variance**: Var(X) = np(1-p)
- **Moment Generating Function**: M_X(t) = (1-p+pe^t)^n
- **Relationship to Bernoulli**: A binomial distribution with n=1 is a Bernoulli distribution

### Visualization

For n = 10, p = 0.3:

```
  p(x)
   │
0.3│
   │      ┌─┐
0.2│      │ │
   │ ┌─┐  │ │
0.1│ │ │  │ │  ┌─┐
   │ │ │  │ │  │ │  ┌─┐
   └─┴─┴──┴─┴──┴─┴──┴─┴─────── x
     0  1  2  3  4  5  6  7  8  9  10
```

### Applications in Machine Learning

1. **A/B Testing**: Modeling the number of conversions in n trials
2. **Quality Control**: Number of defects in a batch
3. **Natural Language Processing**: Word occurrence models
4. **Ensemble Methods**: Modeling votes in random forests

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Binomial distribution with n = 10, p = 0.3
n, p = 10, 0.3
x = np.arange(0, n+1)
pmf = stats.binom.pmf(x, n, p)

plt.figure(figsize=(10, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('Number of Successes (x)')
plt.ylabel('Probability')
plt.title(f'Binomial Distribution (n={n}, p={p})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
print(f"Expected value: {n * p}")
print(f"Variance: {n * p * (1-p)}")
```

## Poisson Distribution

### Definition

The **Poisson distribution** models the number of events occurring in a fixed interval of time or space, assuming these events occur independently at a constant average rate.

A random variable X follows a Poisson distribution with parameter λ (lambda) if it represents the number of events in a fixed interval, where λ is the average number of events in that interval.

### Probability Mass Function (PMF)

$$p(x) = \frac{\lambda^x e^{-\lambda}}{x!} \text{ for } x \in \{0, 1, 2, ...\}$$

### Properties

- **Mean**: E[X] = λ
- **Variance**: Var(X) = λ
- **Moment Generating Function**: M_X(t) = exp(λ(e^t - 1))
- **Sum of Poisson**: If X ~ Poisson(λ₁) and Y ~ Poisson(λ₂) are independent, then X + Y ~ Poisson(λ₁ + λ₂)

### Visualization

For λ = 3:

```
  p(x)
   │
0.2│      ┌─┐
   │      │ │  ┌─┐
0.1│ ┌─┐  │ │  │ │  ┌─┐
   │ │ │  │ │  │ │  │ │  ┌─┐
   └─┴─┴──┴─┴──┴─┴──┴─┴──┴─┴─────── x
     0  1  2  3  4  5  6  7  8  9
```

### Applications in Machine Learning

1. **Event Modeling**: Customer arrivals, website visits, etc.
2. **Rare Event Detection**: Anomaly and fraud detection
3. **Text Analysis**: Word frequency in documents
4. **Network Traffic**: Modeling packet arrivals
5. **Recommender Systems**: Modeling user interactions

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Poisson distribution with lambda = 3
lam = 3
x = np.arange(0, 15)
pmf = stats.poisson.pmf(x, lam)

plt.figure(figsize=(10, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('Number of Events (x)')
plt.ylabel('Probability')
plt.title(f'Poisson Distribution (λ={lam})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
print(f"Expected value: {lam}")
print(f"Variance: {lam}")
```

## Geometric Distribution

### Definition

The **geometric distribution** models the number of trials needed to get the first success in a sequence of independent Bernoulli trials.

A random variable X follows a geometric distribution with parameter p if it represents the number of trials until the first success, where each trial has a success probability p.

### Probability Mass Function (PMF)

There are two common parameterizations:

1. X = number of trials until first success:
   $$p(x) = (1-p)^{x-1} p \text{ for } x \in \{1, 2, 3, ...\}$$

2. X = number of failures before first success:
   $$p(x) = (1-p)^x p \text{ for } x \in \{0, 1, 2, ...\}$$

We'll use the first parameterization here.

### Properties

- **Mean**: E[X] = 1/p
- **Variance**: Var(X) = (1-p)/p²
- **Moment Generating Function**: M_X(t) = pe^t/(1-(1-p)e^t) for t < -ln(1-p)
- **Memoryless Property**: P(X > m+n | X > m) = P(X > n)

### Visualization

For p = 0.3:

```
  p(x)
   │
0.3│ ┌─┐
   │ │ │
0.2│ │ │
   │ │ │  ┌─┐
0.1│ │ │  │ │  ┌─┐
   │ │ │  │ │  │ │  ┌─┐  ┌─┐
   └─┴─┴──┴─┴──┴─┴──┴─┴──┴─┴─────── x
     1  2  3  4  5  6  7  8  9
```

### Applications in Machine Learning

1. **Waiting Time Models**: Time until first click, purchase, etc.
2. **Reliability Analysis**: Time until system failure
3. **Reinforcement Learning**: Modeling time until success
4. **A/B Testing**: Trials until first conversion

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Geometric distribution with p = 0.3
p = 0.3
x = np.arange(1, 15)
pmf = stats.geom.pmf(x, p)  # Note: scipy uses the second parameterization

plt.figure(figsize=(10, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('Number of Trials Until First Success (x)')
plt.ylabel('Probability')
plt.title(f'Geometric Distribution (p={p})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
print(f"Expected value: {1/p}")
print(f"Variance: {(1-p)/p**2}")
```

## Negative Binomial Distribution

### Definition

The **negative binomial distribution** models the number of trials needed to achieve r successes in a sequence of independent Bernoulli trials.

A random variable X follows a negative binomial distribution with parameters r and p if it represents the number of trials needed to get r successes, where each trial has a success probability p.

### Probability Mass Function (PMF)

There are two common parameterizations:

1. X = number of trials to get r successes:
   $$p(x) = \binom{x-1}{r-1} p^r (1-p)^{x-r} \text{ for } x \in \{r, r+1, r+2, ...\}$$

2. X = number of failures before r successes:
   $$p(x) = \binom{x+r-1}{x} p^r (1-p)^x \text{ for } x \in \{0, 1, 2, ...\}$$

We'll use the first parameterization here.

### Properties

- **Mean**: E[X] = r/p
- **Variance**: Var(X) = r(1-p)/p²
- **Relationship to Geometric**: When r=1, the negative binomial reduces to the geometric distribution
- **Sum of Geometrics**: If X₁, X₂, ..., X_r are independent geometric random variables with parameter p, then their sum follows a negative binomial distribution with parameters r and p

### Visualization

For r = 3, p = 0.3:

```
  p(x)
   │
0.2│
   │
0.1│         ┌─┐
   │      ┌─┐│ │┌─┐
   │   ┌─┐│ ││ ││ │┌─┐
   │   │ ││ ││ ││ ││ │┌─┐
   └───┴─┴┴─┴┴─┴┴─┴┴─┴┴─┴─────── x
       3  4  5  6  7  8  9 10
```

### Applications in Machine Learning

1. **Overdispersed Count Data**: When variance exceeds the mean
2. **Risk Modeling**: Number of events until a target is reached
3. **A/B Testing**: Trials until r conversions
4. **Reliability Engineering**: Modeling system failures

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Negative binomial distribution with r = 3, p = 0.3
r, p = 3, 0.3
x = np.arange(r, r+15)
# Note: scipy uses the second parameterization, so we adjust
pmf = stats.nbinom.pmf(x-r, r, p)

plt.figure(figsize=(10, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('Number of Trials to Get r Successes (x)')
plt.ylabel('Probability')
plt.title(f'Negative Binomial Distribution (r={r}, p={p})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
print(f"Expected value: {r/p}")
print(f"Variance: {r*(1-p)/p**2}")
```

## Categorical (Multinoulli) Distribution

### Definition

The **categorical distribution** (also called multinoulli) is a generalization of the Bernoulli distribution to multiple categories. It models a single trial with k possible outcomes.

A random variable X follows a categorical distribution with parameters p₁, p₂, ..., p_k if:
- X = i with probability p_i for i ∈ {1, 2, ..., k}
- Σ p_i = 1

### Probability Mass Function (PMF)

$$p(x) = \prod_{i=1}^k p_i^{[x=i]} \text{ for } x \in \{1, 2, ..., k\}$$

where [x=i] is the Iverson bracket (1 if x=i, 0 otherwise).

Alternatively, using one-hot encoding with x_i = 1 if the outcome is category i and 0 otherwise:

$$p(x_1, x_2, ..., x_k) = \prod_{i=1}^k p_i^{x_i}$$

### Properties

- **Mean**: E[X_i] = p_i (for one-hot encoding)
- **Variance**: Var(X_i) = p_i(1-p_i) (for one-hot encoding)
- **Covariance**: Cov(X_i, X_j) = -p_i p_j for i ≠ j (for one-hot encoding)

### Visualization

For k = 4, p = [0.1, 0.4, 0.2, 0.3]:

```
  p(x)
   │
0.4│      ┌─┐
   │      │ │
0.3│      │ │         ┌─┐
   │      │ │         │ │
0.2│      │ │    ┌─┐  │ │
   │      │ │    │ │  │ │
0.1│ ┌─┐  │ │    │ │  │ │
   │ │ │  │ │    │ │  │ │
   └─┴─┴──┴─┴────┴─┴──┴─┴─── x
     1    2      3    4
```

### Applications in Machine Learning

1. **Multi-class Classification**: Modeling class probabilities
2. **Natural Language Processing**: Word choice models
3. **Recommender Systems**: Item selection models
4. **Reinforcement Learning**: Action selection in discrete action spaces

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Categorical distribution with p = [0.1, 0.4, 0.2, 0.3]
p = np.array([0.1, 0.4, 0.2, 0.3])
categories = np.arange(1, len(p)+1)

plt.figure(figsize=(8, 6))
plt.bar(categories, p, width=0.4)
plt.xticks(categories)
plt.xlabel('Category (x)')
plt.ylabel('Probability')
plt.title('Categorical Distribution')
plt.grid(alpha=0.3)
plt.show()

# Expected value (for one-hot encoding)
print(f"Expected values (one-hot): {p}")
```

## Multinomial Distribution

### Definition

The **multinomial distribution** is a generalization of the binomial distribution to multiple categories. It models the counts of each outcome in n independent trials, where each trial has k possible outcomes.

A random variable X = (X₁, X₂, ..., X_k) follows a multinomial distribution with parameters n and p = (p₁, p₂, ..., p_k) if it represents the counts of each of the k categories in n independent trials, where each trial results in category i with probability p_i.

### Probability Mass Function (PMF)

$$p(x_1, x_2, ..., x_k) = \frac{n!}{x_1! x_2! \cdots x_k!} \prod_{i=1}^k p_i^{x_i}$$

where x_i ≥ 0, Σ x_i = n, and Σ p_i = 1.

### Properties

- **Mean**: E[X_i] = n·p_i
- **Variance**: Var(X_i) = n·p_i·(1-p_i)
- **Covariance**: Cov(X_i, X_j) = -n·p_i·p_j for i ≠ j
- **Relationship to Binomial**: When k=2, the multinomial reduces to the binomial distribution

### Applications in Machine Learning

1. **Text Analysis**: Word count models
2. **Topic Modeling**: Document-topic distributions
3. **Multi-class Classification**: Modeling class counts
4. **Market Basket Analysis**: Item co-occurrence models

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Multinomial distribution with n = 10, p = [0.1, 0.4, 0.2, 0.3]
n = 10
p = np.array([0.1, 0.4, 0.2, 0.3])
categories = np.arange(1, len(p)+1)

# Generate some random samples
np.random.seed(42)
samples = np.random.multinomial(n, p, size=1000)

# Calculate the average counts
avg_counts = np.mean(samples, axis=0)

# Plot the expected vs. observed counts
plt.figure(figsize=(10, 6))
width = 0.35
plt.bar(categories - width/2, n*p, width, label='Expected')
plt.bar(categories + width/2, avg_counts, width, label='Observed (1000 samples)')
plt.xticks(categories)
plt.xlabel('Category')
plt.ylabel('Count')
plt.title(f'Multinomial Distribution (n={n})')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Expected values and variances
print(f"Expected values: {n * p}")
print(f"Variances: {n * p * (1-p)}")
```

## Hypergeometric Distribution

### Definition

The **hypergeometric distribution** models sampling without replacement from a finite population. It gives the probability of getting k successes in n draws from a population of size N containing K success states.

### Probability Mass Function (PMF)

$$p(x) = \frac{\binom{K}{x} \binom{N-K}{n-x}}{\binom{N}{n}}$$

for max(0, n+K-N) ≤ x ≤ min(n, K).

### Properties

- **Mean**: E[X] = n·(K/N)
- **Variance**: Var(X) = n·(K/N)·(1-K/N)·(N-n)/(N-1)
- **Relationship to Binomial**: As N → ∞ with K/N = p fixed, the hypergeometric distribution approaches the binomial distribution

### Applications in Machine Learning

1. **Sampling Without Replacement**: Survey sampling, audit sampling
2. **Quality Control**: Acceptance sampling plans
3. **Anomaly Detection**: Rare category detection
4. **Educational Testing**: Test question selection

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Hypergeometric distribution with N=50, K=20, n=10
N, K, n = 50, 20, 10
x = np.arange(0, min(n, K) + 1)
pmf = stats.hypergeom.pmf(x, N, K, n)

plt.figure(figsize=(10, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('Number of Successes (x)')
plt.ylabel('Probability')
plt.title(f'Hypergeometric Distribution (N={N}, K={K}, n={n})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
mean = n * (K/N)
var = n * (K/N) * (1-K/N) * (N-n)/(N-1)
print(f"Expected value: {mean}")
print(f"Variance: {var}")
```

## Discrete Uniform Distribution

### Definition

The **discrete uniform distribution** models a random variable that is equally likely to take any of k distinct values.

A random variable X follows a discrete uniform distribution on {a, a+1, ..., b} if:
- P(X = x) = 1/(b-a+1) for x ∈ {a, a+1, ..., b}

### Probability Mass Function (PMF)

$$p(x) = \frac{1}{b-a+1} \text{ for } x \in \{a, a+1, ..., b\}$$

### Properties

- **Mean**: E[X] = (a+b)/2
- **Variance**: Var(X) = ((b-a+1)²-1)/12
- **Moment Generating Function**: M_X(t) = (e^{at} - e^{(b+1)t})/(1-e^t)/(b-a+1)

### Visualization

For a = 1, b = 6 (standard die):

```
  p(x)
   │
1/6│ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐
   │ │ │ │ │ │ │ │ │ │ │ │ │
   │ │ │ │ │ │ │ │ │ │ │ │ │
   └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─── x
     1   2   3   4   5   6
```

### Applications in Machine Learning

1. **Random Sampling**: Uniform selection from a discrete set
2. **Initialization**: Random initialization in algorithms
3. **Simulation**: Modeling fair dice or other uniform processes
4. **Exploration**: Uniform exploration in reinforcement learning

### Example in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Discrete uniform distribution on {1, 2, 3, 4, 5, 6}
a, b = 1, 6
x = np.arange(a, b+1)
pmf = stats.randint.pmf(x, a, b+1)  # scipy uses [low, high) convention

plt.figure(figsize=(8, 6))
plt.bar(x, pmf, width=0.4)
plt.xticks(x)
plt.xlabel('x')
plt.ylabel('Probability')
plt.title(f'Discrete Uniform Distribution ({a} to {b})')
plt.grid(alpha=0.3)
plt.show()

# Expected value and variance
mean = (a + b) / 2
var = ((b - a + 1)**2 - 1) / 12
print(f"Expected value: {mean}")
print(f"Variance: {var}")
```

## Summary

Discrete probability distributions are essential tools in machine learning for modeling count data, categorical outcomes, and other discrete phenomena:

1. **Bernoulli Distribution**: Models a single binary outcome (success/failure)
2. **Binomial Distribution**: Models the number of successes in n independent trials
3. **Poisson Distribution**: Models the number of events in a fixed interval
4. **Geometric Distribution**: Models the number of trials until the first success
5. **Negative Binomial Distribution**: Models the number of trials until r successes
6. **Categorical Distribution**: Models a single trial with multiple possible outcomes
7. **Multinomial Distribution**: Models the counts of multiple categories in n trials
8. **Hypergeometric Distribution**: Models sampling without replacement
9. **Discrete Uniform Distribution**: Models equally likely outcomes

Understanding these distributions and their properties is crucial for:
- Choosing appropriate models for different types of data
- Designing effective machine learning algorithms
- Performing statistical inference
- Quantifying uncertainty in predictions

In the next section, we'll explore continuous probability distributions, which model random variables that can take any value in a continuous range.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Ross, S. M. (2014). Introduction to Probability Models (11th ed.). Academic Press.
