# 2.3.2.1 Discrete Random Variables

## Understanding Discrete Random Variables

Discrete random variables are a fundamental concept in probability theory and machine learning. They allow us to mathematically model and analyze processes with countable outcomes, such as the number of emails received in a day, the result of a dice roll, or the class label in a classification problem. In this section, we'll explore discrete random variables, their properties, and their applications in machine learning.

## Definition and Characteristics

### Definition

A **discrete random variable** is a function that maps outcomes from a sample space to numerical values, where the set of possible values is countable (finite or countably infinite).

Formally, if Ω is the sample space of a random experiment, a discrete random variable X is a function X: Ω → S, where S is a countable set of real numbers.

### Examples of Discrete Random Variables

1. **Bernoulli Random Variable**: Models a binary outcome (success/failure)
   - X = 1 if success, X = 0 if failure
   - Example: Whether a customer clicks on an ad (1 = click, 0 = no click)

2. **Binomial Random Variable**: Counts the number of successes in n independent Bernoulli trials
   - Example: Number of heads in 10 coin flips

3. **Poisson Random Variable**: Counts the number of events occurring in a fixed interval
   - Example: Number of customers arriving at a store in an hour

4. **Geometric Random Variable**: Counts the number of trials until the first success
   - Example: Number of attempts until a machine learning model correctly classifies a difficult example

5. **Categorical Random Variable**: Takes one of k different values (categories)
   - Example: The digit (0-9) in a handwritten digit recognition task

## Probability Mass Function (PMF)

### Definition

The **probability mass function (PMF)** of a discrete random variable X, denoted as p_X(x) or simply p(x), gives the probability that X takes the value x:

$$p_X(x) = P(X = x)$$

### Properties of a Valid PMF

1. **Non-negativity**: p(x) ≥ 0 for all x
2. **Normalization**: Σ p(x) = 1, summing over all possible values of x
3. **Range**: 0 ≤ p(x) ≤ 1 for all x

### Example: Dice Roll

For a fair six-sided die, the PMF is:
- p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1/6
- p(x) = 0 for all other values of x

This can be visualized as:

```
  p(x)
   │
   │
1/6│ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐ ┌─┐
   │ │ │ │ │ │ │ │ │ │ │ │ │
   │ │ │ │ │ │ │ │ │ │ │ │ │
   └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─── x
     1   2   3   4   5   6
```

### Example: Binomial Distribution

For a binomial random variable X ~ Bin(n, p) representing the number of successes in n trials with success probability p, the PMF is:

$$p(x) = \binom{n}{x} p^x (1-p)^{n-x} \text{ for } x \in \{0, 1, 2, ..., n\}$$

For n = 5 and p = 0.3:

```
  p(x)
   │
0.4│
   │
0.3│      ┌─┐
   │      │ │
0.2│ ┌─┐  │ │
   │ │ │  │ │  ┌─┐
0.1│ │ │  │ │  │ │  ┌─┐
   │ │ │  │ │  │ │  │ │  ┌─┐
   └─┴─┴──┴─┴──┴─┴──┴─┴──┴─┴─── x
     0    1    2    3    4    5
```

## Cumulative Distribution Function (CDF)

### Definition

The **cumulative distribution function (CDF)** of a discrete random variable X, denoted as F_X(x) or F(x), gives the probability that X is less than or equal to x:

$$F_X(x) = P(X \leq x) = \sum_{t \leq x} p_X(t)$$

### Properties of a CDF

1. **Non-decreasing**: If x₁ < x₂, then F(x₁) ≤ F(x₂)
2. **Right-continuous**: lim F(x + h) = F(x) as h → 0⁺
3. **Limits**: lim F(x) = 0 as x → -∞, and lim F(x) = 1 as x → ∞

### Example: Dice Roll CDF

For a fair six-sided die:
- F(0) = 0
- F(1) = 1/6
- F(2) = 2/6
- F(3) = 3/6
- F(4) = 4/6
- F(5) = 5/6
- F(6) = 1
- F(x) = 0 for x < 1
- F(x) = 1 for x > 6
- F(x) = ⌊x⌋/6 for 1 ≤ x ≤ 6, where ⌊x⌋ is the floor function

This can be visualized as:

```
  F(x)
   │                           ┌───────
 1 │                      ┌────┘
   │                 ┌────┘
5/6│            ┌────┘
   │       ┌────┘
4/6│  ┌────┘
   │  │
3/6│  │
   │  │
2/6│  │
   │  │
1/6│  │
   │  │
 0 └──┴───────────────────────────── x
     1   2   3   4   5   6
```

## Expected Value and Variance

### Expected Value (Mean)

The **expected value** or **mean** of a discrete random variable X, denoted as E[X] or μ, is the weighted average of all possible values, where the weights are given by the PMF:

$$E[X] = \sum_x x \cdot p(x)$$

Intuitively, it represents the "center of mass" of the probability distribution.

### Variance

The **variance** of a discrete random variable X, denoted as Var(X) or σ², measures the spread or dispersion of the distribution around the mean:

$$\text{Var}(X) = E[(X - E[X])^2] = \sum_x (x - E[X])^2 \cdot p(x)$$

An alternative formula for computing variance is:

$$\text{Var}(X) = E[X^2] - (E[X])^2 = \sum_x x^2 \cdot p(x) - \left(\sum_x x \cdot p(x)\right)^2$$

### Standard Deviation

The **standard deviation** of X, denoted as σ, is the square root of the variance:

$$\sigma = \sqrt{\text{Var}(X)}$$

The standard deviation has the same units as the random variable, making it more interpretable than variance.

### Example: Expected Value and Variance of a Dice Roll

For a fair six-sided die:
- E[X] = (1 + 2 + 3 + 4 + 5 + 6) / 6 = 21 / 6 = 3.5
- E[X²] = (1² + 2² + 3² + 4² + 5² + 6²) / 6 = 91 / 6 ≈ 15.17
- Var(X) = E[X²] - (E[X])² = 15.17 - 3.5² = 15.17 - 12.25 = 2.92
- σ = √2.92 ≈ 1.71

## Common Discrete Distributions

### 1. Bernoulli Distribution

Models a single binary outcome (success/failure).

- PMF: p(x) = p^x * (1-p)^(1-x) for x ∈ {0, 1}
- Mean: p
- Variance: p(1-p)

**Applications:**
- Modeling binary outcomes (yes/no, success/failure)
- Binary classification in machine learning

### 2. Binomial Distribution

Models the number of successes in n independent Bernoulli trials.

- PMF: p(x) = (n choose x) * p^x * (1-p)^(n-x) for x ∈ {0, 1, 2, ..., n}
- Mean: np
- Variance: np(1-p)

**Applications:**
- A/B testing
- Quality control
- Modeling count data with fixed number of trials

### 3. Poisson Distribution

Models the number of events occurring in a fixed interval of time or space.

- PMF: p(x) = (λ^x * e^(-λ)) / x! for x ∈ {0, 1, 2, ...}
- Mean: λ
- Variance: λ

**Applications:**
- Modeling rare events
- Arrival processes
- Count data in machine learning

### 4. Geometric Distribution

Models the number of trials until the first success.

- PMF: p(x) = (1-p)^(x-1) * p for x ∈ {1, 2, 3, ...}
- Mean: 1/p
- Variance: (1-p)/p²

**Applications:**
- Modeling waiting times until success
- Reliability testing
- Retry mechanisms

### 5. Negative Binomial Distribution

Models the number of trials until r successes.

- PMF: p(x) = (x-1 choose r-1) * p^r * (1-p)^(x-r) for x ∈ {r, r+1, r+2, ...}
- Mean: r/p
- Variance: r(1-p)/p²

**Applications:**
- Modeling count data with overdispersion
- Risk analysis
- Reliability engineering

### 6. Categorical (Multinomial with n=1) Distribution

Models a single trial with k possible outcomes.

- PMF: p(x) = p_x for x ∈ {1, 2, ..., k}, where p_x is the probability of outcome x
- Constraint: Σ p_x = 1

**Applications:**
- Classification problems
- Modeling categorical data
- Natural language processing

### 7. Hypergeometric Distribution

Models sampling without replacement from a finite population.

- PMF: p(x) = (K choose x)(N-K choose n-x)/(N choose n)
  - N = population size
  - K = number of success states in the population
  - n = number of draws
  - x = number of observed successes
- Mean: n(K/N)
- Variance: n(K/N)(1-K/N)(N-n)/(N-1)

**Applications:**
- Quality control sampling
- Audit sampling
- Card game probabilities

## Functions of Discrete Random Variables

### Transformation of Random Variables

If Y = g(X) is a function of a discrete random variable X, the PMF of Y is:

$$p_Y(y) = \sum_{x: g(x) = y} p_X(x)$$

### Example: Squared Dice Roll

If X represents a fair dice roll and Y = X², then:
- p_Y(1) = p_X(1) = 1/6
- p_Y(4) = p_X(2) = 1/6
- p_Y(9) = p_X(3) = 1/6
- p_Y(16) = p_X(4) = 1/6
- p_Y(25) = p_X(5) = 1/6
- p_Y(36) = p_X(6) = 1/6

### Expected Value of a Function

For a function g(X) of a discrete random variable X:

$$E[g(X)] = \sum_x g(x) \cdot p_X(x)$$

### Linearity of Expectation

For random variables X and Y, and constants a and b:

$$E[aX + bY] = aE[X] + bE[Y]$$

This property holds even if X and Y are not independent.

## Discrete Random Variables in Machine Learning

### 1. Classification Problems

In classification, the target variable Y is a discrete random variable:
- Binary classification: Y follows a Bernoulli distribution
- Multi-class classification: Y follows a categorical distribution

The goal is to model P(Y|X), the conditional probability of the class given the features.

### 2. Count Data Modeling

Many machine learning problems involve count data:
- Number of user interactions with a website
- Number of purchases
- Number of defects in manufacturing

These are often modeled using discrete distributions like Poisson or negative binomial.

### 3. Probabilistic Graphical Models

Discrete random variables are fundamental in probabilistic graphical models:
- Bayesian networks
- Markov random fields
- Hidden Markov models

### 4. Sampling and Monte Carlo Methods

Discrete random variables are used in sampling methods:
- Rejection sampling
- Importance sampling
- Gibbs sampling

### 5. Reinforcement Learning

In reinforcement learning:
- Actions can be modeled as discrete random variables
- Policies define probability distributions over actions
- State transitions can be discrete

## Visualizing Discrete Random Variables

### Probability Mass Function (PMF) Plot

```
  p(x)
   │
   │    ┌─┐
   │    │ │
   │ ┌─┐│ │┌─┐
   │ │ ││ ││ │
   └─┴─┴┴─┴┴─┴─── x
     1 2 3 4 5
```

### Cumulative Distribution Function (CDF) Plot

```
  F(x)
   │               ┌───────
 1 │          ┌────┘
   │     ┌────┘
   │┌────┘
   ││
 0 └┴───────────────── x
     1 2 3 4 5
```

### Bar Charts

```
  Frequency
     │
     │    ┌─┐
     │    │ │
     │ ┌─┐│ │┌─┐
     │ │ ││ ││ │
     └─┴─┴┴─┴┴─┴─── Value
       1 2 3 4 5
```

### Stem-and-Leaf Plots

```
  0 | 1 2 3 5 7 9
  1 | 0 2 5 8
  2 | 3 7
```

## Practical Considerations

### 1. Handling Sparse Data

When working with discrete distributions with many possible values:
- Smoothing techniques can help with sparse data
- Laplace (add-one) smoothing
- Good-Turing estimation

### 2. Computational Efficiency

For large discrete spaces:
- Use sparse representations
- Consider approximations for large factorials
- Use logarithms for numerical stability

### 3. Discrete vs. Continuous Approximations

Sometimes it's useful to approximate discrete distributions with continuous ones:
- Normal approximation to binomial (when np and n(1-p) are large)
- Poisson approximation to binomial (when n is large and p is small)

## Summary

Discrete random variables are a powerful tool for modeling countable outcomes in probability and machine learning:

1. **Definition**: Functions mapping outcomes to countable sets of values
2. **Representation**: Characterized by probability mass functions (PMFs) and cumulative distribution functions (CDFs)
3. **Properties**: Expected value, variance, and other moments describe the distribution
4. **Common Distributions**: Bernoulli, binomial, Poisson, geometric, and others model different types of discrete phenomena
5. **Applications**: Classification, count data modeling, probabilistic graphical models, and reinforcement learning

Understanding discrete random variables provides a foundation for probabilistic modeling in machine learning, allowing us to quantify uncertainty and make principled predictions in scenarios with countable outcomes.

In the next section, we'll explore continuous random variables, which extend these concepts to uncountable sets of values.

## References

1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
4. Ross, S. M. (2014). Introduction to Probability Models (11th ed.). Academic Press.
