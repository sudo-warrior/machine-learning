# 2.4.7 Time Series Analysis - Part 1

## Introduction to Time Series Analysis

Time series analysis involves studying data collected over time to understand underlying patterns, make predictions, and extract insights. Unlike cross-sectional data, time series data exhibits temporal dependencies that require specialized statistical methods. This section introduces the fundamental concepts and techniques of time series analysis.

## Time Series Components

A time series can typically be decomposed into several components:

1. **Trend**: The long-term movement or direction in the data
2. **Seasonality**: Regular, periodic fluctuations
3. **Cyclical Component**: Irregular fluctuations due to business or economic cycles
4. **Irregular Component**: Random, unpredictable variations (noise)

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Generate synthetic time series data
np.random.seed(42)
n = 144  # 12 years of monthly data

# Time index
t = np.arange(n)

# Trend component: quadratic trend
trend = 10 + 0.1 * t + 0.001 * t**2

# Seasonal component: annual cycle with period 12
seasonal = 5 * np.sin(2 * np.pi * t / 12)

# Cyclical component: longer cycle with period 48 (4 years)
cyclical = 3 * np.sin(2 * np.pi * t / 48)

# Irregular component: random noise
irregular = np.random.normal(0, 1, n)

# Combine components
time_series = trend + seasonal + cyclical + irregular

# Create a pandas Series for decomposition
import pandas as pd
ts = pd.Series(time_series)

# Decompose the time series
decomposition = seasonal_decompose(ts, model='additive', period=12)

# Plot the decomposition
plt.figure(figsize=(12, 10))

plt.subplot(5, 1, 1)
plt.plot(t, time_series)
plt.title('Original Time Series')
plt.grid(True, alpha=0.3)

plt.subplot(5, 1, 2)
plt.plot(t, trend)
plt.title('Trend Component')
plt.grid(True, alpha=0.3)

plt.subplot(5, 1, 3)
plt.plot(t, seasonal)
plt.title('Seasonal Component')
plt.grid(True, alpha=0.3)

plt.subplot(5, 1, 4)
plt.plot(t, cyclical)
plt.title('Cyclical Component')
plt.grid(True, alpha=0.3)

plt.subplot(5, 1, 5)
plt.plot(t, irregular)
plt.title('Irregular Component')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Plot the statsmodels decomposition
plt.figure(figsize=(12, 10))

plt.subplot(4, 1, 1)
plt.plot(decomposition.observed)
plt.title('Original Time Series')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 2)
plt.plot(decomposition.trend)
plt.title('Trend Component (Estimated)')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 3)
plt.plot(decomposition.seasonal)
plt.title('Seasonal Component (Estimated)')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 4)
plt.plot(decomposition.resid)
plt.title('Residual Component (Estimated)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Stationarity

**Stationarity** is a key concept in time series analysis:

- A stationary time series has statistical properties that do not change over time
- Specifically, the mean, variance, and autocorrelation structure remain constant

### Testing for Stationarity

The **Augmented Dickey-Fuller (ADF) test** is commonly used to test for stationarity:
- Null hypothesis (H₀): The time series has a unit root (non-stationary)
- Alternative hypothesis (H₁): The time series is stationary

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

# Generate different types of time series
np.random.seed(42)
n = 1000

# 1. Stationary series: white noise
stationary = np.random.normal(0, 1, n)

# 2. Non-stationary series: random walk
non_stationary = np.cumsum(np.random.normal(0, 1, n))

# 3. Trend stationary series: stationary around a trend
trend_stationary = 0.1 * np.arange(n) + np.random.normal(0, 1, n)

# Perform ADF test on each series
def adf_test(series, title):
    result = adfuller(series)
    print(f"ADF Test for {title}:")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"\t{key}: {value:.4f}")
    print(f"Conclusion: {'Stationary' if result[1] < 0.05 else 'Non-stationary'} at 5% significance level")
    print()

adf_test(stationary, "Stationary Series (White Noise)")
adf_test(non_stationary, "Non-stationary Series (Random Walk)")
adf_test(trend_stationary, "Trend Stationary Series")

# Plot the series
plt.figure(figsize=(15, 10))

plt.subplot(3, 2, 1)
plt.plot(stationary)
plt.title('Stationary Series (White Noise)')
plt.grid(True, alpha=0.3)

plt.subplot(3, 2, 2)
plt.hist(stationary, bins=30, alpha=0.7, edgecolor='black')
plt.title('Distribution of Stationary Series')
plt.grid(True, alpha=0.3)

plt.subplot(3, 2, 3)
plt.plot(non_stationary)
plt.title('Non-stationary Series (Random Walk)')
plt.grid(True, alpha=0.3)

plt.subplot(3, 2, 4)
plt.hist(non_stationary, bins=30, alpha=0.7, edgecolor='black')
plt.title('Distribution of Non-stationary Series')
plt.grid(True, alpha=0.3)

plt.subplot(3, 2, 5)
plt.plot(trend_stationary)
plt.title('Trend Stationary Series')
plt.grid(True, alpha=0.3)

plt.subplot(3, 2, 6)
plt.hist(trend_stationary, bins=30, alpha=0.7, edgecolor='black')
plt.title('Distribution of Trend Stationary Series')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Making a Time Series Stationary

Common transformations to achieve stationarity:

1. **Differencing**: Compute the difference between consecutive observations
2. **Logarithmic Transformation**: Apply log transformation to stabilize variance
3. **Seasonal Differencing**: Remove seasonal patterns
4. **Detrending**: Remove the trend component

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

# Generate a non-stationary time series
np.random.seed(42)
n = 1000

# Random walk with drift and exponential growth
t = np.arange(n)
series = 100 + 0.1 * t + np.cumsum(np.random.normal(0, 1, n))
series = np.exp(series / 100)  # Exponential growth

# Apply transformations
log_transform = np.log(series)
diff = np.diff(series)
log_diff = np.diff(log_transform)

# Perform ADF test on each series
adf_test(series, "Original Series")
adf_test(log_transform, "Log Transformed Series")
adf_test(diff, "Differenced Series")
adf_test(log_diff, "Log-Differenced Series")

# Plot the series and transformations
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
plt.plot(series)
plt.title('Original Series')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 2)
plt.plot(log_transform)
plt.title('Log Transformed Series')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 3)
plt.plot(diff)
plt.title('Differenced Series')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 4)
plt.plot(log_diff)
plt.title('Log-Differenced Series')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Autocorrelation and Partial Autocorrelation

**Autocorrelation** measures the correlation between a time series and its lagged values.

**Partial autocorrelation** measures the correlation between a time series and its lagged values, after controlling for the effects of intermediate lags.

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Generate different time series processes
np.random.seed(42)
n = 1000

# 1. White noise: no autocorrelation
white_noise = np.random.normal(0, 1, n)

# 2. AR(1) process: y_t = 0.7 * y_{t-1} + ε_t
ar1 = np.zeros(n)
ar1[0] = np.random.normal(0, 1)
for t in range(1, n):
    ar1[t] = 0.7 * ar1[t-1] + np.random.normal(0, 1)

# 3. MA(1) process: y_t = ε_t + 0.7 * ε_{t-1}
ma1 = np.zeros(n)
e = np.random.normal(0, 1, n)
for t in range(1, n):
    ma1[t] = e[t] + 0.7 * e[t-1]

# 4. ARMA(1,1) process: y_t = 0.7 * y_{t-1} + ε_t + 0.5 * ε_{t-1}
arma11 = np.zeros(n)
e = np.random.normal(0, 1, n)
arma11[0] = e[0]
for t in range(1, n):
    arma11[t] = 0.7 * arma11[t-1] + e[t] + 0.5 * e[t-1]

# Plot the series and their ACF/PACF
plt.figure(figsize=(15, 20))

# White Noise
plt.subplot(4, 3, 1)
plt.plot(white_noise)
plt.title('White Noise')
plt.grid(True, alpha=0.3)

plt.subplot(4, 3, 2)
plot_acf(white_noise, ax=plt.gca(), lags=20)
plt.title('ACF of White Noise')

plt.subplot(4, 3, 3)
plot_pacf(white_noise, ax=plt.gca(), lags=20)
plt.title('PACF of White Noise')

# AR(1)
plt.subplot(4, 3, 4)
plt.plot(ar1)
plt.title('AR(1) Process')
plt.grid(True, alpha=0.3)

plt.subplot(4, 3, 5)
plot_acf(ar1, ax=plt.gca(), lags=20)
plt.title('ACF of AR(1)')

plt.subplot(4, 3, 6)
plot_pacf(ar1, ax=plt.gca(), lags=20)
plt.title('PACF of AR(1)')

# MA(1)
plt.subplot(4, 3, 7)
plt.plot(ma1)
plt.title('MA(1) Process')
plt.grid(True, alpha=0.3)

plt.subplot(4, 3, 8)
plot_acf(ma1, ax=plt.gca(), lags=20)
plt.title('ACF of MA(1)')

plt.subplot(4, 3, 9)
plot_pacf(ma1, ax=plt.gca(), lags=20)
plt.title('PACF of MA(1)')

# ARMA(1,1)
plt.subplot(4, 3, 10)
plt.plot(arma11)
plt.title('ARMA(1,1) Process')
plt.grid(True, alpha=0.3)

plt.subplot(4, 3, 11)
plot_acf(arma11, ax=plt.gca(), lags=20)
plt.title('ACF of ARMA(1,1)')

plt.subplot(4, 3, 12)
plot_pacf(arma11, ax=plt.gca(), lags=20)
plt.title('PACF of ARMA(1,1)')

plt.tight_layout()
plt.show()
```

## Time Series Models

### Autoregressive (AR) Models

An **AR(p) model** expresses the current value as a linear function of its p previous values:

$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \varepsilon_t$$

where:
- $y_t$ is the value at time t
- $c$ is a constant
- $\phi_1, \phi_2, \ldots, \phi_p$ are the parameters
- $\varepsilon_t$ is white noise

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Generate an AR(2) process
np.random.seed(42)
n = 200
ar_params = [0.7, -0.3]  # AR parameters: φ₁ = 0.7, φ₂ = -0.3
ar2 = np.zeros(n)
e = np.random.normal(0, 1, n)
ar2[0] = e[0]
ar2[1] = ar_params[0] * ar2[0] + e[1]
for t in range(2, n):
    ar2[t] = ar_params[0] * ar2[t-1] + ar_params[1] * ar2[t-2] + e[t]

# Fit an AR(2) model
model = ARIMA(ar2, order=(2, 0, 0))
results = model.fit()
print(results.summary())

# Generate forecasts
forecast_steps = 50
forecast = results.get_forecast(steps=forecast_steps)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Plot the series and forecasts
plt.figure(figsize=(12, 6))
plt.plot(range(n), ar2, label='Observed')
plt.plot(range(n, n + forecast_steps), forecast_mean, color='red', label='Forecast')
plt.fill_between(range(n, n + forecast_steps), 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1], 
                color='pink', alpha=0.3)
plt.title('AR(2) Process and Forecasts')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Moving Average (MA) Models

An **MA(q) model** expresses the current value as a linear function of current and past error terms:

$$y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$

where:
- $y_t$ is the value at time t
- $\mu$ is the mean of the series
- $\theta_1, \theta_2, \ldots, \theta_q$ are the parameters
- $\varepsilon_t, \varepsilon_{t-1}, \ldots, \varepsilon_{t-q}$ are white noise error terms

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Generate an MA(2) process
np.random.seed(42)
n = 200
ma_params = [0.6, 0.3]  # MA parameters: θ₁ = 0.6, θ₂ = 0.3
ma2 = np.zeros(n)
e = np.random.normal(0, 1, n)
ma2[0] = e[0]
ma2[1] = e[1] + ma_params[0] * e[0]
for t in range(2, n):
    ma2[t] = e[t] + ma_params[0] * e[t-1] + ma_params[1] * e[t-2]

# Fit an MA(2) model
model = ARIMA(ma2, order=(0, 0, 2))
results = model.fit()
print(results.summary())

# Generate forecasts
forecast_steps = 50
forecast = results.get_forecast(steps=forecast_steps)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Plot the series and forecasts
plt.figure(figsize=(12, 6))
plt.plot(range(n), ma2, label='Observed')
plt.plot(range(n, n + forecast_steps), forecast_mean, color='red', label='Forecast')
plt.fill_between(range(n, n + forecast_steps), 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1], 
                color='pink', alpha=0.3)
plt.title('MA(2) Process and Forecasts')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Autoregressive Moving Average (ARMA) Models

An **ARMA(p,q) model** combines AR and MA models:

$$y_t = c + \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q}$$

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Generate an ARMA(1,1) process
np.random.seed(42)
n = 200
ar_params = [0.7]  # AR parameters: φ₁ = 0.7
ma_params = [0.5]  # MA parameters: θ₁ = 0.5
arma11 = np.zeros(n)
e = np.random.normal(0, 1, n)
arma11[0] = e[0]
for t in range(1, n):
    arma11[t] = ar_params[0] * arma11[t-1] + e[t] + ma_params[0] * e[t-1]

# Fit an ARMA(1,1) model
model = ARIMA(arma11, order=(1, 0, 1))
results = model.fit()
print(results.summary())

# Generate forecasts
forecast_steps = 50
forecast = results.get_forecast(steps=forecast_steps)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Plot the series and forecasts
plt.figure(figsize=(12, 6))
plt.plot(range(n), arma11, label='Observed')
plt.plot(range(n, n + forecast_steps), forecast_mean, color='red', label='Forecast')
plt.fill_between(range(n, n + forecast_steps), 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1], 
                color='pink', alpha=0.3)
plt.title('ARMA(1,1) Process and Forecasts')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Summary

Time series analysis provides tools for understanding and forecasting data collected over time:

1. **Time Series Components**:
   - Trend: Long-term movement
   - Seasonality: Regular, periodic fluctuations
   - Cyclical Component: Irregular fluctuations due to business cycles
   - Irregular Component: Random variations

2. **Stationarity**:
   - A key concept where statistical properties do not change over time
   - Can be tested using the Augmented Dickey-Fuller test
   - Achieved through transformations like differencing and log transformations

3. **Autocorrelation and Partial Autocorrelation**:
   - ACF: Correlation between a series and its lagged values
   - PACF: Correlation after controlling for intermediate lags
   - Help identify appropriate model orders

4. **Time Series Models**:
   - AR(p): Current value depends on p previous values
   - MA(q): Current value depends on q previous error terms
   - ARMA(p,q): Combines AR and MA models

In Part 2, we will explore more advanced time series models, including ARIMA, SARIMA, and state-space models, as well as techniques for handling non-stationary and seasonal data.

## References

1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5th ed.). Wiley.
2. Brockwell, P. J., & Davis, R. A. (2016). Introduction to Time Series and Forecasting (3rd ed.). Springer.
3. Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: Principles and Practice (2nd ed.). OTexts.
4. Shumway, R. H., & Stoffer, D. S. (2017). Time Series Analysis and Its Applications: With R Examples (4th ed.). Springer.
