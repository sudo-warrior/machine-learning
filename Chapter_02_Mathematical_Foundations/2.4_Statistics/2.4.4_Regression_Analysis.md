# 2.4.4 Regression Analysis

## Introduction to Regression Analysis

Regression analysis is a statistical method for modeling the relationship between a dependent variable and one or more independent variables. It's one of the most widely used techniques in statistics and machine learning for prediction and inference. This section covers the fundamental concepts and common regression methods.

## Simple Linear Regression

**Simple linear regression** models the relationship between two variables using a linear equation:

$$y = \beta_0 + \beta_1 x + \varepsilon$$

where:
- $y$ is the dependent variable
- $x$ is the independent variable
- $\beta_0$ is the y-intercept
- $\beta_1$ is the slope
- $\varepsilon$ is the error term

### Ordinary Least Squares (OLS)

OLS estimates the parameters by minimizing the sum of squared residuals:

$$\min_{\beta_0, \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
y = 2 * x + 1 + np.random.normal(0, 2, 100)  # y = 2x + 1 + noise

# Reshape x for sklearn
X = x.reshape(-1, 1)

# Fit linear regression model
model = LinearRegression()
model.fit(X, y)

# Get model parameters
slope = model.coef_[0]
intercept = model.intercept_

# Make predictions
y_pred = model.predict(X)

# Calculate metrics
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

# Print results
print(f"Estimated equation: y = {intercept:.4f} + {slope:.4f}x")
print(f"Mean Squared Error: {mse:.4f}")
print(f"R-squared: {r2:.4f}")

# Plot the data and regression line
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.7, label='Data')
plt.plot(x, y_pred, color='red', linewidth=2, label='Regression Line')
plt.title('Simple Linear Regression')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Assumptions of Linear Regression

1. **Linearity**: The relationship between variables is linear
2. **Independence**: Observations are independent of each other
3. **Homoscedasticity**: Constant variance of errors
4. **Normality**: Errors are normally distributed
5. **No multicollinearity**: Independent variables are not highly correlated (for multiple regression)

### Assessing Model Fit

- **R-squared (R²)**: Proportion of variance explained by the model
- **Adjusted R²**: R² adjusted for the number of predictors
- **Mean Squared Error (MSE)**: Average of squared residuals
- **Root Mean Squared Error (RMSE)**: Square root of MSE

```python
# Residual analysis
residuals = y - y_pred

# Plot residuals
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residuals vs. Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=20, alpha=0.7, edgecolor='black')
plt.title('Histogram of Residuals')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Q-Q plot for normality check
import scipy.stats as stats

plt.figure(figsize=(8, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot of Residuals')
plt.grid(True, alpha=0.3)
plt.show()
```

## Multiple Linear Regression

**Multiple linear regression** extends simple linear regression to include multiple independent variables:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \varepsilon$$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Generate synthetic data with multiple predictors
np.random.seed(42)
n = 100
X = np.random.uniform(0, 10, (n, 3))  # 3 features
y = 1 + 2 * X[:, 0] - 1.5 * X[:, 1] + 0.5 * X[:, 2] + np.random.normal(0, 2, n)  # y = 1 + 2x₁ - 1.5x₂ + 0.5x₃ + noise

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit multiple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Get model parameters
coefficients = model.coef_
intercept = model.intercept_

# Make predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calculate metrics
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Estimated equation: y = {intercept:.4f} + {coefficients[0]:.4f}x₁ + {coefficients[1]:.4f}x₂ + {coefficients[2]:.4f}x₃")
print(f"Training MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Testing MSE: {test_mse:.4f}, R²: {test_r2:.4f}")

# Create a DataFrame for easier interpretation
results = pd.DataFrame({
    'Feature': ['Intercept', 'X1', 'X2', 'X3'],
    'True Coefficient': [1, 2, -1.5, 0.5],
    'Estimated Coefficient': [intercept] + list(coefficients)
})
print("\nCoefficient Comparison:")
print(results)

# Plot actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.grid(True, alpha=0.3)
plt.show()
```

### Feature Selection

Methods for selecting the most relevant features:

1. **Forward Selection**: Start with no features and add one at a time
2. **Backward Elimination**: Start with all features and remove one at a time
3. **Stepwise Regression**: Combination of forward and backward approaches
4. **Regularization**: Penalize model complexity (Lasso, Ridge)

```python
from sklearn.feature_selection import f_regression, RFE

# F-statistic for feature importance
f_statistic, p_values = f_regression(X_train, y_train)

# Create a DataFrame with feature importance
feature_importance = pd.DataFrame({
    'Feature': ['X1', 'X2', 'X3'],
    'F-Statistic': f_statistic,
    'P-Value': p_values
})
print("\nFeature Importance:")
print(feature_importance.sort_values('F-Statistic', ascending=False))

# Recursive Feature Elimination
rfe = RFE(estimator=LinearRegression(), n_features_to_select=2)
rfe.fit(X_train, y_train)

# Print selected features
print("\nRecursive Feature Elimination:")
for i, feature in enumerate(['X1', 'X2', 'X3']):
    print(f"{feature}: {'Selected' if rfe.support_[i] else 'Not Selected'} (Rank: {rfe.ranking_[i]})")
```

## Polynomial Regression

**Polynomial regression** extends linear regression to model non-linear relationships:

$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_d x^d + \varepsilon$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data with non-linear relationship
np.random.seed(42)
x = np.random.uniform(-3, 3, 100)
y = 1 + 2 * x + 1.5 * x**2 - 0.5 * x**3 + np.random.normal(0, 2, 100)  # Cubic relationship with noise

# Reshape x for sklearn
X = x.reshape(-1, 1)

# Create and fit models of different degrees
degrees = [1, 2, 3, 5]
models = []
predictions = []

for degree in degrees:
    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    model.fit(X, y)
    models.append(model)
    predictions.append(model.predict(X))

# Plot the results
plt.figure(figsize=(12, 8))

# Sort x for smooth curve plotting
sort_idx = np.argsort(x)
x_sorted = x[sort_idx]
X_sorted = X[sort_idx]

# Plot data and polynomial fits
plt.scatter(x, y, alpha=0.7, label='Data')

for i, degree in enumerate(degrees):
    plt.plot(x_sorted, predictions[i][sort_idx], label=f'Degree {degree}')
    
    # Calculate metrics
    mse = mean_squared_error(y, predictions[i])
    r2 = r2_score(y, predictions[i])
    print(f"Degree {degree}: MSE = {mse:.4f}, R² = {r2:.4f}")

plt.title('Polynomial Regression')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Regularized Regression

Regularization adds a penalty term to the loss function to prevent overfitting.

### Ridge Regression (L2 Regularization)

Adds the sum of squared coefficients to the loss function:

$$\min_{\beta} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \alpha \sum_{j=1}^{p} \beta_j^2$$

### Lasso Regression (L1 Regularization)

Adds the sum of absolute coefficients to the loss function:

$$\min_{\beta} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \alpha \sum_{j=1}^{p} |\beta_j|$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data with many features (some irrelevant)
np.random.seed(42)
n = 100
p = 20  # Number of features
X = np.random.normal(0, 1, (n, p))
# Only the first 5 features are relevant
true_coef = np.zeros(p)
true_coef[:5] = [1.5, -2, 0.5, -1, 1]
y = X.dot(true_coef) + np.random.normal(0, 1, n)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and fit models
models = {
    'Linear Regression': make_pipeline(StandardScaler(), LinearRegression()),
    'Ridge (α=1.0)': make_pipeline(StandardScaler(), Ridge(alpha=1.0)),
    'Lasso (α=0.1)': make_pipeline(StandardScaler(), Lasso(alpha=0.1))
}

for name, model in models.items():
    model.fit(X_train, y_train)
    
    # Get coefficients (skip the scaler in the pipeline)
    if name == 'Linear Regression':
        coef = model.steps[1][1].coef_
    elif name == 'Ridge (α=1.0)':
        coef = model.steps[1][1].coef_
    else:  # Lasso
        coef = model.steps[1][1].coef_
    
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Calculate metrics
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    
    print(f"\n{name}:")
    print(f"Training MSE: {train_mse:.4f}")
    print(f"Testing MSE: {test_mse:.4f}")
    print(f"Number of non-zero coefficients: {np.sum(np.abs(coef) > 1e-10)}")

# Plot coefficients
plt.figure(figsize=(12, 6))
plt.plot(range(p), true_coef, 'o-', label='True Coefficients')

for name, model in models.items():
    if name == 'Linear Regression':
        coef = model.steps[1][1].coef_
    elif name == 'Ridge (α=1.0)':
        coef = model.steps[1][1].coef_
    else:  # Lasso
        coef = model.steps[1][1].coef_
    
    plt.plot(range(p), coef, 'o-', label=f'{name} Coefficients')

plt.title('Coefficient Comparison')
plt.xlabel('Feature Index')
plt.ylabel('Coefficient Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Logistic Regression

**Logistic regression** models the probability of a binary outcome:

$$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p)}}$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.model_selection import train_test_split

# Generate synthetic data for binary classification
np.random.seed(42)
n = 100
X = np.random.normal(0, 1, (n, 2))  # 2 features
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Binary outcome

# Add some noise
noise_idx = np.random.choice(n, 10, replace=False)
y[noise_idx] = 1 - y[noise_idx]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Get model parameters
coefficients = model.coef_[0]
intercept = model.intercept_[0]

# Make predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
y_test_prob = model.predict_proba(X_test)[:, 1]

# Calculate metrics
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
conf_matrix = confusion_matrix(y_test, y_test_pred)

# Print results
print(f"Estimated equation: log(p/(1-p)) = {intercept:.4f} + {coefficients[0]:.4f}x₁ + {coefficients[1]:.4f}x₂")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Testing Accuracy: {test_accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred))

# Plot decision boundary
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
# Create a meshgrid for visualization
h = 0.02  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot decision boundary and data points
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.RdBu)
plt.title('Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Plot ROC curve
plt.subplot(1, 2, 2)
fpr, tpr, _ = roc_curve(y_test, y_test_prob)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Generalized Linear Models (GLMs)

**GLMs** extend linear regression to handle response variables with non-normal distributions:

1. **Linear Regression**: Normal distribution, identity link
2. **Logistic Regression**: Binomial distribution, logit link
3. **Poisson Regression**: Poisson distribution, log link
4. **Gamma Regression**: Gamma distribution, inverse link

### Poisson Regression

Models count data:

$$\log(E[y|x]) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import PoissonRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Generate synthetic count data
np.random.seed(42)
n = 100
X = np.random.uniform(0, 5, (n, 1))
log_mu = 0.5 + 0.3 * X.flatten()  # log(λ) = 0.5 + 0.3x
y = np.random.poisson(np.exp(log_mu))

# Fit Poisson regression model
model = PoissonRegressor()
model.fit(X, y)

# Get model parameters
coefficient = model.coef_[0]
intercept = model.intercept_

# Make predictions
X_range = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model.predict(X_range)

# Print results
print(f"Estimated equation: log(λ) = {intercept:.4f} + {coefficient:.4f}x")
print(f"True equation: log(λ) = 0.5 + 0.3x")

# Plot the data and model
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.7, label='Data')
plt.plot(X_range, y_pred, color='red', linewidth=2, label='Poisson Regression')
plt.title('Poisson Regression')
plt.xlabel('x')
plt.ylabel('Count (y)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Time Series Regression

**Time series regression** models data collected over time, accounting for temporal dependencies.

### Autoregressive (AR) Models

Models the current value as a function of past values:

$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \varepsilon_t$$

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.ar_model import AutoReg
from sklearn.metrics import mean_squared_error

# Generate synthetic time series data
np.random.seed(42)
n = 200
ar_params = [0.7, -0.3]  # AR(2) parameters
ma_params = []
y = np.zeros(n)

# Generate AR(2) process
for t in range(2, n):
    y[t] = 1 + ar_params[0] * y[t-1] + ar_params[1] * y[t-2] + np.random.normal(0, 1)

# Split data into train and test
train_size = int(0.8 * n)
train, test = y[:train_size], y[train_size:]

# Fit AR model
model = AutoReg(train, lags=2)
model_fit = model.fit()

# Make predictions
predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, dynamic=False)

# Calculate metrics
mse = mean_squared_error(test, predictions)
rmse = np.sqrt(mse)

# Print results
print(f"AR(2) Model Summary:")
print(model_fit.summary())
print(f"\nTest RMSE: {rmse:.4f}")

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(range(n), y, label='Original Series')
plt.plot(range(train_size, n), predictions, color='red', label='Predictions')
plt.title('AR(2) Model Predictions')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Regression Diagnostics

Diagnostics help identify issues with regression models:

1. **Residual Analysis**: Check for patterns in residuals
2. **Influence Measures**: Identify influential observations
3. **Multicollinearity**: Check for correlated predictors
4. **Heteroscedasticity**: Test for non-constant variance

```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Generate synthetic data
np.random.seed(42)
n = 100
X = np.random.uniform(0, 10, (n, 3))  # 3 features
y = 1 + 2 * X[:, 0] - 1.5 * X[:, 1] + 0.5 * X[:, 2] + np.random.normal(0, 2, n)

# Add a constant term
X_with_const = sm.add_constant(X)

# Fit OLS model
model = sm.OLS(y, X_with_const)
results = model.fit()

# Print summary
print(results.summary())

# Residual analysis
residuals = results.resid
fitted_values = results.fittedvalues

# Plot diagnostics
plt.figure(figsize=(12, 10))

# Residuals vs. Fitted Values
plt.subplot(2, 2, 1)
plt.scatter(fitted_values, residuals, alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residuals vs. Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.grid(True, alpha=0.3)

# Q-Q Plot
plt.subplot(2, 2, 2)
sm.qqplot(residuals, line='45', fit=True, ax=plt.gca())
plt.title('Q-Q Plot')
plt.grid(True, alpha=0.3)

# Scale-Location Plot
plt.subplot(2, 2, 3)
plt.scatter(fitted_values, np.sqrt(np.abs(residuals)), alpha=0.7)
plt.title('Scale-Location Plot')
plt.xlabel('Fitted Values')
plt.ylabel('√|Residuals|')
plt.grid(True, alpha=0.3)

# Leverage Plot
plt.subplot(2, 2, 4)
influence = results.get_influence()
leverage = influence.hat_matrix_diag
plt.scatter(leverage, residuals, alpha=0.7)
plt.title('Leverage Plot')
plt.xlabel('Leverage')
plt.ylabel('Residuals')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Feature"] = ["const", "X1", "X2", "X3"]
vif_data["VIF"] = [variance_inflation_factor(X_with_const, i) for i in range(X_with_const.shape[1])]
print("\nVariance Inflation Factors:")
print(vif_data)
```

## Summary

Regression analysis provides powerful tools for modeling relationships between variables:

1. **Simple Linear Regression**:
   - Models relationship between two variables
   - Estimated using Ordinary Least Squares (OLS)

2. **Multiple Linear Regression**:
   - Extends to multiple independent variables
   - Feature selection methods help identify relevant predictors

3. **Polynomial Regression**:
   - Models non-linear relationships
   - Higher degrees can lead to overfitting

4. **Regularized Regression**:
   - Ridge (L2): Shrinks coefficients toward zero
   - Lasso (L1): Can set coefficients exactly to zero

5. **Logistic Regression**:
   - Models probability of binary outcomes
   - Uses logistic function to constrain predictions between 0 and 1

6. **Generalized Linear Models**:
   - Extends regression to non-normal distributions
   - Includes Poisson, Gamma, and other regression types

7. **Time Series Regression**:
   - Models temporal dependencies
   - Autoregressive models use past values as predictors

8. **Regression Diagnostics**:
   - Residual analysis checks model assumptions
   - Influence measures identify outliers and leverage points

Regression analysis forms the foundation for many machine learning algorithms and provides interpretable models for both prediction and inference.

## References

1. Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models (5th ed.). McGraw-Hill/Irwin.
2. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.
3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
4. Fox, J. (2015). Applied Regression Analysis and Generalized Linear Models (3rd ed.). SAGE Publications.
