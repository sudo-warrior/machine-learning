# 2.4.5 Experimental Design - Part 2

## Advanced Experimental Designs

Building on the fundamental principles covered in Part 1, this section explores more complex experimental designs that allow researchers to study multiple factors and their interactions efficiently.

## Factorial Designs

**Factorial designs** involve studying the effects of two or more factors simultaneously, with each factor having multiple levels.

### Full Factorial Design

In a **full factorial design**, all possible combinations of factor levels are tested.

Advantages:
- Allows estimation of main effects and interactions
- Efficient use of experimental units
- Comprehensive exploration of the factor space

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.graphics.factorplots import interaction_plot

# Simulate a 2×3 factorial experiment (2 levels of factor A, 3 levels of factor B)
np.random.seed(42)
n_per_cell = 5  # Number of replicates per treatment combination
n = n_per_cell * 2 * 3  # Total number of subjects

# Create factor combinations
factor_A = np.repeat(['A1', 'A2'], 3 * n_per_cell)
factor_B = np.repeat(np.tile(['B1', 'B2', 'B3'], 2), n_per_cell)

# True effects
effect_A = {'A1': 0, 'A2': 10}
effect_B = {'B1': 0, 'B2': 5, 'B3': 15}
# Interaction effects (additional effect beyond main effects)
interaction_effect = {
    ('A1', 'B1'): 0, ('A1', 'B2'): 0, ('A1', 'B3'): 0,
    ('A2', 'B1'): 0, ('A2', 'B2'): 5, ('A2', 'B3'): -5  # Interaction in some combinations
}

# Generate outcomes
baseline = np.random.normal(50, 5, n)
outcome = baseline.copy()

# Apply main effects and interactions
for i in range(n):
    outcome[i] += effect_A[factor_A[i]] + effect_B[factor_B[i]] + interaction_effect[(factor_A[i], factor_B[i])]
    outcome[i] += np.random.normal(0, 3)  # Add noise

# Create a DataFrame
experiment = pd.DataFrame({
    'Factor_A': factor_A,
    'Factor_B': factor_B,
    'Outcome': outcome
})

# Analyze results with two-way ANOVA
model = ols('Outcome ~ C(Factor_A) * C(Factor_B)', data=experiment).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("Two-way ANOVA results:")
print(anova_table)

# Calculate means for each treatment combination
means = experiment.groupby(['Factor_A', 'Factor_B'])['Outcome'].mean().reset_index()
means_wide = means.pivot(index='Factor_A', columns='Factor_B', values='Outcome')
print("\nMean outcomes for each treatment combination:")
print(means_wide)

# Visualize the interaction
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
interaction_plot(experiment['Factor_B'], experiment['Factor_A'], experiment['Outcome'], 
                colors=['red', 'blue'], markers=['o', '^'], ms=10)
plt.title('Interaction Plot')
plt.xlabel('Factor B')
plt.ylabel('Mean Outcome')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
# Main effect of Factor A
main_A = experiment.groupby('Factor_A')['Outcome'].mean()
plt.bar(main_A.index, main_A.values)
plt.title('Main Effect of Factor A')
plt.xlabel('Factor A')
plt.ylabel('Mean Outcome')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
# Main effect of Factor B
main_B = experiment.groupby('Factor_B')['Outcome'].mean()
plt.bar(main_B.index, main_B.values)
plt.title('Main Effect of Factor B')
plt.xlabel('Factor B')
plt.ylabel('Mean Outcome')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize all treatment combinations
plt.figure(figsize=(10, 6))
for i, a_level in enumerate(['A1', 'A2']):
    subset = experiment[experiment['Factor_A'] == a_level]
    positions = np.array([1, 2, 3]) + 0.3 * (i - 0.5)
    plt.boxplot([subset[subset['Factor_B'] == b]['Outcome'] for b in ['B1', 'B2', 'B3']],
               positions=positions, widths=0.25, patch_artist=True,
               boxprops=dict(facecolor=['lightblue', 'lightgreen', 'lightsalmon'][i]))
    
plt.xticks([1, 2, 3], ['B1', 'B2', 'B3'])
plt.title('Outcomes for All Treatment Combinations')
plt.xlabel('Factor B')
plt.ylabel('Outcome')
plt.grid(True, alpha=0.3)
plt.legend(['A1', 'A2'], title='Factor A')
plt.show()
```

### Fractional Factorial Design

When the number of factors or levels is large, testing all combinations becomes impractical. **Fractional factorial designs** test only a subset of combinations, sacrificing some interaction information.

Advantages:
- Requires fewer experimental units
- Still allows estimation of main effects and some interactions

Disadvantages:
- Some effects are confounded (cannot be distinguished)
- Requires careful selection of which combinations to test

## Latin Square Design

A **Latin square design** controls for two blocking factors simultaneously, with treatments arranged so that each treatment appears exactly once in each row and each column.

Advantages:
- Controls for two sources of variation
- Requires fewer experimental units than a complete block design

Disadvantages:
- Number of treatments must equal number of rows and columns
- Cannot estimate interactions between blocks and treatments

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Create a 4×4 Latin square design
latin_square = np.array([
    ['A', 'B', 'C', 'D'],
    ['B', 'C', 'D', 'A'],
    ['C', 'D', 'A', 'B'],
    ['D', 'A', 'B', 'C']
])

# Simulate an experiment using this design
np.random.seed(42)
n = 16  # Total number of subjects

# Create row and column factors
rows = np.repeat(range(1, 5), 4)
columns = np.tile(range(1, 5), 4)

# Extract treatments from the Latin square
treatments = []
for i in range(4):
    for j in range(4):
        treatments.append(latin_square[i, j])

# True effects
treatment_effects = {'A': 0, 'B': 5, 'C': 10, 'D': 15}
row_effects = {1: 0, 2: 8, 3: -5, 4: 3}  # e.g., different days
column_effects = {1: 0, 2: -3, 3: 7, 4: -2}  # e.g., different locations

# Generate outcomes
baseline = np.random.normal(50, 3, n)
outcome = baseline.copy()

# Apply effects
for i in range(n):
    outcome[i] += treatment_effects[treatments[i]] + row_effects[rows[i]] + column_effects[columns[i]]
    outcome[i] += np.random.normal(0, 2)  # Add noise

# Create a DataFrame
experiment = pd.DataFrame({
    'Row': rows,
    'Column': columns,
    'Treatment': treatments,
    'Outcome': outcome
})

# Analyze results
model = ols('Outcome ~ C(Treatment) + C(Row) + C(Column)', data=experiment).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("Latin Square ANOVA results:")
print(anova_table)

# Calculate adjusted treatment means
treatment_means = experiment.groupby('Treatment')['Outcome'].mean()
print("\nTreatment means:")
print(treatment_means)

# Visualize the Latin square design
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
# Create a heatmap of the design
for i in range(4):
    for j in range(4):
        plt.text(j + 0.5, i + 0.5, latin_square[i, j], ha='center', va='center', fontsize=12)
        color = {'A': 'lightblue', 'B': 'lightgreen', 'C': 'lightsalmon', 'D': 'lightpink'}[latin_square[i, j]]
        plt.fill([j, j+1, j+1, j], [i, i, i+1, i+1], color=color, alpha=0.5)

plt.xlim(0, 4)
plt.ylim(0, 4)
plt.xticks(np.arange(0.5, 4.5), ['1', '2', '3', '4'])
plt.yticks(np.arange(0.5, 4.5), ['1', '2', '3', '4'])
plt.title('Latin Square Design')
plt.xlabel('Column')
plt.ylabel('Row')

plt.subplot(1, 2, 2)
# Plot adjusted treatment effects
plt.bar(treatment_means.index, treatment_means.values)
plt.title('Treatment Effects')
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Split-Plot Design

A **split-plot design** is used when some factors are harder to randomize than others. The experimental units are divided into main plots (with hard-to-change factors) and subplots (with easy-to-change factors).

Advantages:
- Accommodates factors with different randomization requirements
- Efficient for factors that are difficult or costly to change

Disadvantages:
- More complex analysis
- Different precision for different effects

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Simulate a split-plot experiment
np.random.seed(42)
n_main_plots = 4  # Number of main plots
n_subplots_per_main = 3  # Number of subplots per main plot
n = n_main_plots * n_subplots_per_main  # Total number of observations

# Main plot factor (hard to change, e.g., temperature)
main_plot_factor = np.repeat(['Low', 'High'], n_main_plots // 2)
main_plot_id = np.repeat(range(1, n_main_plots + 1), n_subplots_per_main)
main_plot_treatment = np.repeat(main_plot_factor, n_subplots_per_main)

# Subplot factor (easy to change, e.g., fertilizer type)
subplot_factor = np.tile(['None', 'Type A', 'Type B'], n_main_plots)

# True effects
main_effect = {'Low': 0, 'High': 15}
subplot_effect = {'None': 0, 'Type A': 5, 'Type B': 10}
# Interaction effects
interaction_effect = {
    ('Low', 'None'): 0, ('Low', 'Type A'): 0, ('Low', 'Type B'): 0,
    ('High', 'None'): 0, ('High', 'Type A'): 5, ('High', 'Type B'): -5
}

# Main plot error (whole plot variation)
main_plot_error = np.repeat(np.random.normal(0, 5, n_main_plots), n_subplots_per_main)

# Generate outcomes
baseline = np.random.normal(50, 2, n)
outcome = baseline.copy()

# Apply effects
for i in range(n):
    outcome[i] += main_effect[main_plot_treatment[i]] + subplot_effect[subplot_factor[i]]
    outcome[i] += interaction_effect[(main_plot_treatment[i], subplot_factor[i])]
    outcome[i] += main_plot_error[i] + np.random.normal(0, 2)  # Add main plot and subplot errors

# Create a DataFrame
experiment = pd.DataFrame({
    'MainPlot': main_plot_id,
    'MainFactor': main_plot_treatment,
    'SubFactor': subplot_factor,
    'Outcome': outcome
})

# Analyze results (simplified - in practice, use mixed-effects models)
model = ols('Outcome ~ C(MainFactor) * C(SubFactor) + C(MainPlot)', data=experiment).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("Split-plot ANOVA results (simplified):")
print(anova_table)

# Calculate means for each treatment combination
means = experiment.groupby(['MainFactor', 'SubFactor'])['Outcome'].mean().reset_index()
means_wide = means.pivot(index='MainFactor', columns='SubFactor', values='Outcome')
print("\nMean outcomes for each treatment combination:")
print(means_wide)

# Visualize the results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
# Interaction plot
for i, main_level in enumerate(['Low', 'High']):
    subset = means[means['MainFactor'] == main_level]
    plt.plot(subset['SubFactor'], subset['Outcome'], 'o-', label=main_level)
plt.title('Interaction Between Main and Subplot Factors')
plt.xlabel('Subplot Factor')
plt.ylabel('Mean Outcome')
plt.legend(title='Main Factor')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Boxplot by treatment combination
boxplot_data = []
labels = []
for main in ['Low', 'High']:
    for sub in ['None', 'Type A', 'Type B']:
        subset = experiment[(experiment['MainFactor'] == main) & (experiment['SubFactor'] == sub)]
        boxplot_data.append(subset['Outcome'])
        labels.append(f"{main}\n{sub}")

plt.boxplot(boxplot_data)
plt.xticks(range(1, 7), labels)
plt.title('Outcomes by Treatment Combination')
plt.ylabel('Outcome')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Experimental Design in Machine Learning

### Cross-Validation Designs

**Cross-validation** is a resampling procedure used to evaluate machine learning models on limited data.

Types of cross-validation:
1. **k-fold cross-validation**: Data is divided into k subsets, with each subset serving as a test set once
2. **Stratified k-fold**: Maintains the same class distribution in each fold
3. **Leave-one-out**: Uses a single observation as the test set (k = n)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Generate a synthetic classification dataset
np.random.seed(42)
X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,
                          n_clusters_per_class=1, weights=[0.7, 0.3], random_state=42)

# Visualize the dataset
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', alpha=0.7)
plt.title('Classification Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Class')
plt.grid(True, alpha=0.3)
plt.show()

# Implement different cross-validation schemes
cv_schemes = {
    '5-Fold CV': KFold(n_splits=5, shuffle=True, random_state=42),
    'Stratified 5-Fold CV': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    'Leave-One-Out CV': LeaveOneOut()
}

# Compare cross-validation schemes
results = {}

for name, cv in cv_schemes.items():
    fold_accuracies = []
    
    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # Train a simple model
        model = LogisticRegression(random_state=42)
        model.fit(X_train, y_train)
        
        # Evaluate on test fold
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        fold_accuracies.append(accuracy)
    
    results[name] = {
        'Mean Accuracy': np.mean(fold_accuracies),
        'Std Dev': np.std(fold_accuracies),
        'Fold Accuracies': fold_accuracies
    }
    
    print(f"{name}:")
    print(f"  Mean Accuracy: {results[name]['Mean Accuracy']:.4f}")
    print(f"  Std Dev: {results[name]['Std Dev']:.4f}")
    if name != 'Leave-One-Out CV':  # Too many folds to print for LOO
        print(f"  Fold Accuracies: {fold_accuracies}")
    print()

# Visualize cross-validation results
plt.figure(figsize=(10, 6))
for i, (name, result) in enumerate(results.items()):
    if name != 'Leave-One-Out CV':  # Skip LOO for boxplot
        plt.boxplot(result['Fold Accuracies'], positions=[i], widths=0.6)
plt.xticks(range(len(results) - 1), [name for name in results if name != 'Leave-One-Out CV'])
plt.title('Cross-Validation Accuracy Comparison')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)
plt.show()
```

### Hyperparameter Optimization

**Hyperparameter optimization** involves finding the best hyperparameters for a machine learning model.

Common approaches:
1. **Grid Search**: Exhaustive search over specified parameter values
2. **Random Search**: Random sampling from parameter distributions
3. **Bayesian Optimization**: Sequential model-based optimization

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Generate a synthetic classification dataset
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5,
                          n_classes=2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Total number of combinations
total_combinations = 1
for param in param_grid.values():
    total_combinations *= len(param)
print(f"Total parameter combinations in grid: {total_combinations}")

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Random Search
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_grid,
    n_iter=20,  # Number of parameter settings sampled
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

# Train and evaluate
search_methods = {
    'Grid Search': grid_search,
    'Random Search': random_search
}

results = {}

for name, search in search_methods.items():
    print(f"\nPerforming {name}...")
    search.fit(X_train, y_train)
    
    # Best parameters and score
    print(f"Best parameters: {search.best_params_}")
    print(f"Best cross-validation score: {search.best_score_:.4f}")
    
    # Evaluate on test set
    best_model = search.best_estimator_
    test_accuracy = best_model.score(X_test, y_test)
    print(f"Test accuracy: {test_accuracy:.4f}")
    
    results[name] = {
        'best_params': search.best_params_,
        'cv_results': search.cv_results_,
        'best_score': search.best_score_,
        'test_accuracy': test_accuracy
    }

# Visualize search results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
for name, result in results.items():
    cv_results = result['cv_results_']
    plt.plot(range(1, len(cv_results['mean_test_score']) + 1), 
             sorted(cv_results['mean_test_score'], reverse=True),
             'o-', label=name)
plt.title('Performance of Top Parameter Combinations')
plt.xlabel('Rank of Parameter Combination')
plt.ylabel('Mean CV Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
names = list(results.keys())
cv_scores = [results[name]['best_score'] for name in names]
test_scores = [results[name]['test_accuracy'] for name in names]

x = np.arange(len(names))
width = 0.35

plt.bar(x - width/2, cv_scores, width, label='CV Score')
plt.bar(x + width/2, test_scores, width, label='Test Score')
plt.xticks(x, names)
plt.title('Best Model Performance')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

Advanced experimental designs provide powerful tools for studying complex systems:

1. **Factorial Designs**:
   - Full Factorial: Tests all combinations of factor levels
   - Fractional Factorial: Tests a subset of combinations

2. **Latin Square Design**:
   - Controls for two blocking factors simultaneously
   - Each treatment appears once in each row and column

3. **Split-Plot Design**:
   - Accommodates factors with different randomization requirements
   - Divides experimental units into main plots and subplots

4. **Experimental Design in Machine Learning**:
   - Cross-Validation: Evaluates models on limited data
   - Hyperparameter Optimization: Finds optimal model configurations

These designs enable researchers to efficiently study multiple factors and their interactions while controlling for sources of variation.

## References

1. Montgomery, D. C. (2019). Design and Analysis of Experiments (10th ed.). Wiley.
2. Box, G. E. P., Hunter, J. S., & Hunter, W. G. (2005). Statistics for Experimenters: Design, Innovation, and Discovery (2nd ed.). Wiley-Interscience.
3. Kohavi, R., Longbotham, R., Sommerfield, D., & Henne, R. M. (2009). Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1), 140-181.
4. Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13, 281-305.
