# 2.4.6 Bayesian Statistics - Part 1

## Introduction to Bayesian Statistics

Bayesian statistics provides a framework for statistical inference based on Bayes' theorem, which describes how to update beliefs based on new evidence. Unlike frequentist statistics, which treats parameters as fixed but unknown constants, Bayesian statistics treats parameters as random variables with probability distributions. This section introduces the fundamental concepts of Bayesian statistics and their applications.

## Bayes' Theorem

**Bayes' theorem** forms the foundation of Bayesian statistics:

$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$

In the context of statistical inference:

$$P(\theta|D) = \frac{P(D|\theta) \times P(\theta)}{P(D)}$$

where:
- $P(\theta|D)$ is the **posterior distribution** (probability of parameters given the data)
- $P(D|\theta)$ is the **likelihood** (probability of the data given the parameters)
- $P(\theta)$ is the **prior distribution** (initial belief about the parameters)
- $P(D)$ is the **evidence** or **marginal likelihood** (total probability of the data)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate Bayes' theorem with a simple example
# Problem: Testing for a rare disease

# Parameters
prevalence = 0.01  # Prior probability of having the disease (1%)
sensitivity = 0.95  # P(positive test | disease) - True positive rate
specificity = 0.90  # P(negative test | no disease) - True negative rate

# Calculate posterior probability using Bayes' theorem
# P(disease | positive test) = P(positive test | disease) * P(disease) / P(positive test)

# Calculate P(positive test)
p_positive = sensitivity * prevalence + (1 - specificity) * (1 - prevalence)

# Calculate posterior probability
posterior = (sensitivity * prevalence) / p_positive

# Print results
print(f"Prior probability of disease: {prevalence:.4f} (1%)")
print(f"Probability of positive test given disease (sensitivity): {sensitivity:.4f} (95%)")
print(f"Probability of negative test given no disease (specificity): {specificity:.4f} (90%)")
print(f"Probability of positive test: {p_positive:.4f}")
print(f"Posterior probability of disease given positive test: {posterior:.4f} ({posterior*100:.1f}%)")

# Visualize Bayes' theorem
plt.figure(figsize=(12, 6))

# Prior distribution
plt.subplot(1, 3, 1)
plt.bar(['No Disease', 'Disease'], [1-prevalence, prevalence], color=['lightblue', 'salmon'])
plt.title('Prior Probability')
plt.ylabel('Probability')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3)

# Likelihood
plt.subplot(1, 3, 2)
plt.bar(['No Disease', 'Disease'], [1-specificity, sensitivity], color=['lightblue', 'salmon'])
plt.title('Likelihood of Positive Test')
plt.ylabel('Probability')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3)

# Posterior distribution
plt.subplot(1, 3, 3)
plt.bar(['No Disease', 'Disease'], [1-posterior, posterior], color=['lightblue', 'salmon'])
plt.title('Posterior Probability')
plt.ylabel('Probability')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Prior Distributions

The **prior distribution** represents our initial beliefs about the parameters before seeing the data.

### Types of Priors

1. **Informative Priors**: Based on previous studies, expert knowledge, or theoretical considerations
2. **Uninformative Priors**: Designed to have minimal influence on the posterior
3. **Conjugate Priors**: Result in a posterior distribution from the same family as the prior
4. **Improper Priors**: Do not integrate to 1, but can still lead to proper posteriors

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate different types of priors for a binomial likelihood (coin flipping)
plt.figure(figsize=(15, 5))

# Parameter space (probability of heads)
theta = np.linspace(0, 1, 1000)

# 1. Informative prior (Beta distribution)
plt.subplot(1, 3, 1)
# Beta(5, 2) - Prior belief that coin is biased towards heads
alpha_informative, beta_informative = 5, 2
prior_informative = stats.beta.pdf(theta, alpha_informative, beta_informative)
plt.plot(theta, prior_informative, 'b-', linewidth=2, label='Beta(5, 2)')
plt.title('Informative Prior')
plt.xlabel('θ (Probability of Heads)')
plt.ylabel('Density')
plt.grid(True, alpha=0.3)
plt.legend()

# 2. Uninformative prior (Uniform distribution)
plt.subplot(1, 3, 2)
# Beta(1, 1) - No preference for any value of theta
alpha_uninformative, beta_uninformative = 1, 1
prior_uninformative = stats.beta.pdf(theta, alpha_uninformative, beta_uninformative)
plt.plot(theta, prior_uninformative, 'r-', linewidth=2, label='Beta(1, 1)')
plt.title('Uninformative Prior')
plt.xlabel('θ (Probability of Heads)')
plt.ylabel('Density')
plt.grid(True, alpha=0.3)
plt.legend()

# 3. Jeffreys' prior for binomial (Beta(0.5, 0.5))
plt.subplot(1, 3, 3)
alpha_jeffreys, beta_jeffreys = 0.5, 0.5
prior_jeffreys = stats.beta.pdf(theta, alpha_jeffreys, beta_jeffreys)
plt.plot(theta, prior_jeffreys, 'g-', linewidth=2, label='Beta(0.5, 0.5)')
plt.title('Jeffreys\' Prior')
plt.xlabel('θ (Probability of Heads)')
plt.ylabel('Density')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()
```

## Likelihood Function

The **likelihood function** represents the probability of observing the data given the parameters.

For a binomial experiment (e.g., coin flipping):

$$P(D|\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}$$

where:
- $n$ is the number of trials
- $k$ is the number of successes
- $\theta$ is the probability of success

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate likelihood function for coin flipping
plt.figure(figsize=(12, 6))

# Parameter space (probability of heads)
theta = np.linspace(0, 1, 1000)

# Different observed data
data_scenarios = [
    {'n': 10, 'k': 5, 'label': '5 heads in 10 flips'},
    {'n': 10, 'k': 8, 'label': '8 heads in 10 flips'},
    {'n': 20, 'k': 10, 'label': '10 heads in 20 flips'},
    {'n': 100, 'k': 50, 'label': '50 heads in 100 flips'}
]

# Plot likelihood functions
for scenario in data_scenarios:
    n, k = scenario['n'], scenario['k']
    likelihood = stats.binom.pmf(k, n, theta)
    
    # Normalize for better visualization
    likelihood = likelihood / np.max(likelihood)
    
    plt.plot(theta, likelihood, linewidth=2, label=scenario['label'])

plt.title('Likelihood Functions for Different Data')
plt.xlabel('θ (Probability of Heads)')
plt.ylabel('Normalized Likelihood')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Posterior Distribution

The **posterior distribution** combines the prior and the likelihood to represent our updated beliefs after observing the data.

For a binomial likelihood with a Beta prior (conjugate pair):

$$P(\theta|D) \propto \theta^{k+\alpha-1} (1-\theta)^{n-k+\beta-1}$$

which is a Beta distribution with parameters $\alpha' = k + \alpha$ and $\beta' = n - k + \beta$.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate posterior distribution for coin flipping
plt.figure(figsize=(15, 10))

# Parameter space (probability of heads)
theta = np.linspace(0, 1, 1000)

# Different prior and data combinations
scenarios = [
    {
        'prior': {'alpha': 1, 'beta': 1, 'label': 'Uniform Prior'},
        'data': {'n': 10, 'k': 7, 'label': '7 heads in 10 flips'}
    },
    {
        'prior': {'alpha': 5, 'beta': 5, 'label': 'Beta(5, 5) Prior'},
        'data': {'n': 10, 'k': 7, 'label': '7 heads in 10 flips'}
    },
    {
        'prior': {'alpha': 1, 'beta': 1, 'label': 'Uniform Prior'},
        'data': {'n': 100, 'k': 70, 'label': '70 heads in 100 flips'}
    },
    {
        'prior': {'alpha': 5, 'beta': 5, 'label': 'Beta(5, 5) Prior'},
        'data': {'n': 100, 'k': 70, 'label': '70 heads in 100 flips'}
    }
]

for i, scenario in enumerate(scenarios):
    plt.subplot(2, 2, i+1)
    
    # Prior
    alpha_prior = scenario['prior']['alpha']
    beta_prior = scenario['prior']['beta']
    prior = stats.beta.pdf(theta, alpha_prior, beta_prior)
    
    # Data
    n = scenario['data']['n']
    k = scenario['data']['k']
    
    # Likelihood
    likelihood = stats.binom.pmf(k, n, theta)
    
    # Posterior
    alpha_posterior = alpha_prior + k
    beta_posterior = beta_prior + n - k
    posterior = stats.beta.pdf(theta, alpha_posterior, beta_posterior)
    
    # Normalize likelihood for plotting
    likelihood = likelihood / np.max(likelihood) * np.max(posterior)
    
    # Plot
    plt.plot(theta, prior, 'b--', linewidth=2, label='Prior')
    plt.plot(theta, likelihood, 'r-', linewidth=2, label='Likelihood')
    plt.plot(theta, posterior, 'g-', linewidth=2, label='Posterior')
    
    # Add vertical lines for maximum likelihood estimate and posterior mean
    mle = k / n
    posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)
    plt.axvline(mle, color='r', linestyle=':', label=f'MLE: {mle:.3f}')
    plt.axvline(posterior_mean, color='g', linestyle=':', label=f'Posterior Mean: {posterior_mean:.3f}')
    
    plt.title(f"{scenario['prior']['label']}, {scenario['data']['label']}")
    plt.xlabel('θ (Probability of Heads)')
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Bayesian Inference

Bayesian inference involves using the posterior distribution to make probabilistic statements about parameters.

### Point Estimation

Common point estimates from the posterior distribution:
- **Posterior Mean**: $E[\theta|D]$
- **Posterior Median**: The value that divides the posterior into two equal parts
- **Maximum A Posteriori (MAP)**: The value of $\theta$ that maximizes the posterior density

### Credible Intervals

A **credible interval** is a range of values that contains the parameter with a specified probability.

Types of credible intervals:
1. **Equal-tailed Interval**: Equal probability in each tail
2. **Highest Density Interval (HDI)**: The shortest interval with the specified probability

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate Bayesian inference for coin flipping
np.random.seed(42)

# True parameter
theta_true = 0.7

# Generate data
n = 20
data = np.random.binomial(1, theta_true, n)
k = np.sum(data)

# Prior
alpha_prior, beta_prior = 1, 1  # Uniform prior

# Posterior
alpha_posterior = alpha_prior + k
beta_posterior = beta_prior + n - k

# Parameter space
theta = np.linspace(0, 1, 1000)

# Posterior distribution
posterior = stats.beta.pdf(theta, alpha_posterior, beta_posterior)

# Point estimates
posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)
posterior_median = stats.beta.median(alpha_posterior, beta_posterior)
posterior_mode = (alpha_posterior - 1) / (alpha_posterior + beta_posterior - 2) if alpha_posterior > 1 and beta_posterior > 1 else np.nan

# Credible intervals
ci_lower, ci_upper = stats.beta.interval(0.95, alpha_posterior, beta_posterior)

# Plot posterior with point estimates and credible interval
plt.figure(figsize=(10, 6))
plt.plot(theta, posterior, 'b-', linewidth=2, label='Posterior')
plt.axvline(theta_true, color='k', linestyle='-', linewidth=2, label=f'True θ: {theta_true:.3f}')
plt.axvline(posterior_mean, color='r', linestyle='--', linewidth=2, label=f'Posterior Mean: {posterior_mean:.3f}')
plt.axvline(posterior_median, color='g', linestyle='--', linewidth=2, label=f'Posterior Median: {posterior_median:.3f}')
if not np.isnan(posterior_mode):
    plt.axvline(posterior_mode, color='m', linestyle='--', linewidth=2, label=f'Posterior Mode: {posterior_mode:.3f}')

# Shade credible interval
plt.fill_between(theta, 0, posterior, where=(theta >= ci_lower) & (theta <= ci_upper), 
                color='gray', alpha=0.3, label=f'95% Credible Interval: ({ci_lower:.3f}, {ci_upper:.3f})')

plt.title(f'Posterior Distribution after {k} heads in {n} flips')
plt.xlabel('θ (Probability of Heads)')
plt.ylabel('Posterior Density')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Print summary statistics
print(f"Data: {k} heads in {n} flips")
print(f"Prior: Beta({alpha_prior}, {beta_prior})")
print(f"Posterior: Beta({alpha_posterior}, {beta_posterior})")
print(f"True θ: {theta_true:.4f}")
print(f"Posterior Mean: {posterior_mean:.4f}")
print(f"Posterior Median: {posterior_median:.4f}")
if not np.isnan(posterior_mode):
    print(f"Posterior Mode (MAP): {posterior_mode:.4f}")
print(f"95% Credible Interval: ({ci_lower:.4f}, {ci_upper:.4f})")
```

## Bayesian vs. Frequentist Approaches

Bayesian and frequentist statistics differ in their philosophical approaches and practical implementations.

### Key Differences

1. **Parameter Interpretation**:
   - Frequentist: Parameters are fixed but unknown constants
   - Bayesian: Parameters are random variables with probability distributions

2. **Probability Interpretation**:
   - Frequentist: Probability as long-run frequency
   - Bayesian: Probability as degree of belief

3. **Inference**:
   - Frequentist: Confidence intervals, p-values
   - Bayesian: Credible intervals, posterior probabilities

4. **Prior Information**:
   - Frequentist: Generally doesn't incorporate prior information
   - Bayesian: Explicitly incorporates prior information

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Compare Bayesian and frequentist approaches
np.random.seed(42)

# True parameter
theta_true = 0.7

# Generate data
n = 20
data = np.random.binomial(1, theta_true, n)
k = np.sum(data)

# Frequentist approach
mle = k / n  # Maximum likelihood estimate
se = np.sqrt(mle * (1 - mle) / n)  # Standard error
ci_lower_freq = max(0, mle - 1.96 * se)  # 95% confidence interval
ci_upper_freq = min(1, mle + 1.96 * se)

# Bayesian approach
alpha_prior, beta_prior = 1, 1  # Uniform prior
alpha_posterior = alpha_prior + k
beta_posterior = beta_prior + n - k
posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)
ci_lower_bayes, ci_upper_bayes = stats.beta.interval(0.95, alpha_posterior, beta_posterior)

# Parameter space
theta = np.linspace(0, 1, 1000)

# Plot comparison
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
# Frequentist approach
plt.axvline(mle, color='b', linestyle='-', linewidth=2, label=f'MLE: {mle:.3f}')
plt.axvline(ci_lower_freq, color='b', linestyle='--', linewidth=2)
plt.axvline(ci_upper_freq, color='b', linestyle='--', linewidth=2)
plt.axhline(0.5, color='b', linestyle='-', linewidth=0.5)
plt.annotate(f'95% CI: ({ci_lower_freq:.3f}, {ci_upper_freq:.3f})', 
            xy=(0.5, 0.7), xytext=(0.5, 0.7), ha='center')
plt.axvline(theta_true, color='k', linestyle='-', linewidth=2, label=f'True θ: {theta_true:.3f}')
plt.title('Frequentist Approach')
plt.xlabel('θ (Probability of Heads)')
plt.ylim(0, 1)
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Bayesian approach
posterior = stats.beta.pdf(theta, alpha_posterior, beta_posterior)
plt.plot(theta, posterior / np.max(posterior), 'r-', linewidth=2, label='Posterior')
plt.axvline(posterior_mean, color='r', linestyle='-', linewidth=2, label=f'Posterior Mean: {posterior_mean:.3f}')
plt.axvline(ci_lower_bayes, color='r', linestyle='--', linewidth=2)
plt.axvline(ci_upper_bayes, color='r', linestyle='--', linewidth=2)
plt.fill_between(theta, 0, posterior / np.max(posterior), where=(theta >= ci_lower_bayes) & (theta <= ci_upper_bayes), 
                color='red', alpha=0.3)
plt.annotate(f'95% CI: ({ci_lower_bayes:.3f}, {ci_upper_bayes:.3f})', 
            xy=(0.5, 0.7), xytext=(0.5, 0.7), ha='center')
plt.axvline(theta_true, color='k', linestyle='-', linewidth=2, label=f'True θ: {theta_true:.3f}')
plt.title('Bayesian Approach')
plt.xlabel('θ (Probability of Heads)')
plt.ylim(0, 1)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Print comparison
print(f"Data: {k} heads in {n} flips")
print(f"True θ: {theta_true:.4f}")
print("\nFrequentist Approach:")
print(f"Maximum Likelihood Estimate: {mle:.4f}")
print(f"Standard Error: {se:.4f}")
print(f"95% Confidence Interval: ({ci_lower_freq:.4f}, {ci_upper_freq:.4f})")
print("\nBayesian Approach:")
print(f"Posterior Mean: {posterior_mean:.4f}")
print(f"95% Credible Interval: ({ci_lower_bayes:.4f}, {ci_upper_bayes:.4f})")
```

## Summary

Bayesian statistics provides a framework for updating beliefs based on evidence:

1. **Bayes' Theorem**:
   - Posterior ∝ Likelihood × Prior
   - Formalizes the process of updating beliefs

2. **Prior Distributions**:
   - Represent initial beliefs about parameters
   - Can be informative, uninformative, conjugate, or improper

3. **Likelihood Function**:
   - Represents the probability of the data given the parameters
   - Quantifies the evidence provided by the data

4. **Posterior Distribution**:
   - Represents updated beliefs after observing the data
   - Combines prior knowledge with evidence from the data

5. **Bayesian Inference**:
   - Point estimates: Posterior mean, median, mode (MAP)
   - Credible intervals: Ranges with specified posterior probability

6. **Bayesian vs. Frequentist Approaches**:
   - Different philosophical foundations
   - Different interpretations of probability and parameters
   - Different methods for inference and uncertainty quantification

In Part 2, we will explore more advanced Bayesian methods, including Bayesian regression, hierarchical models, and computational techniques for Bayesian inference.

## References

1. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
2. McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.
3. Kruschke, J. K. (2014). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (2nd ed.). Academic Press.
4. Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge University Press.
