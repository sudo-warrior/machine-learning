# 2.4.3 Hypothesis Testing

## Introduction to Hypothesis Testing

Hypothesis testing is a statistical method used to make decisions about populations based on sample data. It provides a framework for determining whether observed differences are statistically significant or merely due to random chance. This section covers the essential concepts and common tests used in machine learning and data science.

## Fundamentals of Hypothesis Testing

### The Hypothesis Testing Framework

1. **State the hypotheses**:
   - **Null Hypothesis (H₀)**: A statement of no effect, no difference, or no relationship
   - **Alternative Hypothesis (H₁ or Hₐ)**: A statement of an effect, difference, or relationship

2. **Choose a significance level (α)**:
   - Typically 0.05 (5%), 0.01 (1%), or 0.001 (0.1%)
   - Represents the probability of rejecting a true null hypothesis

3. **Calculate a test statistic**:
   - A numerical value calculated from the sample data
   - Examples: t-statistic, z-statistic, F-statistic, chi-square statistic

4. **Determine the p-value**:
   - The probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true

5. **Make a decision**:
   - If p-value ≤ α: Reject the null hypothesis
   - If p-value > α: Fail to reject the null hypothesis

### Types of Errors

- **Type I Error (False Positive)**: Rejecting a true null hypothesis (probability = α)
- **Type II Error (False Negative)**: Failing to reject a false null hypothesis (probability = β)
- **Power**: The probability of correctly rejecting a false null hypothesis (1 - β)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Visualize Type I and Type II errors
np.random.seed(42)

# Parameters
mu0 = 0  # Null hypothesis mean
mu1 = 1  # Alternative hypothesis mean
sigma = 1  # Standard deviation
alpha = 0.05  # Significance level
n = 20  # Sample size

# Critical value for two-tailed test
critical_value = stats.norm.ppf(1 - alpha/2) * sigma / np.sqrt(n)
critical_x = mu0 + critical_value

# Generate x values for plotting
x = np.linspace(-3, 4, 1000)

# Calculate PDFs
pdf_h0 = stats.norm.pdf(x, mu0, sigma / np.sqrt(n))
pdf_h1 = stats.norm.pdf(x, mu1, sigma / np.sqrt(n))

# Plot the distributions
plt.figure(figsize=(12, 6))
plt.plot(x, pdf_h0, 'b-', linewidth=2, label='Null Distribution (H₀)')
plt.plot(x, pdf_h1, 'r-', linewidth=2, label='Alternative Distribution (H₁)')

# Shade Type I error region
x_type1 = x[(x >= critical_x) | (x <= -critical_x)]
y_type1 = stats.norm.pdf(x_type1, mu0, sigma / np.sqrt(n))
plt.fill_between(x_type1, y_type1, 0, color='blue', alpha=0.3, label='Type I Error (α)')

# Shade Type II error region
x_type2 = x[(x <= critical_x) & (x >= -critical_x)]
y_type2 = stats.norm.pdf(x_type2, mu1, sigma / np.sqrt(n))
plt.fill_between(x_type2, y_type2, 0, color='red', alpha=0.3, label='Type II Error (β)')

# Add vertical lines for critical values
plt.axvline(critical_x, color='black', linestyle='--', linewidth=1)
plt.axvline(-critical_x, color='black', linestyle='--', linewidth=1)

plt.title('Type I and Type II Errors in Hypothesis Testing')
plt.xlabel('Sample Mean')
plt.ylabel('Probability Density')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Common Hypothesis Tests

### Z-Test

The **z-test** is used when:
- The population standard deviation is known
- The sample size is large (n ≥ 30) or the population is normally distributed

#### One-Sample Z-Test

Tests whether a sample mean differs from a known population mean.

$$z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}$$

```python
import numpy as np
from scipy import stats

# Generate sample data
np.random.seed(42)
population_mean = 100
population_std = 15
sample_size = 50
sample = np.random.normal(loc=105, scale=population_std, size=sample_size)

# Perform one-sample z-test
sample_mean = np.mean(sample)
z_stat = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))
p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test

# Print results
print(f"Sample Mean: {sample_mean:.2f}")
print(f"Population Mean: {population_mean}")
print(f"Z-statistic: {z_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

### T-Test

The **t-test** is used when:
- The population standard deviation is unknown
- The sample size is small (n < 30)

#### One-Sample T-Test

Tests whether a sample mean differs from a hypothesized population mean.

$$t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$$

```python
import numpy as np
from scipy import stats

# Generate sample data
np.random.seed(42)
hypothesized_mean = 100
sample_size = 20
sample = np.random.normal(loc=105, scale=15, size=sample_size)

# Perform one-sample t-test
t_stat, p_value = stats.ttest_1samp(sample, hypothesized_mean)

# Print results
print(f"Sample Mean: {np.mean(sample):.2f}")
print(f"Sample Standard Deviation: {np.std(sample, ddof=1):.2f}")
print(f"Hypothesized Mean: {hypothesized_mean}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

#### Two-Sample T-Test

Tests whether the means of two independent samples differ.

```python
import numpy as np
from scipy import stats

# Generate two sample groups
np.random.seed(42)
group1 = np.random.normal(loc=100, scale=15, size=30)
group2 = np.random.normal(loc=110, scale=15, size=30)

# Perform two-sample t-test
t_stat, p_value = stats.ttest_ind(group1, group2)

# Print results
print(f"Group 1 Mean: {np.mean(group1):.2f}")
print(f"Group 2 Mean: {np.mean(group2):.2f}")
print(f"Mean Difference: {np.mean(group2) - np.mean(group1):.2f}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

#### Paired T-Test

Tests whether the mean difference between paired observations is zero.

```python
import numpy as np
from scipy import stats

# Generate paired data (e.g., before and after measurements)
np.random.seed(42)
before = np.random.normal(loc=100, scale=15, size=30)
effect = 5 + np.random.normal(loc=0, scale=5, size=30)  # True effect plus noise
after = before + effect

# Perform paired t-test
t_stat, p_value = stats.ttest_rel(before, after)

# Print results
print(f"Mean Before: {np.mean(before):.2f}")
print(f"Mean After: {np.mean(after):.2f}")
print(f"Mean Difference: {np.mean(after - before):.2f}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

### ANOVA (Analysis of Variance)

**ANOVA** tests whether the means of three or more independent groups differ.

#### One-Way ANOVA

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Generate data for three groups
np.random.seed(42)
group1 = np.random.normal(loc=100, scale=15, size=30)
group2 = np.random.normal(loc=110, scale=15, size=30)
group3 = np.random.normal(loc=105, scale=15, size=30)

# Perform one-way ANOVA
f_stat, p_value = stats.f_oneway(group1, group2, group3)

# Print results
print(f"Group 1 Mean: {np.mean(group1):.2f}")
print(f"Group 2 Mean: {np.mean(group2):.2f}")
print(f"Group 3 Mean: {np.mean(group3):.2f}")
print(f"F-statistic: {f_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")

# Visualize the groups
plt.figure(figsize=(10, 6))
plt.boxplot([group1, group2, group3], labels=['Group 1', 'Group 2', 'Group 3'])
plt.title('One-Way ANOVA: Comparison of Three Groups')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)
plt.show()
```

### Chi-Square Test

The **chi-square test** is used for categorical data.

#### Chi-Square Test of Independence

Tests whether two categorical variables are independent.

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Create a contingency table
# Rows: Treatment (A, B)
# Columns: Outcome (Success, Failure)
observed = np.array([[40, 10],
                     [25, 25]])

# Perform chi-square test
chi2_stat, p_value, dof, expected = stats.chi2_contingency(observed)

# Print results
print(f"Contingency Table (Observed):")
print(f"            Success  Failure")
print(f"Treatment A    {observed[0, 0]}       {observed[0, 1]}")
print(f"Treatment B    {observed[1, 0]}       {observed[1, 1]}")
print(f"\nChi-square statistic: {chi2_stat:.4f}")
print(f"Degrees of freedom: {dof}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")

# Visualize the observed vs. expected frequencies
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(['A-Success', 'A-Failure', 'B-Success', 'B-Failure'], 
       [observed[0, 0], observed[0, 1], observed[1, 0], observed[1, 1]], 
       color='skyblue')
plt.title('Observed Frequencies')
plt.ylabel('Count')
plt.grid(True, alpha=0.3, axis='y')

plt.subplot(1, 2, 2)
plt.bar(['A-Success', 'A-Failure', 'B-Success', 'B-Failure'], 
       [expected[0, 0], expected[0, 1], expected[1, 0], expected[1, 1]], 
       color='lightgreen')
plt.title('Expected Frequencies (If Independent)')
plt.ylabel('Count')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### Non-Parametric Tests

Non-parametric tests don't assume a specific distribution for the data.

#### Mann-Whitney U Test

A non-parametric alternative to the two-sample t-test.

```python
import numpy as np
from scipy import stats

# Generate two sample groups (one with a different distribution)
np.random.seed(42)
group1 = np.random.normal(loc=100, scale=15, size=30)
group2 = np.random.exponential(scale=15, size=30) + 90  # Skewed distribution

# Perform Mann-Whitney U test
u_stat, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')

# Print results
print(f"Group 1 Median: {np.median(group1):.2f}")
print(f"Group 2 Median: {np.median(group2):.2f}")
print(f"U-statistic: {u_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

#### Wilcoxon Signed-Rank Test

A non-parametric alternative to the paired t-test.

```python
import numpy as np
from scipy import stats

# Generate paired data with a skewed difference
np.random.seed(42)
before = np.random.normal(loc=100, scale=15, size=30)
difference = np.random.exponential(scale=5, size=30)  # Skewed difference
after = before + difference

# Perform Wilcoxon signed-rank test
w_stat, p_value = stats.wilcoxon(before, after)

# Print results
print(f"Median Before: {np.median(before):.2f}")
print(f"Median After: {np.median(after):.2f}")
print(f"Median Difference: {np.median(after - before):.2f}")
print(f"W-statistic: {w_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
```

## Multiple Testing Problem

When performing multiple hypothesis tests, the probability of making at least one Type I error increases.

### Bonferroni Correction

Adjusts the significance level by dividing α by the number of tests.

```python
import numpy as np
from scipy import stats

# Generate multiple independent samples (all from the same distribution)
np.random.seed(42)
num_tests = 20
samples = [np.random.normal(loc=100, scale=15, size=30) for _ in range(num_tests)]

# Perform multiple t-tests against a hypothesized mean
hypothesized_mean = 100
p_values = []

for i, sample in enumerate(samples):
    _, p_value = stats.ttest_1samp(sample, hypothesized_mean)
    p_values.append(p_value)
    print(f"Test {i+1}: p-value = {p_value:.4f}")

# Apply Bonferroni correction
alpha = 0.05
bonferroni_alpha = alpha / num_tests

print(f"\nOriginal significance level (α): {alpha}")
print(f"Bonferroni-corrected significance level: {bonferroni_alpha:.4f}")

# Count significant results before and after correction
sig_original = sum(p < alpha for p in p_values)
sig_bonferroni = sum(p < bonferroni_alpha for p in p_values)

print(f"Number of significant results (original α): {sig_original}")
print(f"Number of significant results (Bonferroni-corrected): {sig_bonferroni}")
```

### False Discovery Rate (FDR) Control

The Benjamini-Hochberg procedure controls the expected proportion of false positives.

```python
import numpy as np
from scipy import stats

# Function to apply Benjamini-Hochberg procedure
def benjamini_hochberg(p_values, alpha=0.05):
    # Sort p-values in ascending order
    sorted_indices = np.argsort(p_values)
    sorted_p_values = np.array(p_values)[sorted_indices]
    n = len(p_values)
    
    # Find the largest k such that P(k) ≤ (k/n) * α
    k = 0
    for i in range(n):
        if sorted_p_values[i] <= (i + 1) / n * alpha:
            k = i + 1
    
    # Adjust p-values
    adjusted_p_values = np.zeros(n)
    for i in range(n):
        adjusted_p_values[sorted_indices[i]] = sorted_p_values[i] * n / (i + 1)
    
    # Ensure monotonicity
    for i in range(n-2, -1, -1):
        adjusted_p_values[sorted_indices[i]] = min(adjusted_p_values[sorted_indices[i]], 
                                                 adjusted_p_values[sorted_indices[i+1]])
    
    return adjusted_p_values, sorted_p_values, k

# Apply Benjamini-Hochberg procedure to the p-values
adjusted_p_values, sorted_p_values, k = benjamini_hochberg(p_values)

print(f"\nBenjamini-Hochberg procedure:")
print(f"Number of significant results: {k}")

# Compare methods
print(f"\nComparison of multiple testing corrections:")
print(f"Original (uncorrected): {sig_original} significant results")
print(f"Bonferroni correction: {sig_bonferroni} significant results")
print(f"Benjamini-Hochberg procedure: {k} significant results")
```

## Hypothesis Testing in Machine Learning

### Model Comparison

Hypothesis testing can be used to compare the performance of different models.

```python
import numpy as np
from scipy import stats
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

# Load a dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Define models
rf = RandomForestClassifier(random_state=42)
lr = LogisticRegression(random_state=42, max_iter=1000)

# Perform cross-validation
np.random.seed(42)
rf_scores = cross_val_score(rf, X, y, cv=10, scoring='accuracy')
lr_scores = cross_val_score(lr, X, y, cv=10, scoring='accuracy')

# Print mean accuracies
print(f"Random Forest mean accuracy: {np.mean(rf_scores):.4f}")
print(f"Logistic Regression mean accuracy: {np.mean(lr_scores):.4f}")

# Perform paired t-test to compare models
t_stat, p_value = stats.ttest_rel(rf_scores, lr_scores)

print(f"\nPaired t-test results:")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
print(f"Interpretation: {'There is' if p_value < 0.05 else 'There is not'} a statistically significant difference between the models.")
```

### Feature Importance

Hypothesis testing can determine whether features have a significant relationship with the target variable.

```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.datasets import load_breast_cancer

# Load a dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Convert to DataFrame for easier manipulation
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y

# Perform t-tests to compare feature distributions between classes
p_values = []
t_stats = []

for feature in data.feature_names:
    # Split data by class
    feature_class0 = df[df['target'] == 0][feature]
    feature_class1 = df[df['target'] == 1][feature]
    
    # Perform t-test
    t_stat, p_value = stats.ttest_ind(feature_class0, feature_class1)
    
    t_stats.append(t_stat)
    p_values.append(p_value)

# Create a DataFrame with results
results = pd.DataFrame({
    'Feature': data.feature_names,
    'T-statistic': t_stats,
    'P-value': p_values
})

# Sort by p-value
results = results.sort_values('P-value')

# Apply Bonferroni correction
alpha = 0.05
bonferroni_alpha = alpha / len(data.feature_names)
results['Significant (Bonferroni)'] = results['P-value'] < bonferroni_alpha

# Display top 10 most significant features
print(results.head(10))

# Count significant features
print(f"\nTotal features: {len(data.feature_names)}")
print(f"Significant features (α = 0.05): {sum(results['P-value'] < 0.05)}")
print(f"Significant features after Bonferroni correction: {sum(results['Significant (Bonferroni)'])}")
```

### A/B Testing in Machine Learning

A/B testing is used to compare different versions of models or systems.

```python
import numpy as np
from scipy import stats

# Simulate A/B test results for two recommendation algorithms
np.random.seed(42)

# Algorithm A: 5% click-through rate
n_a = 10000
clicks_a = np.random.binomial(1, 0.05, n_a)

# Algorithm B: 5.5% click-through rate
n_b = 10000
clicks_b = np.random.binomial(1, 0.055, n_b)

# Calculate click-through rates
ctr_a = np.mean(clicks_a)
ctr_b = np.mean(clicks_b)

# Perform two-proportion z-test
count_a = np.sum(clicks_a)
count_b = np.sum(clicks_b)
p_pooled = (count_a + count_b) / (n_a + n_b)
z_stat = (ctr_b - ctr_a) / np.sqrt(p_pooled * (1 - p_pooled) * (1/n_a + 1/n_b))
p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test

# Print results
print(f"Algorithm A: {count_a} clicks out of {n_a} impressions (CTR: {ctr_a:.4f})")
print(f"Algorithm B: {count_b} clicks out of {n_b} impressions (CTR: {ctr_b:.4f})")
print(f"Absolute difference: {ctr_b - ctr_a:.4f}")
print(f"Relative difference: {(ctr_b - ctr_a) / ctr_a:.4f} ({(ctr_b - ctr_a) / ctr_a * 100:.2f}%)")
print(f"Z-statistic: {z_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Conclusion: {'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis at α = 0.05")
print(f"Interpretation: {'There is' if p_value < 0.05 else 'There is not'} a statistically significant difference between the algorithms.")
```

## Summary

Hypothesis testing provides a framework for making statistical inferences:

1. **Fundamentals**:
   - Null and Alternative Hypotheses
   - Significance Level and p-value
   - Type I and Type II Errors

2. **Common Tests**:
   - Z-Test: Known population standard deviation
   - T-Test: Unknown population standard deviation
   - ANOVA: Comparing multiple groups
   - Chi-Square Test: Categorical data
   - Non-Parametric Tests: No distributional assumptions

3. **Multiple Testing Problem**:
   - Bonferroni Correction: Controls family-wise error rate
   - False Discovery Rate Control: Controls proportion of false positives

4. **Applications in Machine Learning**:
   - Model Comparison: Determining if one model outperforms another
   - Feature Importance: Identifying significant predictors
   - A/B Testing: Comparing different versions of systems

Hypothesis testing is essential for making data-driven decisions and ensuring that observed differences are not merely due to random chance.

## References

1. Wasserman, L. (2013). All of Statistics: A Concise Course in Statistical Inference. Springer.
2. Lehmann, E. L., & Romano, J. P. (2005). Testing Statistical Hypotheses (3rd ed.). Springer.
3. Benjamini, Y., & Hochberg, Y. (1995). Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society, Series B, 57(1), 289-300.
4. Kohavi, R., Longbotham, R., Sommerfield, D., & Henne, R. M. (2009). Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1), 140-181.
