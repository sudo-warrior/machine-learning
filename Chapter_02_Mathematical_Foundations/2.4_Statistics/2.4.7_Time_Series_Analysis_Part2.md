# 2.4.7 Time Series Analysis - Part 2

## Advanced Time Series Models

Building on the foundations covered in Part 1, this section explores more advanced time series models that handle non-stationarity, seasonality, and complex patterns in time series data.

## ARIMA Models

**Autoregressive Integrated Moving Average (ARIMA)** models extend ARMA models to handle non-stationary data through differencing.

An **ARIMA(p,d,q)** model has three components:
- p: Order of the autoregressive (AR) component
- d: Degree of differencing required to make the series stationary
- q: Order of the moving average (MA) component

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller

# Generate a non-stationary time series
np.random.seed(42)
n = 200

# Random walk with drift
drift = 0.1
random_walk = np.zeros(n)
e = np.random.normal(0, 1, n)
random_walk[0] = e[0]
for t in range(1, n):
    random_walk[t] = drift + random_walk[t-1] + e[t]

# Test for stationarity
result = adfuller(random_walk)
print(f"ADF Statistic: {result[0]:.4f}")
print(f"p-value: {result[1]:.4f}")
print(f"Conclusion: {'Stationary' if result[1] < 0.05 else 'Non-stationary'} at 5% significance level")

# First difference
diff1 = np.diff(random_walk)

# Test for stationarity of the differenced series
result = adfuller(diff1)
print(f"\nADF Statistic (First Difference): {result[0]:.4f}")
print(f"p-value: {result[1]:.4f}")
print(f"Conclusion: {'Stationary' if result[1] < 0.05 else 'Non-stationary'} at 5% significance level")

# Fit an ARIMA(1,1,1) model
model = ARIMA(random_walk, order=(1, 1, 1))
results = model.fit()
print("\nARIMA Model Summary:")
print(results.summary().tables[1])

# Generate forecasts
forecast_steps = 50
forecast = results.get_forecast(steps=forecast_steps)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Plot the series and forecasts
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(range(n), random_walk, label='Observed')
plt.plot(range(n, n + forecast_steps), forecast_mean, color='red', label='Forecast')
plt.fill_between(range(n, n + forecast_steps), 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1], 
                color='pink', alpha=0.3)
plt.title('Non-stationary Series with ARIMA(1,1,1) Forecast')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 1, 2)
plt.plot(range(n-1), diff1, label='First Difference')
plt.title('First Difference of the Series')
plt.xlabel('Time')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Seasonal ARIMA (SARIMA) Models

**Seasonal ARIMA (SARIMA)** models extend ARIMA to handle seasonal patterns.

A **SARIMA(p,d,q)(P,D,Q)s** model has:
- (p,d,q): Non-seasonal components
- (P,D,Q): Seasonal components
- s: Seasonal period

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Generate a seasonal time series
np.random.seed(42)
n = 144  # 12 years of monthly data

# Time index
t = np.arange(n)

# Trend component
trend = 0.1 * t

# Seasonal component (period = 12)
seasonal = 5 * np.sin(2 * np.pi * t / 12)

# AR(1) component
ar = np.zeros(n)
ar[0] = np.random.normal(0, 1)
for i in range(1, n):
    ar[i] = 0.7 * ar[i-1] + np.random.normal(0, 1)

# Combine components
time_series = trend + seasonal + ar

# Create a pandas Series with a DateTimeIndex
dates = pd.date_range(start='2010-01-01', periods=n, freq='MS')
ts = pd.Series(time_series, index=dates)

# Fit a SARIMA model
model = SARIMAX(ts, order=(1, 0, 0), seasonal_order=(1, 1, 0, 12))
results = model.fit(disp=False)
print("SARIMA Model Summary:")
print(results.summary().tables[1])

# Generate forecasts
forecast_steps = 24  # 2 years
forecast = results.get_forecast(steps=forecast_steps)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Plot the series and forecasts
plt.figure(figsize=(12, 6))
plt.plot(ts.index, ts, label='Observed')
plt.plot(forecast_mean.index, forecast_mean, color='red', label='Forecast')
plt.fill_between(forecast_mean.index, 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1], 
                color='pink', alpha=0.3)
plt.title('Seasonal Time Series with SARIMA(1,0,0)(1,1,0,12) Forecast')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Plot the components
plt.figure(figsize=(12, 8))

plt.subplot(3, 1, 1)
plt.plot(t, trend)
plt.title('Trend Component')
plt.grid(True, alpha=0.3)

plt.subplot(3, 1, 2)
plt.plot(t, seasonal)
plt.title('Seasonal Component')
plt.grid(True, alpha=0.3)

plt.subplot(3, 1, 3)
plt.plot(t, ar)
plt.title('AR(1) Component')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## GARCH Models

**Generalized Autoregressive Conditional Heteroskedasticity (GARCH)** models are used to model time-varying volatility.

A **GARCH(p,q)** model has:
- p: Order of the GARCH terms (lagged conditional variance)
- q: Order of the ARCH terms (lagged squared residuals)

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from arch import arch_model

# Generate a GARCH(1,1) process
np.random.seed(42)
n = 1000

# Parameters
omega = 0.1  # Constant
alpha = 0.2  # ARCH parameter
beta = 0.7   # GARCH parameter

# Initialize arrays
returns = np.zeros(n)
variance = np.zeros(n)
variance[0] = omega / (1 - alpha - beta)  # Unconditional variance

# Generate the process
for t in range(1, n):
    # Conditional variance
    variance[t] = omega + alpha * returns[t-1]**2 + beta * variance[t-1]
    
    # Return
    returns[t] = np.sqrt(variance[t]) * np.random.normal(0, 1)

# Fit a GARCH(1,1) model
model = arch_model(returns, vol='GARCH', p=1, q=1)
results = model.fit(disp='off')
print(results.summary())

# Generate forecasts
forecast = results.forecast(horizon=50)
forecast_variance = forecast.variance.values[-1, :]

# Plot the series and volatility
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(returns)
plt.title('GARCH(1,1) Process')
plt.xlabel('Time')
plt.ylabel('Returns')
plt.grid(True, alpha=0.3)

plt.subplot(2, 1, 2)
plt.plot(np.sqrt(variance), label='True Volatility')
plt.plot(np.sqrt(results.conditional_volatility), 'r--', label='Estimated Volatility')
plt.title('Volatility')
plt.xlabel('Time')
plt.ylabel('Standard Deviation')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Plot the forecasted volatility
plt.figure(figsize=(10, 6))
plt.plot(range(n, n + 50), np.sqrt(forecast_variance), 'r-', label='Forecasted Volatility')
plt.title('Volatility Forecast')
plt.xlabel('Time')
plt.ylabel('Standard Deviation')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Vector Autoregression (VAR)

**Vector Autoregression (VAR)** models extend univariate autoregressive models to multivariate time series.

A **VAR(p)** model for a k-dimensional time series $y_t$ is:

$$y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \ldots + A_p y_{t-p} + \varepsilon_t$$

where:
- $y_t$ is a k×1 vector of variables
- $c$ is a k×1 vector of constants
- $A_i$ are k×k matrices of coefficients
- $\varepsilon_t$ is a k×1 vector of error terms

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.api import VAR

# Generate a bivariate VAR(1) process
np.random.seed(42)
n = 200

# VAR(1) parameters
A = np.array([[0.5, 0.3],
              [0.2, 0.7]])  # Coefficient matrix

# Initialize arrays
y = np.zeros((n, 2))
e = np.random.multivariate_normal(mean=[0, 0], cov=np.eye(2), size=n)

# Generate the process
for t in range(1, n):
    y[t] = A @ y[t-1] + e[t]

# Create a pandas DataFrame
dates = pd.date_range(start='2010-01-01', periods=n, freq='MS')
df = pd.DataFrame(y, index=dates, columns=['y1', 'y2'])

# Fit a VAR model
model = VAR(df)
results = model.fit(1)  # VAR(1)
print(results.summary())

# Generate forecasts
forecast_steps = 20
forecast = results.forecast(df.values, steps=forecast_steps)

# Plot the series and forecasts
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(df.index, df['y1'], label='y1 (Observed)')
plt.plot(pd.date_range(start=df.index[-1], periods=forecast_steps+1, freq='MS')[1:], 
         forecast[:, 0], 'r--', label='y1 (Forecast)')
plt.title('VAR(1) Process - Variable y1')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 1, 2)
plt.plot(df.index, df['y2'], label='y2 (Observed)')
plt.plot(pd.date_range(start=df.index[-1], periods=forecast_steps+1, freq='MS')[1:], 
         forecast[:, 1], 'r--', label='y2 (Forecast)')
plt.title('VAR(1) Process - Variable y2')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Impulse Response Functions
irf = results.irf(10)
irf.plot()
plt.suptitle('Impulse Response Functions')
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
```

## State Space Models

**State space models** represent a time series in terms of state variables, which evolve over time according to probabilistic rules.

### Kalman Filter

The **Kalman filter** is an algorithm for estimating the state of a linear dynamic system from noisy measurements.

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.kalman_filter import KalmanFilter

# Generate a simple state space model
np.random.seed(42)
n = 100

# True state (random walk)
state_true = np.zeros(n)
state_noise = np.random.normal(0, 1, n)
for t in range(1, n):
    state_true[t] = state_true[t-1] + state_noise[t]

# Observations (state + noise)
observation_noise = np.random.normal(0, 2, n)
observations = state_true + observation_noise

# Set up the Kalman filter
# State transition: x_t = x_{t-1} + w_t, w_t ~ N(0, 1)
# Observation: y_t = x_t + v_t, v_t ~ N(0, 4)
kf = KalmanFilter(
    k_states=1,
    k_endog=1,
    transition=np.array([[1]]),
    selection=np.array([[1]]),
    state_cov=np.array([[1]]),
    design=np.array([[1]]),
    obs_cov=np.array([[4]])
)

# Initialize the state
initial_state = np.array([0])
initial_state_cov = np.array([[1]])
kf.initialize_known(initial_state, initial_state_cov)

# Apply the Kalman filter
filtered_state, filtered_state_cov = kf.filter(observations)

# Extract the filtered state and its variance
filtered_mean = filtered_state[0]
filtered_var = filtered_state_cov[0, 0, :]

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(state_true, 'k-', label='True State')
plt.plot(observations, 'b.', alpha=0.5, label='Observations')
plt.plot(filtered_mean, 'r-', label='Filtered State')
plt.fill_between(range(n), 
                filtered_mean - 2 * np.sqrt(filtered_var), 
                filtered_mean + 2 * np.sqrt(filtered_var), 
                color='red', alpha=0.2)
plt.title('Kalman Filter Example')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Model Selection and Evaluation

### Information Criteria

**Information criteria** help select the best model by balancing goodness of fit and model complexity:

- **Akaike Information Criterion (AIC)**: $AIC = -2 \ln(L) + 2k$
- **Bayesian Information Criterion (BIC)**: $BIC = -2 \ln(L) + k \ln(n)$

where:
- $L$ is the likelihood of the model
- $k$ is the number of parameters
- $n$ is the number of observations

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Generate an ARMA(2,1) process
np.random.seed(42)
n = 200
ar_params = [0.7, -0.3]  # AR parameters
ma_params = [0.5]        # MA parameters
arma21 = np.zeros(n)
e = np.random.normal(0, 1, n)
arma21[0] = e[0]
arma21[1] = ar_params[0] * arma21[0] + e[1]
for t in range(2, n):
    arma21[t] = ar_params[0] * arma21[t-1] + ar_params[1] * arma21[t-2] + e[t] + ma_params[0] * e[t-1]

# Fit different ARMA models
max_p = 3
max_q = 3
results = {}

for p in range(max_p + 1):
    for q in range(max_q + 1):
        if p == 0 and q == 0:
            continue  # Skip the trivial model
        
        try:
            model = ARIMA(arma21, order=(p, 0, q))
            model_fit = model.fit()
            results[(p, q)] = {
                'aic': model_fit.aic,
                'bic': model_fit.bic,
                'model': model_fit
            }
            print(f"ARMA({p},{q}): AIC = {model_fit.aic:.4f}, BIC = {model_fit.bic:.4f}")
        except:
            print(f"ARMA({p},{q}): Failed to converge")

# Find the best model according to AIC and BIC
best_aic = min(results.items(), key=lambda x: x[1]['aic'])
best_bic = min(results.items(), key=lambda x: x[1]['bic'])

print(f"\nBest model according to AIC: ARMA{best_aic[0]} with AIC = {best_aic[1]['aic']:.4f}")
print(f"Best model according to BIC: ARMA{best_bic[0]} with BIC = {best_bic[1]['bic']:.4f}")
print(f"True model: ARMA(2,1)")

# Plot AIC and BIC values
plt.figure(figsize=(15, 6))

# AIC
plt.subplot(1, 2, 1)
aic_values = np.zeros((max_p + 1, max_q + 1))
aic_values.fill(np.nan)  # Fill with NaN for models that failed to converge
for (p, q), res in results.items():
    aic_values[p, q] = res['aic']

plt.imshow(aic_values, interpolation='nearest', cmap='viridis')
plt.colorbar(label='AIC')
plt.title('AIC Values for ARMA(p,q) Models')
plt.xlabel('q (MA order)')
plt.ylabel('p (AR order)')
plt.xticks(range(max_q + 1))
plt.yticks(range(max_p + 1))
for p in range(max_p + 1):
    for q in range(max_q + 1):
        if not np.isnan(aic_values[p, q]):
            plt.text(q, p, f"{aic_values[p, q]:.1f}", ha='center', va='center', 
                    color='white' if aic_values[p, q] > np.nanmean(aic_values) else 'black')

# BIC
plt.subplot(1, 2, 2)
bic_values = np.zeros((max_p + 1, max_q + 1))
bic_values.fill(np.nan)  # Fill with NaN for models that failed to converge
for (p, q), res in results.items():
    bic_values[p, q] = res['bic']

plt.imshow(bic_values, interpolation='nearest', cmap='viridis')
plt.colorbar(label='BIC')
plt.title('BIC Values for ARMA(p,q) Models')
plt.xlabel('q (MA order)')
plt.ylabel('p (AR order)')
plt.xticks(range(max_q + 1))
plt.yticks(range(max_p + 1))
for p in range(max_p + 1):
    for q in range(max_q + 1):
        if not np.isnan(bic_values[p, q]):
            plt.text(q, p, f"{bic_values[p, q]:.1f}", ha='center', va='center', 
                    color='white' if bic_values[p, q] > np.nanmean(bic_values) else 'black')

plt.tight_layout()
plt.show()
```

### Forecast Evaluation

Common metrics for evaluating forecast accuracy:

- **Mean Absolute Error (MAE)**: $MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$
- **Mean Squared Error (MSE)**: $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
- **Root Mean Squared Error (RMSE)**: $RMSE = \sqrt{MSE}$
- **Mean Absolute Percentage Error (MAPE)**: $MAPE = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|$

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

# Generate an ARMA(1,1) process
np.random.seed(42)
n = 300  # Longer series for train-test split
ar_params = [0.7]  # AR parameters
ma_params = [0.5]  # MA parameters
arma11 = np.zeros(n)
e = np.random.normal(0, 1, n)
arma11[0] = e[0]
for t in range(1, n):
    arma11[t] = ar_params[0] * arma11[t-1] + e[t] + ma_params[0] * e[t-1]

# Split into training and test sets
train_size = 250
train, test = arma11[:train_size], arma11[train_size:]

# Fit different models
models = {
    'AR(1)': ARIMA(train, order=(1, 0, 0)),
    'MA(1)': ARIMA(train, order=(0, 0, 1)),
    'ARMA(1,1)': ARIMA(train, order=(1, 0, 1)),
    'ARMA(2,1)': ARIMA(train, order=(2, 0, 1))
}

# Generate forecasts and evaluate
results = {}
for name, model in models.items():
    # Fit the model
    model_fit = model.fit()
    
    # Generate forecasts
    forecast = model_fit.forecast(steps=len(test))
    
    # Calculate error metrics
    mae = mean_absolute_error(test, forecast)
    mse = mean_squared_error(test, forecast)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(test, forecast) * 100  # Convert to percentage
    
    # Store results
    results[name] = {
        'forecast': forecast,
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'mape': mape
    }
    
    print(f"{name}:")
    print(f"  MAE: {mae:.4f}")
    print(f"  MSE: {mse:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAPE: {mape:.4f}%")

# Plot the forecasts
plt.figure(figsize=(12, 8))

# Plot the data
plt.plot(range(train_size), train, 'b-', label='Training Data')
plt.plot(range(train_size, n), test, 'k-', label='Test Data')

# Plot the forecasts
colors = ['red', 'green', 'purple', 'orange']
for i, (name, res) in enumerate(results.items()):
    plt.plot(range(train_size, n), res['forecast'], color=colors[i], linestyle='--', label=f'{name} Forecast')

plt.title('Forecast Comparison')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Plot error metrics
plt.figure(figsize=(15, 5))

metrics = ['mae', 'rmse', 'mape']
titles = ['Mean Absolute Error', 'Root Mean Squared Error', 'Mean Absolute Percentage Error (%)']

for i, metric in enumerate(metrics):
    plt.subplot(1, 3, i+1)
    values = [results[model][metric] for model in models.keys()]
    plt.bar(models.keys(), values, color=colors)
    plt.title(titles[i])
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

Advanced time series models provide powerful tools for analyzing complex temporal patterns:

1. **ARIMA Models**:
   - Handle non-stationary data through differencing
   - ARIMA(p,d,q) combines autoregression, integration, and moving average

2. **Seasonal ARIMA (SARIMA) Models**:
   - Extend ARIMA to handle seasonal patterns
   - SARIMA(p,d,q)(P,D,Q)s includes seasonal components

3. **GARCH Models**:
   - Model time-varying volatility
   - Particularly useful for financial time series

4. **Vector Autoregression (VAR)**:
   - Model multiple interrelated time series
   - Capture dynamic relationships between variables

5. **State Space Models**:
   - Represent time series in terms of state variables
   - Kalman filter provides optimal state estimation

6. **Model Selection and Evaluation**:
   - Information criteria (AIC, BIC) balance fit and complexity
   - Forecast evaluation metrics (MAE, MSE, RMSE, MAPE) assess predictive accuracy

These advanced models enable analysts to handle a wide range of time series patterns and make accurate forecasts for complex real-world data.

## References

1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5th ed.). Wiley.
2. Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31(3), 307-327.
3. Hamilton, J. D. (1994). Time Series Analysis. Princeton University Press.
4. Durbin, J., & Koopman, S. J. (2012). Time Series Analysis by State Space Methods (2nd ed.). Oxford University Press.
