# 2.4.6 Bayesian Statistics - Part 2

## Bayesian Models

Building on the foundations covered in Part 1, this section explores practical Bayesian models for common statistical problems. These models demonstrate how to apply Bayesian principles to real-world data analysis tasks.

## Bayesian Estimation of Proportions

Estimating a proportion (e.g., probability of success) is one of the simplest Bayesian models.

### Beta-Binomial Model

For a binomial likelihood with a Beta prior:
- **Prior**: $\theta \sim \text{Beta}(\alpha, \beta)$
- **Likelihood**: $y \sim \text{Binomial}(n, \theta)$
- **Posterior**: $\theta|y \sim \text{Beta}(\alpha + y, \beta + n - y)$

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate sequential updating with Beta-Binomial model
np.random.seed(42)

# True parameter
theta_true = 0.7

# Prior parameters
alpha_0, beta_0 = 1, 1  # Start with uniform prior

# Generate data in batches
n_per_batch = 10
n_batches = 5
data_batches = [np.random.binomial(1, theta_true, n_per_batch) for _ in range(n_batches)]

# Sequential updating
alpha = alpha_0
beta = beta_0
posteriors = []

for i, batch in enumerate(data_batches):
    # Update posterior
    y = np.sum(batch)
    alpha += y
    beta += n_per_batch - y
    
    # Store posterior parameters
    posteriors.append((alpha, beta))
    
    print(f"Batch {i+1}: {y} successes in {n_per_batch} trials")
    print(f"Updated posterior: Beta({alpha}, {beta})")
    print(f"Posterior mean: {alpha / (alpha + beta):.4f}")
    print()

# Plot sequential updating
theta = np.linspace(0, 1, 1000)
plt.figure(figsize=(12, 6))

# Prior
prior = stats.beta.pdf(theta, alpha_0, beta_0)
plt.plot(theta, prior, 'k--', linewidth=2, label='Prior: Beta(1, 1)')

# Posteriors after each batch
colors = ['blue', 'green', 'red', 'purple', 'orange']
for i, (a, b) in enumerate(posteriors):
    posterior = stats.beta.pdf(theta, a, b)
    plt.plot(theta, posterior, color=colors[i], linewidth=2, 
             label=f'After Batch {i+1}: Beta({a}, {b})')

# True parameter
plt.axvline(theta_true, color='black', linestyle='-', linewidth=2, label=f'True θ: {theta_true}')

plt.title('Sequential Bayesian Updating')
plt.xlabel('θ (Probability of Success)')
plt.ylabel('Density')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Plot convergence of posterior mean
plt.figure(figsize=(10, 6))
prior_mean = alpha_0 / (alpha_0 + beta_0)
means = [prior_mean] + [a / (a + b) for a, b in posteriors]
plt.plot(range(n_batches + 1), means, 'bo-', linewidth=2)
plt.axhline(theta_true, color='red', linestyle='--', linewidth=2, label=f'True θ: {theta_true}')
plt.title('Convergence of Posterior Mean')
plt.xlabel('Number of Batches')
plt.ylabel('Posterior Mean')
plt.xticks(range(n_batches + 1), ['Prior'] + [f'Batch {i+1}' for i in range(n_batches)])
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Bayesian Linear Regression

**Bayesian linear regression** extends the classical linear regression model by placing prior distributions on the parameters.

### Normal-Normal Model

For a linear regression with normal priors and normal likelihood:
- **Prior**: $\beta \sim \mathcal{N}(\mu_0, \Sigma_0)$, $\sigma^2 \sim \text{InvGamma}(a_0, b_0)$
- **Likelihood**: $y \sim \mathcal{N}(X\beta, \sigma^2 I)$
- **Posterior**: $\beta|y, \sigma^2 \sim \mathcal{N}(\mu_n, \Sigma_n)$

where:
- $\Sigma_n = (\Sigma_0^{-1} + \frac{1}{\sigma^2}X^TX)^{-1}$
- $\mu_n = \Sigma_n(\Sigma_0^{-1}\mu_0 + \frac{1}{\sigma^2}X^Ty)$

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate Bayesian linear regression
np.random.seed(42)

# Generate synthetic data
n = 20  # Number of observations
x = np.linspace(0, 10, n)
X = np.column_stack([np.ones(n), x])  # Design matrix with intercept
beta_true = np.array([2, 0.5])  # True coefficients (intercept, slope)
sigma_true = 1.0  # True noise standard deviation

# Generate noisy observations
y = X @ beta_true + np.random.normal(0, sigma_true, n)

# Prior parameters (weakly informative)
mu_0 = np.array([0, 0])  # Prior mean
sigma_0 = 10.0  # Prior standard deviation
Sigma_0 = sigma_0**2 * np.eye(2)  # Prior covariance matrix

# Known noise variance for simplicity
sigma_sq = sigma_true**2

# Posterior parameters
Sigma_n_inv = np.linalg.inv(Sigma_0) + (1/sigma_sq) * X.T @ X
Sigma_n = np.linalg.inv(Sigma_n_inv)
mu_n = Sigma_n @ (np.linalg.inv(Sigma_0) @ mu_0 + (1/sigma_sq) * X.T @ y)

# Print results
print(f"True coefficients: {beta_true}")
print(f"Prior mean: {mu_0}")
print(f"Posterior mean: {mu_n}")
print(f"Posterior standard deviations: {np.sqrt(np.diag(Sigma_n))}")

# Generate predictions
x_new = np.linspace(0, 12, 100)
X_new = np.column_stack([np.ones(100), x_new])

# Frequentist prediction (OLS)
beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y
y_ols = X_new @ beta_ols

# Bayesian prediction
y_mean = X_new @ mu_n
y_var = np.array([X_new[i] @ Sigma_n @ X_new[i].T + sigma_sq for i in range(100)])
y_std = np.sqrt(y_var)

# Plot the results
plt.figure(figsize=(12, 6))

# Data and regression lines
plt.subplot(1, 2, 1)
plt.scatter(x, y, color='blue', alpha=0.7, label='Data')
plt.plot(x_new, X_new @ beta_true, 'k-', linewidth=2, label='True Line')
plt.plot(x_new, y_ols, 'g--', linewidth=2, label='OLS Line')
plt.plot(x_new, y_mean, 'r-', linewidth=2, label='Bayesian Mean')
plt.fill_between(x_new, y_mean - 2*y_std, y_mean + 2*y_std, color='red', alpha=0.2, label='95% Credible Interval')
plt.title('Bayesian Linear Regression')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Posterior distribution of parameters
plt.subplot(1, 2, 2)
# Generate grid of parameter values
beta0_range = np.linspace(mu_n[0] - 3*np.sqrt(Sigma_n[0, 0]), mu_n[0] + 3*np.sqrt(Sigma_n[0, 0]), 100)
beta1_range = np.linspace(mu_n[1] - 3*np.sqrt(Sigma_n[1, 1]), mu_n[1] + 3*np.sqrt(Sigma_n[1, 1]), 100)
Beta0, Beta1 = np.meshgrid(beta0_range, beta1_range)

# Compute posterior density
Z = np.zeros_like(Beta0)
for i in range(len(beta0_range)):
    for j in range(len(beta1_range)):
        beta = np.array([Beta0[i, j], Beta1[i, j]])
        Z[i, j] = stats.multivariate_normal.pdf(beta, mean=mu_n, cov=Sigma_n)

# Plot posterior contours
plt.contourf(Beta0, Beta1, Z, levels=20, cmap='viridis', alpha=0.7)
plt.colorbar(label='Posterior Density')
plt.scatter(beta_true[0], beta_true[1], color='black', marker='*', s=200, label='True Parameters')
plt.scatter(mu_n[0], mu_n[1], color='red', marker='o', s=100, label='Posterior Mean')
plt.scatter(beta_ols[0], beta_ols[1], color='green', marker='x', s=100, label='OLS Estimate')
plt.title('Posterior Distribution of Parameters')
plt.xlabel('Intercept (β₀)')
plt.ylabel('Slope (β₁)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Bayesian Model Comparison

Bayesian model comparison involves evaluating the relative evidence for different models.

### Bayes Factors

The **Bayes factor** is the ratio of marginal likelihoods for two models:

$$BF_{12} = \frac{p(D|M_1)}{p(D|M_2)}$$

where $p(D|M_i)$ is the marginal likelihood (evidence) for model $i$.

### Interpretation of Bayes Factors

| Bayes Factor | Evidence Against M₂ |
|--------------|---------------------|
| 1 to 3       | Barely worth mentioning |
| 3 to 10      | Substantial |
| 10 to 30     | Strong |
| 30 to 100    | Very strong |
| > 100        | Decisive |

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.special import logsumexp

# Demonstrate Bayesian model comparison
np.random.seed(42)

# Generate data from a linear model
n = 30
x = np.linspace(0, 10, n)
X_linear = np.column_stack([np.ones(n), x])
X_quadratic = np.column_stack([np.ones(n), x, x**2])

# True model is linear
beta_true = np.array([2, 0.5])
sigma_true = 1.0
y = X_linear @ beta_true + np.random.normal(0, sigma_true, n)

# Define models
models = {
    'Linear': {'X': X_linear, 'k': 2},
    'Quadratic': {'X': X_quadratic, 'k': 3}
}

# Prior parameters
mu_0 = np.zeros(3)  # Prior mean (use only first k elements for each model)
sigma_0 = 10.0  # Prior standard deviation
sigma_sq = sigma_true**2  # Assume known noise variance for simplicity

# Compute marginal likelihoods and Bayes factors
results = {}

for name, model in models.items():
    X = model['X']
    k = model['k']
    
    # Prior covariance
    Sigma_0 = sigma_0**2 * np.eye(k)
    
    # Posterior parameters
    Sigma_n_inv = np.linalg.inv(Sigma_0) + (1/sigma_sq) * X.T @ X
    Sigma_n = np.linalg.inv(Sigma_n_inv)
    mu_n = Sigma_n @ ((1/sigma_sq) * X.T @ y)  # Assuming zero prior mean
    
    # Compute log marginal likelihood
    log_ml = -0.5 * n * np.log(2 * np.pi * sigma_sq)
    log_ml -= 0.5 * np.log(np.linalg.det(Sigma_0) / np.linalg.det(Sigma_n))
    log_ml -= 0.5 * (y.T @ y - mu_n.T @ Sigma_n_inv @ mu_n) / sigma_sq
    
    # Store results
    results[name] = {
        'log_ml': log_ml,
        'mu_n': mu_n,
        'Sigma_n': Sigma_n
    }

# Compute Bayes factor
log_bf = results['Linear']['log_ml'] - results['Quadratic']['log_ml']
bf = np.exp(log_bf)

# Compute posterior model probabilities (assuming equal prior probabilities)
log_posterior = np.array([results[model]['log_ml'] for model in ['Linear', 'Quadratic']])
log_posterior -= logsumexp(log_posterior)  # Normalize
posterior_probs = np.exp(log_posterior)

# Print results
print(f"Log marginal likelihood - Linear: {results['Linear']['log_ml']:.4f}")
print(f"Log marginal likelihood - Quadratic: {results['Quadratic']['log_ml']:.4f}")
print(f"Log Bayes factor (Linear/Quadratic): {log_bf:.4f}")
print(f"Bayes factor (Linear/Quadratic): {bf:.4f}")
print(f"Posterior probability - Linear: {posterior_probs[0]:.4f}")
print(f"Posterior probability - Quadratic: {posterior_probs[1]:.4f}")

# Generate predictions
x_new = np.linspace(0, 12, 100)
X_linear_new = np.column_stack([np.ones(100), x_new])
X_quadratic_new = np.column_stack([np.ones(100), x_new, x_new**2])

# Linear model predictions
mu_linear = X_linear_new @ results['Linear']['mu_n']
var_linear = np.array([X_linear_new[i] @ results['Linear']['Sigma_n'] @ X_linear_new[i].T + sigma_sq 
                      for i in range(100)])
std_linear = np.sqrt(var_linear)

# Quadratic model predictions
mu_quadratic = X_quadratic_new @ results['Quadratic']['mu_n']
var_quadratic = np.array([X_quadratic_new[i] @ results['Quadratic']['Sigma_n'] @ X_quadratic_new[i].T + sigma_sq 
                         for i in range(100)])
std_quadratic = np.sqrt(var_quadratic)

# Model averaged predictions
mu_avg = posterior_probs[0] * mu_linear + posterior_probs[1] * mu_quadratic
var_avg = posterior_probs[0] * (var_linear + mu_linear**2) + posterior_probs[1] * (var_quadratic + mu_quadratic**2) - mu_avg**2
std_avg = np.sqrt(var_avg)

# Plot the results
plt.figure(figsize=(15, 5))

# Data and individual model predictions
plt.subplot(1, 3, 1)
plt.scatter(x, y, color='blue', alpha=0.7, label='Data')
plt.plot(x_new, X_linear_new @ np.array([2, 0.5]), 'k-', linewidth=2, label='True Line')
plt.plot(x_new, mu_linear, 'r-', linewidth=2, label='Linear Model')
plt.fill_between(x_new, mu_linear - 2*std_linear, mu_linear + 2*std_linear, color='red', alpha=0.2)
plt.plot(x_new, mu_quadratic, 'g-', linewidth=2, label='Quadratic Model')
plt.fill_between(x_new, mu_quadratic - 2*std_quadratic, mu_quadratic + 2*std_quadratic, color='green', alpha=0.2)
plt.title('Model Predictions')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Model averaged predictions
plt.subplot(1, 3, 2)
plt.scatter(x, y, color='blue', alpha=0.7, label='Data')
plt.plot(x_new, X_linear_new @ np.array([2, 0.5]), 'k-', linewidth=2, label='True Line')
plt.plot(x_new, mu_avg, 'purple', linewidth=2, label='Model Average')
plt.fill_between(x_new, mu_avg - 2*std_avg, mu_avg + 2*std_avg, color='purple', alpha=0.2, label='95% CI')
plt.title('Model Averaged Predictions')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Posterior model probabilities
plt.subplot(1, 3, 3)
plt.bar(['Linear', 'Quadratic'], posterior_probs, color=['red', 'green'])
plt.title('Posterior Model Probabilities')
plt.ylabel('Probability')
plt.ylim(0, 1)
for i, prob in enumerate(posterior_probs):
    plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

This section explored practical Bayesian models for common statistical problems:

1. **Bayesian Estimation of Proportions**:
   - Beta-Binomial model for binary data
   - Sequential updating of beliefs with new data

2. **Bayesian Linear Regression**:
   - Normal-Normal model for regression parameters
   - Posterior distribution of coefficients
   - Prediction with uncertainty quantification

3. **Bayesian Model Comparison**:
   - Bayes factors for comparing models
   - Posterior model probabilities
   - Model averaging for robust predictions

These models demonstrate how Bayesian methods provide a coherent framework for parameter estimation, prediction, and model selection, with full quantification of uncertainty.

In Part 3, we will explore more advanced topics in Bayesian statistics, including hierarchical models and computational methods for Bayesian inference.

## References

1. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
2. McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.
3. Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773-795.
4. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
