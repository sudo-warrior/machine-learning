# 2.4.6 Bayesian Statistics - Part 3

## Computational Methods for Bayesian Inference

As Bayesian models become more complex, analytical solutions for posterior distributions become intractable. This section explores computational methods that enable Bayesian inference for complex models.

## Markov Chain Monte Carlo (MCMC)

**Markov Chain Monte Carlo** methods generate samples from the posterior distribution when it cannot be computed analytically.

### Metropolis-Hastings Algorithm

The **Metropolis-Hastings algorithm** is a general MCMC method:

1. Start with an initial parameter value θ₀
2. For each iteration t:
   - Propose a new value θ* from a proposal distribution q(θ*|θₜ₋₁)
   - Calculate the acceptance ratio: α = min(1, [p(θ*|D) × q(θₜ₋₁|θ*)] / [p(θₜ₋₁|D) × q(θ*|θₜ₋₁)])
   - Accept θ* with probability α; otherwise, keep θₜ₋₁

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate Metropolis-Hastings for a simple example
np.random.seed(42)

# Target distribution: Beta(3, 7)
def target_log_pdf(theta):
    return stats.beta.logpdf(theta, 3, 7)

# Proposal distribution: Normal centered at current value
def proposal(theta, sigma=0.1):
    return np.random.normal(theta, sigma)

# Metropolis-Hastings algorithm
def metropolis_hastings(target_log_pdf, proposal, initial_value, n_samples, sigma=0.1):
    samples = np.zeros(n_samples)
    current = initial_value
    accepted = 0
    
    for i in range(n_samples):
        # Propose new value
        proposed = proposal(current, sigma)
        
        # Handle boundary conditions (0 <= theta <= 1)
        if proposed < 0 or proposed > 1:
            samples[i] = current
            continue
        
        # Calculate log acceptance ratio
        log_target_current = target_log_pdf(current)
        log_target_proposed = target_log_pdf(proposed)
        
        # Symmetric proposal, so proposal terms cancel out
        log_acceptance_ratio = log_target_proposed - log_target_current
        
        # Accept or reject
        if np.log(np.random.uniform()) < log_acceptance_ratio:
            current = proposed
            accepted += 1
        
        samples[i] = current
    
    acceptance_rate = accepted / n_samples
    return samples, acceptance_rate

# Run MCMC
initial_value = 0.5
n_samples = 10000
samples, acceptance_rate = metropolis_hastings(target_log_pdf, proposal, initial_value, n_samples)

# Print results
print(f"Acceptance rate: {acceptance_rate:.4f}")
print(f"Mean of samples: {np.mean(samples):.4f}")
print(f"True mean of Beta(3, 7): {3 / (3 + 7):.4f}")

# Plot results
plt.figure(figsize=(15, 5))

# Trace plot
plt.subplot(1, 3, 1)
plt.plot(samples)
plt.title('Trace Plot')
plt.xlabel('Iteration')
plt.ylabel('θ')
plt.grid(True, alpha=0.3)

# Histogram of samples
plt.subplot(1, 3, 2)
plt.hist(samples, bins=30, density=True, alpha=0.7)
# Plot true distribution
theta = np.linspace(0, 1, 1000)
plt.plot(theta, stats.beta.pdf(theta, 3, 7), 'r-', linewidth=2, label='True Beta(3, 7)')
plt.title('Histogram of Samples')
plt.xlabel('θ')
plt.ylabel('Density')
plt.legend()
plt.grid(True, alpha=0.3)

# Autocorrelation plot
plt.subplot(1, 3, 3)
max_lag = 50
acf = np.zeros(max_lag)
for lag in range(max_lag):
    acf[lag] = np.corrcoef(samples[:-lag], samples[lag:])[0, 1] if lag > 0 else 1.0
plt.bar(range(max_lag), acf)
plt.title('Autocorrelation Function')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Gibbs Sampling

**Gibbs sampling** is a special case of Metropolis-Hastings where proposals are always accepted:

1. Start with initial values for all parameters
2. For each iteration, update each parameter by sampling from its conditional posterior distribution given all other parameters

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate Gibbs sampling for a bivariate normal distribution
np.random.seed(42)

# Target: Bivariate normal with parameters
mu = np.array([0, 0])
rho = 0.8  # Correlation
sigma = np.array([1, 1])
cov = np.array([[sigma[0]**2, rho*sigma[0]*sigma[1]], 
                [rho*sigma[0]*sigma[1], sigma[1]**2]])

# Conditional distributions for Gibbs sampling
def conditional_x1_given_x2(x2):
    cond_mean = mu[0] + rho * sigma[0] / sigma[1] * (x2 - mu[1])
    cond_var = sigma[0]**2 * (1 - rho**2)
    return np.random.normal(cond_mean, np.sqrt(cond_var))

def conditional_x2_given_x1(x1):
    cond_mean = mu[1] + rho * sigma[1] / sigma[0] * (x1 - mu[0])
    cond_var = sigma[1]**2 * (1 - rho**2)
    return np.random.normal(cond_mean, np.sqrt(cond_var))

# Gibbs sampling
def gibbs_sampler(n_samples, burn_in=500):
    samples = np.zeros((n_samples, 2))
    
    # Initialize
    current = np.random.normal(0, 1, 2)
    
    # Burn-in
    for _ in range(burn_in):
        current[0] = conditional_x1_given_x2(current[1])
        current[1] = conditional_x2_given_x1(current[0])
    
    # Sampling
    for i in range(n_samples):
        current[0] = conditional_x1_given_x2(current[1])
        current[1] = conditional_x2_given_x1(current[0])
        samples[i] = current
    
    return samples

# Run Gibbs sampler
n_samples = 5000
samples = gibbs_sampler(n_samples)

# Print results
print(f"Sample mean: {np.mean(samples, axis=0)}")
print(f"Sample covariance:\n{np.cov(samples.T)}")
print(f"True mean: {mu}")
print(f"True covariance:\n{cov}")

# Plot results
plt.figure(figsize=(15, 5))

# Scatter plot of samples
plt.subplot(1, 3, 1)
plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=1)
plt.title('Gibbs Samples')
plt.xlabel('x₁')
plt.ylabel('x₂')
plt.axis('equal')
plt.grid(True, alpha=0.3)

# Trace plots
plt.subplot(1, 3, 2)
plt.plot(samples[:500, 0], label='x₁')
plt.plot(samples[:500, 1], label='x₂')
plt.title('Trace Plot (First 500 Samples)')
plt.xlabel('Iteration')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Contour plot of true distribution
plt.subplot(1, 3, 3)
x1 = np.linspace(-3, 3, 100)
x2 = np.linspace(-3, 3, 100)
X1, X2 = np.meshgrid(x1, x2)
pos = np.dstack((X1, X2))
rv = stats.multivariate_normal(mu, cov)
plt.contourf(X1, X2, rv.pdf(pos), levels=20, cmap='viridis', alpha=0.7)
plt.scatter(samples[::20, 0], samples[::20, 1], alpha=0.5, s=1, color='red')
plt.title('True Density and Samples')
plt.xlabel('x₁')
plt.ylabel('x₂')
plt.axis('equal')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Hierarchical Bayesian Models

**Hierarchical Bayesian models** (also called multilevel models) involve parameters at multiple levels, with higher-level parameters governing the distribution of lower-level parameters.

### Structure of Hierarchical Models

1. **Data level**: $y_i \sim p(y_i|\theta_i)$
2. **Parameter level**: $\theta_i \sim p(\theta_i|\phi)$
3. **Hyperparameter level**: $\phi \sim p(\phi)$

### Example: Hierarchical Normal Model

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Demonstrate a hierarchical normal model
np.random.seed(42)

# Generate data from multiple groups
n_groups = 8
n_per_group = 20

# Hyperparameters
mu_mu = 0  # Mean of group means
sigma_mu = 10  # Standard deviation of group means
sigma_y = 2  # Within-group standard deviation

# Generate group means
group_means = np.random.normal(mu_mu, sigma_mu, n_groups)

# Generate data for each group
data = []
group_ids = []

for i in range(n_groups):
    group_data = np.random.normal(group_means[i], sigma_y, n_per_group)
    data.extend(group_data)
    group_ids.extend([i] * n_per_group)

data = np.array(data)
group_ids = np.array(group_ids)

# Compute group statistics
group_sample_means = [np.mean(data[group_ids == i]) for i in range(n_groups)]
group_sample_stds = [np.std(data[group_ids == i], ddof=1) for i in range(n_groups)]
overall_mean = np.mean(data)

# Bayesian hierarchical estimation
# Prior for hyperparameters
mu_0 = 0
sigma_0 = 20
kappa_0 = 0.1
nu_0 = 1
sigma2_0 = 10

# Posterior for hyperparameters
n_total = len(data)
kappa_n = kappa_0 + n_groups
nu_n = nu_0 + n_total
mu_n = (kappa_0 * mu_0 + n_groups * np.mean(group_sample_means)) / kappa_n

# Posterior for group means (partial pooling)
lambda_factor = sigma_y**2 / (sigma_y**2 + sigma_mu**2)
shrinkage_estimates = lambda_factor * mu_mu + (1 - lambda_factor) * np.array(group_sample_means)

# Plot results
plt.figure(figsize=(15, 10))

# Raw data by group
plt.subplot(2, 2, 1)
for i in range(n_groups):
    group_data = data[group_ids == i]
    plt.scatter([i] * len(group_data), group_data, alpha=0.5)
plt.axhline(overall_mean, color='red', linestyle='--', label='Overall Mean')
plt.title('Raw Data by Group')
plt.xlabel('Group')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)
plt.legend()

# Group means: True vs. Sample vs. Shrinkage
plt.subplot(2, 2, 2)
plt.scatter(range(n_groups), group_means, color='blue', label='True Means')
plt.scatter(range(n_groups), group_sample_means, color='green', label='Sample Means')
plt.scatter(range(n_groups), shrinkage_estimates, color='red', label='Shrinkage Estimates')
plt.axhline(mu_mu, color='blue', linestyle='--', label='True Overall Mean')
plt.axhline(overall_mean, color='green', linestyle='--', label='Sample Overall Mean')
plt.title('Group Means: True vs. Sample vs. Shrinkage')
plt.xlabel('Group')
plt.ylabel('Mean')
plt.grid(True, alpha=0.3)
plt.legend()

# Shrinkage effect
plt.subplot(2, 2, 3)
for i in range(n_groups):
    plt.plot([0, 1], [group_sample_means[i], shrinkage_estimates[i]], 'k-', alpha=0.5)
plt.scatter([0] * n_groups, group_sample_means, color='green', label='Sample Means')
plt.scatter([1] * n_groups, shrinkage_estimates, color='red', label='Shrinkage Estimates')
plt.axhline(overall_mean, color='blue', linestyle='--', label='Overall Mean')
plt.title('Shrinkage Effect')
plt.xticks([0, 1], ['Sample', 'Shrinkage'])
plt.ylabel('Mean')
plt.grid(True, alpha=0.3)
plt.legend()

# Error in estimation
plt.subplot(2, 2, 4)
sample_errors = np.abs(np.array(group_sample_means) - group_means)
shrinkage_errors = np.abs(shrinkage_estimates - group_means)
plt.bar(np.arange(n_groups) - 0.2, sample_errors, width=0.4, label='Sample Error')
plt.bar(np.arange(n_groups) + 0.2, shrinkage_errors, width=0.4, label='Shrinkage Error')
plt.title('Absolute Error in Estimation')
plt.xlabel('Group')
plt.ylabel('Absolute Error')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

# Print summary statistics
print(f"Mean absolute error - Sample means: {np.mean(sample_errors):.4f}")
print(f"Mean absolute error - Shrinkage estimates: {np.mean(shrinkage_errors):.4f}")
```

## Bayesian Nonparametrics

**Bayesian nonparametric** methods allow the complexity of the model to grow with the data.

### Dirichlet Process Mixture Models

**Dirichlet Process Mixture Models** (DPMMs) are a flexible class of models for clustering and density estimation:

1. The number of clusters is not fixed in advance
2. New clusters can be created as more data is observed
3. The model automatically determines the appropriate complexity

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.cluster import KMeans

# Demonstrate a simplified Dirichlet Process Mixture Model
np.random.seed(42)

# Generate data from a mixture of Gaussians
n_samples = 300
true_means = [-5, 0, 5]
true_stds = [1, 1, 1]
true_weights = [0.3, 0.4, 0.3]

# Generate component assignments
z_true = np.random.choice(len(true_means), size=n_samples, p=true_weights)

# Generate data
data = np.array([np.random.normal(true_means[z], true_stds[z]) for z in z_true])

# Simplified Dirichlet Process Mixture Model using Chinese Restaurant Process
def crp_gibbs_sampler(data, alpha=1.0, sigma=1.0, n_iter=100):
    n = len(data)
    # Initialize: each data point in its own cluster
    z = np.arange(n)
    
    # Track cluster assignments over iterations
    z_samples = np.zeros((n_iter, n), dtype=int)
    
    for it in range(n_iter):
        # For each data point
        for i in range(n):
            # Remove point from its cluster
            z_i = z[i]
            z[i] = -1
            
            # Count number of points in each cluster
            unique_clusters = np.unique(z[z >= 0])
            counts = np.array([np.sum(z == j) for j in unique_clusters])
            
            # Compute probabilities for existing clusters
            probs = np.zeros(len(unique_clusters) + 1)
            for j, cluster in enumerate(unique_clusters):
                # Probability proportional to count and likelihood
                mean_j = np.mean(data[z == cluster])
                probs[j] = counts[j] * stats.norm.pdf(data[i], mean_j, sigma)
            
            # Probability for a new cluster
            probs[-1] = alpha * stats.norm.pdf(data[i], 0, np.sqrt(sigma**2 + 10**2))
            
            # Normalize probabilities
            probs = probs / np.sum(probs)
            
            # Sample new cluster assignment
            if len(unique_clusters) == 0:
                z[i] = 0  # First cluster
            else:
                choice = np.random.choice(len(probs), p=probs)
                if choice < len(unique_clusters):
                    z[i] = unique_clusters[choice]
                else:
                    z[i] = np.max(unique_clusters) + 1 if len(unique_clusters) > 0 else 0
        
        # Relabel clusters to be consecutive integers
        unique_z = np.unique(z)
        z_map = {old: new for new, old in enumerate(unique_z)}
        z = np.array([z_map[z_i] for z_i in z])
        
        # Store sample
        z_samples[it] = z
    
    return z_samples

# Run DPMM
n_iter = 100
z_samples = crp_gibbs_sampler(data, alpha=1.0, sigma=1.0, n_iter=n_iter)

# Get final clustering
z_final = z_samples[-1]
unique_clusters = np.unique(z_final)
n_clusters = len(unique_clusters)

# Compare with K-means
kmeans = KMeans(n_clusters=3, random_state=42).fit(data.reshape(-1, 1))
z_kmeans = kmeans.labels_

# Plot results
plt.figure(figsize=(15, 10))

# True data
plt.subplot(3, 1, 1)
for k in range(len(true_means)):
    plt.scatter(data[z_true == k], np.zeros_like(data[z_true == k]) + 0.1, 
               alpha=0.7, label=f'True Cluster {k+1}')
plt.title('True Data Generation')
plt.xlabel('Value')
plt.yticks([])
plt.grid(True, alpha=0.3)
plt.legend()

# K-means clustering
plt.subplot(3, 1, 2)
for k in range(3):
    plt.scatter(data[z_kmeans == k], np.zeros_like(data[z_kmeans == k]) + 0.1, 
               alpha=0.7, label=f'K-means Cluster {k+1}')
plt.title(f'K-means Clustering (K=3)')
plt.xlabel('Value')
plt.yticks([])
plt.grid(True, alpha=0.3)
plt.legend()

# DPMM clustering
plt.subplot(3, 1, 3)
for k in unique_clusters:
    plt.scatter(data[z_final == k], np.zeros_like(data[z_final == k]) + 0.1, 
               alpha=0.7, label=f'DPMM Cluster {k+1}')
plt.title(f'DPMM Clustering (Found {n_clusters} clusters)')
plt.xlabel('Value')
plt.yticks([])
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

# Plot number of clusters over iterations
plt.figure(figsize=(10, 6))
n_clusters_per_iter = [len(np.unique(z_samples[i])) for i in range(n_iter)]
plt.plot(n_clusters_per_iter)
plt.title('Number of Clusters per Iteration')
plt.xlabel('Iteration')
plt.ylabel('Number of Clusters')
plt.grid(True, alpha=0.3)
plt.show()
```

## Bayesian Deep Learning

**Bayesian deep learning** combines Bayesian inference with deep neural networks.

### Bayesian Neural Networks

**Bayesian Neural Networks** (BNNs) place prior distributions over the weights and biases:

1. Instead of point estimates, weights are represented by probability distributions
2. Predictions include uncertainty estimates
3. Regularization emerges naturally from the prior

### Monte Carlo Dropout

**Monte Carlo Dropout** provides a simple approximation to Bayesian neural networks:

1. Train a neural network with dropout
2. At test time, keep dropout active and perform multiple forward passes
3. The variation in predictions represents model uncertainty

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.models import Model

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Generate synthetic data with heteroscedastic noise
def generate_data(n=100):
    x = np.linspace(-3, 3, n)
    # True function: y = x^3
    y_true = x**3
    # Heteroscedastic noise: more noise for larger |x|
    noise_level = 1 + 2 * np.abs(x)
    y = y_true + np.random.normal(0, noise_level)
    return x, y, y_true

# Generate data
x_train, y_train, y_true_train = generate_data(100)
x_test = np.linspace(-4, 4, 200)  # Include extrapolation regions
y_true_test = x_test**3

# Build a Bayesian neural network with MC Dropout
def build_mc_dropout_model(dropout_rate=0.1):
    inputs = Input(shape=(1,))
    x = Dense(64, activation='relu')(inputs)
    x = Dropout(dropout_rate)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(1)(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mse')
    
    return model

# Train the model
model = build_mc_dropout_model(dropout_rate=0.1)
model.fit(x_train.reshape(-1, 1), y_train, epochs=500, verbose=0)

# Perform MC Dropout inference
def mc_dropout_predict(model, x, n_samples=100):
    predictions = np.zeros((n_samples, len(x)))
    
    for i in range(n_samples):
        predictions[i, :] = model(x, training=True).numpy().flatten()
    
    return predictions

# Generate predictions
n_samples = 100
x_test_reshaped = x_test.reshape(-1, 1)
mc_predictions = mc_dropout_predict(model, x_test_reshaped, n_samples)

# Calculate statistics
mean_prediction = np.mean(mc_predictions, axis=0)
std_prediction = np.std(mc_predictions, axis=0)

# Plot results
plt.figure(figsize=(15, 10))

# Data and predictions
plt.subplot(2, 2, 1)
plt.scatter(x_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.plot(x_test, y_true_test, 'k--', label='True Function')
plt.plot(x_test, mean_prediction, 'r-', label='Mean Prediction')
plt.fill_between(x_test, mean_prediction - 2*std_prediction, mean_prediction + 2*std_prediction, 
                color='red', alpha=0.2, label='±2σ (Epistemic Uncertainty)')
plt.title('Bayesian Neural Network with MC Dropout')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Uncertainty vs. x
plt.subplot(2, 2, 2)
plt.plot(x_test, std_prediction, 'r-', label='Predictive Standard Deviation')
plt.axvspan(-3, 3, color='yellow', alpha=0.2, label='Training Data Range')
plt.title('Uncertainty vs. x')
plt.xlabel('x')
plt.ylabel('Standard Deviation')
plt.legend()
plt.grid(True, alpha=0.3)

# Sample predictions
plt.subplot(2, 2, 3)
plt.plot(x_test, y_true_test, 'k--', label='True Function')
for i in range(10):
    plt.plot(x_test, mc_predictions[i], alpha=0.5)
plt.scatter(x_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.title('Sample Predictions')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Predictive distribution at specific points
plt.subplot(2, 2, 4)
test_points = [-3.5, 0, 3.5]  # Points to examine
colors = ['blue', 'green', 'red']
for i, point in enumerate(test_points):
    idx = np.argmin(np.abs(x_test - point))
    plt.hist(mc_predictions[:, idx], bins=20, alpha=0.5, color=colors[i], 
            label=f'x = {point:.1f}')
plt.title('Predictive Distributions at Specific Points')
plt.xlabel('Predicted y')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

This section explored computational methods and advanced models in Bayesian statistics:

1. **Markov Chain Monte Carlo (MCMC)**:
   - Metropolis-Hastings algorithm for general posterior sampling
   - Gibbs sampling for conditionally conjugate models

2. **Hierarchical Bayesian Models**:
   - Multi-level structure with hyperparameters
   - Partial pooling of information across groups
   - Shrinkage of estimates toward the population mean

3. **Bayesian Nonparametrics**:
   - Dirichlet Process Mixture Models for flexible clustering
   - Automatic determination of model complexity

4. **Bayesian Deep Learning**:
   - Bayesian Neural Networks with distributions over weights
   - Monte Carlo Dropout for approximate Bayesian inference
   - Uncertainty quantification in predictions

These advanced methods enable Bayesian inference for complex models, providing a principled approach to uncertainty quantification and model selection.

## References

1. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.
2. Robert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods (2nd ed.). Springer.
3. Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (pp. 1050-1059).
4. Teh, Y. W. (2010). Dirichlet process. Encyclopedia of Machine Learning, 280-287.
