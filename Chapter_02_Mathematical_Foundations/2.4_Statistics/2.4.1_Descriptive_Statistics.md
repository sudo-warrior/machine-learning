# 2.4.1 Descriptive Statistics

## Introduction to Descriptive Statistics

Descriptive statistics summarize and organize characteristics of a data set. These summaries may be quantitative (numerical) or visual (graphical) and form the foundation of statistical analysis. In machine learning, descriptive statistics help us understand our data before building models, identify patterns, detect outliers, and communicate findings effectively.

## Measures of Central Tendency

Measures of central tendency identify the "center" of a data distribution.

### Mean (Average)

The **mean** is the sum of all values divided by the number of values:

$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

```python
import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
data = np.random.normal(loc=50, scale=10, size=100)

# Calculate mean
mean_value = np.mean(data)

# Plot histogram with mean
plt.figure(figsize=(10, 6))
plt.hist(data, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_value:.2f}')
plt.title('Distribution with Mean')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Median

The **median** is the middle value when data is arranged in order:

- For odd number of observations: middle value
- For even number: average of two middle values

The median is less sensitive to outliers than the mean.

```python
# Calculate median
median_value = np.median(data)

# Add outliers to demonstrate robustness
data_with_outliers = np.append(data, [100, 110, 120])
mean_with_outliers = np.mean(data_with_outliers)
median_with_outliers = np.median(data_with_outliers)

# Plot comparison
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(data, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_value:.2f}')
plt.axvline(median_value, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_value:.2f}')
plt.title('Original Data')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(data_with_outliers, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mean_with_outliers, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_with_outliers:.2f}')
plt.axvline(median_with_outliers, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_with_outliers:.2f}')
plt.title('Data with Outliers')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Mode

The **mode** is the most frequently occurring value in a dataset. A distribution can have multiple modes:

- **Unimodal**: One mode
- **Bimodal**: Two modes
- **Multimodal**: Multiple modes

```python
from scipy import stats

# Generate bimodal data
np.random.seed(42)
data1 = np.random.normal(loc=40, scale=5, size=100)
data2 = np.random.normal(loc=70, scale=5, size=100)
bimodal_data = np.concatenate([data1, data2])

# Calculate mode
mode_value = stats.mode(np.round(data)).mode[0]
bimodal_mode = stats.mode(np.round(bimodal_data)).mode[0]

# Plot unimodal and bimodal distributions
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(data, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mode_value, color='purple', linestyle='dashed', linewidth=2, label=f'Mode: {mode_value:.2f}')
plt.title('Unimodal Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(bimodal_data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Bimodal Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Measures of Dispersion

Measures of dispersion describe how spread out the data is.

### Range

The **range** is the difference between the maximum and minimum values:

$$\text{Range} = \max(x) - \min(x)$$

### Variance

The **variance** measures the average squared deviation from the mean:

$$\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

For a sample (rather than the entire population), we use:

$$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

### Standard Deviation

The **standard deviation** is the square root of the variance:

$$\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$

```python
# Calculate dispersion measures
range_value = np.max(data) - np.min(data)
variance = np.var(data, ddof=1)  # ddof=1 for sample variance
std_dev = np.std(data, ddof=1)   # ddof=1 for sample standard deviation

# Generate data with different standard deviations
np.random.seed(42)
data_low_std = np.random.normal(loc=50, scale=5, size=100)
data_high_std = np.random.normal(loc=50, scale=15, size=100)

# Plot comparison
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(data_low_std, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(data_low_std), color='red', linestyle='dashed', linewidth=2, 
           label=f'Mean: {np.mean(data_low_std):.2f}')
plt.title(f'Low Standard Deviation: {np.std(data_low_std, ddof=1):.2f}')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(data_high_std, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(data_high_std), color='red', linestyle='dashed', linewidth=2, 
           label=f'Mean: {np.mean(data_high_std):.2f}')
plt.title(f'High Standard Deviation: {np.std(data_high_std, ddof=1):.2f}')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interquartile Range (IQR)

The **interquartile range** is the difference between the 75th percentile (Q3) and the 25th percentile (Q1):

$$\text{IQR} = Q3 - Q1$$

The IQR is robust to outliers and is used in box plots and for outlier detection.

```python
# Calculate quartiles and IQR
q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)
iqr = q3 - q1

# Create a box plot
plt.figure(figsize=(10, 6))
plt.boxplot(data, vert=False, patch_artist=True, boxprops=dict(facecolor='skyblue'))
plt.title('Box Plot with Quartiles')
plt.xlabel('Value')
plt.grid(True, alpha=0.3)
plt.annotate(f'Q1: {q1:.2f}', xy=(q1, 1), xytext=(q1, 1.2), 
            arrowprops=dict(facecolor='black', shrink=0.05))
plt.annotate(f'Median: {median_value:.2f}', xy=(median_value, 1), xytext=(median_value, 1.4), 
            arrowprops=dict(facecolor='black', shrink=0.05))
plt.annotate(f'Q3: {q3:.2f}', xy=(q3, 1), xytext=(q3, 1.2), 
            arrowprops=dict(facecolor='black', shrink=0.05))
plt.annotate(f'IQR: {iqr:.2f}', xy=((q1+q3)/2, 1), xytext=((q1+q3)/2, 1.6), 
            arrowprops=dict(facecolor='black', shrink=0.05))
plt.show()
```

## Measures of Shape

Measures of shape describe the form of the distribution.

### Skewness

**Skewness** measures the asymmetry of a distribution:

- **Positive skew**: Right tail is longer (mean > median)
- **Negative skew**: Left tail is longer (mean < median)
- **Zero skew**: Symmetric distribution (mean = median)

$$\text{Skewness} = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{\sigma}\right)^3$$

### Kurtosis

**Kurtosis** measures the "tailedness" of a distribution:

- **Leptokurtic** (positive kurtosis): Heavy tails, more outliers
- **Mesokurtic** (zero kurtosis): Normal distribution
- **Platykurtic** (negative kurtosis): Light tails, fewer outliers

$$\text{Kurtosis} = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{\sigma}\right)^4 - 3$$

```python
from scipy import stats

# Generate data with different skewness
np.random.seed(42)
normal_data = np.random.normal(loc=0, scale=1, size=1000)
right_skewed = np.random.exponential(scale=1, size=1000)
left_skewed = -np.random.exponential(scale=1, size=1000)

# Calculate skewness
normal_skew = stats.skew(normal_data)
right_skew = stats.skew(right_skewed)
left_skew = stats.skew(left_skewed)

# Plot distributions with different skewness
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.hist(normal_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(normal_data), color='red', linestyle='dashed', linewidth=2, label='Mean')
plt.axvline(np.median(normal_data), color='green', linestyle='dashed', linewidth=2, label='Median')
plt.title(f'Normal: Skewness = {normal_skew:.2f}')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.hist(right_skewed, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(right_skewed), color='red', linestyle='dashed', linewidth=2, label='Mean')
plt.axvline(np.median(right_skewed), color='green', linestyle='dashed', linewidth=2, label='Median')
plt.title(f'Right Skewed: Skewness = {right_skew:.2f}')
plt.xlabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.hist(left_skewed, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(left_skewed), color='red', linestyle='dashed', linewidth=2, label='Mean')
plt.axvline(np.median(left_skewed), color='green', linestyle='dashed', linewidth=2, label='Median')
plt.title(f'Left Skewed: Skewness = {left_skew:.2f}')
plt.xlabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Generate data with different kurtosis
high_kurtosis = np.random.standard_t(df=3, size=1000)  # t-distribution with 3 degrees of freedom
low_kurtosis = np.random.uniform(size=1000)  # Uniform distribution

# Calculate kurtosis
normal_kurt = stats.kurtosis(normal_data)
high_kurt = stats.kurtosis(high_kurtosis)
low_kurt = stats.kurtosis(low_kurtosis)

# Plot distributions with different kurtosis
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.hist(normal_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title(f'Normal: Kurtosis = {normal_kurt:.2f}')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.hist(high_kurtosis, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title(f'High Kurtosis: Kurtosis = {high_kurt:.2f}')
plt.xlabel('Value')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.hist(low_kurtosis, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title(f'Low Kurtosis: Kurtosis = {low_kurt:.2f}')
plt.xlabel('Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Data Visualization

Visualizations help understand the distribution and relationships in data.

### Histograms

**Histograms** show the frequency distribution of a continuous variable by dividing it into bins.

```python
# Create a histogram with different bin sizes
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.hist(data, bins=5, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Histogram with 5 Bins')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.hist(data, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Histogram with 10 Bins')
plt.xlabel('Value')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.hist(data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Histogram with 20 Bins')
plt.xlabel('Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Box Plots

**Box plots** (box-and-whisker plots) display the five-number summary:
- Minimum
- First quartile (Q1)
- Median
- Third quartile (Q3)
- Maximum

They also identify outliers.

```python
# Generate multiple datasets
np.random.seed(42)
data1 = np.random.normal(loc=50, scale=10, size=100)
data2 = np.random.normal(loc=60, scale=15, size=100)
data3 = np.random.normal(loc=45, scale=5, size=100)
data4 = np.random.normal(loc=55, scale=8, size=100)

# Create a box plot comparison
plt.figure(figsize=(10, 6))
plt.boxplot([data1, data2, data3, data4], labels=['Group 1', 'Group 2', 'Group 3', 'Group 4'], 
           patch_artist=True, boxprops=dict(facecolor='skyblue'))
plt.title('Box Plot Comparison')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)
plt.show()
```

### Scatter Plots

**Scatter plots** show the relationship between two continuous variables.

```python
# Generate correlated data
np.random.seed(42)
x = np.random.normal(loc=50, scale=10, size=100)
y = x + np.random.normal(loc=0, scale=5, size=100)  # Positive correlation
z = -x + np.random.normal(loc=100, scale=5, size=100)  # Negative correlation

# Create scatter plots
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(x, y, alpha=0.7, color='blue')
plt.title('Positive Correlation')
plt.xlabel('X')
plt.ylabel('Y')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(x, z, alpha=0.7, color='red')
plt.title('Negative Correlation')
plt.xlabel('X')
plt.ylabel('Z')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Bar Charts

**Bar charts** display the distribution of categorical variables.

```python
import pandas as pd

# Create categorical data
categories = ['Category A', 'Category B', 'Category C', 'Category D', 'Category E']
values = [25, 40, 30, 55, 15]

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(categories, values, color='skyblue', edgecolor='black')
plt.title('Bar Chart of Categories')
plt.xlabel('Category')
plt.ylabel('Value')
plt.grid(True, alpha=0.3, axis='y')
plt.show()
```

### Pie Charts

**Pie charts** show the proportion of categories in a whole.

```python
# Create a pie chart
plt.figure(figsize=(10, 6))
plt.pie(values, labels=categories, autopct='%1.1f%%', startangle=90, 
       colors=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])
plt.title('Pie Chart of Categories')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.show()
```

## Descriptive Statistics in Machine Learning

### Data Preprocessing

Descriptive statistics guide data preprocessing decisions:

- **Outlier Detection**: Using IQR or z-scores to identify outliers
- **Feature Scaling**: Normalizing or standardizing based on mean and standard deviation
- **Missing Value Imputation**: Using mean, median, or mode to fill missing values

```python
# Example: Outlier detection using IQR
def detect_outliers_iqr(data, k=1.5):
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    iqr = q3 - q1
    lower_bound = q1 - k * iqr
    upper_bound = q3 + k * iqr
    outliers = np.where((data < lower_bound) | (data > upper_bound))
    return outliers, lower_bound, upper_bound

# Add outliers to data
data_with_outliers = np.append(data, [100, 110, 120])

# Detect outliers
outliers, lower_bound, upper_bound = detect_outliers_iqr(data_with_outliers)

# Plot data with outliers highlighted
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data_with_outliers)), data_with_outliers, alpha=0.7, color='blue')
plt.scatter(outliers, data_with_outliers[outliers], alpha=1, color='red', label='Outliers')
plt.axhline(lower_bound, color='green', linestyle='dashed', linewidth=2, label=f'Lower Bound: {lower_bound:.2f}')
plt.axhline(upper_bound, color='green', linestyle='dashed', linewidth=2, label=f'Upper Bound: {upper_bound:.2f}')
plt.title('Outlier Detection using IQR Method')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Feature Engineering

Descriptive statistics inform feature engineering:

- **Binning**: Converting continuous variables to categorical based on percentiles
- **Interaction Terms**: Creating new features based on statistical relationships
- **Polynomial Features**: Adding non-linear terms based on distribution shape

```python
# Example: Binning based on quartiles
def bin_data(data):
    q1 = np.percentile(data, 25)
    q2 = np.percentile(data, 50)
    q3 = np.percentile(data, 75)
    
    bins = []
    for value in data:
        if value < q1:
            bins.append('Low')
        elif value < q2:
            bins.append('Medium-Low')
        elif value < q3:
            bins.append('Medium-High')
        else:
            bins.append('High')
    
    return bins

# Bin the data
binned_data = bin_data(data)

# Count occurrences of each bin
bin_counts = pd.Series(binned_data).value_counts().sort_index()

# Plot the binned data
plt.figure(figsize=(10, 6))
plt.bar(bin_counts.index, bin_counts.values, color='skyblue', edgecolor='black')
plt.title('Data Binned by Quartiles')
plt.xlabel('Bin')
plt.ylabel('Count')
plt.grid(True, alpha=0.3, axis='y')
plt.show()
```

### Model Evaluation

Descriptive statistics help evaluate model performance:

- **Error Metrics**: Mean, median, and standard deviation of errors
- **Residual Analysis**: Distribution and patterns in prediction errors
- **Performance Comparison**: Statistical summaries of different models

```python
# Example: Residual analysis
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
y_true = 2 * x + 1
y_pred = 2 * x + 1 + np.random.normal(0, 1, 100)  # Add some noise

# Calculate residuals
residuals = y_true - y_pred

# Plot residuals
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(x, residuals, alpha=0.7, color='blue')
plt.axhline(0, color='red', linestyle='dashed', linewidth=2)
plt.title('Residuals vs. X')
plt.xlabel('X')
plt.ylabel('Residual')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(np.mean(residuals), color='red', linestyle='dashed', linewidth=2, 
           label=f'Mean: {np.mean(residuals):.2f}')
plt.title('Residual Distribution')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Calculate descriptive statistics of residuals
residual_stats = {
    'Mean': np.mean(residuals),
    'Median': np.median(residuals),
    'Std Dev': np.std(residuals),
    'Min': np.min(residuals),
    'Max': np.max(residuals),
    'Skewness': stats.skew(residuals),
    'Kurtosis': stats.kurtosis(residuals)
}

for stat, value in residual_stats.items():
    print(f"{stat}: {value:.4f}")
```

## Summary

Descriptive statistics provide essential tools for understanding and summarizing data:

1. **Measures of Central Tendency**:
   - Mean: Average of all values
   - Median: Middle value when data is ordered
   - Mode: Most frequent value

2. **Measures of Dispersion**:
   - Range: Difference between maximum and minimum
   - Variance: Average squared deviation from the mean
   - Standard Deviation: Square root of variance
   - Interquartile Range (IQR): Difference between Q3 and Q1

3. **Measures of Shape**:
   - Skewness: Asymmetry of the distribution
   - Kurtosis: Tailedness of the distribution

4. **Data Visualization**:
   - Histograms: Frequency distribution of continuous variables
   - Box Plots: Five-number summary and outliers
   - Scatter Plots: Relationship between two variables
   - Bar Charts: Distribution of categorical variables
   - Pie Charts: Proportion of categories in a whole

5. **Applications in Machine Learning**:
   - Data Preprocessing: Outlier detection, scaling, imputation
   - Feature Engineering: Binning, interaction terms, polynomial features
   - Model Evaluation: Error metrics, residual analysis, performance comparison

Descriptive statistics form the foundation for more advanced statistical methods and machine learning techniques, providing insights that guide the entire data analysis process.

## References

1. Agresti, A., & Finlay, B. (2009). Statistical Methods for the Social Sciences (4th ed.). Pearson.
2. Moore, D. S., McCabe, G. P., & Craig, B. A. (2017). Introduction to the Practice of Statistics (9th ed.). W. H. Freeman.
3. McKinney, W. (2017). Python for Data Analysis (2nd ed.). O'Reilly Media.
4. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
