# 2.2.2 Partial Derivatives

## Introduction to Partial Derivatives

When working with functions of multiple variables, which is common in machine learning, we need to understand how the function changes with respect to each individual variable while holding the others constant. This is where partial derivatives come in.

A partial derivative measures the rate of change of a function with respect to one variable while keeping all other variables fixed.

## Definition of Partial Derivatives

For a function f(x₁, x₂, ..., xₙ), the partial derivative with respect to xᵢ, denoted as ∂f/∂xᵢ, is defined as:

$$\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, x_2, \ldots, x_i + h, \ldots, x_n) - f(x_1, x_2, \ldots, x_i, \ldots, x_n)}{h}$$

This is similar to the ordinary derivative, but we only vary one variable at a time.

## Notation

Partial derivatives can be denoted in several ways:
- ∂f/∂x (Leibniz notation)
- fₓ (Subscript notation)
- D₁f (Operator notation)

## Computing Partial Derivatives

### Analytical Computation

To compute a partial derivative analytically, we treat all variables except the one we're differentiating with respect to as constants, and then apply the standard rules of differentiation.

**Example**: For f(x, y) = x² + xy + y³
- ∂f/∂x = 2x + y (treating y as a constant)
- ∂f/∂y = x + 3y² (treating x as a constant)

### Numerical Computation in Python

We can compute partial derivatives numerically using finite differences:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a function of two variables
def f(x, y):
    return x**2 + x*y + y**3

# Compute partial derivatives numerically
def partial_x(f, x, y, h=1e-5):
    return (f(x + h, y) - f(x, y)) / h

def partial_y(f, x, y, h=1e-5):
    return (f(x, y + h) - f(x, y)) / h

# Compute partial derivatives at a specific point
x0, y0 = 1.0, 2.0
df_dx = partial_x(f, x0, y0)
df_dy = partial_y(f, x0, y0)

print(f"At point ({x0}, {y0}):")
print(f"∂f/∂x = {df_dx}")
print(f"∂f/∂y = {df_dy}")

# Verify with analytical derivatives
def df_dx_analytical(x, y):
    return 2*x + y

def df_dy_analytical(x, y):
    return x + 3*y**2

print(f"Analytical ∂f/∂x = {df_dx_analytical(x0, y0)}")
print(f"Analytical ∂f/∂y = {df_dy_analytical(x0, y0)}")
```

### Symbolic Computation in Python

We can also compute partial derivatives symbolically using SymPy:

```python
import sympy as sp

# Define symbolic variables
x, y = sp.symbols('x y')

# Define the function
f = x**2 + x*y + y**3

# Compute partial derivatives
df_dx = sp.diff(f, x)
df_dy = sp.diff(f, y)

print(f"Symbolic ∂f/∂x = {df_dx}")
print(f"Symbolic ∂f/∂y = {df_dy}")

# Evaluate at a specific point
x0, y0 = 1.0, 2.0
df_dx_val = df_dx.subs({x: x0, y: y0})
df_dy_val = df_dy.subs({x: x0, y: y0})

print(f"At point ({x0}, {y0}):")
print(f"∂f/∂x = {df_dx_val}")
print(f"∂f/∂y = {df_dy_val}")
```

## Visualizing Partial Derivatives

Partial derivatives can be visualized as slopes of the function along specific directions:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a function of two variables
def f(x, y):
    return x**2 + x*y + y**3

# Create a grid of points
x = np.linspace(-2, 2, 100)
y = np.linspace(-2, 2, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Choose a specific point for visualization
x0, y0 = 1.0, 2.0
z0 = f(x0, y0)

# Compute partial derivatives at that point
df_dx = 2*x0 + y0
df_dy = x0 + 3*y0**2

# Create slices through the point in x and y directions
x_slice = np.linspace(-2, 2, 100)
y_slice = np.linspace(-2, 2, 100)
z_x_slice = f(x_slice, y0)
z_y_slice = f(x0, y_slice)

# Create tangent lines at the point
x_tangent = x_slice
y_tangent = y_slice
z_x_tangent = z0 + df_dx * (x_tangent - x0)
z_y_tangent = z0 + df_dy * (y_tangent - y0)

# Create 3D plot
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')

# Plot the surface
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')

# Plot the point
ax.scatter([x0], [y0], [z0], color='red', s=50, label='Point (x₀, y₀, f(x₀, y₀))')

# Plot the slices
ax.plot(x_slice, [y0] * len(x_slice), z_x_slice, 'b-', linewidth=2, label='Slice along x')
ax.plot([x0] * len(y_slice), y_slice, z_y_slice, 'g-', linewidth=2, label='Slice along y')

# Plot the tangent lines
ax.plot(x_tangent, [y0] * len(x_tangent), z_x_tangent, 'b--', linewidth=2, label='Tangent along x')
ax.plot([x0] * len(y_tangent), y_tangent, z_y_tangent, 'g--', linewidth=2, label='Tangent along y')

# Add labels and legend
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('Visualization of Partial Derivatives')
ax.legend()

plt.tight_layout()
plt.show()

# Create 2D plots of the slices
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot slice along x
ax1.plot(x_slice, z_x_slice, 'b-', linewidth=2, label='f(x, y₀)')
ax1.plot(x_tangent, z_x_tangent, 'r--', linewidth=2, label=f'Tangent with slope ∂f/∂x = {df_dx}')
ax1.scatter([x0], [z0], color='red', s=50)
ax1.set_xlabel('x')
ax1.set_ylabel('f(x, y₀)')
ax1.set_title('Partial Derivative with respect to x')
ax1.grid(True)
ax1.legend()

# Plot slice along y
ax2.plot(y_slice, z_y_slice, 'g-', linewidth=2, label='f(x₀, y)')
ax2.plot(y_tangent, z_y_tangent, 'r--', linewidth=2, label=f'Tangent with slope ∂f/∂y = {df_dy}')
ax2.scatter([y0], [z0], color='red', s=50)
ax2.set_xlabel('y')
ax2.set_ylabel('f(x₀, y)')
ax2.set_title('Partial Derivative with respect to y')
ax2.grid(True)
ax2.legend()

plt.tight_layout()
plt.show()
```

## Higher-Order Partial Derivatives

Just as with ordinary derivatives, we can compute higher-order partial derivatives by differentiating multiple times.

### Second-Order Partial Derivatives

For a function f(x, y), there are four second-order partial derivatives:

1. ∂²f/∂x² (differentiating twice with respect to x)
2. ∂²f/∂y² (differentiating twice with respect to y)
3. ∂²f/∂x∂y (differentiating first with respect to x, then with respect to y)
4. ∂²f/∂y∂x (differentiating first with respect to y, then with respect to x)

For well-behaved functions, the mixed partial derivatives are equal: ∂²f/∂x∂y = ∂²f/∂y∂x (Clairaut's theorem).

```python
import sympy as sp

# Define symbolic variables
x, y = sp.symbols('x y')

# Define the function
f = x**3 * y**2 + x*y**4

# Compute first-order partial derivatives
df_dx = sp.diff(f, x)
df_dy = sp.diff(f, y)

print(f"∂f/∂x = {df_dx}")
print(f"∂f/∂y = {df_dy}")

# Compute second-order partial derivatives
d2f_dx2 = sp.diff(df_dx, x)
d2f_dy2 = sp.diff(df_dy, y)
d2f_dxdy = sp.diff(df_dx, y)
d2f_dydx = sp.diff(df_dy, x)

print(f"∂²f/∂x² = {d2f_dx2}")
print(f"∂²f/∂y² = {d2f_dy2}")
print(f"∂²f/∂x∂y = {d2f_dxdy}")
print(f"∂²f/∂y∂x = {d2f_dydx}")

# Verify Clairaut's theorem
print(f"Are the mixed partial derivatives equal? {d2f_dxdy == d2f_dydx}")
```

### The Hessian Matrix

The Hessian matrix is a square matrix of second-order partial derivatives. For a function f(x₁, x₂, ..., xₙ), the Hessian matrix H is defined as:

$$H(f) = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}$$

The Hessian matrix is symmetric for well-behaved functions (due to Clairaut's theorem).

```python
import sympy as sp
import numpy as np

# Define symbolic variables
x, y, z = sp.symbols('x y z')

# Define the function
f = x**2 * y + y**2 * z + z**2 * x

# Compute the gradient
grad_f = [sp.diff(f, var) for var in (x, y, z)]
print("Gradient:")
for i, var in enumerate([x, y, z]):
    print(f"∂f/∂{var} = {grad_f[i]}")

# Compute the Hessian matrix
hessian = [[sp.diff(grad_f[i], var) for i, _ in enumerate([x, y, z])] for var in [x, y, z]]

print("\nHessian Matrix:")
for row in hessian:
    print(row)

# Verify symmetry
is_symmetric = all(hessian[i][j] == hessian[j][i] for i in range(3) for j in range(3))
print(f"\nIs the Hessian matrix symmetric? {is_symmetric}")

# Evaluate the Hessian at a specific point
point = {x: 1, y: 2, z: 3}
hessian_at_point = [[float(h.subs(point)) for h in row] for row in hessian]

print("\nHessian at point (1, 2, 3):")
for row in hessian_at_point:
    print(row)

# Compute eigenvalues to determine the nature of the critical point
eigenvalues = np.linalg.eigvals(hessian_at_point)
print(f"\nEigenvalues of the Hessian: {eigenvalues}")

if all(eigenvalues > 0):
    print("The critical point is a local minimum")
elif all(eigenvalues < 0):
    print("The critical point is a local maximum")
else:
    print("The critical point is a saddle point")
```

## Applications in Machine Learning

### 1. Gradient Descent in Multiple Dimensions

In machine learning, we often need to optimize functions of many variables. Gradient descent uses partial derivatives to find the direction of steepest descent:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a function of two variables (e.g., a loss function)
def f(x, y):
    return (x - 2)**2 + 2*(y - 1)**2

# Compute the gradient (vector of partial derivatives)
def gradient(x, y):
    df_dx = 2 * (x - 2)
    df_dy = 4 * (y - 1)
    return np.array([df_dx, df_dy])

# Implement gradient descent
def gradient_descent(start_x, start_y, learning_rate=0.1, n_iterations=100):
    # Initialize
    x, y = start_x, start_y
    path = [(x, y)]
    
    for _ in range(n_iterations):
        # Compute gradient
        grad = gradient(x, y)
        
        # Update parameters
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        
        # Store the path
        path.append((x, y))
    
    return np.array(path)

# Run gradient descent from different starting points
start_points = [(-1, 3), (4, 0), (3, 3)]
paths = [gradient_descent(x0, y0) for x0, y0 in start_points]

# Create a grid of points for the contour plot
x = np.linspace(-2, 5, 100)
y = np.linspace(-1, 4, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Create 3D surface plot
fig = plt.figure(figsize=(15, 10))
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
for path in paths:
    ax1.plot(path[:, 0], path[:, 1], [f(x, y) for x, y in path], 'r-o', markersize=3)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('Gradient Descent on 3D Surface')

# Create contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax2)
for path in paths:
    ax2.plot(path[:, 0], path[:, 1], 'r-o', markersize=3)
    ax2.annotate('Start', (path[0, 0], path[0, 1]), xytext=(10, 10), 
                textcoords='offset points', color='blue')
    ax2.annotate('End', (path[-1, 0], path[-1, 1]), xytext=(10, -10), 
                textcoords='offset points', color='green')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Gradient Descent on Contour Plot')
ax2.grid(True)

plt.tight_layout()
plt.show()
```

### 2. Neural Network Training

In neural networks, we use partial derivatives to compute how the loss changes with respect to each weight and bias. This is done using the backpropagation algorithm, which efficiently computes these partial derivatives.

```python
import numpy as np
import matplotlib.pyplot as plt

# Define a simple neural network with one hidden layer
class SimpleNN:
    def __init__(self, input_size, hidden_size, output_size):
        # Initialize weights and biases
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -30, 30)))  # Clip to avoid overflow
    
    def forward(self, X):
        # Forward pass
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def compute_loss(self, y_pred, y_true):
        # Binary cross-entropy loss
        m = y_true.shape[0]
        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m
        return loss
    
    def backward(self, X, y):
        # Backward pass (compute partial derivatives)
        m = X.shape[0]
        
        # Output layer
        dz2 = self.a2 - y
        dW2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        # Hidden layer
        da1 = np.dot(dz2, self.W2.T)
        dz1 = da1 * self.a1 * (1 - self.a1)  # Derivative of sigmoid
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        return dW1, db1, dW2, db2
    
    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):
        # Update weights and biases using gradient descent
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
    
    def train(self, X, y, learning_rate=0.1, n_iterations=1000):
        losses = []
        
        for i in range(n_iterations):
            # Forward pass
            y_pred = self.forward(X)
            
            # Compute loss
            loss = self.compute_loss(y_pred, y)
            losses.append(loss)
            
            # Backward pass
            dW1, db1, dW2, db2 = self.backward(X, y)
            
            # Update parameters
            self.update_parameters(dW1, db1, dW2, db2, learning_rate)
            
            # Print progress
            if i % 100 == 0:
                print(f"Iteration {i}, Loss: {loss}")
        
        return losses

# Generate synthetic data for binary classification
np.random.seed(42)
X = np.random.randn(100, 2)
y = (X[:, 0]**2 + X[:, 1]**2 < 1).astype(int).reshape(-1, 1)

# Create and train the neural network
nn = SimpleNN(input_size=2, hidden_size=4, output_size=1)
losses = nn.train(X, y, learning_rate=0.5, n_iterations=1000)

# Plot the loss curve
plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualize the decision boundary
plt.figure(figsize=(10, 8))

# Create a grid of points
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
grid_points = np.c_[xx.ravel(), yy.ravel()]

# Make predictions on the grid
Z = nn.forward(grid_points)
Z = Z.reshape(xx.shape)

# Plot the decision boundary and data points
plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap='coolwarm', edgecolors='k', s=50)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Neural Network Decision Boundary')
plt.colorbar(label='Probability')
plt.grid(True)
plt.tight_layout()
plt.show()
```

### 3. Feature Importance in Decision Trees

In decision trees, we use partial derivatives to compute feature importance:

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a decision tree
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Get feature importances
importances = clf.feature_importances_

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Print feature ranking
print("Feature ranking:")
for i in range(X.shape[1]):
    print(f"{i+1}. {feature_names[indices[i]]} ({importances[indices[i]]:.4f})")

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()
```

## Summary

In this section, we've covered:

1. **Partial Derivatives**: How to compute and interpret derivatives with respect to individual variables
2. **Visualization**: How to visualize partial derivatives as slopes along specific directions
3. **Higher-Order Partial Derivatives**: Second derivatives and the Hessian matrix
4. **Applications in Machine Learning**: Gradient descent, neural network training, and feature importance

Partial derivatives are essential for understanding how functions of multiple variables behave, which is crucial in machine learning where we often work with high-dimensional data and complex models. In the next section, we'll explore the chain rule, which is fundamental for understanding backpropagation in neural networks.

## References

1. Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Stewart, J. (2015). Calculus: Early Transcendentals (8th ed.). Cengage Learning.
4. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
