# 2.2.3.1 Basic Chain Rule Concept

## Introduction to the Chain Rule

The Chain Rule is one of the most important concepts in calculus, especially for machine learning. It allows us to compute derivatives of composite functions, which are functions formed by combining simpler functions.

In machine learning, the Chain Rule is the mathematical foundation of the backpropagation algorithm, which is used to train neural networks. Understanding the Chain Rule is essential for grasping how neural networks learn from data.

## Definition of the Chain Rule

If a function f is composed of two functions g and h, such that f(x) = g(h(x)), then the derivative of f with respect to x is:

$$\frac{df}{dx} = \frac{dg}{dh} \cdot \frac{dh}{dx}$$

In Leibniz notation, if y = g(u) and u = h(x), then:

$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$

This formula tells us that to find the derivative of a composite function, we multiply the derivative of the outer function (evaluated at the inner function) by the derivative of the inner function.

## Simple Examples

### Example 1: f(x) = sin(x²)

Here, g(u) = sin(u) and h(x) = x².

1. Find dg/du: The derivative of sin(u) with respect to u is cos(u).
2. Find dh/dx: The derivative of x² with respect to x is 2x.
3. Apply the Chain Rule: df/dx = (dg/du) · (dh/dx) = cos(u) · 2x = cos(x²) · 2x = 2x · cos(x²).

### Example 2: f(x) = (3x + 1)⁵

Here, g(u) = u⁵ and h(x) = 3x + 1.

1. Find dg/du: The derivative of u⁵ with respect to u is 5u⁴.
2. Find dh/dx: The derivative of 3x + 1 with respect to x is 3.
3. Apply the Chain Rule: df/dx = (dg/du) · (dh/dx) = 5u⁴ · 3 = 5(3x + 1)⁴ · 3 = 15(3x + 1)⁴.

## Implementing the Chain Rule in Python

Let's implement these examples in Python:

```python
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

# Example 1: f(x) = sin(x²)
def f1(x):
    return np.sin(x**2)

def df1_dx(x):
    return 2 * x * np.cos(x**2)

# Example 2: f(x) = (3x + 1)⁵
def f2(x):
    return (3*x + 1)**5

def df2_dx(x):
    return 15 * (3*x + 1)**4

# Verify with symbolic differentiation
x = sp.Symbol('x')

# Example 1
f1_sym = sp.sin(x**2)
df1_sym = sp.diff(f1_sym, x)
print(f"Symbolic derivative of sin(x²): {df1_sym}")

# Example 2
f2_sym = (3*x + 1)**5
df2_sym = sp.diff(f2_sym, x)
print(f"Symbolic derivative of (3x + 1)⁵: {df2_sym}")

# Plot the functions and their derivatives
x_vals = np.linspace(-2, 2, 1000)

plt.figure(figsize=(15, 10))

# Example 1
plt.subplot(2, 2, 1)
plt.plot(x_vals, f1(x_vals), 'b-', label='f(x) = sin(x²)')
plt.grid(True)
plt.legend()
plt.title('Function: sin(x²)')

plt.subplot(2, 2, 2)
plt.plot(x_vals, df1_dx(x_vals), 'r-', label="f'(x) = 2x·cos(x²)")
plt.grid(True)
plt.legend()
plt.title('Derivative: 2x·cos(x²)')

# Example 2
plt.subplot(2, 2, 3)
plt.plot(x_vals, f2(x_vals), 'b-', label='f(x) = (3x + 1)⁵')
plt.grid(True)
plt.legend()
plt.title('Function: (3x + 1)⁵')

plt.subplot(2, 2, 4)
plt.plot(x_vals, df2_dx(x_vals), 'r-', label="f'(x) = 15(3x + 1)⁴")
plt.grid(True)
plt.legend()
plt.title('Derivative: 15(3x + 1)⁴')

plt.tight_layout()
plt.show()
```

## Chain Rule for Multiple Compositions

The Chain Rule can be extended to functions with multiple compositions. If f(x) = g(h(j(x))), then:

$$\frac{df}{dx} = \frac{dg}{dh} \cdot \frac{dh}{dj} \cdot \frac{dj}{dx}$$

### Example: f(x) = sin(e^(x²))

Here, we have three functions composed together:
- j(x) = x²
- h(u) = e^u
- g(v) = sin(v)

So f(x) = g(h(j(x))) = sin(e^(x²)).

1. Find dg/dh: The derivative of sin(v) with respect to v is cos(v).
2. Find dh/dj: The derivative of e^u with respect to u is e^u.
3. Find dj/dx: The derivative of x² with respect to x is 2x.
4. Apply the Chain Rule: df/dx = (dg/dh) · (dh/dj) · (dj/dx) = cos(e^(x²)) · e^(x²) · 2x = 2x · e^(x²) · cos(e^(x²)).

```python
# Example with multiple compositions: f(x) = sin(e^(x²))
def f3(x):
    return np.sin(np.exp(x**2))

def df3_dx(x):
    return 2 * x * np.exp(x**2) * np.cos(np.exp(x**2))

# Verify with symbolic differentiation
f3_sym = sp.sin(sp.exp(x**2))
df3_sym = sp.diff(f3_sym, x)
print(f"Symbolic derivative of sin(e^(x²)): {df3_sym}")

# Plot the function and its derivative
x_vals = np.linspace(-1, 1, 1000)  # Smaller range due to rapid growth

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(x_vals, f3(x_vals), 'b-', label='f(x) = sin(e^(x²))')
plt.grid(True)
plt.legend()
plt.title('Function: sin(e^(x²))')

plt.subplot(1, 2, 2)
plt.plot(x_vals, df3_dx(x_vals), 'r-', label="f'(x) = 2x·e^(x²)·cos(e^(x²))")
plt.grid(True)
plt.legend()
plt.title('Derivative: 2x·e^(x²)·cos(e^(x²))')

plt.tight_layout()
plt.show()
```

## Intuitive Understanding of the Chain Rule

The Chain Rule can be understood intuitively as follows:

1. **Rate of Change Perspective**: If y changes with respect to u at a certain rate, and u changes with respect to x at another rate, then the rate at which y changes with respect to x is the product of these two rates.

2. **Infinitesimal Perspective**: If we make a small change dx in x, it causes a small change du in u, which in turn causes a small change dy in y. The ratio dy/dx is equal to (dy/du) · (du/dx).

3. **Unit Conversion Perspective**: The Chain Rule can be viewed as a unit conversion. If dy/du is measured in "y units per u unit" and du/dx is measured in "u units per x unit", then their product gives "y units per x unit".

## Visualizing the Chain Rule

Let's visualize how the Chain Rule works for a simple example:

```python
import numpy as np
import matplotlib.pyplot as plt

# Define functions
def h(x):
    return x**2

def g(u):
    return np.sin(u)

def f(x):
    return g(h(x))

# Define derivatives
def dh_dx(x):
    return 2*x

def dg_du(u):
    return np.cos(u)

def df_dx(x):
    return dg_du(h(x)) * dh_dx(x)

# Create data for visualization
x_vals = np.linspace(-2, 2, 1000)
u_vals = h(x_vals)
y_vals = g(u_vals)

# Create figure
plt.figure(figsize=(15, 10))

# Plot h(x) = x²
plt.subplot(2, 2, 1)
plt.plot(x_vals, u_vals, 'b-')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True)
plt.xlabel('x')
plt.ylabel('u = h(x)')
plt.title('h(x) = x²')

# Plot g(u) = sin(u)
plt.subplot(2, 2, 2)
plt.plot(u_vals, y_vals, 'g-')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True)
plt.xlabel('u')
plt.ylabel('y = g(u)')
plt.title('g(u) = sin(u)')

# Plot f(x) = g(h(x)) = sin(x²)
plt.subplot(2, 2, 3)
plt.plot(x_vals, y_vals, 'r-')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y = f(x)')
plt.title('f(x) = g(h(x)) = sin(x²)')

# Plot the derivative df/dx = (dg/du) * (dh/dx)
plt.subplot(2, 2, 4)
plt.plot(x_vals, df_dx(x_vals), 'm-')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True)
plt.xlabel('x')
plt.ylabel("f'(x)")
plt.title("f'(x) = (dg/du) * (dh/dx) = 2x·cos(x²)")

plt.tight_layout()
plt.show()
```

## Common Mistakes and Pitfalls

1. **Forgetting to Evaluate the Outer Derivative at the Inner Function**: When applying the Chain Rule, remember that dg/du must be evaluated at u = h(x).

2. **Incorrect Order of Multiplication**: The order matters! Always multiply the derivative of the outer function by the derivative of the inner function.

3. **Missing Terms in Complex Compositions**: When dealing with multiple compositions, make sure to include all the terms in the chain.

## Summary

In this section, we've covered:

1. **The Chain Rule Definition**: How to compute derivatives of composite functions
2. **Simple Examples**: Applying the Chain Rule to basic functions
3. **Multiple Compositions**: Extending the Chain Rule to functions with multiple layers
4. **Intuitive Understanding**: Different perspectives on the Chain Rule
5. **Visualization**: How the Chain Rule works graphically

The Chain Rule is a powerful tool in calculus that allows us to break down complex derivatives into simpler parts. In the next section, we'll explore how the Chain Rule extends to functions of multiple variables, which is crucial for understanding gradient-based optimization in machine learning.

## References

1. Stewart, J. (2015). Calculus: Early Transcendentals (8th ed.). Cengage Learning.
2. Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press.
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
