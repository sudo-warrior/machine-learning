# 2.2.3.5 Automatic Differentiation

## Introduction to Automatic Differentiation

Automatic Differentiation (AD) is a set of techniques for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. Unlike numerical differentiation (which uses finite differences) or symbolic differentiation (which manipulates mathematical expressions), automatic differentiation computes exact derivatives by applying the chain rule systematically to elementary operations.

In machine learning, automatic differentiation is the engine behind gradient-based optimization algorithms. Modern deep learning frameworks like TensorFlow, PyTorch, and JAX all implement automatic differentiation to compute gradients for training neural networks.

## Why Automatic Differentiation?

There are three main approaches to computing derivatives:

1. **Numerical Differentiation**: Approximates derivatives using finite differences
   - Simple to implement
   - Prone to numerical errors
   - Computationally expensive for high-dimensional functions

2. **Symbolic Differentiation**: Manipulates mathematical expressions to derive analytical derivatives
   - Produces exact derivatives
   - Can lead to expression swell (very complex expressions)
   - Difficult to implement for complex programs

3. **Automatic Differentiation**: Computes exact derivatives by tracking operations during program execution
   - Provides exact derivatives (unlike numerical differentiation)
   - Avoids expression swell (unlike symbolic differentiation)
   - Efficiently handles complex programs and high-dimensional functions

## Modes of Automatic Differentiation

There are two primary modes of automatic differentiation:

### Forward Mode

In forward mode, derivatives are computed alongside the function evaluation, from inputs to outputs. For each intermediate variable v, we compute both its value and its derivative with respect to the input variables.

Forward mode is efficient when the number of input variables is small compared to the number of output variables.

### Reverse Mode

In reverse mode, the function is first evaluated forward to compute all intermediate values, and then derivatives are computed backward from outputs to inputs. This is essentially what backpropagation does.

Reverse mode is efficient when the number of output variables is small compared to the number of input variables, which is typically the case in machine learning (where we have many parameters but a single loss value).

## The Computational Graph Perspective

Automatic differentiation can be understood through computational graphs:

1. **Forward Pass**: Evaluate the function and build the computational graph
2. **Backward Pass**: Traverse the graph in reverse to compute derivatives

Each node in the graph represents an operation, and each edge represents data flow. During the backward pass, we apply the chain rule at each node to compute derivatives.

## Implementing Automatic Differentiation

Let's implement a simple automatic differentiation system to understand how it works:

```python
import numpy as np
import matplotlib.pyplot as plt

class Variable:
    def __init__(self, value, name=None):
        self.value = value
        self.grad = 0
        self.name = name
        self.children = []
        self.op = ""
    
    def __add__(self, other):
        other = other if isinstance(other, Variable) else Variable(other)
        result = Variable(self.value + other.value)
        result.children = [(self, 1.0), (other, 1.0)]
        result.op = "+"
        return result
    
    def __mul__(self, other):
        other = other if isinstance(other, Variable) else Variable(other)
        result = Variable(self.value * other.value)
        result.children = [(self, other.value), (other, self.value)]
        result.op = "*"
        return result
    
    def __pow__(self, power):
        assert isinstance(power, (int, float))
        result = Variable(self.value ** power)
        result.children = [(self, power * self.value ** (power - 1))]
        result.op = f"^{power}"
        return result
    
    def __neg__(self):
        return self * -1
    
    def __sub__(self, other):
        return self + (-other)
    
    def __truediv__(self, other):
        other = other if isinstance(other, Variable) else Variable(other)
        return self * (other ** -1)
    
    def backward(self, grad=1.0):
        self.grad += grad
        for child, grad_wrt_child in self.children:
            child.backward(grad * grad_wrt_child)
    
    def zero_grad(self):
        self.grad = 0
        for child, _ in self.children:
            child.zero_grad()

def sin(x):
    result = Variable(np.sin(x.value))
    result.children = [(x, np.cos(x.value))]
    result.op = "sin"
    return result

def cos(x):
    result = Variable(np.cos(x.value))
    result.children = [(x, -np.sin(x.value))]
    result.op = "cos"
    return result

def exp(x):
    result = Variable(np.exp(x.value))
    result.children = [(x, np.exp(x.value))]
    result.op = "exp"
    return result

def log(x):
    result = Variable(np.log(x.value))
    result.children = [(x, 1.0 / x.value)]
    result.op = "log"
    return result

def visualize_computational_graph(var, filename=None):
    import networkx as nx
    
    G = nx.DiGraph()
    
    def build_graph(v, visited=None):
        if visited is None:
            visited = set()
        
        if v in visited:
            return
        
        visited.add(v)
        
        # Add node
        node_id = id(v)
        label = f"{v.name if v.name else ''}\nvalue: {v.value:.4f}\ngrad: {v.grad:.4f}"
        G.add_node(node_id, label=label)
        
        # Add edges
        for child, grad_wrt_child in v.children:
            child_id = id(child)
            build_graph(child, visited)
            G.add_edge(child_id, node_id, label=f"{grad_wrt_child:.4f}")
    
    build_graph(var)
    
    # Draw the graph
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, seed=42)
    
    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='lightblue', alpha=0.8)
    
    # Draw edges
    nx.draw_networkx_edges(G, pos, width=2, alpha=0.7, arrows=True, arrowsize=20)
    
    # Draw labels
    node_labels = nx.get_node_attributes(G, 'label')
    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10)
    
    # Draw edge labels
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
    
    plt.axis('off')
    plt.title('Computational Graph')
    plt.tight_layout()
    
    if filename:
        plt.savefig(filename)
    
    plt.show()

# Example 1: f(x) = x^2
def example1():
    x = Variable(3.0, name="x")
    f = x ** 2
    f.name = "f"
    
    print(f"f({x.value}) = {f.value}")
    
    # Compute gradient
    f.backward()
    
    print(f"df/dx = {x.grad}")
    
    # Visualize the computational graph
    visualize_computational_graph(f)

# Example 2: f(x, y) = (x + y) * (x - y)
def example2():
    x = Variable(3.0, name="x")
    y = Variable(2.0, name="y")
    
    a = x + y
    a.name = "a"
    
    b = x - y
    b.name = "b"
    
    f = a * b
    f.name = "f"
    
    print(f"f({x.value}, {y.value}) = {f.value}")
    
    # Compute gradients
    f.backward()
    
    print(f"df/dx = {x.grad}")
    print(f"df/dy = {y.grad}")
    
    # Visualize the computational graph
    visualize_computational_graph(f)

# Example 3: f(x, y, z) = sin(x) * exp(y) / log(z)
def example3():
    x = Variable(1.0, name="x")
    y = Variable(2.0, name="y")
    z = Variable(3.0, name="z")
    
    a = sin(x)
    a.name = "sin(x)"
    
    b = exp(y)
    b.name = "exp(y)"
    
    c = log(z)
    c.name = "log(z)"
    
    d = a * b
    d.name = "sin(x)*exp(y)"
    
    f = d / c
    f.name = "f"
    
    print(f"f({x.value}, {y.value}, {z.value}) = {f.value}")
    
    # Compute gradients
    f.backward()
    
    print(f"df/dx = {x.grad}")
    print(f"df/dy = {y.grad}")
    print(f"df/dz = {z.grad}")
    
    # Visualize the computational graph
    visualize_computational_graph(f)

# Run the examples
print("Example 1: f(x) = x^2")
example1()

print("\nExample 2: f(x, y) = (x + y) * (x - y)")
example2()

print("\nExample 3: f(x, y, z) = sin(x) * exp(y) / log(z)")
example3()
```

## Automatic Differentiation in Deep Learning Frameworks

Modern deep learning frameworks implement automatic differentiation to compute gradients efficiently. Let's see how it works in TensorFlow and PyTorch:

### TensorFlow

TensorFlow uses a technique called "tape-based automatic differentiation" through the `GradientTape` API:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Example 1: f(x) = x^2
def tf_example1():
    x = tf.Variable(3.0)
    
    with tf.GradientTape() as tape:
        f = x ** 2
    
    df_dx = tape.gradient(f, x)
    
    print(f"f({x.numpy()}) = {f.numpy()}")
    print(f"df/dx = {df_dx.numpy()}")

# Example 2: f(x, y) = (x + y) * (x - y)
def tf_example2():
    x = tf.Variable(3.0)
    y = tf.Variable(2.0)
    
    with tf.GradientTape() as tape:
        a = x + y
        b = x - y
        f = a * b
    
    gradients = tape.gradient(f, [x, y])
    
    print(f"f({x.numpy()}, {y.numpy()}) = {f.numpy()}")
    print(f"df/dx = {gradients[0].numpy()}")
    print(f"df/dy = {gradients[1].numpy()}")

# Example 3: f(x, y, z) = sin(x) * exp(y) / log(z)
def tf_example3():
    x = tf.Variable(1.0)
    y = tf.Variable(2.0)
    z = tf.Variable(3.0)
    
    with tf.GradientTape() as tape:
        a = tf.sin(x)
        b = tf.exp(y)
        c = tf.math.log(z)
        d = a * b
        f = d / c
    
    gradients = tape.gradient(f, [x, y, z])
    
    print(f"f({x.numpy()}, {y.numpy()}, {z.numpy()}) = {f.numpy()}")
    print(f"df/dx = {gradients[0].numpy()}")
    print(f"df/dy = {gradients[1].numpy()}")
    print(f"df/dz = {gradients[2].numpy()}")

# Run the examples
print("TensorFlow Example 1: f(x) = x^2")
tf_example1()

print("\nTensorFlow Example 2: f(x, y) = (x + y) * (x - y)")
tf_example2()

print("\nTensorFlow Example 3: f(x, y, z) = sin(x) * exp(y) / log(z)")
tf_example3()
```

### PyTorch

PyTorch uses dynamic computational graphs and the `backward()` method to compute gradients:

```python
import torch
import matplotlib.pyplot as plt

# Example 1: f(x) = x^2
def torch_example1():
    x = torch.tensor(3.0, requires_grad=True)
    f = x ** 2
    
    f.backward()
    
    print(f"f({x.item()}) = {f.item()}")
    print(f"df/dx = {x.grad.item()}")

# Example 2: f(x, y) = (x + y) * (x - y)
def torch_example2():
    x = torch.tensor(3.0, requires_grad=True)
    y = torch.tensor(2.0, requires_grad=True)
    
    a = x + y
    b = x - y
    f = a * b
    
    f.backward()
    
    print(f"f({x.item()}, {y.item()}) = {f.item()}")
    print(f"df/dx = {x.grad.item()}")
    print(f"df/dy = {y.grad.item()}")

# Example 3: f(x, y, z) = sin(x) * exp(y) / log(z)
def torch_example3():
    x = torch.tensor(1.0, requires_grad=True)
    y = torch.tensor(2.0, requires_grad=True)
    z = torch.tensor(3.0, requires_grad=True)
    
    a = torch.sin(x)
    b = torch.exp(y)
    c = torch.log(z)
    d = a * b
    f = d / c
    
    f.backward()
    
    print(f"f({x.item()}, {y.item()}, {z.item()}) = {f.item()}")
    print(f"df/dx = {x.grad.item()}")
    print(f"df/dy = {y.grad.item()}")
    print(f"df/dz = {z.grad.item()}")

# Run the examples
print("PyTorch Example 1: f(x) = x^2")
torch_example1()

print("\nPyTorch Example 2: f(x, y) = (x + y) * (x - y)")
torch_example2()

print("\nPyTorch Example 3: f(x, y, z) = sin(x) * exp(y) / log(z)")
torch_example3()
```

## Higher-Order Derivatives

Automatic differentiation can also compute higher-order derivatives by applying the process recursively:

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Compute first and second derivatives of f(x) = sin(x)
def higher_order_derivatives():
    x = tf.linspace(-2*np.pi, 2*np.pi, 1000)
    
    with tf.GradientTape() as tape2:
        tape2.watch(x)
        with tf.GradientTape() as tape1:
            tape1.watch(x)
            y = tf.sin(x)
        dy_dx = tape1.gradient(y, x)
    d2y_dx2 = tape2.gradient(dy_dx, x)
    
    plt.figure(figsize=(12, 6))
    plt.plot(x.numpy(), y.numpy(), label='f(x) = sin(x)')
    plt.plot(x.numpy(), dy_dx.numpy(), label="f'(x) = cos(x)")
    plt.plot(x.numpy(), d2y_dx2.numpy(), label="f''(x) = -sin(x)")
    plt.grid(True)
    plt.legend()
    plt.title('Function and its Derivatives')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.tight_layout()
    plt.show()

higher_order_derivatives()
```

## Jacobian and Hessian Matrices

Automatic differentiation can compute Jacobian and Hessian matrices for vector-valued functions:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Compute the Jacobian of f(x, y) = [x^2 + y, x*y]
def compute_jacobian():
    x = tf.Variable(1.0)
    y = tf.Variable(2.0)
    
    with tf.GradientTape() as tape:
        f1 = x**2 + y
        f2 = x * y
        f = tf.stack([f1, f2])
    
    jacobian = tape.jacobian(f, [x, y])
    
    print("Jacobian matrix:")
    print(f"∂f1/∂x = {jacobian[0][0].numpy()}")
    print(f"∂f1/∂y = {jacobian[0][1].numpy()}")
    print(f"∂f2/∂x = {jacobian[1][0].numpy()}")
    print(f"∂f2/∂y = {jacobian[1][1].numpy()}")

# Compute the Hessian of f(x, y) = x^2 + 2*x*y + y^2
def compute_hessian():
    x = tf.Variable(1.0)
    y = tf.Variable(2.0)
    
    with tf.GradientTape() as tape2:
        tape2.watch([x, y])
        with tf.GradientTape() as tape1:
            tape1.watch([x, y])
            f = x**2 + 2*x*y + y**2
        grad = tape1.gradient(f, [x, y])
    hessian = tape2.jacobian(grad, [x, y])
    
    print("Hessian matrix:")
    print(f"∂²f/∂x² = {hessian[0][0].numpy()}")
    print(f"∂²f/∂x∂y = {hessian[0][1].numpy()}")
    print(f"∂²f/∂y∂x = {hessian[1][0].numpy()}")
    print(f"∂²f/∂y² = {hessian[1][1].numpy()}")
    
    # Visualize the function and its Hessian
    x_vals = np.linspace(-2, 2, 100)
    y_vals = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x_vals, y_vals)
    Z = X**2 + 2*X*Y + Y**2
    
    fig = plt.figure(figsize=(12, 6))
    
    # 3D surface plot
    ax1 = fig.add_subplot(121, projection='3d')
    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_zlabel('f(x, y)')
    ax1.set_title('f(x, y) = x² + 2xy + y²')
    
    # Contour plot
    ax2 = fig.add_subplot(122)
    contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax2)
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')
    ax2.set_title('Contour Plot')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# Run the examples
print("Computing Jacobian:")
compute_jacobian()

print("\nComputing Hessian:")
compute_hessian()
```

## Applications in Machine Learning

### Gradient Descent Optimization

Automatic differentiation is used to compute gradients for gradient-based optimization algorithms:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a function to optimize: f(x, y) = (x - 2)^2 + 2*(y - 1)^2
def f(x, y):
    return (x - 2)**2 + 2*(y - 1)**2

# Gradient descent with automatic differentiation
def gradient_descent_with_ad():
    # Initial point
    x = tf.Variable(0.0)
    y = tf.Variable(0.0)
    
    # Optimization parameters
    learning_rate = 0.1
    n_iterations = 50
    
    # Store the path
    path = [(x.numpy(), y.numpy(), f(x, y).numpy())]
    
    for i in range(n_iterations):
        with tf.GradientTape() as tape:
            loss = f(x, y)
        
        # Compute gradients
        gradients = tape.gradient(loss, [x, y])
        
        # Update parameters
        x.assign_sub(learning_rate * gradients[0])
        y.assign_sub(learning_rate * gradients[1])
        
        # Store the path
        path.append((x.numpy(), y.numpy(), f(x, y).numpy()))
        
        if i % 10 == 0:
            print(f"Iteration {i}: x = {x.numpy()}, y = {y.numpy()}, f(x, y) = {loss.numpy()}")
    
    print(f"Final: x = {x.numpy()}, y = {y.numpy()}, f(x, y) = {f(x, y).numpy()}")
    
    # Visualize the optimization path
    path = np.array(path)
    
    # Create a grid of points
    x_vals = np.linspace(-1, 3, 100)
    y_vals = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x_vals, y_vals)
    Z = f(X, Y).numpy()
    
    fig = plt.figure(figsize=(15, 10))
    
    # 3D surface plot
    ax1 = fig.add_subplot(121, projection='3d')
    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
    ax1.plot(path[:, 0], path[:, 1], path[:, 2], 'r-o', markersize=3)
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_zlabel('f(x, y)')
    ax1.set_title('Gradient Descent Path on 3D Surface')
    
    # Contour plot
    ax2 = fig.add_subplot(122)
    contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax2)
    ax2.plot(path[:, 0], path[:, 1], 'r-o', markersize=3)
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')
    ax2.set_title('Gradient Descent Path on Contour Plot')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# Run the example
gradient_descent_with_ad()
```

### Neural Network Training

Automatic differentiation is used to compute gradients for training neural networks:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1)

# Convert to TensorFlow tensors
X_tf = tf.convert_to_tensor(X, dtype=tf.float32)
y_tf = tf.convert_to_tensor(y, dtype=tf.float32)

# Define a simple linear model
class LinearModel(tf.Module):
    def __init__(self):
        self.w = tf.Variable(tf.random.normal([1, 1]))
        self.b = tf.Variable(tf.zeros([1]))
    
    def __call__(self, x):
        return x @ self.w + self.b

# Define the loss function
def loss_fn(model, x, y):
    y_pred = model(x)
    return tf.reduce_mean(tf.square(y_pred - y))

# Train the model using automatic differentiation
def train_model():
    model = LinearModel()
    optimizer = tf.optimizers.SGD(learning_rate=0.01)
    
    n_epochs = 100
    losses = []
    
    for epoch in range(n_epochs):
        with tf.GradientTape() as tape:
            loss = loss_fn(model, X_tf, y_tf)
        
        gradients = tape.gradient(loss, [model.w, model.b])
        optimizer.apply_gradients(zip(gradients, [model.w, model.b]))
        
        losses.append(loss.numpy())
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {loss.numpy()}, w = {model.w.numpy()}, b = {model.b.numpy()}")
    
    print(f"Final: w = {model.w.numpy()}, b = {model.b.numpy()}")
    
    # Plot the data and the fitted line
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, alpha=0.7)
    
    # Plot the fitted line
    x_range = np.linspace(0, 10, 100).reshape(-1, 1)
    y_pred = model(tf.convert_to_tensor(x_range, dtype=tf.float32)).numpy()
    plt.plot(x_range, y_pred, 'r-', linewidth=2)
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Linear Regression using Automatic Differentiation')
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    
    # Plot the loss curve
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Run the example
train_model()
```

## Advanced Topics in Automatic Differentiation

### Checkpointing

For very deep networks, storing all intermediate values for reverse-mode AD can be memory-intensive. Checkpointing is a technique that trades computation for memory by recomputing some intermediate values during the backward pass.

### Vectorized Jacobian Products (VJPs) and Jacobian-Vector Products (JVPs)

Modern AD systems often use VJPs (for reverse mode) and JVPs (for forward mode) as building blocks for efficient gradient computation.

### Custom Gradients

Sometimes, we may want to define custom gradients for operations, either for numerical stability or to implement non-differentiable operations:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Define a custom gradient for the ReLU function
@tf.custom_gradient
def custom_relu(x):
    def grad(dy):
        return dy * tf.cast(x > 0, tf.float32)
    return tf.maximum(0.0, x), grad

# Test the custom gradient
def test_custom_gradient():
    x = tf.linspace(-2.0, 2.0, 1000)
    
    with tf.GradientTape() as tape:
        tape.watch(x)
        y = custom_relu(x)
    
    dy_dx = tape.gradient(y, x)
    
    plt.figure(figsize=(10, 6))
    plt.plot(x.numpy(), y.numpy(), label='ReLU(x)')
    plt.plot(x.numpy(), dy_dx.numpy(), label="ReLU'(x)")
    plt.grid(True)
    plt.legend()
    plt.title('ReLU Function and its Derivative')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.tight_layout()
    plt.show()

# Run the example
test_custom_gradient()
```

## Summary

In this section, we've covered:

1. **Automatic Differentiation**: A technique for efficiently computing exact derivatives of functions
2. **Forward and Reverse Modes**: Different approaches to computing derivatives
3. **Implementation**: Building a simple automatic differentiation system
4. **Deep Learning Frameworks**: How TensorFlow and PyTorch implement automatic differentiation
5. **Higher-Order Derivatives**: Computing second derivatives and beyond
6. **Jacobian and Hessian Matrices**: Computing derivatives for vector-valued functions
7. **Applications**: Using automatic differentiation for optimization and neural network training
8. **Advanced Topics**: Checkpointing, VJPs/JVPs, and custom gradients

Automatic differentiation is a powerful tool that enables efficient gradient-based optimization in machine learning. By systematically applying the chain rule to elementary operations, it provides exact derivatives without the limitations of numerical or symbolic differentiation.

## References

1. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18(153), 1-43.
2. Griewank, A., & Walther, A. (2008). Evaluating derivatives: principles and techniques of algorithmic differentiation. Society for Industrial and Applied Mathematics.
3. Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (pp. 8026-8037).
4. Abadi, M., et al. (2016). TensorFlow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) (pp. 265-283).
5. Bradbury, J., et al. (2018). JAX: composable transformations of Python+NumPy programs. GitHub.
