# 2.2.5.1 Basic Function Visualization

## Introduction to Function Visualization

Visualizing functions is a crucial skill in machine learning and optimization. It helps us understand the behavior of loss functions, identify potential challenges in optimization, and gain intuition about how different algorithms might perform. In this section, we'll explore various techniques for visualizing functions in both 2D and 3D.

## Visualizing 1D Functions

Let's start with the simplest case: visualizing a function of one variable. This involves plotting the function value (y-axis) against the input variable (x-axis).

```python
import numpy as np
import matplotlib.pyplot as plt

# Define a simple 1D function
def f(x):
    return x**2 - 4*x + 4  # (x - 2)^2

# Create a range of x values
x = np.linspace(-2, 6, 1000)

# Compute the function values
y = f(x)

# Plot the function
plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', linewidth=2)
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('f(x) = x² - 4x + 4 = (x - 2)²')

# Mark the minimum
plt.plot(2, 0, 'ro', markersize=8)
plt.annotate('Minimum: (2, 0)', xy=(2, 0), xytext=(3, 1),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.show()
```

### Visualizing Derivatives

We can also visualize the derivative of a function to understand how it changes and where critical points (minima, maxima, saddle points) occur:

```python
# Define the derivative of f(x)
def df_dx(x):
    return 2*x - 4  # 2(x - 2)

# Compute the derivative values
dy_dx = df_dx(x)

# Plot the function and its derivative
plt.figure(figsize=(12, 8))

# Plot the function
plt.subplot(2, 1, 1)
plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x² - 4x + 4')
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=2, color='r', linestyle='--', alpha=0.5, label='x = 2 (Critical Point)')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Function: f(x) = x² - 4x + 4')
plt.legend()

# Plot the derivative
plt.subplot(2, 1, 2)
plt.plot(x, dy_dx, 'g-', linewidth=2, label="f'(x) = 2x - 4")
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=2, color='r', linestyle='--', alpha=0.5, label='x = 2 (f\'(x) = 0)')
plt.xlabel('x')
plt.ylabel("f'(x)")
plt.title('Derivative: f\'(x) = 2x - 4')
plt.legend()

plt.tight_layout()
plt.show()
```

### Multiple Functions

We can compare multiple functions on the same plot:

```python
# Define multiple functions
def f1(x):
    return x**2

def f2(x):
    return x**3

def f3(x):
    return np.sin(x)

def f4(x):
    return np.exp(-x**2)

# Compute function values
y1 = f1(x)
y2 = f2(x)
y3 = f3(x)
y4 = f4(x)

# Plot multiple functions
plt.figure(figsize=(12, 8))
plt.plot(x, y1, 'b-', linewidth=2, label='f₁(x) = x²')
plt.plot(x, y2, 'r-', linewidth=2, label='f₂(x) = x³')
plt.plot(x, y3, 'g-', linewidth=2, label='f₃(x) = sin(x)')
plt.plot(x, y4, 'y-', linewidth=2, label='f₄(x) = e^(-x²)')
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Comparison of Different Functions')
plt.legend()
plt.show()
```

## Visualizing 2D Functions (3D Plots)

For functions of two variables, we can create 3D surface plots to visualize how the function value changes with both input variables:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a 2D function
def f(x, y):
    return x**2 + y**2  # Bowl-shaped function (paraboloid)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Create a 3D surface plot
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('3D Surface Plot: f(x, y) = x² + y²')
plt.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='f(x, y) value')
plt.show()
```

### Contour Plots

Contour plots provide a 2D view of a function of two variables, where contour lines connect points of equal function value:

```python
# Create a contour plot
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Contour Plot: f(x, y) = x² + y²')
plt.grid(True)
plt.axis('equal')
plt.show()
```

### Filled Contour Plots

Filled contour plots color the regions between contour lines, providing a clearer visualization of the function's behavior:

```python
# Create a filled contour plot
plt.figure(figsize=(10, 8))
contourf = plt.contourf(X, Y, Z, 20, cmap='viridis', alpha=0.8)
contour = plt.contour(X, Y, Z, 20, colors='k', alpha=0.3)
plt.colorbar(label='f(x, y) value')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Filled Contour Plot: f(x, y) = x² + y²')
plt.grid(True)
plt.axis('equal')
plt.show()
```

### Combining 3D and Contour Plots

We can combine 3D surface plots and contour plots to get a more complete understanding of the function:

```python
# Create a combined 3D surface and contour plot
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('3D Surface Plot')

# Contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot')
ax2.grid(True)
ax2.axis('equal')

plt.suptitle('f(x, y) = x² + y²', fontsize=16)
plt.tight_layout()
plt.show()
```

## Visualizing More Complex Functions

Let's visualize some more complex functions that are common in optimization problems:

### Rosenbrock Function

The Rosenbrock function is a non-convex function often used as a test case for optimization algorithms:

```python
# Define the Rosenbrock function
def rosenbrock(x, y, a=1, b=100):
    return (a - x)**2 + b * (y - x**2)**2

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

# Create a combined 3D surface and contour plot
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('3D Surface Plot')

# Contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot')
ax2.grid(True)

# Mark the global minimum
ax1.plot([1], [1], [0], 'ro', markersize=8)
ax2.plot(1, 1, 'ro', markersize=8)
ax2.annotate('Global Minimum: (1, 1)', xy=(1, 1), xytext=(0, 2),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.suptitle('Rosenbrock Function: f(x, y) = (1 - x)² + 100(y - x²)²', fontsize=16)
plt.tight_layout()
plt.show()

# Create a log-scale contour plot to better visualize the valley
plt.figure(figsize=(10, 8))
contourf = plt.contourf(X, Y, np.log(Z + 1), 50, cmap='viridis', alpha=0.8)
contour = plt.contour(X, Y, np.log(Z + 1), 50, colors='k', alpha=0.3)
plt.colorbar(label='log(f(x, y) + 1) value')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Log-scale Contour Plot of Rosenbrock Function')
plt.grid(True)
plt.plot(1, 1, 'ro', markersize=8)
plt.annotate('Global Minimum: (1, 1)', xy=(1, 1), xytext=(0, 2),
             arrowprops=dict(facecolor='black', shrink=0.05))
plt.show()
```

### Himmelblau's Function

Himmelblau's function is another non-convex function with multiple local minima:

```python
# Define Himmelblau's function
def himmelblau(x, y):
    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = himmelblau(X, Y)

# Create a combined 3D surface and contour plot
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('3D Surface Plot')

# Contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot')
ax2.grid(True)

# Mark the local minima
minima = [
    (3.0, 2.0),
    (-2.805118, 3.131312),
    (-3.779310, -3.283186),
    (3.584428, -1.848126)
]

for i, (x_min, y_min) in enumerate(minima):
    z_min = himmelblau(x_min, y_min)
    ax1.plot([x_min], [y_min], [z_min], 'ro', markersize=8)
    ax2.plot(x_min, y_min, 'ro', markersize=8)
    ax2.annotate(f'Minimum {i+1}: ({x_min:.1f}, {y_min:.1f})', 
                 xy=(x_min, y_min), xytext=(x_min + 0.5, y_min + 0.5),
                 arrowprops=dict(facecolor='black', shrink=0.05))

plt.suptitle("Himmelblau's Function: f(x, y) = (x² + y - 11)² + (x + y² - 7)²", fontsize=16)
plt.tight_layout()
plt.show()

# Create a log-scale contour plot
plt.figure(figsize=(10, 8))
contourf = plt.contourf(X, Y, np.log(Z + 1), 50, cmap='viridis', alpha=0.8)
contour = plt.contour(X, Y, np.log(Z + 1), 50, colors='k', alpha=0.3)
plt.colorbar(label='log(f(x, y) + 1) value')
plt.xlabel('x')
plt.ylabel('y')
plt.title("Log-scale Contour Plot of Himmelblau's Function")
plt.grid(True)

for i, (x_min, y_min) in enumerate(minima):
    plt.plot(x_min, y_min, 'ro', markersize=8)
    plt.annotate(f'Minimum {i+1}', xy=(x_min, y_min), xytext=(x_min + 0.5, y_min + 0.5),
                 arrowprops=dict(facecolor='black', shrink=0.05))

plt.show()
```

### Rastrigin Function

The Rastrigin function is a non-convex function with many local minima, making it challenging for optimization algorithms:

```python
# Define the Rastrigin function
def rastrigin(x, y, A=10):
    return 2*A + (x**2 - A*np.cos(2*np.pi*x)) + (y**2 - A*np.cos(2*np.pi*y))

# Create a grid of x and y values
x = np.linspace(-5.12, 5.12, 100)
y = np.linspace(-5.12, 5.12, 100)
X, Y = np.meshgrid(x, y)
Z = rastrigin(X, Y)

# Create a combined 3D surface and contour plot
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('3D Surface Plot')

# Contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot')
ax2.grid(True)

# Mark the global minimum
ax1.plot([0], [0], [0], 'ro', markersize=8)
ax2.plot(0, 0, 'ro', markersize=8)
ax2.annotate('Global Minimum: (0, 0)', xy=(0, 0), xytext=(1, 1),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.suptitle('Rastrigin Function: f(x, y) = 20 + x² - 10cos(2πx) + y² - 10cos(2πy)', fontsize=16)
plt.tight_layout()
plt.show()
```

## Visualizing Gradients

We can also visualize the gradient of a function to understand how optimization algorithms might behave:

```python
# Define a simple 2D function and its gradient
def f(x, y):
    return x**2 + y**2

def grad_f(x, y):
    return np.array([2*x, 2*y])

# Create a grid of x and y values
x = np.linspace(-5, 5, 20)
y = np.linspace(-5, 5, 20)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Compute the gradient at each point
U = np.zeros_like(X)
V = np.zeros_like(Y)

for i in range(len(x)):
    for j in range(len(y)):
        grad = grad_f(X[j, i], Y[j, i])
        U[j, i] = grad[0]
        V[j, i] = grad[1]

# Normalize the gradient vectors for better visualization
norm = np.sqrt(U**2 + V**2)
U_norm = U / (norm + 1e-10)  # Add small constant to avoid division by zero
V_norm = V / (norm + 1e-10)

# Create a contour plot with gradient vectors
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')
plt.quiver(X, Y, U_norm, V_norm, color='r', scale=30)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Contour Plot with Gradient Vectors: f(x, y) = x² + y²')
plt.grid(True)
plt.axis('equal')
plt.show()
```

## Interactive Visualizations

For a more interactive experience, we can use libraries like Plotly to create interactive 3D plots:

```python
import plotly.graph_objects as go
import numpy as np

# Define a 2D function
def f(x, y):
    return x**2 + y**2

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Create an interactive 3D surface plot
fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, colorscale='viridis')])
fig.update_layout(
    title='Interactive 3D Surface Plot: f(x, y) = x² + y²',
    scene=dict(
        xaxis_title='x',
        yaxis_title='y',
        zaxis_title='f(x, y)'
    ),
    width=800,
    height=800
)
fig.show()
```

## Summary

In this section, we've explored various techniques for visualizing functions:

1. **1D Function Visualization**: Plotting function values against a single input variable
2. **Derivative Visualization**: Understanding how functions change by visualizing their derivatives
3. **3D Surface Plots**: Visualizing functions of two variables in three dimensions
4. **Contour Plots**: Providing a 2D view of functions of two variables
5. **Complex Functions**: Visualizing challenging optimization landscapes like the Rosenbrock, Himmelblau, and Rastrigin functions
6. **Gradient Visualization**: Understanding the direction of steepest ascent at each point
7. **Interactive Visualizations**: Creating more engaging and explorable visualizations

These visualization techniques are essential tools for understanding optimization problems and the behavior of optimization algorithms. In the next section, we'll explore optimization landscapes in more detail, focusing on features like local minima, saddle points, and plateaus that can challenge optimization algorithms.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
3. Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of a function. The Computer Journal, 3(3), 175-184.
4. Himmelblau, D. M. (1972). Applied Nonlinear Programming. McGraw-Hill.
5. Rastrigin, L. A. (1974). Systems of extremal control. Mir Publishers.
