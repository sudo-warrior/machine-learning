# 2.2.5.3.2.1.1 Batch Gradient Descent

## Introduction to Batch Gradient Descent

Batch Gradient Descent (BGD), also known as Vanilla Gradient Descent, is the most straightforward implementation of gradient descent. It computes the gradient of the cost function with respect to the parameters using the entire training dataset in each iteration.

The update rule for batch gradient descent is:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t)$$

where:
- $\theta_t$ is the current parameter value
- $\theta_{t+1}$ is the updated parameter value
- $\alpha$ is the learning rate
- $\nabla_\theta J(\theta_t)$ is the gradient of the cost function $J$ with respect to $\theta$ at the point $\theta_t$, computed over the entire dataset

## Visualizing Batch Gradient Descent

Let's visualize how batch gradient descent works on a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Compute gradient for batch gradient descent
def compute_gradient(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    gradient = (1/m) * X.T.dot(predictions - y)
    return gradient

# Implement batch gradient descent
def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        gradient = compute_gradient(X, y, theta)
        theta = theta - learning_rate * gradient
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        
        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run batch gradient descent
theta_final, theta_history, cost_history = batch_gradient_descent(X_b, y, learning_rate=0.1, n_iterations=100)

print(f"Final parameters: theta_0 = {theta_final[0][0]}, theta_1 = {theta_final[1][0]}")
print(f"Final cost: {cost_history[-1]}")
```

### Visualizing the Cost Function

Let's visualize how the cost function changes during the optimization process:

```python
# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history, 'b-')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration in Batch Gradient Descent')
plt.grid(True)
plt.show()
```

### Visualizing the Regression Line

We can visualize how the regression line evolves during the optimization process:

```python
# Create a function to plot the regression line
def plot_regression_line(X, y, theta, iteration, cost):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 1], y, alpha=0.7)
    
    # Plot the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    y_pred = x_range_b.dot(theta)
    
    plt.plot(x_range, y_pred, 'r-', linewidth=2, 
             label=f'y = {theta[0][0]:.4f} + {theta[1][0]:.4f}x')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Batch Gradient Descent - Iteration {iteration}, Cost: {cost:.4f}')
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the initial, middle, and final regression lines
plot_regression_line(X_b, y, theta_history[0], 0, compute_cost(X_b, y, theta_history[0]))
plot_regression_line(X_b, y, theta_history[50], 50, compute_cost(X_b, y, theta_history[50]))
plot_regression_line(X_b, y, theta_history[-1], 100, compute_cost(X_b, y, theta_history[-1]))
```

### Animating the Optimization Process

We can create an animation to visualize how the regression line evolves during the optimization process:

```python
# Create an animation of the regression line
def create_regression_animation(X, y, theta_history, cost_history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Set up the scatter plot
    ax1.scatter(X[:, 1], y, alpha=0.7)
    ax1.set_xlabel('X')
    ax1.set_ylabel('y')
    ax1.set_title('Batch Gradient Descent')
    ax1.grid(True)
    
    # Initialize the regression line
    line, = ax1.plot([], [], 'r-', linewidth=2)
    
    # Set up the cost plot
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Cost (MSE)')
    ax2.set_title('Cost vs. Iteration')
    ax2.grid(True)
    
    # Initialize the cost line
    cost_line, = ax2.plot([], [], 'b-')
    
    # Set the x-range for the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    
    # Animation initialization function
    def init():
        ax2.set_xlim(0, len(cost_history))
        ax2.set_ylim(0, cost_history[0] * 1.1)
        line.set_data([], [])
        cost_line.set_data([], [])
        return line, cost_line
    
    # Animation update function
    def update(frame):
        # Update regression line
        theta = theta_history[frame]
        y_pred = x_range_b.dot(theta)
        line.set_data(x_range, y_pred)
        
        # Update cost plot
        cost_line.set_data(range(frame+1), cost_history[:frame+1])
        
        # Update title
        ax1.set_title(f'Batch Gradient Descent - Iteration {frame}, Cost: {cost_history[frame]:.4f}')
        
        return line, cost_line
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(len(theta_history)),
                        init_func=init, blit=True, interval=100)
    
    plt.tight_layout()
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_regression_animation(X_b, y, theta_history, cost_history)
# To save the animation: ani.save('batch_gradient_descent.gif', writer='pillow', fps=10)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing Batch Gradient Descent in 2D Parameter Space

We can also visualize the optimization path in the parameter space (theta_0, theta_1):

```python
# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path
plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')
plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6, label='Final Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Batch Gradient Descent in Parameter Space')
plt.grid(True)
plt.legend()
plt.show()
```

## Advantages and Disadvantages of Batch Gradient Descent

### Advantages

1. **Stable Convergence**: Since we use the entire dataset to compute the gradient, the updates are less noisy and the convergence path is smoother.
2. **Guaranteed Convergence**: For convex problems with an appropriate learning rate, batch gradient descent is guaranteed to converge to the global minimum.
3. **Parallelization**: The gradient computation can be parallelized across multiple cores or machines, which can speed up the computation for large datasets.

### Disadvantages

1. **Computational Inefficiency**: For large datasets, computing the gradient over the entire dataset can be computationally expensive and slow.
2. **Memory Requirements**: The entire dataset needs to be loaded into memory, which can be a limitation for very large datasets.
3. **Redundancy**: In many cases, there is redundancy in the data, and we don't need to process the entire dataset to make progress.
4. **Local Minima**: For non-convex problems, batch gradient descent can get stuck in local minima.

## Batch Gradient Descent on Different Types of Functions

Let's visualize how batch gradient descent behaves on different types of functions:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Implement batch gradient descent
def batch_gd_2d(start_x, start_y, grad_func, learning_rate=0.1, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        grad = grad_func(x, y)
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run batch gradient descent
path = batch_gd_2d(4, 4, grad_convex, learning_rate=0.1, n_iterations=50)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='Batch GD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Batch Gradient Descent on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run batch gradient descent from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths = [batch_gd_2d(x0, y0, grad_non_convex, learning_rate=0.1, n_iterations=50) for x0, y0 in start_points]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Batch Gradient Descent on a Non-convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run batch gradient descent
path = batch_gd_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, n_iterations=50)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='Batch GD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Batch Gradient Descent on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Learning Rate on Batch Gradient Descent

The learning rate is a crucial hyperparameter in batch gradient descent. Let's visualize how different learning rates affect the optimization process:

```python
# Define a simple function
def simple_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_simple(x, y):
    return np.array([2*x, 2*y])

# Implement batch gradient descent with different learning rates
def batch_gd_multi_lr(start_x, start_y, grad_func, learning_rates, n_iterations=50):
    paths = []
    
    for lr in learning_rates:
        path = [(start_x, start_y)]
        x, y = start_x, start_y
        
        for _ in range(n_iterations):
            grad = grad_func(x, y)
            x = x - lr * grad[0]
            y = y - lr * grad[1]
            path.append((x, y))
        
        paths.append(np.array(path))
    
    return paths

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = simple_function(X, Y)

# Run batch gradient descent with different learning rates
learning_rates = [0.01, 0.05, 0.1, 0.5]
paths = batch_gd_multi_lr(4, 4, grad_simple, learning_rates, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'o-', markersize=3, label=f'Learning Rate: {learning_rates[i]}')
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Effect of Learning Rate on Batch Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()
```

## Summary

In this section, we've explored batch gradient descent and visualized its behavior on various functions:

1. **Basic Concept**: Batch gradient descent computes the gradient using the entire dataset in each iteration.
2. **Visualization Techniques**: We've visualized the cost function, regression line, optimization path in parameter space, and the effect of different learning rates.
3. **Behavior on Different Functions**: We've seen how batch gradient descent behaves on convex functions, non-convex functions, and functions with ravines.
4. **Advantages and Disadvantages**: We've discussed the pros and cons of batch gradient descent, including its stability, convergence guarantees, computational efficiency, and memory requirements.

Batch gradient descent is a fundamental optimization algorithm that forms the basis for more advanced techniques. In the next section, we'll explore stochastic gradient descent, which addresses some of the limitations of batch gradient descent.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
4. Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
