# 2.2.5.3.2.1.3 Mini-Batch Gradient Descent

## Introduction to Mini-Batch Gradient Descent

Mini-Batch Gradient Descent (MBGD) is a compromise between batch gradient descent and stochastic gradient descent. It computes the gradient using a small random subset (mini-batch) of the training data in each iteration. This approach combines the advantages of both batch and stochastic gradient descent.

The update rule for mini-batch gradient descent is:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J_{B}(\theta_t)$$

where:
- $\theta_t$ is the current parameter value
- $\theta_{t+1}$ is the updated parameter value
- $\alpha$ is the learning rate
- $\nabla_\theta J_{B}(\theta_t)$ is the gradient of the cost function $J$ with respect to $\theta$ at the point $\theta_t$, computed over a mini-batch $B$ of training examples

## Visualizing Mini-Batch Gradient Descent

Let's visualize how mini-batch gradient descent works on a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Compute gradient for a mini-batch
def compute_gradient_mini_batch(X_batch, y_batch, theta):
    m_batch = len(y_batch)
    predictions = X_batch.dot(theta)
    gradient = (1/m_batch) * X_batch.T.dot(predictions - y_batch)
    return gradient

# Implement mini-batch gradient descent
def mini_batch_gradient_descent(X, y, batch_size=10, learning_rate=0.01, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Shuffle the training data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # Process mini-batches
        for i in range(0, m, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            gradient = compute_gradient_mini_batch(X_batch, y_batch, theta)
            theta = theta - learning_rate * gradient
            
            # Store theta
            theta_history.append(theta.copy())
            
            # Compute cost (using the entire dataset for comparison)
            cost = compute_cost(X, y, theta)
            cost_history.append(cost)
        
        # Print progress
        if iteration % 10 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run mini-batch gradient descent
theta_final, theta_history, cost_history = mini_batch_gradient_descent(X_b, y, batch_size=10, learning_rate=0.01, n_iterations=50)

print(f"Final parameters: theta_0 = {theta_final[0][0]}, theta_1 = {theta_final[1][0]}")
print(f"Final cost: {cost_history[-1]}")
```

### Visualizing the Cost Function

Let's visualize how the cost function changes during the optimization process:

```python
# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history, 'b-')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration in Mini-Batch Gradient Descent')
plt.grid(True)
plt.show()
```

### Visualizing the Regression Line

We can visualize how the regression line evolves during the optimization process:

```python
# Create a function to plot the regression line
def plot_regression_line(X, y, theta, iteration, cost):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 1], y, alpha=0.7)
    
    # Plot the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    y_pred = x_range_b.dot(theta)
    
    plt.plot(x_range, y_pred, 'r-', linewidth=2, 
             label=f'y = {theta[0][0]:.4f} + {theta[1][0]:.4f}x')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Mini-Batch Gradient Descent - Iteration {iteration}, Cost: {cost:.4f}')
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the initial, middle, and final regression lines
plot_regression_line(X_b, y, theta_history[0], 0, compute_cost(X_b, y, theta_history[0]))
plot_regression_line(X_b, y, theta_history[len(theta_history)//2], len(theta_history)//2, 
                    compute_cost(X_b, y, theta_history[len(theta_history)//2]))
plot_regression_line(X_b, y, theta_history[-1], len(theta_history)-1, compute_cost(X_b, y, theta_history[-1]))
```

### Animating the Optimization Process

We can create an animation to visualize how the regression line evolves during the optimization process:

```python
# Create an animation of the regression line
def create_regression_animation(X, y, theta_history, cost_history, n_frames=100):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Set up the scatter plot
    ax1.scatter(X[:, 1], y, alpha=0.7)
    ax1.set_xlabel('X')
    ax1.set_ylabel('y')
    ax1.set_title('Mini-Batch Gradient Descent')
    ax1.grid(True)
    
    # Initialize the regression line
    line, = ax1.plot([], [], 'r-', linewidth=2)
    
    # Set up the cost plot
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Cost (MSE)')
    ax2.set_title('Cost vs. Iteration')
    ax2.grid(True)
    
    # Initialize the cost line
    cost_line, = ax2.plot([], [], 'b-')
    
    # Set the x-range for the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    
    # Select frames to display (to avoid too many frames)
    total_frames = len(theta_history)
    frame_indices = np.linspace(0, total_frames-1, n_frames, dtype=int)
    
    # Animation initialization function
    def init():
        ax2.set_xlim(0, len(cost_history))
        ax2.set_ylim(0, cost_history[0] * 1.1)
        line.set_data([], [])
        cost_line.set_data([], [])
        return line, cost_line
    
    # Animation update function
    def update(frame_idx):
        idx = frame_indices[frame_idx]
        # Update regression line
        theta = theta_history[idx]
        y_pred = x_range_b.dot(theta)
        line.set_data(x_range, y_pred)
        
        # Update cost plot
        cost_line.set_data(range(idx+1), cost_history[:idx+1])
        
        # Update title
        ax1.set_title(f'Mini-Batch Gradient Descent - Iteration {idx}, Cost: {cost_history[idx]:.4f}')
        
        return line, cost_line
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(n_frames),
                        init_func=init, blit=True, interval=100)
    
    plt.tight_layout()
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_regression_animation(X_b, y, theta_history, cost_history, n_frames=100)
# To save the animation: ani.save('mini_batch_gradient_descent.gif', writer='pillow', fps=10)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing Mini-Batch Gradient Descent in 2D Parameter Space

We can also visualize the optimization path in the parameter space (theta_0, theta_1):

```python
# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path (sample points to avoid overcrowding)
sample_indices = np.linspace(0, len(theta_history)-1, 100, dtype=int)
sampled_theta_history = theta_history[sample_indices]

plt.plot(sampled_theta_history[:, 0, 0], sampled_theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')
plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6, label='Final Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Mini-Batch Gradient Descent in Parameter Space')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Batch Size on Mini-Batch Gradient Descent

The batch size is a crucial hyperparameter in mini-batch gradient descent. Let's visualize how different batch sizes affect the optimization process:

```python
# Implement mini-batch gradient descent with different batch sizes
def mini_batch_gd_multi_batch_sizes(X, y, batch_sizes, learning_rate=0.01, n_iterations=50):
    results = []
    
    for batch_size in batch_sizes:
        m = len(y)
        theta = np.random.randn(2, 1)  # Random initialization
        cost_history = []
        theta_history = [theta.copy()]
        
        for iteration in range(n_iterations):
            # Shuffle the training data
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            # Process mini-batches
            for i in range(0, m, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                gradient = compute_gradient_mini_batch(X_batch, y_batch, theta)
                theta = theta - learning_rate * gradient
                
                # Store theta
                theta_history.append(theta.copy())
                
                # Compute cost (using the entire dataset for comparison)
                cost = compute_cost(X, y, theta)
                cost_history.append(cost)
        
        results.append((batch_size, np.array(theta_history), np.array(cost_history)))
    
    return results

# Run mini-batch gradient descent with different batch sizes
batch_sizes = [1, 10, 50, 100]  # 1 = SGD, 100 = BGD (for this dataset)
results = mini_batch_gd_multi_batch_sizes(X_b, y, batch_sizes, learning_rate=0.01, n_iterations=5)

# Plot the cost histories
plt.figure(figsize=(12, 6))

for batch_size, theta_history, cost_history in results:
    label = "SGD" if batch_size == 1 else "BGD" if batch_size == 100 else f"MBGD (batch_size={batch_size})"
    plt.plot(cost_history, label=label)

plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration for Different Batch Sizes')
plt.grid(True)
plt.legend()
plt.show()

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

for batch_size, theta_history, cost_history in results:
    # Sample points to avoid overcrowding
    sample_indices = np.linspace(0, len(theta_history)-1, 100, dtype=int)
    sampled_theta_history = theta_history[sample_indices]
    
    label = "SGD" if batch_size == 1 else "BGD" if batch_size == 100 else f"MBGD (batch_size={batch_size})"
    plt.plot(sampled_theta_history[:, 0, 0], sampled_theta_history[:, 1, 0], 'o-', markersize=3, label=label)
    plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'o', markersize=6)  # Initial parameters
    plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'o', markersize=6)  # Final parameters

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Optimization Paths for Different Batch Sizes')
plt.grid(True)
plt.legend()
plt.show()
```

## Mini-Batch Gradient Descent on Different Types of Functions

Let's visualize how mini-batch gradient descent behaves on different types of functions:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Implement mini-batch gradient descent with artificial noise
def mbgd_2d(start_x, start_y, grad_func, learning_rate=0.1, n_iterations=50, noise_level=0.2):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        # Compute true gradient
        grad = grad_func(x, y)
        
        # Add moderate noise to simulate mini-batch behavior
        noise_x = np.random.randn() * noise_level
        noise_y = np.random.randn() * noise_level
        noisy_grad = grad + np.array([noise_x, noise_y])
        
        # Update parameters
        x = x - learning_rate * noisy_grad[0]
        y = y - learning_rate * noisy_grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run mini-batch gradient descent
path = mbgd_2d(4, 4, grad_convex, learning_rate=0.1, n_iterations=50, noise_level=0.2)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='MBGD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Mini-Batch Gradient Descent on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run mini-batch gradient descent from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths = [mbgd_2d(x0, y0, grad_non_convex, learning_rate=0.1, n_iterations=50, noise_level=0.2) for x0, y0 in start_points]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Mini-Batch Gradient Descent on a Non-convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run mini-batch gradient descent
path = mbgd_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, n_iterations=50, noise_level=0.02)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='MBGD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Mini-Batch Gradient Descent on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

## Advantages and Disadvantages of Mini-Batch Gradient Descent

### Advantages

1. **Balance of Efficiency and Stability**: MBGD provides a good balance between the computational efficiency of SGD and the stability of batch gradient descent.
2. **Reduced Variance**: Compared to SGD, MBGD has less variance in the parameter updates, leading to more stable convergence.
3. **Parallelization**: MBGD can leverage matrix operations and parallel computing for the mini-batch, making it more efficient than SGD.
4. **Memory Efficiency**: MBGD only needs to store a mini-batch of examples at a time, making it suitable for large datasets.

### Disadvantages

1. **Batch Size Tuning**: MBGD introduces an additional hyperparameter (batch size) that needs to be tuned.
2. **Still Requires Learning Rate Tuning**: Like other gradient descent variants, MBGD requires careful tuning of the learning rate.
3. **Not as Noisy as SGD**: MBGD may not escape local minima as effectively as SGD due to reduced noise in the updates.
4. **Not as Stable as BGD**: MBGD is not as stable as batch gradient descent, especially for small batch sizes.

## Summary

In this section, we've explored mini-batch gradient descent and visualized its behavior on various functions:

1. **Basic Concept**: Mini-batch gradient descent computes the gradient using a small random subset of the training data in each iteration, providing a compromise between batch and stochastic gradient descent.
2. **Visualization Techniques**: We've visualized the cost function, regression line, optimization path in parameter space, and the effect of different batch sizes.
3. **Batch Size Effect**: We've seen how the batch size affects the optimization process, with smaller batch sizes leading to noisier updates and larger batch sizes leading to more stable but potentially slower convergence.
4. **Behavior on Different Functions**: We've seen how MBGD behaves on convex functions, non-convex functions, and functions with ravines.
5. **Advantages and Disadvantages**: We've discussed the pros and cons of MBGD, including its balance of efficiency and stability, reduced variance compared to SGD, and the need for batch size tuning.

Mini-batch gradient descent is the most commonly used variant of gradient descent in practice, especially for deep learning, as it provides a good balance between the computational efficiency of SGD and the stability of batch gradient descent. In the next section, we'll compare all three variants side by side.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
4. Li, M., Zhang, T., Chen, Y., & Smola, A. J. (2014). Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 661-670).
