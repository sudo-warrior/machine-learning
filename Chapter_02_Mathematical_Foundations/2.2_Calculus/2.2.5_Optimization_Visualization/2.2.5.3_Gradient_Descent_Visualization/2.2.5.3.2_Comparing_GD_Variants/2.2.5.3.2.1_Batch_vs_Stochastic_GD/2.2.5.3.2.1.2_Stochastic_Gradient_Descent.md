# 2.2.5.3.2.1.2 Stochastic Gradient Descent

## Introduction to Stochastic Gradient Descent

Stochastic Gradient Descent (SGD) is a variation of gradient descent that computes the gradient using a single randomly selected training example in each iteration. This approach addresses some of the limitations of batch gradient descent, particularly for large datasets.

The update rule for stochastic gradient descent is:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J_i(\theta_t)$$

where:
- $\theta_t$ is the current parameter value
- $\theta_{t+1}$ is the updated parameter value
- $\alpha$ is the learning rate
- $\nabla_\theta J_i(\theta_t)$ is the gradient of the cost function $J$ with respect to $\theta$ at the point $\theta_t$, computed for a single randomly selected training example $i$

## Visualizing Stochastic Gradient Descent

Let's visualize how stochastic gradient descent works on a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Compute gradient for a single example
def compute_gradient_single(X_i, y_i, theta):
    prediction = X_i.dot(theta)
    gradient = X_i.T.dot(prediction - y_i)
    return gradient

# Implement stochastic gradient descent
def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=50):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    cost_history = []
    theta_history = [theta.copy()]
    
    for epoch in range(n_epochs):
        # Shuffle the training data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(m):
            X_i = X_shuffled[i:i+1]
            y_i = y_shuffled[i:i+1]
            
            gradient = compute_gradient_single(X_i, y_i, theta)
            theta = theta - learning_rate * gradient
            
            # Store theta
            theta_history.append(theta.copy())
            
            # Compute cost (using the entire dataset for comparison)
            cost = compute_cost(X, y, theta)
            cost_history.append(cost)
        
        # Print progress
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run stochastic gradient descent
theta_final, theta_history, cost_history = stochastic_gradient_descent(X_b, y, learning_rate=0.01, n_epochs=5)

print(f"Final parameters: theta_0 = {theta_final[0][0]}, theta_1 = {theta_final[1][0]}")
print(f"Final cost: {cost_history[-1]}")
```

### Visualizing the Cost Function

Let's visualize how the cost function changes during the optimization process:

```python
# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history, 'b-')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration in Stochastic Gradient Descent')
plt.grid(True)
plt.show()
```

### Visualizing the Regression Line

We can visualize how the regression line evolves during the optimization process:

```python
# Create a function to plot the regression line
def plot_regression_line(X, y, theta, iteration, cost):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 1], y, alpha=0.7)
    
    # Plot the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    y_pred = x_range_b.dot(theta)
    
    plt.plot(x_range, y_pred, 'r-', linewidth=2, 
             label=f'y = {theta[0][0]:.4f} + {theta[1][0]:.4f}x')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Stochastic Gradient Descent - Iteration {iteration}, Cost: {cost:.4f}')
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the initial, middle, and final regression lines
plot_regression_line(X_b, y, theta_history[0], 0, compute_cost(X_b, y, theta_history[0]))
plot_regression_line(X_b, y, theta_history[len(theta_history)//2], len(theta_history)//2, 
                    compute_cost(X_b, y, theta_history[len(theta_history)//2]))
plot_regression_line(X_b, y, theta_history[-1], len(theta_history)-1, compute_cost(X_b, y, theta_history[-1]))
```

### Animating the Optimization Process

We can create an animation to visualize how the regression line evolves during the optimization process:

```python
# Create an animation of the regression line
def create_regression_animation(X, y, theta_history, cost_history, n_frames=100):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Set up the scatter plot
    ax1.scatter(X[:, 1], y, alpha=0.7)
    ax1.set_xlabel('X')
    ax1.set_ylabel('y')
    ax1.set_title('Stochastic Gradient Descent')
    ax1.grid(True)
    
    # Initialize the regression line
    line, = ax1.plot([], [], 'r-', linewidth=2)
    
    # Set up the cost plot
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Cost (MSE)')
    ax2.set_title('Cost vs. Iteration')
    ax2.grid(True)
    
    # Initialize the cost line
    cost_line, = ax2.plot([], [], 'b-')
    
    # Set the x-range for the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    
    # Select frames to display (to avoid too many frames)
    total_frames = len(theta_history)
    frame_indices = np.linspace(0, total_frames-1, n_frames, dtype=int)
    
    # Animation initialization function
    def init():
        ax2.set_xlim(0, len(cost_history))
        ax2.set_ylim(0, cost_history[0] * 1.1)
        line.set_data([], [])
        cost_line.set_data([], [])
        return line, cost_line
    
    # Animation update function
    def update(frame_idx):
        idx = frame_indices[frame_idx]
        # Update regression line
        theta = theta_history[idx]
        y_pred = x_range_b.dot(theta)
        line.set_data(x_range, y_pred)
        
        # Update cost plot
        cost_line.set_data(range(idx+1), cost_history[:idx+1])
        
        # Update title
        ax1.set_title(f'Stochastic Gradient Descent - Iteration {idx}, Cost: {cost_history[idx]:.4f}')
        
        return line, cost_line
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(n_frames),
                        init_func=init, blit=True, interval=100)
    
    plt.tight_layout()
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_regression_animation(X_b, y, theta_history, cost_history, n_frames=100)
# To save the animation: ani.save('stochastic_gradient_descent.gif', writer='pillow', fps=10)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing Stochastic Gradient Descent in 2D Parameter Space

We can also visualize the optimization path in the parameter space (theta_0, theta_1):

```python
# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path (sample points to avoid overcrowding)
sample_indices = np.linspace(0, len(theta_history)-1, 100, dtype=int)
sampled_theta_history = theta_history[sample_indices]

plt.plot(sampled_theta_history[:, 0, 0], sampled_theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')
plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6, label='Final Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Stochastic Gradient Descent in Parameter Space')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing the Stochastic Nature of SGD

One of the key characteristics of stochastic gradient descent is its noisy updates. Let's visualize this by comparing the paths of multiple runs of SGD:

```python
# Run stochastic gradient descent multiple times
n_runs = 5
all_theta_history = []

for run in range(n_runs):
    np.random.seed(run)  # Different seed for each run
    theta, theta_history, _ = stochastic_gradient_descent(X_b, y, learning_rate=0.01, n_epochs=5)
    
    # Sample points to avoid overcrowding
    sample_indices = np.linspace(0, len(theta_history)-1, 100, dtype=int)
    sampled_theta_history = theta_history[sample_indices]
    
    all_theta_history.append(sampled_theta_history)

# Create a contour plot with multiple optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization paths
for i, theta_history in enumerate(all_theta_history):
    plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'o-', markersize=3, label=f'Run {i+1}')
    plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6)  # Initial parameters
    plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6)  # Final parameters

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Multiple Runs of Stochastic Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()
```

## Stochastic Gradient Descent on Different Types of Functions

Let's visualize how stochastic gradient descent behaves on different types of functions:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Implement stochastic gradient descent with artificial noise
def sgd_2d(start_x, start_y, grad_func, learning_rate=0.1, n_iterations=50, noise_level=0.5):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        # Compute true gradient
        grad = grad_func(x, y)
        
        # Add noise to simulate stochasticity
        noise_x = np.random.randn() * noise_level
        noise_y = np.random.randn() * noise_level
        noisy_grad = grad + np.array([noise_x, noise_y])
        
        # Update parameters
        x = x - learning_rate * noisy_grad[0]
        y = y - learning_rate * noisy_grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run stochastic gradient descent
path = sgd_2d(4, 4, grad_convex, learning_rate=0.1, n_iterations=50, noise_level=0.5)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='SGD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Stochastic Gradient Descent on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run stochastic gradient descent from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths = [sgd_2d(x0, y0, grad_non_convex, learning_rate=0.1, n_iterations=50, noise_level=0.5) for x0, y0 in start_points]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Stochastic Gradient Descent on a Non-convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run stochastic gradient descent
path = sgd_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, n_iterations=50, noise_level=0.05)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='SGD Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Stochastic Gradient Descent on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Learning Rate and Noise Level on Stochastic Gradient Descent

The learning rate and the level of noise (stochasticity) are crucial parameters in stochastic gradient descent. Let's visualize how they affect the optimization process:

```python
# Define a simple function
def simple_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_simple(x, y):
    return np.array([2*x, 2*y])

# Implement stochastic gradient descent with different learning rates and noise levels
def sgd_multi_params(start_x, start_y, grad_func, learning_rates, noise_levels, n_iterations=50):
    paths = []
    
    for lr in learning_rates:
        for noise in noise_levels:
            path = [(start_x, start_y)]
            x, y = start_x, start_y
            
            for _ in range(n_iterations):
                # Compute true gradient
                grad = grad_func(x, y)
                
                # Add noise to simulate stochasticity
                noise_x = np.random.randn() * noise
                noise_y = np.random.randn() * noise
                noisy_grad = grad + np.array([noise_x, noise_y])
                
                # Update parameters
                x = x - lr * noisy_grad[0]
                y = y - lr * noisy_grad[1]
                path.append((x, y))
            
            paths.append((np.array(path), lr, noise))
    
    return paths

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = simple_function(X, Y)

# Run stochastic gradient descent with different parameters
learning_rates = [0.01, 0.1]
noise_levels = [0.1, 1.0]
paths = sgd_multi_params(4, 4, grad_simple, learning_rates, noise_levels, n_iterations=50)

# Create a 2x2 grid of contour plots
fig, axes = plt.subplots(len(learning_rates), len(noise_levels), figsize=(15, 12))

for i, lr in enumerate(learning_rates):
    for j, noise in enumerate(noise_levels):
        # Find the corresponding path
        for path_data in paths:
            if path_data[1] == lr and path_data[2] == noise:
                path = path_data[0]
                break
        
        # Create a contour plot
        ax = axes[i, j]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        
        # Plot the optimization path
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3)
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'Learning Rate: {lr}, Noise Level: {noise}')
        ax.grid(True)

plt.tight_layout()
plt.show()
```

## Advantages and Disadvantages of Stochastic Gradient Descent

### Advantages

1. **Computational Efficiency**: SGD processes one example at a time, making it much faster per iteration than batch gradient descent, especially for large datasets.
2. **Memory Efficiency**: SGD only needs to store one example at a time, making it suitable for datasets that don't fit in memory.
3. **Online Learning**: SGD can learn from new data on-the-fly, making it suitable for online learning scenarios.
4. **Escaping Local Minima**: The noise in SGD updates can help the algorithm escape local minima in non-convex optimization problems.

### Disadvantages

1. **Noisy Updates**: The updates in SGD are noisy, which can cause the optimization path to oscillate and potentially slow down convergence.
2. **Requires Tuning**: SGD often requires careful tuning of the learning rate and potentially a learning rate schedule.
3. **No Parallelization**: Unlike batch gradient descent, SGD processes examples sequentially, which limits the use of parallel computing.
4. **Convergence**: SGD doesn't converge as nicely as batch gradient descent; it tends to oscillate around the minimum rather than reaching it precisely.

## Summary

In this section, we've explored stochastic gradient descent and visualized its behavior on various functions:

1. **Basic Concept**: Stochastic gradient descent computes the gradient using a single randomly selected training example in each iteration.
2. **Visualization Techniques**: We've visualized the cost function, regression line, optimization path in parameter space, and the effect of different learning rates and noise levels.
3. **Stochastic Nature**: We've seen how the stochastic nature of SGD leads to noisy updates and a more erratic optimization path compared to batch gradient descent.
4. **Behavior on Different Functions**: We've seen how SGD behaves on convex functions, non-convex functions, and functions with ravines.
5. **Advantages and Disadvantages**: We've discussed the pros and cons of SGD, including its computational efficiency, memory efficiency, and ability to escape local minima, as well as its noisy updates and convergence properties.

Stochastic gradient descent is a powerful optimization algorithm that addresses some of the limitations of batch gradient descent, particularly for large datasets. In the next section, we'll explore mini-batch gradient descent, which combines the best of both worlds.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.
4. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
