# 2.2.5.3.2.2.2.3 NAG Visualization

## Visualizing Nesterov Accelerated Gradient

In this section, we'll visualize the behavior of Nesterov Accelerated Gradient (NAG) on various functions. Visualization helps us understand how NAG navigates the optimization landscape and how it compares to other optimization algorithms.

## Visualizing NAG on a Simple Linear Regression Problem

Let's start by visualizing NAG on a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Implement Nesterov Accelerated Gradient
def nesterov_accelerated_gradient(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    velocity_history = [velocity.copy()]
    
    for iteration in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_theta = theta + momentum * velocity
        
        # Compute gradient at the look-ahead position
        gradient = (1/m) * X.T.dot(X.dot(look_ahead_theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost, theta, and velocity
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        velocity_history.append(velocity.copy())
    
    return theta, np.array(theta_history), np.array(velocity_history), np.array(cost_history)

# Run Nesterov Accelerated Gradient
theta_final, theta_history, velocity_history, cost_history = nesterov_accelerated_gradient(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)
```

### Visualizing the Cost Function

Let's visualize how the cost function changes during the optimization process:

```python
# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history, 'b-')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration in Nesterov Accelerated Gradient')
plt.grid(True)
plt.show()
```

### Visualizing the Regression Line

We can visualize how the regression line evolves during the optimization process:

```python
# Create a function to plot the regression line
def plot_regression_line(X, y, theta, iteration, cost):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 1], y, alpha=0.7)
    
    # Plot the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    y_pred = x_range_b.dot(theta)
    
    plt.plot(x_range, y_pred, 'r-', linewidth=2, 
             label=f'y = {theta[0][0]:.4f} + {theta[1][0]:.4f}x')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Nesterov Accelerated Gradient - Iteration {iteration}, Cost: {cost:.4f}')
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the initial, middle, and final regression lines
plot_regression_line(X_b, y, theta_history[0], 0, compute_cost(X_b, y, theta_history[0]))
plot_regression_line(X_b, y, theta_history[50], 50, compute_cost(X_b, y, theta_history[50]))
plot_regression_line(X_b, y, theta_history[-1], 100, compute_cost(X_b, y, theta_history[-1]))
```

### Visualizing in Parameter Space

We can visualize the optimization path in the parameter space (theta_0, theta_1):

```python
# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path
plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')
plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6, label='Final Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Nesterov Accelerated Gradient in Parameter Space')
plt.grid(True)
plt.legend()
plt.show()
```

### Visualizing Look-ahead Steps

One of the key features of NAG is the look-ahead step. Let's visualize this:

```python
# Create a contour plot with look-ahead steps
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path
plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')

# Plot look-ahead steps for selected iterations
for i in range(0, len(theta_history), 10):
    if i < len(theta_history) - 1:  # Skip the last point
        # Current position
        current_pos = (theta_history[i, 0, 0], theta_history[i, 1, 0])
        
        # Look-ahead position
        look_ahead_pos = (theta_history[i, 0, 0] + momentum * velocity_history[i, 0, 0],
                          theta_history[i, 1, 0] + momentum * velocity_history[i, 1, 0])
        
        # Plot the look-ahead step
        plt.arrow(current_pos[0], current_pos[1],
                  look_ahead_pos[0] - current_pos[0], look_ahead_pos[1] - current_pos[1],
                  head_width=0.1, head_length=0.1, fc='blue', ec='blue', alpha=0.5)

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Nesterov Accelerated Gradient - Look-ahead Steps')
plt.grid(True)
plt.legend()
plt.show()
```

### Animating the Optimization Process

We can create an animation to visualize how the optimization process evolves over time:

```python
# Create an animation of the optimization process
def create_nag_animation(X, y, theta_history, velocity_history, cost_history, momentum=0.9):
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create a contour plot
    contour = ax.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='Cost (MSE)')
    
    # Initialize the point and path
    point, = ax.plot([], [], 'ro', markersize=6)
    path, = ax.plot([], [], 'r-', linewidth=2)
    
    # Initialize the look-ahead point and arrow
    look_ahead_point, = ax.plot([], [], 'bo', markersize=6)
    look_ahead_arrow = ax.arrow(0, 0, 0, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')
    
    # Set up the plot
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    ax.set_title('Nesterov Accelerated Gradient')
    ax.grid(True)
    
    # Animation initialization function
    def init():
        point.set_data([], [])
        path.set_data([], [])
        look_ahead_point.set_data([], [])
        look_ahead_arrow.set_visible(False)
        return point, path, look_ahead_point, look_ahead_arrow
    
    # Animation update function
    def update(frame):
        # Update point and path
        point.set_data([theta_history[frame, 0, 0]], [theta_history[frame, 1, 0]])
        path.set_data(theta_history[:frame+1, 0, 0], theta_history[:frame+1, 1, 0])
        
        # Compute look-ahead position
        if frame < len(theta_history) - 1:  # Skip the last point
            look_ahead_x = theta_history[frame, 0, 0] + momentum * velocity_history[frame, 0, 0]
            look_ahead_y = theta_history[frame, 1, 0] + momentum * velocity_history[frame, 1, 0]
            look_ahead_point.set_data([look_ahead_x], [look_ahead_y])
            
            # Remove old arrow and create a new one
            look_ahead_arrow.remove()
            look_ahead_arrow = ax.arrow(theta_history[frame, 0, 0], theta_history[frame, 1, 0],
                                      look_ahead_x - theta_history[frame, 0, 0],
                                      look_ahead_y - theta_history[frame, 1, 0],
                                      head_width=0.1, head_length=0.1, fc='blue', ec='blue', alpha=0.5)
        else:
            look_ahead_point.set_data([], [])
            look_ahead_arrow.set_visible(False)
        
        # Update title
        ax.set_title(f'Nesterov Accelerated Gradient - Iteration {frame}, Cost: {cost_history[frame]:.4f}')
        
        return point, path, look_ahead_point, look_ahead_arrow
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(len(theta_history)),
                        init_func=init, blit=True, interval=100)
    
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_nag_animation(X_b, y, theta_history, velocity_history, cost_history, momentum=0.9)
# To save the animation: ani.save('nesterov_accelerated_gradient.gif', writer='pillow', fps=10)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing NAG on Different Functions

Let's visualize how NAG behaves on different types of functions:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Implement NAG for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run NAG
path = nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run NAG
path = nag_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run NAG from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths = [nag_2d(x0, y0, grad_non_convex, learning_rate=0.1, momentum=0.9, n_iterations=50) for x0, y0 in start_points]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient on a Non-convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing the Effect of Momentum Parameter

The momentum parameter $\gamma$ controls how much of the past velocity is preserved. Let's visualize the effect of different momentum values:

```python
# Run NAG with different momentum values
momentum_values = [0.0, 0.5, 0.9, 0.99]
paths = [nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=m, n_iterations=50) for m in momentum_values]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    label = "Standard GD" if momentum_values[i] == 0.0 else f"Momentum = {momentum_values[i]}"
    plt.plot(path[:, 0], path[:, 1], 'o-', markersize=3, label=label)
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'o', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Effect of Momentum Parameter on Nesterov Accelerated Gradient')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing the Effect of Learning Rate

The learning rate $\alpha$ controls the step size. Let's visualize the effect of different learning rates:

```python
# Run NAG with different learning rates
learning_rates = [0.01, 0.05, 0.1, 0.5]
paths = [nag_2d(4, 4, grad_convex, learning_rate=lr, momentum=0.9, n_iterations=50) for lr in learning_rates]

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'o-', markersize=3, label=f"Learning Rate = {learning_rates[i]}")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'o', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Effect of Learning Rate on Nesterov Accelerated Gradient')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing NAG vs. Standard Momentum

Let's compare NAG with standard momentum to see the benefits of the "look-ahead" approach:

```python
# Implement standard momentum for 2D functions
def momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Run standard momentum and NAG
path_momentum = momentum_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)
path_nag = nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'b-o', markersize=3, label='Standard Momentum')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='Nesterov Accelerated Gradient')

plt.plot(path_momentum[0, 0], path_momentum[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_momentum[-1, 0], path_momentum[-1, 1], 'bo', markersize=6, label='Momentum Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard Momentum vs. Nesterov Accelerated Gradient')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing NAG on the Rosenbrock Function

The Rosenbrock function is a classic test function for optimization algorithms:

```python
# Define the Rosenbrock function
def rosenbrock(x, y, a=1, b=100):
    return (a - x)**2 + b * (y - x**2)**2

# Define the gradient of the Rosenbrock function
def grad_rosenbrock(x, y, a=1, b=100):
    df_dx = -2*(a - x) - 4*b*x*(y - x**2)
    df_dy = 2*b*(y - x**2)
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

# Run NAG
path = nag_2d(0, 0, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=1000)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')  # Log scale for better visualization
plt.colorbar(label='log(f(x, y) + 1) value')

plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG Path')
plt.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')

# Mark the global minimum
plt.plot(1, 1, 'ko', markersize=8, label='Global Minimum (1, 1)')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient on the Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing NAG in 3D

We can also create 3D visualizations to better understand how NAG navigates the optimization landscape:

```python
from mpl_toolkits.mplot3d import Axes3D

# Create a 3D visualization of NAG on a convex function
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

# Create a surface plot
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')

# Compute the z-values for the path
path_z = [convex_function(x, y) for x, y in path]

# Plot the optimization path
ax.plot(path[:, 0], path[:, 1], path_z, 'r-o', markersize=3, label='NAG Path')
ax.plot([path[0, 0]], [path[0, 1]], [path_z[0]], 'go', markersize=6, label='Initial Point')
ax.plot([path[-1, 0]], [path[-1, 1]], [path_z[-1]], 'bo', markersize=6, label='Final Point')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('Nesterov Accelerated Gradient on a Convex Function')
plt.legend()
plt.show()
```

## Summary

In this section, we've visualized the behavior of Nesterov Accelerated Gradient on various functions:

1. **Linear Regression**: We visualized NAG on a simple linear regression problem, showing the cost function, regression line, and optimization path in parameter space.

2. **Look-ahead Steps**: We visualized the key feature of NAG, the look-ahead step, which allows it to be more responsive to changes in the gradient.

3. **Different Functions**: We visualized NAG on convex functions, non-convex functions, functions with ravines, and the Rosenbrock function.

4. **Effect of Parameters**: We visualized the effect of different momentum values and learning rates on the optimization path.

5. **Comparison with Standard Momentum**: We compared NAG with standard momentum, showing how the "look-ahead" approach leads to faster convergence and reduced oscillation.

6. **3D Visualization**: We created 3D visualizations to better understand how NAG navigates the optimization landscape.

These visualizations help us understand why NAG is often more effective than standard gradient descent or momentum, particularly for complex optimization landscapes.

In the next section, we'll compare NAG with other optimization algorithms to understand its relative strengths and weaknesses.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
