# 2.2.5.3.2.2.2.2 NAG Implementation

## Implementing Nesterov Accelerated Gradient

In this section, we'll explore how to implement Nesterov Accelerated Gradient (NAG) in Python. We'll start with a basic implementation for a simple linear regression problem and then extend it to more complex scenarios.

## Basic Implementation for Linear Regression

Let's implement NAG for a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Implement Nesterov Accelerated Gradient
def nesterov_accelerated_gradient(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    velocity_history = [velocity.copy()]
    
    for iteration in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_theta = theta + momentum * velocity
        
        # Compute gradient at the look-ahead position
        gradient = (1/m) * X.T.dot(X.dot(look_ahead_theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost, theta, and velocity
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        velocity_history.append(velocity.copy())
        
        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(velocity_history), np.array(cost_history)

# Run Nesterov Accelerated Gradient
theta_final, theta_history, velocity_history, cost_history = nesterov_accelerated_gradient(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

print(f"Final parameters: theta_0 = {theta_final[0][0]}, theta_1 = {theta_final[1][0]}")
print(f"Final cost: {cost_history[-1]}")
```

## Alternative Implementation

As mentioned in the previous section, there's an alternative formulation of NAG that is often used in practice. Let's implement this formulation:

```python
# Implement Nesterov Accelerated Gradient (alternative formulation)
def nesterov_accelerated_gradient_alt(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    theta_prev = theta.copy()  # Previous theta
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_theta = theta + momentum * (theta - theta_prev)
        
        # Compute gradient at the look-ahead position
        gradient = (1/m) * X.T.dot(X.dot(look_ahead_theta) - y)
        
        # Store previous theta
        theta_prev = theta.copy()
        
        # Update parameters
        theta = look_ahead_theta - learning_rate * gradient
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        
        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run Nesterov Accelerated Gradient (alternative formulation)
theta_final_alt, theta_history_alt, cost_history_alt = nesterov_accelerated_gradient_alt(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

print(f"Final parameters (alt): theta_0 = {theta_final_alt[0][0]}, theta_1 = {theta_final_alt[1][0]}")
print(f"Final cost (alt): {cost_history_alt[-1]}")
```

## Implementation for 2D Functions

Let's implement NAG for 2D functions, which will be useful for visualization:

```python
# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)
```

## Implementation for Mini-batch Gradient Descent with NAG

In practice, NAG is often used with mini-batch gradient descent. Let's implement this combination:

```python
# Implement mini-batch gradient descent with Nesterov Accelerated Gradient
def mini_batch_nag(X, y, batch_size=10, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Shuffle the training data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # Process mini-batches
        for i in range(0, m, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # Compute the look-ahead position
            look_ahead_theta = theta + momentum * velocity
            
            # Compute gradient at the look-ahead position
            gradient = (1/len(X_batch)) * X_batch.T.dot(X_batch.dot(look_ahead_theta) - y_batch)
            
            # Update velocity
            velocity = momentum * velocity - learning_rate * gradient
            
            # Update parameters
            theta = theta + velocity
            
            # Store cost and theta
            cost = compute_cost(X, y, theta)
            cost_history.append(cost)
            theta_history.append(theta.copy())
    
    return theta, np.array(theta_history), np.array(cost_history)
```

## Implementation in TensorFlow/Keras

NAG is also available in popular deep learning frameworks like TensorFlow/Keras:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

# Create a simple model
model = Sequential([
    Dense(1, input_shape=(1,), activation='linear')
])

# Compile the model with NAG optimizer
optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
model.compile(optimizer=optimizer, loss='mse')

# Train the model
history = model.fit(X, y, epochs=100, verbose=0)

# Get the trained parameters
weights = model.get_weights()
print(f"TensorFlow parameters: w = {weights[0][0][0]}, b = {weights[1][0]}")
```

## Implementation in PyTorch

Similarly, NAG is available in PyTorch:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Convert data to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Create a simple model
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)
    
    def forward(self, x):
        return self.linear(x)

model = LinearRegression()

# Define loss function and optimizer with NAG
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)

# Train the model
for epoch in range(100):
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    
    # Backward pass and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Get the trained parameters
w = model.linear.weight.item()
b = model.linear.bias.item()
print(f"PyTorch parameters: w = {w}, b = {b}")
```

## Implementation for Neural Networks

Let's implement NAG for a simple neural network:

```python
# Implement a simple neural network with NAG
def neural_network_with_nag(X, y, hidden_size=10, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    # Initialize parameters
    input_size = X.shape[1]
    output_size = 1
    
    # Initialize weights and biases
    np.random.seed(42)
    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros((1, output_size))
    
    # Initialize velocities
    vW1 = np.zeros_like(W1)
    vb1 = np.zeros_like(b1)
    vW2 = np.zeros_like(W2)
    vb2 = np.zeros_like(b2)
    
    # Store costs
    costs = []
    
    # Training loop
    for i in range(n_iterations):
        # Forward pass
        # Compute look-ahead parameters
        W1_look_ahead = W1 + momentum * vW1
        b1_look_ahead = b1 + momentum * vb1
        W2_look_ahead = W2 + momentum * vW2
        b2_look_ahead = b2 + momentum * vb2
        
        # Forward pass with look-ahead parameters
        Z1 = np.dot(X, W1_look_ahead) + b1_look_ahead
        A1 = np.tanh(Z1)
        Z2 = np.dot(A1, W2_look_ahead) + b2_look_ahead
        A2 = Z2  # Linear activation for regression
        
        # Compute cost
        cost = np.mean((A2 - y) ** 2) / 2
        costs.append(cost)
        
        # Backward pass
        dZ2 = A2 - y
        dW2 = np.dot(A1.T, dZ2) / X.shape[0]
        db2 = np.sum(dZ2, axis=0, keepdims=True) / X.shape[0]
        dZ1 = np.dot(dZ2, W2_look_ahead.T) * (1 - np.power(A1, 2))
        dW1 = np.dot(X.T, dZ1) / X.shape[0]
        db1 = np.sum(dZ1, axis=0, keepdims=True) / X.shape[0]
        
        # Update velocities
        vW1 = momentum * vW1 - learning_rate * dW1
        vb1 = momentum * vb1 - learning_rate * db1
        vW2 = momentum * vW2 - learning_rate * dW2
        vb2 = momentum * vb2 - learning_rate * db2
        
        # Update parameters
        W1 = W1 + vW1
        b1 = b1 + vb1
        W2 = W2 + vW2
        b2 = b2 + vb2
        
        # Print progress
        if i % 100 == 0:
            print(f"Iteration {i}, Cost: {cost}")
    
    # Return parameters and costs
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
    return parameters, costs
```

## Practical Implementation Tips

When implementing NAG, consider the following tips:

1. **Initialization**: Initialize the velocity to zero and the parameters using an appropriate initialization scheme (e.g., Xavier/Glorot for neural networks).

2. **Learning Rate**: NAG often works well with a smaller learning rate compared to standard gradient descent. Start with a small value (e.g., 0.01) and adjust as needed.

3. **Momentum Parameter**: A common default value for the momentum parameter is 0.9. For fine-tuning, values between 0.5 and 0.99 are typically used.

4. **Scheduling**: Consider using a momentum schedule, where the momentum parameter increases over time (e.g., from 0.5 to 0.99).

5. **Gradient Clipping**: For deep networks, consider using gradient clipping to prevent exploding gradients.

6. **Batch Normalization**: When using NAG with deep networks, batch normalization can help stabilize training.

7. **Early Stopping**: Implement early stopping to prevent overfitting.

## Common Pitfalls and Solutions

When implementing NAG, be aware of these common pitfalls:

1. **Incorrect Look-ahead Calculation**: Make sure you're computing the gradient at the look-ahead position, not at the current position.

2. **Numerical Stability**: For deep networks, ensure numerical stability by using techniques like gradient clipping and proper initialization.

3. **Learning Rate Too High**: If training is unstable, try reducing the learning rate.

4. **Momentum Too High**: If the algorithm overshoots the minimum, try reducing the momentum parameter.

5. **Incorrect Update Order**: Make sure you're updating the velocity and parameters in the correct order.

## Summary

In this section, we've explored various implementations of Nesterov Accelerated Gradient:

1. **Basic Implementation**: We implemented NAG for a simple linear regression problem.
2. **Alternative Formulation**: We implemented the alternative formulation of NAG, which is often used in practice.
3. **2D Functions**: We implemented NAG for 2D functions, which will be useful for visualization.
4. **Mini-batch Gradient Descent**: We combined NAG with mini-batch gradient descent.
5. **Deep Learning Frameworks**: We showed how to use NAG in TensorFlow/Keras and PyTorch.
6. **Neural Networks**: We implemented NAG for a simple neural network.

We also discussed practical implementation tips and common pitfalls to avoid when implementing NAG.

In the next section, we'll visualize the behavior of NAG on various functions and compare it with other optimization algorithms.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
