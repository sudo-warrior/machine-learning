# 2.2.5.3.2.2.2.1 Nesterov Accelerated Gradient Basics

## Introduction to Nesterov Accelerated Gradient

Nesterov Accelerated Gradient (NAG), also known as Nesterov Momentum, is an advanced optimization algorithm that builds upon the concept of momentum. It was introduced by Yurii Nesterov in 1983 and has become a popular choice for training neural networks and solving other optimization problems.

While standard momentum computes the gradient at the current position and then takes a step influenced by the accumulated velocity, Nesterov Accelerated Gradient takes a more sophisticated approach. It first makes a partial update based on the accumulated velocity, then computes the gradient at this "look-ahead" position, and finally makes the full update. This "look-ahead" approach allows NAG to be more responsive to changes in the gradient and achieve faster convergence.

## The Intuition Behind Nesterov Accelerated Gradient

To understand the intuition behind NAG, consider a ball rolling down a hill:

1. **Standard Momentum**: The ball accumulates velocity as it rolls down, which helps it move faster. However, if the slope changes suddenly (e.g., the hill flattens out or curves upward), the ball will continue with its accumulated momentum before eventually responding to the new slope.

2. **Nesterov Accelerated Gradient**: The ball first makes a tentative move based on its current velocity, then checks the slope at this new position, and adjusts its direction accordingly. This allows the ball to be more responsive to changes in the terrain, slowing down earlier when approaching a minimum or changing direction more quickly when needed.

This "look-ahead" property makes NAG particularly effective for functions with complex landscapes, such as those encountered in deep learning.

## Mathematical Formulation

The update rule for Nesterov Accelerated Gradient is:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t - \gamma v_{t-1})$$
$$\theta_{t+1} = \theta_t - v_t$$

where:
- $v_t$ is the velocity vector (initially $v_0 = 0$)
- $\gamma$ is the momentum parameter (typically around 0.9)
- $\alpha$ is the learning rate
- $\nabla_\theta J(\theta_t - \gamma v_{t-1})$ is the gradient of the cost function $J$ with respect to $\theta$ at the "look-ahead" point $\theta_t - \gamma v_{t-1}$

The key difference from standard momentum is that the gradient is computed at the "look-ahead" position $\theta_t - \gamma v_{t-1}$ rather than at the current position $\theta_t$.

## Alternative Formulation

An alternative, equivalent formulation of NAG that is often used in practice is:

$$\tilde{\theta}_t = \theta_t + \gamma (\theta_t - \theta_{t-1})$$
$$\theta_{t+1} = \tilde{\theta}_t - \alpha \nabla_\theta J(\tilde{\theta}_t)$$

where $\tilde{\theta}_t$ is the "look-ahead" position. This formulation makes the "look-ahead" step more explicit and is often easier to implement.

## Theoretical Advantages of NAG

Nesterov Accelerated Gradient has several theoretical advantages:

1. **Improved Convergence Rate**: For convex functions, NAG achieves a convergence rate of $O(1/t^2)$, which is better than the $O(1/t)$ rate of standard gradient descent. This means NAG converges quadratically faster than standard gradient descent.

2. **Better Responsiveness**: The "look-ahead" step allows NAG to respond more quickly to changes in the gradient, making it more effective for functions with complex landscapes.

3. **Improved Stability**: NAG tends to be more stable than standard momentum, especially when approaching a minimum, as it can slow down more effectively.

## When to Use Nesterov Accelerated Gradient

Nesterov Accelerated Gradient is particularly useful in the following scenarios:

1. **Complex Optimization Landscapes**: When dealing with functions that have complex landscapes with multiple minima, saddle points, or ravines.

2. **Deep Learning**: NAG is widely used in training deep neural networks, where it often outperforms standard momentum.

3. **Convex Optimization**: For convex optimization problems, NAG provides theoretical guarantees of faster convergence.

4. **When Standard Momentum Overshoots**: If standard momentum tends to overshoot the minimum, NAG's "look-ahead" approach can help mitigate this issue.

## Comparison with Standard Momentum

Here's a comparison of Nesterov Accelerated Gradient and standard momentum:

| Aspect | Standard Momentum | Nesterov Accelerated Gradient |
|--------|-------------------|------------------------------|
| **Gradient Computation** | At the current position | At the "look-ahead" position |
| **Responsiveness to Changes** | Less responsive | More responsive |
| **Convergence Rate (Convex)** | $O(1/t)$ | $O(1/t^2)$ |
| **Overshooting** | More likely to overshoot | Less likely to overshoot |
| **Implementation Complexity** | Simpler | Slightly more complex |
| **Computational Cost** | One gradient computation per iteration | One gradient computation per iteration |

## Practical Considerations

When implementing Nesterov Accelerated Gradient, consider the following:

1. **Momentum Parameter**: Like standard momentum, NAG requires tuning the momentum parameter $\gamma$. A common default value is 0.9, but values between 0.5 and 0.99 are typically used.

2. **Learning Rate**: NAG may require a different learning rate than standard gradient descent or momentum. Often, a smaller learning rate works well with NAG.

3. **Initialization**: Proper weight initialization is important for NAG, as it is for other optimization algorithms.

4. **Scheduling**: Consider using a momentum schedule, where the momentum parameter increases over time (e.g., from 0.5 to 0.99).

5. **Combining with Other Techniques**: NAG can be combined with other optimization techniques, such as learning rate schedules or adaptive methods.

## Summary

Nesterov Accelerated Gradient is an advanced optimization algorithm that builds upon the concept of momentum by introducing a "look-ahead" step. This allows NAG to be more responsive to changes in the gradient and achieve faster convergence, particularly for complex optimization landscapes.

The key features of NAG are:
- Computing the gradient at a "look-ahead" position
- Improved convergence rate for convex functions
- Better responsiveness to changes in the gradient
- Reduced overshooting compared to standard momentum

In the next sections, we'll explore the implementation of NAG, visualize its behavior on various functions, and compare it with other optimization algorithms.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
