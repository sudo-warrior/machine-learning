# 2.2.5.3.2.2.2.4.2.3.3.2.1 Simple Saddle Implementation

## Implementation of Optimization Algorithms for Simple Saddle Points

In this section, we'll implement the optimization algorithms we'll use to compare Nesterov Accelerated Gradient (NAG) and standard momentum on simple functions with saddle points. We'll focus on implementing these algorithms in a way that allows us to visualize and analyze their behavior around saddle points.

## Basic Optimization Algorithms

Let's start by implementing the three main optimization algorithms we'll compare:

1. **Standard Gradient Descent (GD)**: The baseline algorithm that updates parameters in the direction of the negative gradient.
2. **Gradient Descent with Momentum (GDM)**: Adds a momentum term to standard gradient descent to accelerate convergence.
3. **Nesterov Accelerated Gradient (NAG)**: Computes the gradient at a "look-ahead" position to improve responsiveness.

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Implement standard gradient descent for 2D functions
def gradient_descent_2d(start_x, start_y, grad_func, learning_rate=0.1, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update parameters
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        
        path.append((x, y))
    
    return np.array(path)

# Implement gradient descent with momentum for 2D functions
def momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)
```

## Simple Saddle Functions

Next, let's define some simple functions with saddle points that we'll use for our comparisons:

```python
# Define a basic saddle function: f(x, y) = x^2 - y^2
def basic_saddle(x, y):
    return x**2 - y**2

# Define the gradient of the basic saddle function
def grad_basic_saddle(x, y):
    return np.array([2*x, -2*y])

# Define a cubic saddle function: f(x, y) = x^3 - 3xy^2
def cubic_saddle(x, y):
    return x**3 - 3*x*y**2

# Define the gradient of the cubic saddle function
def grad_cubic_saddle(x, y):
    return np.array([3*x**2 - 3*y**2, -6*x*y])

# Define a modified saddle function with a plateau: f(x, y) = x^2 - y^2 + 0.1*sin(5*x)*sin(5*y)
def plateau_saddle(x, y):
    return x**2 - y**2 + 0.1*np.sin(5*x)*np.sin(5*y)

# Define the gradient of the plateau saddle function
def grad_plateau_saddle(x, y):
    df_dx = 2*x + 0.5*np.cos(5*x)*np.sin(5*y)
    df_dy = -2*y + 0.5*np.sin(5*x)*np.cos(5*y)
    return np.array([df_dx, df_dy])
```

## Visualization Functions

Now, let's implement functions to visualize the optimization paths on these saddle functions:

```python
# Function to create a contour plot with optimization paths
def plot_optimization_paths(function, grad_function, x_range, y_range, start_points, 
                           learning_rate=0.1, momentum=0.9, n_iterations=50, title=None):
    # Create a grid of x and y values
    x = np.linspace(x_range[0], x_range[1], 100)
    y = np.linspace(y_range[0], y_range[1], 100)
    X, Y = np.meshgrid(x, y)
    Z = function(X, Y)
    
    # Run the optimization algorithms from each starting point
    paths_gd = []
    paths_momentum = []
    paths_nag = []
    
    for start_x, start_y in start_points:
        path_gd = gradient_descent_2d(start_x, start_y, grad_function, 
                                     learning_rate=learning_rate, n_iterations=n_iterations)
        path_momentum = momentum_2d(start_x, start_y, grad_function, 
                                   learning_rate=learning_rate, momentum=momentum, n_iterations=n_iterations)
        path_nag = nag_2d(start_x, start_y, grad_function, 
                         learning_rate=learning_rate, momentum=momentum, n_iterations=n_iterations)
        
        paths_gd.append(path_gd)
        paths_momentum.append(path_momentum)
        paths_nag.append(path_nag)
    
    # Create a 1x3 grid of contour plots
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # Gradient Descent
    ax = axes[0]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    for i, path in enumerate(paths_gd):
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point
    
    ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('Gradient Descent')
    ax.grid(True)
    ax.legend()
    
    # Momentum
    ax = axes[1]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    for i, path in enumerate(paths_momentum):
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point
    
    ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('Gradient Descent with Momentum')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[2]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    for i, path in enumerate(paths_nag):
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point
    
    ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('Nesterov Accelerated Gradient')
    ax.grid(True)
    ax.legend()
    
    plt.tight_layout()
    if title:
        fig.suptitle(title, fontsize=16)
        plt.subplots_adjust(top=0.9)
    plt.show()
    
    return paths_gd, paths_momentum, paths_nag

# Function to create a 3D surface plot of a function
def plot_3d_surface(function, x_range, y_range, title=None):
    # Create a grid of x and y values
    x = np.linspace(x_range[0], x_range[1], 100)
    y = np.linspace(y_range[0], y_range[1], 100)
    X, Y = np.meshgrid(x, y)
    Z = function(X, Y)
    
    # Create a 3D surface plot
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('f(x, y)')
    if title:
        ax.set_title(title)
    plt.show()
```

## Functions for Analyzing Saddle Point Behavior

Finally, let's implement functions to analyze the behavior of the optimization algorithms around saddle points:

```python
# Function to compute the distance to the saddle point for each iteration
def compute_distance_to_saddle(paths, saddle_point=np.array([0, 0])):
    distances = []
    for path in paths:
        distance = np.sqrt(np.sum((path - saddle_point)**2, axis=1))
        distances.append(distance)
    return distances

# Function to plot the distance to the saddle point vs. iteration
def plot_distance_to_saddle(distances_gd, distances_momentum, distances_nag, title=None):
    plt.figure(figsize=(10, 6))
    
    # Plot for a selected path (e.g., the first one)
    plt.plot(distances_gd[0], 'b-', linewidth=2, label='Gradient Descent')
    plt.plot(distances_momentum[0], 'g-', linewidth=2, label='Momentum')
    plt.plot(distances_nag[0], 'r-', linewidth=2, label='NAG')
    
    plt.xlabel('Iteration')
    plt.ylabel('Distance to Saddle Point')
    if title:
        plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()

# Function to compute the function values along the optimization paths
def compute_function_values(paths, function):
    values = []
    for path in paths:
        value = [function(x, y) for x, y in path]
        values.append(value)
    return values

# Function to plot the function values vs. iteration
def plot_function_values(values_gd, values_momentum, values_nag, title=None):
    plt.figure(figsize=(10, 6))
    
    # Plot for a selected path (e.g., the first one)
    plt.plot(values_gd[0], 'b-', linewidth=2, label='Gradient Descent')
    plt.plot(values_momentum[0], 'g-', linewidth=2, label='Momentum')
    plt.plot(values_nag[0], 'r-', linewidth=2, label='NAG')
    
    plt.xlabel('Iteration')
    plt.ylabel('Function Value')
    if title:
        plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()

# Function to analyze the escape behavior from the saddle point
def analyze_escape_behavior(paths_gd, paths_momentum, paths_nag, saddle_point=np.array([0, 0]), threshold=0.1):
    # Compute the number of iterations to escape the saddle point
    iterations_to_escape_gd = []
    iterations_to_escape_momentum = []
    iterations_to_escape_nag = []
    
    for path in paths_gd:
        distances = np.sqrt(np.sum((path - saddle_point)**2, axis=1))
        # Find the first iteration where the distance exceeds the threshold
        escape_iterations = np.where(distances > threshold)[0]
        if len(escape_iterations) > 0:
            iterations_to_escape_gd.append(escape_iterations[0])
        else:
            iterations_to_escape_gd.append(len(path))
    
    for path in paths_momentum:
        distances = np.sqrt(np.sum((path - saddle_point)**2, axis=1))
        escape_iterations = np.where(distances > threshold)[0]
        if len(escape_iterations) > 0:
            iterations_to_escape_momentum.append(escape_iterations[0])
        else:
            iterations_to_escape_momentum.append(len(path))
    
    for path in paths_nag:
        distances = np.sqrt(np.sum((path - saddle_point)**2, axis=1))
        escape_iterations = np.where(distances > threshold)[0]
        if len(escape_iterations) > 0:
            iterations_to_escape_nag.append(escape_iterations[0])
        else:
            iterations_to_escape_nag.append(len(path))
    
    # Print the results
    print(f"Average iterations to escape (GD): {np.mean(iterations_to_escape_gd):.2f}")
    print(f"Average iterations to escape (Momentum): {np.mean(iterations_to_escape_momentum):.2f}")
    print(f"Average iterations to escape (NAG): {np.mean(iterations_to_escape_nag):.2f}")
    
    return iterations_to_escape_gd, iterations_to_escape_momentum, iterations_to_escape_nag
```

## Utility Functions for Hyperparameter Analysis

Let's also implement functions to analyze the effect of hyperparameters on the optimization algorithms:

```python
# Function to analyze the effect of learning rate
def analyze_learning_rate_effect(function, grad_function, start_point, learning_rates, 
                               momentum=0.9, n_iterations=50, title=None):
    results_gd = []
    results_momentum = []
    results_nag = []
    
    for lr in learning_rates:
        path_gd = gradient_descent_2d(start_point[0], start_point[1], grad_function, 
                                     learning_rate=lr, n_iterations=n_iterations)
        path_momentum = momentum_2d(start_point[0], start_point[1], grad_function, 
                                   learning_rate=lr, momentum=momentum, n_iterations=n_iterations)
        path_nag = nag_2d(start_point[0], start_point[1], grad_function, 
                         learning_rate=lr, momentum=momentum, n_iterations=n_iterations)
        
        results_gd.append((lr, path_gd))
        results_momentum.append((lr, path_momentum))
        results_nag.append((lr, path_nag))
    
    # Create a grid of plots for each learning rate
    n_rows = len(learning_rates)
    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
    
    # Create a grid of x and y values for contour plots
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x, y)
    Z = function(X, Y)
    
    for i, lr in enumerate(learning_rates):
        # Gradient Descent
        ax = axes[i, 0] if n_rows > 1 else axes[0]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        plt.colorbar(contour, ax=ax, label='f(x, y) value')
        
        path = results_gd[i][1]
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='GD Path')
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
        ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'GD (lr={lr})')
        ax.grid(True)
        ax.legend()
        
        # Momentum
        ax = axes[i, 1] if n_rows > 1 else axes[1]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        plt.colorbar(contour, ax=ax, label='f(x, y) value')
        
        path = results_momentum[i][1]
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='Momentum Path')
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
        ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'Momentum (lr={lr})')
        ax.grid(True)
        ax.legend()
        
        # NAG
        ax = axes[i, 2] if n_rows > 1 else axes[2]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        plt.colorbar(contour, ax=ax, label='f(x, y) value')
        
        path = results_nag[i][1]
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG Path')
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
        ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'NAG (lr={lr})')
        ax.grid(True)
        ax.legend()
    
    plt.tight_layout()
    if title:
        fig.suptitle(title, fontsize=16)
        plt.subplots_adjust(top=0.95)
    plt.show()
    
    return results_gd, results_momentum, results_nag

# Function to analyze the effect of momentum parameter
def analyze_momentum_effect(function, grad_function, start_point, momentum_values, 
                          learning_rate=0.1, n_iterations=50, title=None):
    results_momentum = []
    results_nag = []
    
    for m in momentum_values:
        path_momentum = momentum_2d(start_point[0], start_point[1], grad_function, 
                                   learning_rate=learning_rate, momentum=m, n_iterations=n_iterations)
        path_nag = nag_2d(start_point[0], start_point[1], grad_function, 
                         learning_rate=learning_rate, momentum=m, n_iterations=n_iterations)
        
        results_momentum.append((m, path_momentum))
        results_nag.append((m, path_nag))
    
    # Create a grid of plots for each momentum value
    n_rows = len(momentum_values)
    fig, axes = plt.subplots(n_rows, 2, figsize=(10, 5*n_rows))
    
    # Create a grid of x and y values for contour plots
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x, y)
    Z = function(X, Y)
    
    for i, m in enumerate(momentum_values):
        # Momentum
        ax = axes[i, 0] if n_rows > 1 else axes[0]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        plt.colorbar(contour, ax=ax, label='f(x, y) value')
        
        path = results_momentum[i][1]
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='Momentum Path')
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
        ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'Momentum (m={m})')
        ax.grid(True)
        ax.legend()
        
        # NAG
        ax = axes[i, 1] if n_rows > 1 else axes[1]
        contour = ax.contour(X, Y, Z, 20, cmap='viridis')
        plt.colorbar(contour, ax=ax, label='f(x, y) value')
        
        path = results_nag[i][1]
        ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG Path')
        ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
        ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
        ax.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
        
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'NAG (m={m})')
        ax.grid(True)
        ax.legend()
    
    plt.tight_layout()
    if title:
        fig.suptitle(title, fontsize=16)
        plt.subplots_adjust(top=0.95)
    plt.show()
    
    return results_momentum, results_nag
```

## Summary

In this section, we've implemented the necessary functions to compare Nesterov Accelerated Gradient (NAG) and standard momentum on simple functions with saddle points:

1. **Basic Optimization Algorithms**: We've implemented standard gradient descent, gradient descent with momentum, and Nesterov Accelerated Gradient for 2D functions.

2. **Simple Saddle Functions**: We've defined several simple functions with saddle points, including a basic saddle function, a cubic saddle function, and a saddle function with a plateau.

3. **Visualization Functions**: We've implemented functions to visualize the optimization paths on these saddle functions, including contour plots and 3D surface plots.

4. **Analysis Functions**: We've implemented functions to analyze the behavior of the optimization algorithms around saddle points, including computing distances to the saddle point, function values along the paths, and escape behavior.

5. **Hyperparameter Analysis**: We've implemented functions to analyze the effect of learning rate and momentum parameter on the optimization algorithms.

In the next sections, we'll use these functions to compare NAG and standard momentum on various simple functions with saddle points, starting with the basic saddle function $f(x, y) = x^2 - y^2$.

## References

1. Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems (pp. 2933-2941).
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., & Jordan, M. I. (2017). How to escape saddle points efficiently. In International Conference on Machine Learning (pp. 1724-1732).
4. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
5. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
