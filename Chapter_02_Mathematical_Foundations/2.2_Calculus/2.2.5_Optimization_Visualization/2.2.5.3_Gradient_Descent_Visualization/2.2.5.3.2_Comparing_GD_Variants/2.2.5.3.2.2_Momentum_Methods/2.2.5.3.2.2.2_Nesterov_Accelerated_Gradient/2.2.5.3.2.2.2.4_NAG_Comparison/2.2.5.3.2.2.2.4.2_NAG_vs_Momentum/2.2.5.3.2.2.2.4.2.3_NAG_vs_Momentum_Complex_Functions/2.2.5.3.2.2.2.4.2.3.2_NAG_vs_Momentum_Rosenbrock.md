# 2.2.5.3.2.2.2.4.2.3.2 NAG vs Momentum Rosenbrock

## Comparing Nesterov Accelerated Gradient and Standard Momentum on the Rosenbrock Function

In this section, we'll compare Nesterov Accelerated Gradient (NAG) and standard momentum on the Rosenbrock function, a classic test function for optimization algorithms. The Rosenbrock function, also known as the "banana function," is particularly challenging due to its long, narrow, curved valley. It's a standard benchmark for optimization algorithms because it tests their ability to navigate a curved valley where the gradient changes direction.

## The Rosenbrock Function

The Rosenbrock function is defined as:

$$f(x, y) = (a - x)^2 + b(y - x^2)^2$$

where $a$ and $b$ are constants, typically set to $a = 1$ and $b = 100$. The global minimum is at $(a, a^2) = (1, 1)$ with a function value of 0.

The gradient of the Rosenbrock function is:

$$\nabla f(x, y) = \begin{bmatrix} -2(a - x) - 4bx(y - x^2) \\ 2b(y - x^2) \end{bmatrix}$$

## Implementation

Let's start by implementing both algorithms for the Rosenbrock function:

```python
import numpy as np
import matplotlib.pyplot as plt

# Implement standard momentum for 2D functions
def momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Define the Rosenbrock function
def rosenbrock(x, y, a=1, b=100):
    return (a - x)**2 + b * (y - x**2)**2

# Define the gradient of the Rosenbrock function
def grad_rosenbrock(x, y, a=1, b=100):
    df_dx = -2*(a - x) - 4*b*x*(y - x**2)
    df_dy = 2*b*(y - x**2)
    return np.array([df_dx, df_dy])
```

## Visualizing the Rosenbrock Function

Let's first visualize the Rosenbrock function to understand its challenging landscape:

```python
# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

# Create a 3D surface plot
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('Rosenbrock Function')
plt.show()

# Create a contour plot
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')  # Log scale for better visualization
plt.colorbar(label='log(f(x, y) + 1) value')
plt.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Contour Plot of Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Comparing Optimization Paths

Now, let's compare the optimization paths of standard momentum and NAG on the Rosenbrock function:

```python
# Run standard momentum and NAG from different starting points
start_points = [(-1.5, -0.5), (-1, 0), (0, 0), (2, 2), (0, 2), (-1.2, 1)]
paths_momentum = []
paths_nag = []

for start_x, start_y in start_points:
    path_momentum = momentum_2d(start_x, start_y, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=1000)
    path_nag = nag_2d(start_x, start_y, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=1000)
    paths_momentum.append(path_momentum)
    paths_nag.append(path_nag)

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard Momentum
ax = axes[0]
contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')

for i, path in enumerate(paths_momentum):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Momentum on Rosenbrock Function')
ax.grid(True)
ax.legend()

# NAG
ax = axes[1]
contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')

for i, path in enumerate(paths_nag):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Nesterov Accelerated Gradient on Rosenbrock Function')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Optimization Paths

When comparing the optimization paths on the Rosenbrock function, we observe several key differences:

1. **Valley Navigation**: The Rosenbrock function has a long, narrow, curved valley. NAG typically navigates this valley more efficiently than standard momentum, with less oscillation across the valley.

2. **Convergence to Global Minimum**: Both algorithms may eventually converge to the global minimum at (1, 1), but NAG often reaches it in fewer iterations.

3. **Path Smoothness**: NAG's path is typically smoother than standard momentum's path, with less zigzagging, especially in the curved part of the valley.

4. **Sensitivity to Starting Point**: Both algorithms are sensitive to the starting point, but NAG tends to be more robust, converging to the global minimum from a wider range of starting points.

Let's quantify these differences by computing the distance to the global minimum for each iteration:

```python
# Compute the distance to the global minimum for each iteration
min_point = np.array([1, 1])  # The global minimum of the Rosenbrock function is at (1, 1)
dist_momentum = [np.sqrt(np.sum((path - min_point)**2, axis=1)) for path in paths_momentum]
dist_nag = [np.sqrt(np.sum((path - min_point)**2, axis=1)) for path in paths_nag]

# Plot the distance to the minimum vs. iteration for a selected starting point (e.g., the first one)
plt.figure(figsize=(10, 6))
plt.plot(dist_momentum[0], 'b-', linewidth=2, label='Standard Momentum')
plt.plot(dist_nag[0], 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Global Minimum')
plt.title('Convergence Speed: Standard Momentum vs. NAG on Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()

# Compute the final function values for each starting point
final_values_momentum = [rosenbrock(path[-1, 0], path[-1, 1]) for path in paths_momentum]
final_values_nag = [rosenbrock(path[-1, 0], path[-1, 1]) for path in paths_nag]

# Print the results
print("Final function values for Standard Momentum:")
for i, value in enumerate(final_values_momentum):
    print(f"Starting point {start_points[i]}: {value:.6f}")

print("\nFinal function values for NAG:")
for i, value in enumerate(final_values_nag):
    print(f"Starting point {start_points[i]}: {value:.6f}")

# Compare the average final function value
print(f"\nAverage final value for Standard Momentum: {np.mean(final_values_momentum):.6f}")
print(f"Average final value for NAG: {np.mean(final_values_nag):.6f}")

# Count how many times each algorithm found the global minimum (approximately)
global_min_threshold = 0.01  # Consider values below this threshold as having found the global minimum
global_min_count_momentum = sum(1 for value in final_values_momentum if value < global_min_threshold)
global_min_count_nag = sum(1 for value in final_values_nag if value < global_min_threshold)

print(f"\nNumber of times Standard Momentum found the global minimum: {global_min_count_momentum}/{len(start_points)}")
print(f"Number of times NAG found the global minimum: {global_min_count_nag}/{len(start_points)}")
```

## Effect of Learning Rate

The learning rate is a crucial hyperparameter for both algorithms on the Rosenbrock function. Let's compare how different learning rates affect their performance:

```python
# Run standard momentum and NAG with different learning rates
learning_rates = [0.00001, 0.0001, 0.001]
paths_momentum = []
paths_nag = []

for lr in learning_rates:
    path_momentum = momentum_2d(-1, 0, grad_rosenbrock, learning_rate=lr, momentum=0.9, n_iterations=1000)
    path_nag = nag_2d(-1, 0, grad_rosenbrock, learning_rate=lr, momentum=0.9, n_iterations=1000)
    paths_momentum.append((lr, path_momentum))
    paths_nag.append((lr, path_nag))

# Create a 3x2 grid of contour plots
fig, axes = plt.subplots(3, 2, figsize=(15, 20))

for i, lr in enumerate(learning_rates):
    # Standard Momentum
    ax = axes[i, 0]
    contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')
    
    path = paths_momentum[i][1]
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label='Standard Momentum')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
    ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Standard Momentum (α = {lr})')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[i, 1]
    contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')
    
    path = paths_nag[i][1]
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6, label='Final Point')
    ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Nesterov Accelerated Gradient (α = {lr})')
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Learning Rate Effect

When comparing the effect of different learning rates on both algorithms for the Rosenbrock function, we observe:

1. **Low Learning Rate (0.00001)**: Both algorithms make slow progress, but NAG typically makes more efficient use of the small steps, navigating the valley more directly.

2. **Medium Learning Rate (0.0001)**: This is often a good balance for both algorithms on the Rosenbrock function, with NAG showing better convergence and less oscillation.

3. **High Learning Rate (0.001)**: Both algorithms may become unstable with too high a learning rate on the Rosenbrock function, but NAG is often more robust to higher learning rates due to its "look-ahead" approach.

4. **Sensitivity to Learning Rate**: The Rosenbrock function requires careful tuning of the learning rate for both algorithms, but NAG is generally less sensitive to the exact choice.

## Effect of Momentum Parameter

The momentum parameter $\gamma$ also significantly affects the behavior of both algorithms on the Rosenbrock function. Let's compare how different momentum values affect their performance:

```python
# Run standard momentum and NAG with different momentum values
momentum_values = [0.5, 0.9, 0.99]
paths_momentum = []
paths_nag = []

for m in momentum_values:
    path_momentum = momentum_2d(-1, 0, grad_rosenbrock, learning_rate=0.0001, momentum=m, n_iterations=1000)
    path_nag = nag_2d(-1, 0, grad_rosenbrock, learning_rate=0.0001, momentum=m, n_iterations=1000)
    paths_momentum.append((m, path_momentum))
    paths_nag.append((m, path_nag))

# Create a 3x2 grid of contour plots
fig, axes = plt.subplots(3, 2, figsize=(15, 20))

for i, m in enumerate(momentum_values):
    # Standard Momentum
    ax = axes[i, 0]
    contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')
    
    path = paths_momentum[i][1]
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label='Standard Momentum')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
    ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Standard Momentum (γ = {m})')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[i, 1]
    contour = ax.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='log(f(x, y) + 1) value')
    
    path = paths_nag[i][1]
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6, label='Final Point')
    ax.plot(1, 1, 'ro', markersize=8, label='Global Minimum (1, 1)')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Nesterov Accelerated Gradient (γ = {m})')
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Momentum Parameter Effect

When comparing the effect of different momentum values on both algorithms for the Rosenbrock function, we observe:

1. **Low Momentum (0.5)**: Both algorithms make slower progress along the valley, with NAG showing a slight advantage in directness.

2. **Medium Momentum (0.9)**: This is often a good balance for both algorithms on the Rosenbrock function, with NAG showing better navigation of the curved valley.

3. **High Momentum (0.99)**: High momentum can help both algorithms progress faster along the valley, but may also lead to overshooting. NAG's "look-ahead" approach helps it maintain better control even with high momentum.

4. **Valley Navigation**: Higher momentum values help both algorithms navigate the long valley more efficiently, but NAG consistently shows better performance in following the curve of the valley.

## Convergence Analysis

Let's analyze the convergence behavior of both algorithms on the Rosenbrock function in more detail:

```python
# Run standard momentum and NAG for a longer number of iterations
path_momentum = momentum_2d(-1, 0, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=5000)
path_nag = nag_2d(-1, 0, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=5000)

# Compute function values along the paths
values_momentum = [rosenbrock(x, y) for x, y in path_momentum]
values_nag = [rosenbrock(x, y) for x, y in path_nag]

# Plot the function values vs. iteration (log scale for better visualization)
plt.figure(figsize=(10, 6))
plt.semilogy(values_momentum, 'b-', linewidth=2, label='Standard Momentum')
plt.semilogy(values_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Function Value (log scale)')
plt.title('Convergence: Standard Momentum vs. NAG on Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()

# Compute the distance to the global minimum for each iteration
min_point = np.array([1, 1])
dist_momentum = np.sqrt(np.sum((path_momentum - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.semilogy(dist_momentum, 'b-', linewidth=2, label='Standard Momentum')
plt.semilogy(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Global Minimum (log scale)')
plt.title('Convergence Speed: Standard Momentum vs. NAG on Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of Convergence Behavior

When analyzing the convergence behavior of both algorithms on the Rosenbrock function, we observe:

1. **Convergence Rate**: NAG typically converges faster than standard momentum, reaching lower function values and getting closer to the global minimum in fewer iterations.

2. **Convergence Phases**: Both algorithms exhibit different phases of convergence:
   - Initial phase: Rapid progress down the steep sides of the valley
   - Middle phase: Slower progress along the valley floor
   - Final phase: Convergence to the global minimum at the end of the valley

3. **Stability Near Minimum**: NAG often shows better stability near the global minimum, with less oscillation and more precise convergence.

4. **Long-term Behavior**: Over a large number of iterations, both algorithms eventually converge to the global minimum, but NAG typically reaches a given precision with fewer iterations.

## Visualizing the "Look-ahead" Step on Rosenbrock

To better understand why NAG performs better on the Rosenbrock function, let's visualize the "look-ahead" step:

```python
# Create a contour plot with look-ahead steps
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')
plt.colorbar(label='log(f(x, y) + 1) value')

# Run NAG for a few iterations to visualize the look-ahead step
x, y = -1, 0
velocity_x, velocity_y = 0, 0
path = [(x, y)]
look_ahead_path = []

for i in range(20):
    # Compute the look-ahead position
    look_ahead_x = x + momentum * velocity_x
    look_ahead_y = y + momentum * velocity_y
    look_ahead_path.append((look_ahead_x, look_ahead_y))
    
    # Compute gradient at the look-ahead position
    grad = grad_rosenbrock(look_ahead_x, look_ahead_y)
    
    # Update velocity
    velocity_x = momentum * velocity_x - learning_rate * grad[0]
    velocity_y = momentum * velocity_y - learning_rate * grad[1]
    
    # Update parameters
    x = x + velocity_x
    y = y + velocity_y
    path.append((x, y))

# Convert to numpy arrays
path = np.array(path)
look_ahead_path = np.array(look_ahead_path)

# Plot the path and look-ahead points
plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=6, label='NAG Path')
plt.plot(look_ahead_path[:, 0], look_ahead_path[:, 1], 'b*', markersize=8, label='Look-ahead Points')
plt.plot(1, 1, 'go', markersize=8, label='Global Minimum (1, 1)')

# Draw arrows from each point to its look-ahead point
for i in range(len(look_ahead_path)):
    plt.arrow(path[i, 0], path[i, 1], 
              look_ahead_path[i, 0] - path[i, 0], look_ahead_path[i, 1] - path[i, 1],
              head_width=0.05, head_length=0.05, fc='blue', ec='blue', alpha=0.5)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient - Look-ahead Steps on Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of the "Look-ahead" Step on Rosenbrock

The visualization of the "look-ahead" step on the Rosenbrock function reveals:

1. **Anticipatory Behavior**: NAG anticipates where the parameters will be in the next iteration and computes the gradient at this anticipated position. This is particularly beneficial in the curved valley of the Rosenbrock function, where the gradient direction changes significantly along the path.

2. **Valley Navigation**: The "look-ahead" step helps NAG better navigate the curved valley by anticipating the change in gradient direction, leading to a more direct path along the valley floor.

3. **Reduced Oscillation**: By computing the gradient at the "look-ahead" position, NAG reduces oscillation across the valley, which is a common issue for standard momentum on the Rosenbrock function.

4. **Adaptive Step Size**: The "look-ahead" step effectively provides an adaptive step size that adjusts to the local curvature of the function, allowing NAG to take larger steps in flat regions and smaller steps in curved regions.

## Summary

In this section, we've compared Nesterov Accelerated Gradient and standard momentum on the Rosenbrock function:

1. **Valley Navigation**: We've seen that NAG navigates the long, narrow, curved valley of the Rosenbrock function more efficiently than standard momentum, with less oscillation and a more direct path.

2. **Convergence Speed**: We've observed that NAG typically converges faster than standard momentum, reaching the global minimum in fewer iterations.

3. **Effect of Hyperparameters**: We've examined how different learning rates and momentum values affect the performance of both algorithms, finding that NAG is generally more robust to hyperparameter choices.

4. **"Look-ahead" Advantage**: We've visualized the key difference between NAG and standard momentum, the "look-ahead" step, which allows NAG to anticipate changes in the gradient direction and navigate the curved valley more effectively.

5. **Convergence Analysis**: We've analyzed the convergence behavior of both algorithms, showing that NAG exhibits better long-term convergence properties on the Rosenbrock function.

The Rosenbrock function is a particularly good showcase for the advantages of NAG over standard momentum, as it highlights NAG's ability to navigate curved valleys more efficiently. This is relevant for many machine learning problems, where the loss landscape often contains similar curved valleys.

In the next section, we'll compare the two algorithms on functions with saddle points, another challenging feature of optimization landscapes in machine learning.

## References

1. Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of a function. The Computer Journal, 3(3), 175-184.
2. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
3. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
5. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
