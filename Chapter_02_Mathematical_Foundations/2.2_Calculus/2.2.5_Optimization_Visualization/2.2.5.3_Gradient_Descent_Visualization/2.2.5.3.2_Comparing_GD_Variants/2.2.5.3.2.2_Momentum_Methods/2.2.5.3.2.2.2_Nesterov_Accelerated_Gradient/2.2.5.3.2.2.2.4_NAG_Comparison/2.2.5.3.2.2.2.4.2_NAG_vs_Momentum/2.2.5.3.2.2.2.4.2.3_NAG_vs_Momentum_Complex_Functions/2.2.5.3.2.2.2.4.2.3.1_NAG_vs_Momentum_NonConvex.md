# 2.2.5.3.2.2.2.4.2.3.1 NAG vs Momentum NonConvex

## Comparing Nesterov Accelerated Gradient and Standard Momentum on Non-Convex Functions

In this section, we'll compare Nesterov Accelerated Gradient (NAG) and standard momentum on non-convex functions. Non-convex functions are particularly important in machine learning, as most neural network loss landscapes are non-convex. These functions present unique challenges for optimization algorithms, including multiple local minima, saddle points, and complex topologies.

## Implementation

Let's start by implementing both algorithms for 2D functions:

```python
import numpy as np
import matplotlib.pyplot as plt

# Implement standard momentum for 2D functions
def momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)
```

## Simple Non-Convex Function

Let's first compare the two algorithms on a simple non-convex function with multiple local minima:

```python
# Define a simple non-convex function
def simple_non_convex(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_simple_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = simple_non_convex(X, Y)

# Run standard momentum and NAG from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths_momentum = []
paths_nag = []

for start_x, start_y in start_points:
    path_momentum = momentum_2d(start_x, start_y, grad_simple_non_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)
    path_nag = nag_2d(start_x, start_y, grad_simple_non_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)
    paths_momentum.append(path_momentum)
    paths_nag.append(path_nag)

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard Momentum
ax = axes[0]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_momentum):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Momentum')
ax.grid(True)
ax.legend()

# NAG
ax = axes[1]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_nag):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Nesterov Accelerated Gradient')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Results on Simple Non-Convex Function

When comparing the optimization paths on a simple non-convex function, we observe several key differences:

1. **Local Minima Convergence**: Both algorithms may converge to different local minima depending on the starting point, but NAG often finds better local minima (with lower function values) than standard momentum.

2. **Path Stability**: NAG's paths are typically more stable and direct, while standard momentum's paths may exhibit more oscillation and exploration.

3. **Convergence Speed**: NAG generally converges faster to local minima than standard momentum, requiring fewer iterations to reach a stable point.

4. **Exploration vs. Exploitation**: Standard momentum tends to explore the function landscape more widely, while NAG is more focused on exploiting the current gradient information to find the nearest local minimum.

Let's compute the final function values for each starting point to quantify these differences:

```python
# Compute the final function values for each starting point
final_values_momentum = [simple_non_convex(path[-1, 0], path[-1, 1]) for path in paths_momentum]
final_values_nag = [simple_non_convex(path[-1, 0], path[-1, 1]) for path in paths_nag]

# Print the results
print("Final function values for Standard Momentum:")
for i, value in enumerate(final_values_momentum):
    print(f"Starting point {start_points[i]}: {value:.6f}")

print("\nFinal function values for NAG:")
for i, value in enumerate(final_values_nag):
    print(f"Starting point {start_points[i]}: {value:.6f}")

# Compare the average final function value
print(f"\nAverage final value for Standard Momentum: {np.mean(final_values_momentum):.6f}")
print(f"Average final value for NAG: {np.mean(final_values_nag):.6f}")
```

## Complex Non-Convex Function

Next, let's compare the two algorithms on a more complex non-convex function with multiple local minima and saddle points:

```python
# Define a complex non-convex function (Rastrigin function)
def rastrigin(x, y, A=10):
    return 2*A + (x**2 - A*np.cos(2*np.pi*x)) + (y**2 - A*np.cos(2*np.pi*y))

# Define the gradient of the function
def grad_rastrigin(x, y, A=10):
    df_dx = 2*x + 2*np.pi*A*np.sin(2*np.pi*x)
    df_dy = 2*y + 2*np.pi*A*np.sin(2*np.pi*y)
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = rastrigin(X, Y)

# Run standard momentum and NAG from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0), (2.5, 2.5), (-2.5, -2.5)]
paths_momentum = []
paths_nag = []

for start_x, start_y in start_points:
    path_momentum = momentum_2d(start_x, start_y, grad_rastrigin, learning_rate=0.01, momentum=0.9, n_iterations=100)
    path_nag = nag_2d(start_x, start_y, grad_rastrigin, learning_rate=0.01, momentum=0.9, n_iterations=100)
    paths_momentum.append(path_momentum)
    paths_nag.append(path_nag)

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard Momentum
ax = axes[0]
contour = ax.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_momentum):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Momentum on Rastrigin Function')
ax.grid(True)
ax.legend()

# NAG
ax = axes[1]
contour = ax.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_nag):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Nesterov Accelerated Gradient on Rastrigin Function')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Results on Complex Non-Convex Function

When comparing the optimization paths on the Rastrigin function, which is a more challenging non-convex function, we observe:

1. **Local Minima Trapping**: Both algorithms can get trapped in different local minima, but NAG tends to find better local minima more consistently.

2. **Oscillation in Valleys**: Standard momentum tends to oscillate more in the valleys between local minima, while NAG navigates these regions more smoothly.

3. **Escape from Local Minima**: In some cases, the momentum in both algorithms can help escape shallow local minima, but NAG's "look-ahead" approach can make it more effective at escaping certain types of local minima.

4. **Convergence to Global Minimum**: Neither algorithm is guaranteed to find the global minimum (at (0, 0) for the Rastrigin function), but NAG may have a slightly better chance due to its more efficient navigation of the landscape.

Let's compute the final function values for each starting point to quantify these differences:

```python
# Compute the final function values for each starting point
final_values_momentum = [rastrigin(path[-1, 0], path[-1, 1]) for path in paths_momentum]
final_values_nag = [rastrigin(path[-1, 0], path[-1, 1]) for path in paths_nag]

# Print the results
print("Final function values for Standard Momentum:")
for i, value in enumerate(final_values_momentum):
    print(f"Starting point {start_points[i]}: {value:.6f}")

print("\nFinal function values for NAG:")
for i, value in enumerate(final_values_nag):
    print(f"Starting point {start_points[i]}: {value:.6f}")

# Compare the average final function value
print(f"\nAverage final value for Standard Momentum: {np.mean(final_values_momentum):.6f}")
print(f"Average final value for NAG: {np.mean(final_values_nag):.6f}")

# Count how many times each algorithm found the global minimum (approximately)
global_min_threshold = 0.1  # Consider values below this threshold as having found the global minimum
global_min_count_momentum = sum(1 for value in final_values_momentum if value < global_min_threshold)
global_min_count_nag = sum(1 for value in final_values_nag if value < global_min_threshold)

print(f"\nNumber of times Standard Momentum found the global minimum: {global_min_count_momentum}/{len(start_points)}")
print(f"Number of times NAG found the global minimum: {global_min_count_nag}/{len(start_points)}")
```

## Effect of Momentum Parameter on Non-Convex Functions

The momentum parameter $\gamma$ can significantly affect the behavior of both algorithms on non-convex functions. Let's compare how different momentum values affect their performance:

```python
# Run standard momentum and NAG with different momentum values
momentum_values = [0.5, 0.9, 0.99]
paths_momentum = []
paths_nag = []

for m in momentum_values:
    path_momentum = momentum_2d(2.5, 2.5, grad_rastrigin, learning_rate=0.01, momentum=m, n_iterations=100)
    path_nag = nag_2d(2.5, 2.5, grad_rastrigin, learning_rate=0.01, momentum=m, n_iterations=100)
    paths_momentum.append((m, path_momentum))
    paths_nag.append((m, path_nag))

# Create a 3x2 grid of contour plots
fig, axes = plt.subplots(3, 2, figsize=(15, 20))

for i, m in enumerate(momentum_values):
    # Standard Momentum
    ax = axes[i, 0]
    contour = ax.contour(X, Y, Z, 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_momentum[i][1]
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label='Standard Momentum')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Standard Momentum (γ = {m})')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[i, 1]
    contour = ax.contour(X, Y, Z, 50, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_nag[i][1]
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Nesterov Accelerated Gradient (γ = {m})')
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Momentum Parameter Effect on Non-Convex Functions

When comparing the effect of different momentum values on both algorithms for non-convex functions, we observe:

1. **Low Momentum (0.5)**: Both algorithms tend to get trapped in local minima more easily, with NAG showing a slight advantage in finding better local minima.

2. **Medium Momentum (0.9)**: Both algorithms can escape some shallow local minima, with NAG showing better navigation of the complex landscape.

3. **High Momentum (0.99)**: Both algorithms can escape more local minima, but may also overshoot and oscillate more. NAG's "look-ahead" approach helps it maintain better control even with high momentum.

4. **Exploration vs. Exploitation Trade-off**: Higher momentum values increase exploration (ability to escape local minima) at the cost of exploitation (precise convergence to minima). NAG generally maintains a better balance between these two aspects.

## Ackley Function

Let's also compare the two algorithms on the Ackley function, another challenging non-convex function with many local minima:

```python
# Define the Ackley function
def ackley(x, y, a=20, b=0.2, c=2*np.pi):
    term1 = -a * np.exp(-b * np.sqrt(0.5 * (x**2 + y**2)))
    term2 = -np.exp(0.5 * (np.cos(c*x) + np.cos(c*y)))
    return term1 + term2 + a + np.exp(1)

# Define the gradient of the Ackley function
def grad_ackley(x, y, a=20, b=0.2, c=2*np.pi):
    term1_factor = a * b * np.exp(-b * np.sqrt(0.5 * (x**2 + y**2))) / (2 * np.sqrt(0.5 * (x**2 + y**2)))
    df_dx = term1_factor * x + 0.5 * c * np.exp(0.5 * (np.cos(c*x) + np.cos(c*y))) * np.sin(c*x)
    df_dy = term1_factor * y + 0.5 * c * np.exp(0.5 * (np.cos(c*x) + np.cos(c*y))) * np.sin(c*y)
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = ackley(X, Y)

# Run standard momentum and NAG from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (2, 2), (-2, -2)]
paths_momentum = []
paths_nag = []

for start_x, start_y in start_points:
    path_momentum = momentum_2d(start_x, start_y, grad_ackley, learning_rate=0.01, momentum=0.9, n_iterations=100)
    path_nag = nag_2d(start_x, start_y, grad_ackley, learning_rate=0.01, momentum=0.9, n_iterations=100)
    paths_momentum.append(path_momentum)
    paths_nag.append(path_nag)

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard Momentum
ax = axes[0]
contour = ax.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_momentum):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Momentum on Ackley Function')
ax.grid(True)
ax.legend()

# NAG
ax = axes[1]
contour = ax.contour(X, Y, Z, 50, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_nag):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Nesterov Accelerated Gradient on Ackley Function')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Results on Ackley Function

The Ackley function is particularly challenging due to its many local minima and a steep global minimum at (0, 0). When comparing the two algorithms on this function, we observe:

1. **Difficulty Escaping Local Minima**: Both algorithms struggle to escape the numerous local minima in the Ackley function, but NAG may have a slight advantage due to its "look-ahead" approach.

2. **Convergence to Global Minimum**: Neither algorithm consistently finds the global minimum at (0, 0), highlighting the limitations of first-order methods on highly non-convex functions.

3. **Path Stability**: NAG's paths are generally more stable and direct than standard momentum's paths, which may exhibit more oscillation.

4. **Sensitivity to Starting Point**: Both algorithms are highly sensitive to the starting point, with some starting points leading to better minima than others.

Let's compute the final function values for each starting point to quantify these differences:

```python
# Compute the final function values for each starting point
final_values_momentum = [ackley(path[-1, 0], path[-1, 1]) for path in paths_momentum]
final_values_nag = [ackley(path[-1, 0], path[-1, 1]) for path in paths_nag]

# Print the results
print("Final function values for Standard Momentum:")
for i, value in enumerate(final_values_momentum):
    print(f"Starting point {start_points[i]}: {value:.6f}")

print("\nFinal function values for NAG:")
for i, value in enumerate(final_values_nag):
    print(f"Starting point {start_points[i]}: {value:.6f}")

# Compare the average final function value
print(f"\nAverage final value for Standard Momentum: {np.mean(final_values_momentum):.6f}")
print(f"Average final value for NAG: {np.mean(final_values_nag):.6f}")

# Count how many times each algorithm found the global minimum (approximately)
global_min_threshold = 1.0  # Consider values below this threshold as having found the global minimum
global_min_count_momentum = sum(1 for value in final_values_momentum if value < global_min_threshold)
global_min_count_nag = sum(1 for value in final_values_nag if value < global_min_threshold)

print(f"\nNumber of times Standard Momentum found the global minimum: {global_min_count_momentum}/{len(start_points)}")
print(f"Number of times NAG found the global minimum: {global_min_count_nag}/{len(start_points)}")
```

## Summary

In this section, we've compared Nesterov Accelerated Gradient and standard momentum on non-convex functions:

1. **Simple Non-Convex Function**: We've seen that NAG generally finds better local minima and converges faster than standard momentum on simple non-convex functions.

2. **Complex Non-Convex Functions**: We've observed that both algorithms can get trapped in local minima on more complex functions like the Rastrigin and Ackley functions, but NAG often finds better local minima and navigates the landscape more efficiently.

3. **Effect of Momentum Parameter**: We've examined how different momentum values affect the performance of both algorithms on non-convex functions, finding that NAG maintains a better balance between exploration and exploitation across different momentum values.

4. **Global Minimum Convergence**: We've seen that neither algorithm is guaranteed to find the global minimum on challenging non-convex functions, highlighting the limitations of first-order methods on such problems.

5. **Path Stability**: We've observed that NAG's paths are generally more stable and direct than standard momentum's paths, with less oscillation and more efficient navigation of the landscape.

Overall, our empirical results on non-convex functions suggest that NAG often outperforms standard momentum, finding better local minima and converging faster. However, both algorithms have limitations on highly non-convex functions, and neither is guaranteed to find the global minimum.

In the next section, we'll compare the two algorithms on the Rosenbrock function, a classic test function for optimization algorithms.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
5. Rastrigin, L. A. (1974). Systems of extremal control. Mir Publishers.
6. Ackley, D. H. (1987). A connectionist machine for genetic hillclimbing. Kluwer Academic Publishers.
