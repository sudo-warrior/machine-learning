# 2.2.5.3.2.2.2.4.2.2 NAG vs Momentum Simple Functions

## Comparing Nesterov Accelerated Gradient and Standard Momentum on Simple Functions

In this section, we'll empirically compare Nesterov Accelerated Gradient (NAG) and standard momentum on simple functions to understand how the theoretical differences translate into practical performance. We'll focus on convex functions and functions with ravines, which are common in optimization problems.

## Implementation

Let's start by implementing both algorithms for 2D functions:

```python
import numpy as np
import matplotlib.pyplot as plt

# Implement standard momentum for 2D functions
def momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)
```

## Convex Function

Let's first compare the two algorithms on a simple convex function:

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run standard momentum and NAG
path_momentum = momentum_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)
path_nag = nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'b-o', markersize=3, label='Standard Momentum')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='Nesterov Accelerated Gradient')

plt.plot(path_momentum[0, 0], path_momentum[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_momentum[-1, 0], path_momentum[-1, 1], 'bo', markersize=6, label='Momentum Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard Momentum vs. Nesterov Accelerated Gradient on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of Results on Convex Function

When comparing the optimization paths on a convex function, we observe several key differences:

1. **Convergence Speed**: NAG typically converges faster than standard momentum, reaching the vicinity of the minimum in fewer iterations.

2. **Path Smoothness**: NAG's path is often smoother than standard momentum's path, with less oscillation.

3. **Final Accuracy**: NAG typically achieves a more accurate solution (closer to the true minimum) than standard momentum with the same number of iterations.

4. **Approach to Minimum**: As NAG approaches the minimum, it slows down more effectively than standard momentum, reducing the risk of overshooting.

Let's quantify these differences by computing the distance to the minimum for each iteration:

```python
# Compute the distance to the minimum for each iteration
min_point = np.array([0, 0])  # The minimum of the convex function is at (0, 0)
dist_momentum = np.sqrt(np.sum((path_momentum - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.plot(dist_momentum, 'b-', linewidth=2, label='Standard Momentum')
plt.plot(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum')
plt.title('Convergence Speed: Standard Momentum vs. NAG on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Function with a Ravine

Next, let's compare the two algorithms on a function with a ravine, which is a common challenge in optimization:

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run standard momentum and NAG
path_momentum = momentum_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.9, n_iterations=50)
path_nag = nag_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'b-o', markersize=3, label='Standard Momentum')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='Nesterov Accelerated Gradient')

plt.plot(path_momentum[0, 0], path_momentum[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_momentum[-1, 0], path_momentum[-1, 1], 'bo', markersize=6, label='Momentum Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard Momentum vs. Nesterov Accelerated Gradient on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of Results on Ravine Function

When comparing the optimization paths on a function with a ravine, we observe even more pronounced differences:

1. **Oscillation Reduction**: NAG significantly reduces oscillation compared to standard momentum, which tends to bounce back and forth across the ravine.

2. **Directional Stability**: NAG maintains a more stable direction along the ravine, while standard momentum tends to zigzag.

3. **Convergence Speed**: NAG converges much faster than standard momentum in ravines, as it can navigate the narrow valley more effectively.

4. **Approach to Minimum**: NAG approaches the minimum more directly, while standard momentum may overshoot and oscillate around the minimum.

Let's quantify these differences by computing the distance to the minimum for each iteration:

```python
# Compute the distance to the minimum for each iteration
min_point = np.array([0, 0])  # The minimum of the ravine function is at (0, 0)
dist_momentum = np.sqrt(np.sum((path_momentum - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.plot(dist_momentum, 'b-', linewidth=2, label='Standard Momentum')
plt.plot(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum')
plt.title('Convergence Speed: Standard Momentum vs. NAG on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Momentum Parameter

The momentum parameter $\gamma$ is crucial for both algorithms. Let's compare how different momentum values affect their performance:

```python
# Run standard momentum and NAG with different momentum values
momentum_values = [0.5, 0.9, 0.99]
paths_momentum = []
paths_nag = []

for m in momentum_values:
    path_momentum = momentum_2d(4, 4, grad_convex, learning_rate=0.1, momentum=m, n_iterations=50)
    path_nag = nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=m, n_iterations=50)
    paths_momentum.append((m, path_momentum))
    paths_nag.append((m, path_nag))

# Create a 3x2 grid of contour plots
fig, axes = plt.subplots(3, 2, figsize=(15, 20))

for i, m in enumerate(momentum_values):
    # Standard Momentum
    ax = axes[i, 0]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_momentum[i][1]
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label='Standard Momentum')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Standard Momentum (γ = {m})')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[i, 1]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_nag[i][1]
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Nesterov Accelerated Gradient (γ = {m})')
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Momentum Parameter Effect

When comparing the effect of different momentum values on both algorithms, we observe:

1. **Low Momentum (0.5)**: Both algorithms behave similarly with low momentum, with NAG showing a slight advantage in convergence speed.

2. **Medium Momentum (0.9)**: The differences become more pronounced, with NAG showing significantly better convergence and less oscillation.

3. **High Momentum (0.99)**: With high momentum, standard momentum tends to overshoot and oscillate wildly, while NAG maintains better control and converges more effectively.

4. **Sensitivity to Momentum**: NAG is generally less sensitive to the choice of momentum parameter than standard momentum, making it more robust in practice.

## Effect of Learning Rate

The learning rate $\alpha$ is another crucial hyperparameter. Let's compare how different learning rates affect the performance of both algorithms:

```python
# Run standard momentum and NAG with different learning rates
learning_rates = [0.01, 0.1, 0.5]
paths_momentum = []
paths_nag = []

for lr in learning_rates:
    path_momentum = momentum_2d(4, 4, grad_convex, learning_rate=lr, momentum=0.9, n_iterations=50)
    path_nag = nag_2d(4, 4, grad_convex, learning_rate=lr, momentum=0.9, n_iterations=50)
    paths_momentum.append((lr, path_momentum))
    paths_nag.append((lr, path_nag))

# Create a 3x2 grid of contour plots
fig, axes = plt.subplots(3, 2, figsize=(15, 20))

for i, lr in enumerate(learning_rates):
    # Standard Momentum
    ax = axes[i, 0]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_momentum[i][1]
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label='Standard Momentum')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Standard Momentum (α = {lr})')
    ax.grid(True)
    ax.legend()
    
    # NAG
    ax = axes[i, 1]
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    path = paths_nag[i][1]
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label='NAG')
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6, label='Initial Point')
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6, label='Final Point')
    
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Nesterov Accelerated Gradient (α = {lr})')
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
```

### Analysis of Learning Rate Effect

When comparing the effect of different learning rates on both algorithms, we observe:

1. **Low Learning Rate (0.01)**: Both algorithms converge slowly but steadily, with NAG showing a slight advantage.

2. **Medium Learning Rate (0.1)**: Both algorithms converge well, with NAG showing better convergence and less oscillation.

3. **High Learning Rate (0.5)**: Standard momentum tends to overshoot and oscillate, while NAG maintains better control and converges more effectively.

4. **Sensitivity to Learning Rate**: NAG is generally less sensitive to the choice of learning rate than standard momentum, making it more robust in practice.

## Visualizing the "Look-ahead" Step

To better understand the key difference between NAG and standard momentum, let's visualize the "look-ahead" step in NAG:

```python
# Create a contour plot with look-ahead steps
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

# Run NAG for a few iterations to visualize the look-ahead step
x, y = 4, 4
velocity_x, velocity_y = 0, 0
path = [(x, y)]
look_ahead_path = []

for i in range(10):
    # Compute the look-ahead position
    look_ahead_x = x + momentum * velocity_x
    look_ahead_y = y + momentum * velocity_y
    look_ahead_path.append((look_ahead_x, look_ahead_y))
    
    # Compute gradient at the look-ahead position
    grad = grad_convex(look_ahead_x, look_ahead_y)
    
    # Update velocity
    velocity_x = momentum * velocity_x - learning_rate * grad[0]
    velocity_y = momentum * velocity_y - learning_rate * grad[1]
    
    # Update parameters
    x = x + velocity_x
    y = y + velocity_y
    path.append((x, y))

# Convert to numpy arrays
path = np.array(path)
look_ahead_path = np.array(look_ahead_path)

# Plot the path and look-ahead points
plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=6, label='NAG Path')
plt.plot(look_ahead_path[:, 0], look_ahead_path[:, 1], 'b*', markersize=8, label='Look-ahead Points')

# Draw arrows from each point to its look-ahead point
for i in range(len(look_ahead_path)):
    plt.arrow(path[i, 0], path[i, 1], 
              look_ahead_path[i, 0] - path[i, 0], look_ahead_path[i, 1] - path[i, 1],
              head_width=0.1, head_length=0.1, fc='blue', ec='blue', alpha=0.5)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Nesterov Accelerated Gradient - Look-ahead Steps')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of the "Look-ahead" Step

The visualization of the "look-ahead" step reveals:

1. **Anticipatory Behavior**: NAG anticipates where the parameters will be in the next iteration and computes the gradient at this anticipated position.

2. **Direction Adjustment**: This allows NAG to adjust its direction more effectively, especially when approaching a minimum or navigating a ravine.

3. **Smoother Trajectory**: The "look-ahead" step leads to a smoother trajectory with less oscillation.

4. **Earlier Slowdown**: NAG slows down earlier when approaching a minimum, reducing the risk of overshooting.

## Linear Regression Example

Let's also compare the two algorithms on a simple linear regression problem:

```python
# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Implement standard momentum
def momentum(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Compute gradient at the current position
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
    
    return theta, np.array(theta_history), np.array(cost_history)

# Implement Nesterov Accelerated Gradient
def nesterov_accelerated_gradient(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_theta = theta + momentum * velocity
        
        # Compute gradient at the look-ahead position
        gradient = (1/m) * X.T.dot(X.dot(look_ahead_theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run standard momentum and NAG
np.random.seed(42)  # Same initialization for fair comparison
theta_momentum, theta_history_momentum, cost_history_momentum = momentum(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

np.random.seed(42)  # Same initialization for fair comparison
theta_nag, theta_history_nag, cost_history_nag = nesterov_accelerated_gradient(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

# Plot the cost histories
plt.figure(figsize=(10, 6))
plt.plot(cost_history_momentum, 'b-', linewidth=2, label='Standard Momentum')
plt.plot(cost_history_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration: Standard Momentum vs. NAG')
plt.grid(True)
plt.legend()
plt.show()
```

### Analysis of Linear Regression Results

When comparing the two algorithms on a linear regression problem, we observe:

1. **Convergence Speed**: NAG typically converges faster than standard momentum, reaching a lower cost in fewer iterations.

2. **Smoothness of Convergence**: NAG's convergence curve is often smoother than standard momentum's, with less oscillation.

3. **Final Cost**: NAG typically achieves a lower final cost than standard momentum with the same number of iterations.

4. **Parameter Trajectory**: NAG's parameter trajectory is more direct and efficient than standard momentum's.

## Summary

In this section, we've compared Nesterov Accelerated Gradient and standard momentum on simple functions:

1. **Convex Function**: We've seen that NAG converges faster and more smoothly than standard momentum on convex functions.

2. **Ravine Function**: We've observed that NAG significantly reduces oscillation and navigates ravines more effectively than standard momentum.

3. **Effect of Hyperparameters**: We've examined how different momentum values and learning rates affect the performance of both algorithms, finding that NAG is generally more robust to hyperparameter choices.

4. **"Look-ahead" Step**: We've visualized the key difference between NAG and standard momentum, the "look-ahead" step, which allows NAG to anticipate changes and adjust its direction more effectively.

5. **Linear Regression**: We've compared the two algorithms on a simple linear regression problem, confirming NAG's advantages in convergence speed and stability.

Overall, our empirical results on simple functions align with the theoretical advantages of NAG over standard momentum. NAG consistently shows faster convergence, reduced oscillation, and better stability, particularly in challenging scenarios like ravines.

In the next section, we'll extend our comparison to more complex functions to see if these advantages hold in more challenging optimization landscapes.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
