# 2.2.5.3.2.2.2.4.1 NAG vs Standard GD

## Comparing Nesterov Accelerated Gradient with Standard Gradient Descent

In this section, we'll compare Nesterov Accelerated Gradient (NAG) with standard gradient descent (GD) to understand the advantages and limitations of each approach. We'll examine their behavior on various functions, convergence properties, and practical considerations.

## Theoretical Comparison

Before diving into empirical comparisons, let's understand the theoretical differences between NAG and standard GD:

### Convergence Rate

For convex functions:
- **Standard GD**: Converges at a rate of $O(1/t)$, where $t$ is the number of iterations.
- **NAG**: Converges at a rate of $O(1/t^2)$, which is quadratically faster than standard GD.

This theoretical advantage makes NAG particularly attractive for convex optimization problems.

### Update Rules

The update rules for the two algorithms are:

**Standard Gradient Descent**:
$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t)$$

**Nesterov Accelerated Gradient**:
$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t - \gamma v_{t-1})$$
$$\theta_{t+1} = \theta_t - v_t$$

The key difference is that NAG computes the gradient at a "look-ahead" position, which allows it to be more responsive to changes in the gradient.

## Implementation for Comparison

Let's implement both algorithms for a fair comparison:

```python
import numpy as np
import matplotlib.pyplot as plt

# Implement standard gradient descent for 2D functions
def standard_gd_2d(start_x, start_y, grad_func, learning_rate=0.1, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        # Compute gradient at the current position
        grad = grad_func(x, y)
        
        # Update parameters
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        
        path.append((x, y))
    
    return np.array(path)

# Implement Nesterov Accelerated Gradient for 2D functions
def nag_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        
        # Compute gradient at the look-ahead position
        grad = grad_func(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)
```

## Comparison on Different Functions

Let's compare the two algorithms on various functions to understand their behavior in different scenarios:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run standard GD and NAG
path_gd = standard_gd_2d(4, 4, grad_convex, learning_rate=0.1, n_iterations=50)
path_nag = nag_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_gd[:, 0], path_gd[:, 1], 'b-o', markersize=3, label='Standard GD')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='NAG')

plt.plot(path_gd[0, 0], path_gd[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_gd[-1, 0], path_gd[-1, 1], 'bo', markersize=6, label='GD Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard GD vs. NAG on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()

# Compute the distance to the minimum for each iteration
min_point = np.array([0, 0])  # The minimum of the convex function is at (0, 0)
dist_gd = np.sqrt(np.sum((path_gd - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.plot(dist_gd, 'b-', linewidth=2, label='Standard GD')
plt.plot(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum')
plt.title('Convergence Speed: Standard GD vs. NAG on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run standard GD and NAG
path_gd = standard_gd_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, n_iterations=50)
path_nag = nag_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_gd[:, 0], path_gd[:, 1], 'b-o', markersize=3, label='Standard GD')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='NAG')

plt.plot(path_gd[0, 0], path_gd[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_gd[-1, 0], path_gd[-1, 1], 'bo', markersize=6, label='GD Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard GD vs. NAG on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()

# Compute the distance to the minimum for each iteration
min_point = np.array([0, 0])  # The minimum of the ravine function is at (0, 0)
dist_gd = np.sqrt(np.sum((path_gd - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.plot(dist_gd, 'b-', linewidth=2, label='Standard GD')
plt.plot(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum')
plt.title('Convergence Speed: Standard GD vs. NAG on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run standard GD and NAG from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths_gd = [standard_gd_2d(x0, y0, grad_non_convex, learning_rate=0.1, n_iterations=50) for x0, y0 in start_points]
paths_nag = [nag_2d(x0, y0, grad_non_convex, learning_rate=0.1, momentum=0.9, n_iterations=50) for x0, y0 in start_points]

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard GD
ax = axes[0]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_gd):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Gradient Descent')
ax.grid(True)
ax.legend()

# NAG
ax = axes[1]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_nag):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Nesterov Accelerated Gradient')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

### Rosenbrock Function

```python
# Define the Rosenbrock function
def rosenbrock(x, y, a=1, b=100):
    return (a - x)**2 + b * (y - x**2)**2

# Define the gradient of the Rosenbrock function
def grad_rosenbrock(x, y, a=1, b=100):
    df_dx = -2*(a - x) - 4*b*x*(y - x**2)
    df_dy = 2*b*(y - x**2)
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

# Run standard GD and NAG
path_gd = standard_gd_2d(0, 0, grad_rosenbrock, learning_rate=0.0001, n_iterations=1000)
path_nag = nag_2d(0, 0, grad_rosenbrock, learning_rate=0.0001, momentum=0.9, n_iterations=1000)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')  # Log scale for better visualization
plt.colorbar(label='log(f(x, y) + 1) value')

plt.plot(path_gd[:, 0], path_gd[:, 1], 'b-o', markersize=3, label='Standard GD')
plt.plot(path_nag[:, 0], path_nag[:, 1], 'r-o', markersize=3, label='NAG')

plt.plot(path_gd[0, 0], path_gd[0, 1], 'go', markersize=6, label='Initial Point')
plt.plot(path_gd[-1, 0], path_gd[-1, 1], 'bo', markersize=6, label='GD Final')
plt.plot(path_nag[-1, 0], path_nag[-1, 1], 'ro', markersize=6, label='NAG Final')

# Mark the global minimum
plt.plot(1, 1, 'ko', markersize=8, label='Global Minimum (1, 1)')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Standard GD vs. NAG on the Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()

# Compute the distance to the minimum for each iteration
min_point = np.array([1, 1])  # The minimum of the Rosenbrock function is at (1, 1)
dist_gd = np.sqrt(np.sum((path_gd - min_point)**2, axis=1))
dist_nag = np.sqrt(np.sum((path_nag - min_point)**2, axis=1))

# Plot the distance to the minimum vs. iteration
plt.figure(figsize=(10, 6))
plt.plot(dist_gd, 'b-', linewidth=2, label='Standard GD')
plt.plot(dist_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum')
plt.title('Convergence Speed: Standard GD vs. NAG on the Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Comparison on Linear Regression

Let's compare the two algorithms on a simple linear regression problem:

```python
# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Implement standard gradient descent
def standard_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        theta = theta - learning_rate * gradient
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
    
    return theta, np.array(theta_history), np.array(cost_history)

# Implement Nesterov Accelerated Gradient
def nesterov_accelerated_gradient(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        # Compute the look-ahead position
        look_ahead_theta = theta + momentum * velocity
        
        # Compute gradient at the look-ahead position
        gradient = (1/m) * X.T.dot(X.dot(look_ahead_theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run standard GD and NAG
np.random.seed(42)  # Same initialization for fair comparison
theta_gd, theta_history_gd, cost_history_gd = standard_gradient_descent(
    X_b, y, learning_rate=0.01, n_iterations=100)

np.random.seed(42)  # Same initialization for fair comparison
theta_nag, theta_history_nag, cost_history_nag = nesterov_accelerated_gradient(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

# Plot the cost histories
plt.figure(figsize=(10, 6))
plt.plot(cost_history_gd, 'b-', linewidth=2, label='Standard GD')
plt.plot(cost_history_nag, 'r-', linewidth=2, label='NAG')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration: Standard GD vs. NAG')
plt.grid(True)
plt.legend()
plt.show()

# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

plt.plot(theta_history_gd[:, 0, 0], theta_history_gd[:, 1, 0], 'b-o', markersize=3, label='Standard GD')
plt.plot(theta_history_nag[:, 0, 0], theta_history_nag[:, 1, 0], 'r-o', markersize=3, label='NAG')

plt.plot(theta_history_gd[0, 0, 0], theta_history_gd[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history_gd[-1, 0, 0], theta_history_gd[-1, 1, 0], 'bo', markersize=6, label='GD Final')
plt.plot(theta_history_nag[-1, 0, 0], theta_history_nag[-1, 1, 0], 'ro', markersize=6, label='NAG Final')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Optimization Paths: Standard GD vs. NAG')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Learning Rate

The learning rate is a crucial hyperparameter for both algorithms. Let's compare how different learning rates affect their performance:

```python
# Run standard GD and NAG with different learning rates
learning_rates = [0.001, 0.01, 0.1, 0.5]
results_gd = []
results_nag = []

for lr in learning_rates:
    # Standard GD
    np.random.seed(42)  # Same initialization for fair comparison
    theta, theta_history, cost_history = standard_gradient_descent(
        X_b, y, learning_rate=lr, n_iterations=100)
    results_gd.append((lr, theta, theta_history, cost_history))
    
    # NAG
    np.random.seed(42)  # Same initialization for fair comparison
    theta, theta_history, cost_history = nesterov_accelerated_gradient(
        X_b, y, learning_rate=lr, momentum=0.9, n_iterations=100)
    results_nag.append((lr, theta, theta_history, cost_history))

# Plot the cost histories for different learning rates
plt.figure(figsize=(15, 10))

for i, lr in enumerate(learning_rates):
    plt.subplot(2, 2, i+1)
    plt.plot(results_gd[i][3], 'b-', linewidth=2, label='Standard GD')
    plt.plot(results_nag[i][3], 'r-', linewidth=2, label='NAG')
    plt.xlabel('Iteration')
    plt.ylabel('Cost (MSE)')
    plt.title(f'Learning Rate = {lr}')
    plt.grid(True)
    plt.legend()

plt.tight_layout()
plt.show()
```

## Advantages of NAG over Standard GD

Based on our comparisons, we can identify several advantages of NAG over standard GD:

1. **Faster Convergence**: NAG typically converges faster than standard GD, especially for convex functions, due to its improved convergence rate of $O(1/t^2)$ compared to $O(1/t)$ for standard GD.

2. **Reduced Oscillation**: NAG's "look-ahead" approach helps reduce oscillation in ravines, leading to a more direct path to the minimum.

3. **Better Navigation of Complex Landscapes**: NAG is better at navigating complex optimization landscapes, such as those with ravines or multiple local minima.

4. **Improved Stability**: NAG tends to be more stable than standard GD, especially when approaching a minimum, as it can slow down more effectively.

5. **Responsiveness to Changes**: The "look-ahead" step allows NAG to respond more quickly to changes in the gradient, making it more effective for functions with complex landscapes.

## Limitations of NAG Compared to Standard GD

Despite its advantages, NAG also has some limitations compared to standard GD:

1. **Additional Hyperparameter**: NAG introduces an additional hyperparameter (momentum), which needs to be tuned.

2. **Implementation Complexity**: NAG is slightly more complex to implement than standard GD.

3. **Potential Overshooting**: With high momentum, NAG might overshoot the minimum and take longer to converge.

4. **Memory Requirements**: NAG requires storing the velocity vector, which increases memory usage (though usually negligible).

## When to Use NAG vs. Standard GD

Based on our comparisons, here are some guidelines for when to use NAG vs. standard GD:

**Use Standard GD when**:
- The problem is simple and well-conditioned
- You want a simpler implementation with fewer hyperparameters
- Memory is extremely limited

**Use NAG when**:
- The problem has a complex optimization landscape (e.g., ravines, multiple local minima)
- Faster convergence is important
- The problem is convex and you want to leverage the theoretical guarantees
- You're willing to tune an additional hyperparameter (momentum)

## Summary

In this section, we've compared Nesterov Accelerated Gradient with standard gradient descent:

1. **Theoretical Comparison**: We've examined the theoretical differences between the two algorithms, including their convergence rates and update rules.

2. **Empirical Comparison**: We've visualized their behavior on various functions, including convex functions, functions with ravines, non-convex functions, and the Rosenbrock function.

3. **Linear Regression**: We've compared their performance on a simple linear regression problem.

4. **Effect of Learning Rate**: We've examined how different learning rates affect their performance.

5. **Advantages and Limitations**: We've discussed the advantages and limitations of NAG compared to standard GD.

6. **Usage Guidelines**: We've provided guidelines for when to use each algorithm.

Overall, NAG offers significant advantages over standard GD, particularly for complex optimization problems, at the cost of slightly increased complexity and an additional hyperparameter to tune.

In the next section, we'll compare NAG with standard momentum to understand the benefits of the "look-ahead" approach.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
