# 2.2.5.3.2.2.2.4.2.3.3.2.2 Basic Saddle Comparison

## Comparing NAG and Momentum on the Basic Saddle Function

In this section, we'll compare Nesterov Accelerated Gradient (NAG) and standard momentum on the basic saddle function $f(x, y) = x^2 - y^2$. This is the simplest example of a function with a saddle point, and it provides a clear illustration of how different optimization algorithms behave around saddle points.

## The Basic Saddle Function

The basic saddle function is defined as:

$$f(x, y) = x^2 - y^2$$

This function has a saddle point at $(0, 0)$ with a function value of $0$. The function curves upward in the $x$ direction and downward in the $y$ direction, creating a classic saddle shape.

The gradient of this function is:

$$\nabla f(x, y) = \begin{bmatrix} 2x \\ -2y \end{bmatrix}$$

Let's first visualize this function to understand its landscape:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define the basic saddle function
def basic_saddle(x, y):
    return x**2 - y**2

# Define the gradient of the basic saddle function
def grad_basic_saddle(x, y):
    return np.array([2*x, -2*y])

# Create a 3D surface plot of the basic saddle function
def plot_basic_saddle_3d():
    # Create a grid of x and y values
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x, y)
    Z = basic_saddle(X, Y)
    
    # Create a 3D surface plot
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('f(x, y)')
    ax.set_title('Basic Saddle Function: f(x, y) = x² - y²')
    plt.show()

# Create a contour plot of the basic saddle function
def plot_basic_saddle_contour():
    # Create a grid of x and y values
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x, y)
    Z = basic_saddle(X, Y)
    
    # Create a contour plot
    plt.figure(figsize=(10, 8))
    contour = plt.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(label='f(x, y) value')
    plt.plot(0, 0, 'ro', markersize=8, label='Saddle Point (0, 0)')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Contour Plot of Basic Saddle Function')
    plt.grid(True)
    plt.legend()
    plt.show()

# Visualize the basic saddle function
plot_basic_saddle_3d()
plot_basic_saddle_contour()
```

## Comparing Optimization Paths

Now, let's compare the optimization paths of standard gradient descent, gradient descent with momentum, and Nesterov Accelerated Gradient on the basic saddle function. We'll start from different initial points near the saddle point to see how each algorithm behaves.

```python
# Import the optimization algorithms and visualization functions from the previous section
# (Assuming they are defined in a module called 'saddle_optimization')
from saddle_optimization import (
    gradient_descent_2d, momentum_2d, nag_2d,
    plot_optimization_paths, compute_distance_to_saddle,
    plot_distance_to_saddle, compute_function_values,
    plot_function_values, analyze_escape_behavior
)

# Define starting points near the saddle point
start_points = [
    (0.1, 0.1),   # Quadrant 1
    (-0.1, 0.1),  # Quadrant 2
    (-0.1, -0.1), # Quadrant 3
    (0.1, -0.1)   # Quadrant 4
]

# Run the optimization algorithms and visualize the paths
paths_gd, paths_momentum, paths_nag = plot_optimization_paths(
    basic_saddle, grad_basic_saddle, 
    x_range=(-2, 2), y_range=(-2, 2),
    start_points=start_points,
    learning_rate=0.1, momentum=0.9, n_iterations=50,
    title='Optimization Paths on Basic Saddle Function'
)
```

### Analysis of Optimization Paths

When comparing the optimization paths on the basic saddle function, we observe several key differences:

1. **Gradient Descent**: Standard gradient descent moves directly away from the saddle point along the directions of negative curvature (the $y$ axis) and towards the saddle point along the directions of positive curvature (the $x$ axis). This results in paths that initially move away from the saddle point in the $y$ direction but may get stuck or progress very slowly in the $x$ direction.

2. **Momentum**: Gradient descent with momentum also moves away from the saddle point, but the accumulated momentum helps it maintain progress in both the $x$ and $y$ directions. This results in paths that escape the saddle point more efficiently than standard gradient descent.

3. **NAG**: Nesterov Accelerated Gradient shows the most efficient escape from the saddle point. The "look-ahead" step allows it to anticipate changes in the gradient, resulting in paths that escape the saddle point more directly and with less oscillation.

Let's quantify these differences by analyzing the distance to the saddle point and the function values along the optimization paths:

```python
# Compute the distance to the saddle point for each iteration
distances_gd = compute_distance_to_saddle(paths_gd)
distances_momentum = compute_distance_to_saddle(paths_momentum)
distances_nag = compute_distance_to_saddle(paths_nag)

# Plot the distance to the saddle point vs. iteration
plot_distance_to_saddle(
    distances_gd, distances_momentum, distances_nag,
    title='Distance to Saddle Point vs. Iteration'
)

# Compute the function values along the optimization paths
values_gd = compute_function_values(paths_gd, basic_saddle)
values_momentum = compute_function_values(paths_momentum, basic_saddle)
values_nag = compute_function_values(paths_nag, basic_saddle)

# Plot the function values vs. iteration
plot_function_values(
    values_gd, values_momentum, values_nag,
    title='Function Value vs. Iteration'
)

# Analyze the escape behavior from the saddle point
iterations_to_escape_gd, iterations_to_escape_momentum, iterations_to_escape_nag = analyze_escape_behavior(
    paths_gd, paths_momentum, paths_nag,
    threshold=0.5
)
```

### Analysis of Escape Behavior

The analysis of escape behavior reveals:

1. **Escape Speed**: NAG typically escapes the saddle point faster than standard momentum, which in turn escapes faster than standard gradient descent. This is reflected in the average number of iterations required to reach a certain distance from the saddle point.

2. **Escape Direction**: All three algorithms tend to escape the saddle point primarily along the direction of negative curvature (the $y$ axis) initially. However, NAG and momentum are better at making progress in the direction of positive curvature (the $x$ axis) as well.

3. **Function Value Progression**: The function values along the paths show that NAG typically achieves more negative function values (indicating better progress in the $y$ direction) in fewer iterations than the other algorithms.

## Effect of Learning Rate

The learning rate is a crucial hyperparameter for all three algorithms. Let's analyze how different learning rates affect their performance on the basic saddle function:

```python
# Define learning rates to analyze
learning_rates = [0.01, 0.1, 0.5]

# Analyze the effect of learning rate
results_gd, results_momentum, results_nag = analyze_learning_rate_effect(
    basic_saddle, grad_basic_saddle,
    start_point=(0.1, 0.1),
    learning_rates=learning_rates,
    momentum=0.9, n_iterations=50,
    title='Effect of Learning Rate on Optimization Paths'
)
```

### Analysis of Learning Rate Effect

When comparing the effect of different learning rates on the three algorithms, we observe:

1. **Low Learning Rate (0.01)**: All three algorithms make slow progress away from the saddle point. NAG shows a slight advantage in terms of escape speed, but the difference is not dramatic.

2. **Medium Learning Rate (0.1)**: This is often a good balance for all three algorithms. NAG shows a more significant advantage in escape speed and directness of the path.

3. **High Learning Rate (0.5)**: With a high learning rate, standard gradient descent may become unstable and oscillate, especially in the direction of negative curvature. Momentum and NAG are more robust to higher learning rates, with NAG showing the most stable behavior.

4. **Sensitivity to Learning Rate**: NAG is generally less sensitive to the choice of learning rate than standard momentum or gradient descent, making it more robust in practice.

## Effect of Momentum Parameter

The momentum parameter $\gamma$ is another crucial hyperparameter for both momentum and NAG. Let's analyze how different momentum values affect their performance:

```python
# Define momentum values to analyze
momentum_values = [0.5, 0.9, 0.99]

# Analyze the effect of momentum parameter
results_momentum, results_nag = analyze_momentum_effect(
    basic_saddle, grad_basic_saddle,
    start_point=(0.1, 0.1),
    momentum_values=momentum_values,
    learning_rate=0.1, n_iterations=50,
    title='Effect of Momentum Parameter on Optimization Paths'
)
```

### Analysis of Momentum Parameter Effect

When comparing the effect of different momentum values on both algorithms, we observe:

1. **Low Momentum (0.5)**: Both algorithms show moderate improvement over standard gradient descent, with NAG having a slight edge in escape speed.

2. **Medium Momentum (0.9)**: This is often a good balance for both algorithms. NAG shows a more significant advantage in escape speed and directness of the path.

3. **High Momentum (0.99)**: With high momentum, both algorithms escape the saddle point very quickly, but may overshoot and oscillate. NAG's "look-ahead" approach helps it maintain better control even with high momentum.

4. **Escape Direction**: Higher momentum values help both algorithms make better progress in the direction of positive curvature (the $x$ axis), which is challenging for standard gradient descent.

## Visualizing the "Look-ahead" Step

To better understand why NAG performs better on the basic saddle function, let's visualize the "look-ahead" step:

```python
# Create a contour plot with look-ahead steps
def visualize_look_ahead_steps():
    # Create a grid of x and y values
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(x, y)
    Z = basic_saddle(X, Y)
    
    plt.figure(figsize=(10, 8))
    contour = plt.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(label='f(x, y) value')
    
    # Run NAG for a few iterations to visualize the look-ahead step
    x, y = 0.1, 0.1
    velocity_x, velocity_y = 0, 0
    momentum = 0.9
    learning_rate = 0.1
    path = [(x, y)]
    look_ahead_path = []
    
    for i in range(10):
        # Compute the look-ahead position
        look_ahead_x = x + momentum * velocity_x
        look_ahead_y = y + momentum * velocity_y
        look_ahead_path.append((look_ahead_x, look_ahead_y))
        
        # Compute gradient at the look-ahead position
        grad = grad_basic_saddle(look_ahead_x, look_ahead_y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        path.append((x, y))
    
    # Convert to numpy arrays
    path = np.array(path)
    look_ahead_path = np.array(look_ahead_path)
    
    # Plot the path and look-ahead points
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=6, label='NAG Path')
    plt.plot(look_ahead_path[:, 0], look_ahead_path[:, 1], 'b*', markersize=8, label='Look-ahead Points')
    
    # Draw arrows from each point to its look-ahead point
    for i in range(len(look_ahead_path)):
        plt.arrow(path[i, 0], path[i, 1], 
                  look_ahead_path[i, 0] - path[i, 0], look_ahead_path[i, 1] - path[i, 1],
                  head_width=0.05, head_length=0.05, fc='blue', ec='blue', alpha=0.5)
    
    plt.plot(0, 0, 'ko', markersize=8, label='Saddle Point (0, 0)')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Nesterov Accelerated Gradient - Look-ahead Steps')
    plt.grid(True)
    plt.legend()
    plt.show()

# Visualize the look-ahead steps
visualize_look_ahead_steps()
```

### Analysis of the "Look-ahead" Step

The visualization of the "look-ahead" step on the basic saddle function reveals:

1. **Anticipatory Behavior**: NAG anticipates where the parameters will be in the next iteration and computes the gradient at this anticipated position. This is particularly beneficial near the saddle point, where the gradient changes significantly as we move away from the point.

2. **Direction Adjustment**: The "look-ahead" step allows NAG to adjust its direction more effectively, especially in the direction of negative curvature (the $y$ axis), leading to faster escape from the saddle point.

3. **Momentum Utilization**: NAG makes more efficient use of the momentum by computing the gradient at a position that takes into account the current velocity, resulting in more informed updates.

4. **Reduced Oscillation**: By anticipating the next position, NAG can reduce oscillation, especially with high momentum values, leading to more stable optimization.

## Mathematical Analysis

Let's analyze the behavior of the three algorithms on the basic saddle function mathematically:

For the basic saddle function $f(x, y) = x^2 - y^2$, the gradient is $\nabla f(x, y) = (2x, -2y)$.

### Standard Gradient Descent

The update rule for standard gradient descent is:

$$x_{t+1} = x_t - \alpha \cdot 2x_t = (1 - 2\alpha) x_t$$
$$y_{t+1} = y_t - \alpha \cdot (-2y_t) = (1 + 2\alpha) y_t$$

For $\alpha > 0$, we have:
- If $|1 - 2\alpha| < 1$ (i.e., $0 < \alpha < 1$), then $x_t$ converges to 0.
- Since $1 + 2\alpha > 1$, $y_t$ grows exponentially, moving away from the saddle point.

This explains why gradient descent moves towards the saddle point in the $x$ direction and away from it in the $y$ direction.

### Gradient Descent with Momentum

The update rule for gradient descent with momentum is:

$$v_t^x = \gamma v_{t-1}^x - \alpha \cdot 2x_t$$
$$v_t^y = \gamma v_{t-1}^y - \alpha \cdot (-2y_t) = \gamma v_{t-1}^y + \alpha \cdot 2y_t$$
$$x_{t+1} = x_t + v_t^x$$
$$y_{t+1} = y_t + v_t^y$$

The accumulated momentum in the $y$ direction helps the algorithm escape the saddle point more efficiently. Even if $y_t$ is small, the velocity $v_t^y$ can grow over time, leading to faster escape.

### Nesterov Accelerated Gradient

The update rule for Nesterov Accelerated Gradient is:

$$v_t^x = \gamma v_{t-1}^x - \alpha \cdot 2(x_t + \gamma v_{t-1}^x)$$
$$v_t^y = \gamma v_{t-1}^y - \alpha \cdot (-2)(y_t + \gamma v_{t-1}^y) = \gamma v_{t-1}^y + \alpha \cdot 2(y_t + \gamma v_{t-1}^y)$$
$$x_{t+1} = x_t + v_t^x$$
$$y_{t+1} = y_t + v_t^y$$

The "look-ahead" term $(x_t + \gamma v_{t-1}^x, y_t + \gamma v_{t-1}^y)$ allows NAG to compute the gradient at a position that anticipates the next update. This provides more accurate gradient information, especially in the direction of negative curvature (the $y$ axis), leading to faster escape from the saddle point.

## Summary

In this section, we've compared Nesterov Accelerated Gradient and standard momentum on the basic saddle function $f(x, y) = x^2 - y^2$:

1. **Optimization Paths**: We've visualized and analyzed the optimization paths of standard gradient descent, gradient descent with momentum, and Nesterov Accelerated Gradient, showing that NAG escapes the saddle point more efficiently.

2. **Escape Behavior**: We've quantified the escape behavior of the three algorithms, demonstrating that NAG typically requires fewer iterations to escape the saddle point.

3. **Effect of Hyperparameters**: We've examined how different learning rates and momentum values affect the performance of the algorithms, finding that NAG is generally more robust to hyperparameter choices.

4. **"Look-ahead" Advantage**: We've visualized and analyzed the key difference between NAG and standard momentum, the "look-ahead" step, which allows NAG to anticipate changes in the gradient and adjust its direction more effectively.

5. **Mathematical Analysis**: We've provided a mathematical analysis of the behavior of the three algorithms on the basic saddle function, explaining why NAG performs better.

The basic saddle function provides a clear illustration of the advantages of NAG over standard momentum for escaping saddle points. In the next section, we'll extend our comparison to more complex saddle functions to see if these advantages hold in more challenging scenarios.

## References

1. Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems (pp. 2933-2941).
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., & Jordan, M. I. (2017). How to escape saddle points efficiently. In International Conference on Machine Learning (pp. 1724-1732).
4. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
5. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
