# 2.2.5.3.2.2.2.4.2.1 NAG vs Momentum Theory

## Theoretical Comparison Between Nesterov Accelerated Gradient and Standard Momentum

In this section, we'll explore the theoretical differences between Nesterov Accelerated Gradient (NAG) and standard momentum. Both methods build upon the basic gradient descent algorithm by incorporating information from previous updates, but they do so in subtly different ways that lead to significant differences in performance.

## Update Rules

Let's start by comparing the update rules for both algorithms:

### Standard Momentum

The update rule for standard momentum is:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

where:
- $v_t$ is the velocity vector (initially $v_0 = 0$)
- $\gamma$ is the momentum parameter (typically around 0.9)
- $\alpha$ is the learning rate
- $\nabla_\theta J(\theta_t)$ is the gradient of the cost function $J$ with respect to $\theta$ at the point $\theta_t$

### Nesterov Accelerated Gradient

The update rule for Nesterov Accelerated Gradient is:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t - \gamma v_{t-1})$$
$$\theta_{t+1} = \theta_t - v_t$$

The key difference is that NAG computes the gradient at the "look-ahead" position $\theta_t - \gamma v_{t-1}$ rather than at the current position $\theta_t$.

### Alternative Formulation of NAG

An alternative, equivalent formulation of NAG that is often used in practice is:

$$\tilde{\theta}_t = \theta_t + \gamma (\theta_t - \theta_{t-1})$$
$$\theta_{t+1} = \tilde{\theta}_t - \alpha \nabla_\theta J(\tilde{\theta}_t)$$

where $\tilde{\theta}_t$ is the "look-ahead" position. This formulation makes the "look-ahead" step more explicit.

## The "Look-ahead" Advantage

The key theoretical advantage of NAG over standard momentum is the "look-ahead" step. This allows NAG to:

1. **Anticipate Changes**: By computing the gradient at the "look-ahead" position, NAG can anticipate changes in the gradient and adjust its direction accordingly.

2. **Respond More Quickly**: NAG can respond more quickly to changes in the gradient, making it more effective for functions with complex landscapes.

3. **Slow Down Earlier**: When approaching a minimum, NAG can slow down earlier than standard momentum, reducing the risk of overshooting.

## Convergence Rates

For convex functions, both standard momentum and NAG have improved convergence rates compared to standard gradient descent:

- **Standard Gradient Descent**: $O(1/t)$
- **Standard Momentum**: $O(1/t)$ with improved constants
- **Nesterov Accelerated Gradient**: $O(1/t^2)$

This means that NAG converges quadratically faster than both standard gradient descent and standard momentum for convex optimization problems.

## Intuitive Understanding

To understand the difference between standard momentum and NAG intuitively, consider the following analogy:

Imagine a ball rolling down a hill:

- **Standard Momentum**: The ball accumulates velocity as it rolls down, which helps it move faster. However, if the slope changes suddenly (e.g., the hill flattens out or curves upward), the ball will continue with its accumulated momentum before eventually responding to the new slope.

- **Nesterov Accelerated Gradient**: The ball first makes a tentative move based on its current velocity, then checks the slope at this new position, and adjusts its direction accordingly. This allows the ball to be more responsive to changes in the terrain, slowing down earlier when approaching a minimum or changing direction more quickly when needed.

## Mathematical Insight

To gain deeper mathematical insight into the difference between standard momentum and NAG, let's consider a simple quadratic function:

$$f(\theta) = \frac{1}{2} \theta^T A \theta - b^T \theta + c$$

where $A$ is a positive definite matrix, $b$ is a vector, and $c$ is a constant.

The gradient of this function is:

$$\nabla f(\theta) = A \theta - b$$

For standard momentum, the update rule becomes:

$$v_t = \gamma v_{t-1} + \alpha (A \theta_t - b)$$
$$\theta_{t+1} = \theta_t - v_t$$

For NAG, the update rule becomes:

$$v_t = \gamma v_{t-1} + \alpha (A (\theta_t - \gamma v_{t-1}) - b)$$
$$\theta_{t+1} = \theta_t - v_t$$

Expanding the NAG update:

$$v_t = \gamma v_{t-1} + \alpha (A \theta_t - \gamma A v_{t-1} - b)$$
$$\theta_{t+1} = \theta_t - v_t$$

Comparing the two updates, we can see that NAG includes an additional term $-\gamma A v_{t-1}$ in the gradient computation. This term allows NAG to take into account the curvature of the function when computing the gradient, leading to more informed updates.

## Theoretical Guarantees

NAG provides stronger theoretical guarantees than standard momentum for convex optimization problems:

1. **Optimal Convergence Rate**: For smooth convex functions, NAG achieves the optimal convergence rate of $O(1/t^2)$, which is the best possible rate for first-order methods.

2. **Robustness to Ill-Conditioning**: NAG is more robust to ill-conditioning (large condition number of the Hessian) than standard momentum.

3. **Stability Near Minima**: NAG exhibits better stability near minima, reducing the risk of overshooting.

## Relationship to Second-Order Methods

Both standard momentum and NAG can be viewed as approximations to second-order methods (methods that use the Hessian or an approximation of it):

- **Standard Momentum**: Can be viewed as a crude approximation to the Newton method, where the momentum term provides some information about the curvature of the function.

- **Nesterov Accelerated Gradient**: Provides a better approximation to second-order information by computing the gradient at the "look-ahead" position, which allows it to capture more information about the curvature of the function.

## Theoretical Limitations

Despite its advantages, NAG also has some theoretical limitations:

1. **Non-convex Functions**: The theoretical guarantees for NAG primarily apply to convex functions. For non-convex functions (like those in deep learning), the theoretical advantages are less clear.

2. **Stochastic Settings**: In stochastic settings (like mini-batch gradient descent), the theoretical advantages of NAG over standard momentum are less pronounced.

3. **Hyperparameter Sensitivity**: NAG can be more sensitive to the choice of hyperparameters (learning rate and momentum) than standard momentum.

## Summary

In this section, we've explored the theoretical differences between Nesterov Accelerated Gradient and standard momentum:

1. **Update Rules**: We've compared the update rules for both algorithms, highlighting the key difference: NAG computes the gradient at a "look-ahead" position.

2. **The "Look-ahead" Advantage**: We've discussed how the "look-ahead" step allows NAG to anticipate changes, respond more quickly, and slow down earlier.

3. **Convergence Rates**: We've noted that NAG achieves a better convergence rate ($O(1/t^2)$) than standard momentum ($O(1/t)$ with improved constants) for convex functions.

4. **Intuitive Understanding**: We've provided an intuitive analogy to understand the difference between the two algorithms.

5. **Mathematical Insight**: We've delved deeper into the mathematical differences between the algorithms, showing how NAG takes into account the curvature of the function.

6. **Theoretical Guarantees and Limitations**: We've discussed the theoretical guarantees and limitations of NAG compared to standard momentum.

In the next sections, we'll empirically compare NAG and standard momentum on various functions to see how these theoretical differences translate into practical performance.

## References

1. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$. Doklady ANSSSR, 269, 543-547.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
5. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
