# 2.2.5.3.2.2.1 Gradient Descent with Momentum

## Introduction to Momentum

Gradient Descent with Momentum is an extension of the standard gradient descent algorithm that helps accelerate convergence and reduce oscillation. It was introduced to address two key challenges in gradient descent:

1. **Slow convergence** in regions where the gradient is small
2. **Oscillation** in ravines, where the surface curves much more steeply in one dimension than in another

The momentum method introduces a velocity term that accumulates the gradient of past steps to determine the direction to move in the current step. This is analogous to a ball rolling down a hill, which accumulates momentum as it descends, helping it move faster and avoid getting stuck in local depressions.

## The Mathematics of Momentum

The update rule for gradient descent with momentum is:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

where:
- $v_t$ is the velocity vector (initially $v_0 = 0$)
- $\gamma$ is the momentum parameter (typically around 0.9)
- $\alpha$ is the learning rate
- $\nabla_\theta J(\theta_t)$ is the gradient of the cost function $J$ with respect to $\theta$ at the point $\theta_t$

The momentum parameter $\gamma$ determines how much of the past velocity is preserved. A higher value gives more weight to past gradients, resulting in more momentum.

## Visualizing Gradient Descent with Momentum

Let's visualize how gradient descent with momentum works on a simple linear regression problem:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Compute cost (MSE)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

# Implement gradient descent with momentum
def gradient_descent_with_momentum(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    velocity = np.zeros_like(theta)  # Initialize velocity
    cost_history = []
    theta_history = [theta.copy()]
    velocity_history = [velocity.copy()]
    
    for iteration in range(n_iterations):
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradient
        
        # Update parameters
        theta = theta + velocity
        
        # Store cost, theta, and velocity
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        velocity_history.append(velocity.copy())
        
        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(velocity_history), np.array(cost_history)

# Run gradient descent with momentum
theta_final, theta_history, velocity_history, cost_history = gradient_descent_with_momentum(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

print(f"Final parameters: theta_0 = {theta_final[0][0]}, theta_1 = {theta_final[1][0]}")
print(f"Final cost: {cost_history[-1]}")
```

### Visualizing the Cost Function

Let's visualize how the cost function changes during the optimization process:

```python
# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history, 'b-')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration in Gradient Descent with Momentum')
plt.grid(True)
plt.show()
```

### Visualizing the Regression Line

We can visualize how the regression line evolves during the optimization process:

```python
# Create a function to plot the regression line
def plot_regression_line(X, y, theta, iteration, cost):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 1], y, alpha=0.7)
    
    # Plot the regression line
    x_range = np.array([[0], [2]])
    x_range_b = np.c_[np.ones((2, 1)), x_range]
    y_pred = x_range_b.dot(theta)
    
    plt.plot(x_range, y_pred, 'r-', linewidth=2, 
             label=f'y = {theta[0][0]:.4f} + {theta[1][0]:.4f}x')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Gradient Descent with Momentum - Iteration {iteration}, Cost: {cost:.4f}')
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the initial, middle, and final regression lines
plot_regression_line(X_b, y, theta_history[0], 0, compute_cost(X_b, y, theta_history[0]))
plot_regression_line(X_b, y, theta_history[50], 50, compute_cost(X_b, y, theta_history[50]))
plot_regression_line(X_b, y, theta_history[-1], 100, compute_cost(X_b, y, theta_history[-1]))
```

### Visualizing in Parameter Space

We can visualize the optimization path in the parameter space (theta_0, theta_1):

```python
# Create a grid of theta_0 and theta_1 values
theta_0 = np.linspace(0, 8, 100)
theta_1 = np.linspace(0, 6, 100)
Theta_0, Theta_1 = np.meshgrid(theta_0, theta_1)

# Compute cost for each combination of theta_0 and theta_1
J = np.zeros_like(Theta_0)
for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        theta = np.array([[Theta_0[j, i]], [Theta_1[j, i]]])
        J[j, i] = compute_cost(X_b, y, theta)

# Create a contour plot with the optimization path
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path
plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')
plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'go', markersize=6, label='Initial Parameters')
plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'bo', markersize=6, label='Final Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Gradient Descent with Momentum in Parameter Space')
plt.grid(True)
plt.legend()
plt.show()
```

### Visualizing Velocity Vectors

One of the key aspects of momentum is the velocity vector. Let's visualize how the velocity changes during the optimization process:

```python
# Create a contour plot with velocity vectors
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization path
plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'r-o', markersize=3, label='Optimization Path')

# Plot velocity vectors at selected points
for i in range(0, len(theta_history), 10):
    plt.arrow(theta_history[i, 0, 0], theta_history[i, 1, 0], 
              velocity_history[i, 0, 0], velocity_history[i, 1, 0], 
              head_width=0.1, head_length=0.1, fc='blue', ec='blue', alpha=0.5)

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Gradient Descent with Momentum - Velocity Vectors')
plt.grid(True)
plt.legend()
plt.show()
```

### Animating the Optimization Process

We can create an animation to visualize how the optimization process evolves over time:

```python
# Create an animation of the optimization process
def create_momentum_animation(X, y, theta_history, velocity_history, cost_history):
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create a contour plot
    contour = ax.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='Cost (MSE)')
    
    # Initialize the point and path
    point, = ax.plot([], [], 'ro', markersize=6)
    path, = ax.plot([], [], 'r-', linewidth=2)
    
    # Initialize the velocity vector
    velocity_arrow = ax.arrow(0, 0, 0, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')
    
    # Set up the plot
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    ax.set_title('Gradient Descent with Momentum')
    ax.grid(True)
    
    # Animation initialization function
    def init():
        point.set_data([], [])
        path.set_data([], [])
        velocity_arrow.set_visible(False)
        return point, path, velocity_arrow
    
    # Animation update function
    def update(frame):
        # Update point and path
        point.set_data([theta_history[frame, 0, 0]], [theta_history[frame, 1, 0]])
        path.set_data(theta_history[:frame+1, 0, 0], theta_history[:frame+1, 1, 0])
        
        # Remove old velocity arrow and create a new one
        velocity_arrow.remove()
        velocity_arrow = ax.arrow(theta_history[frame, 0, 0], theta_history[frame, 1, 0], 
                                 velocity_history[frame, 0, 0], velocity_history[frame, 1, 0], 
                                 head_width=0.1, head_length=0.1, fc='blue', ec='blue', alpha=0.5)
        
        # Update title
        ax.set_title(f'Gradient Descent with Momentum - Iteration {frame}, Cost: {cost_history[frame]:.4f}')
        
        return point, path, velocity_arrow
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(len(theta_history)),
                        init_func=init, blit=True, interval=100)
    
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_momentum_animation(X_b, y, theta_history, velocity_history, cost_history)
# To save the animation: ani.save('gradient_descent_with_momentum.gif', writer='pillow', fps=10)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Comparing Gradient Descent With and Without Momentum

Let's compare gradient descent with and without momentum to see the benefits of adding momentum:

```python
# Implement standard gradient descent (without momentum)
def standard_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    m = len(y)
    theta = np.random.randn(2, 1)  # Random initialization
    cost_history = []
    theta_history = [theta.copy()]
    
    for iteration in range(n_iterations):
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        theta = theta - learning_rate * gradient
        
        # Store cost and theta
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
        theta_history.append(theta.copy())
        
        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}, Cost: {cost}")
    
    return theta, np.array(theta_history), np.array(cost_history)

# Run standard gradient descent
np.random.seed(42)  # Same initialization for fair comparison
theta_std, theta_history_std, cost_history_std = standard_gradient_descent(
    X_b, y, learning_rate=0.01, n_iterations=100)

# Run gradient descent with momentum
np.random.seed(42)  # Same initialization for fair comparison
theta_momentum, theta_history_momentum, velocity_history_momentum, cost_history_momentum = gradient_descent_with_momentum(
    X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

# Plot the cost histories
plt.figure(figsize=(10, 6))
plt.plot(cost_history_std, 'b-', linewidth=2, label='Standard Gradient Descent')
plt.plot(cost_history_momentum, 'r-', linewidth=2, label='Gradient Descent with Momentum')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration: Standard GD vs. GD with Momentum')
plt.grid(True)
plt.legend()
plt.show()

# Create a contour plot with both optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

# Plot the optimization paths
plt.plot(theta_history_std[:, 0, 0], theta_history_std[:, 1, 0], 'b-o', markersize=3, label='Standard GD')
plt.plot(theta_history_momentum[:, 0, 0], theta_history_momentum[:, 1, 0], 'r-o', markersize=3, label='GD with Momentum')

plt.plot(theta_history_std[0, 0, 0], theta_history_std[0, 1, 0], 'ko', markersize=6, label='Initial Parameters')
plt.plot(theta_history_std[-1, 0, 0], theta_history_std[-1, 1, 0], 'bo', markersize=6, label='Standard GD Final')
plt.plot(theta_history_momentum[-1, 0, 0], theta_history_momentum[-1, 1, 0], 'ro', markersize=6, label='Momentum Final')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Optimization Paths: Standard GD vs. GD with Momentum')
plt.grid(True)
plt.legend()
plt.show()
```

## Effect of Momentum Parameter

The momentum parameter $\gamma$ controls how much of the past velocity is preserved. Let's visualize the effect of different momentum values:

```python
# Run gradient descent with different momentum values
momentum_values = [0.0, 0.5, 0.9, 0.99]
results = []

for momentum in momentum_values:
    np.random.seed(42)  # Same initialization for fair comparison
    theta, theta_history, velocity_history, cost_history = gradient_descent_with_momentum(
        X_b, y, learning_rate=0.01, momentum=momentum, n_iterations=100)
    results.append((momentum, theta, theta_history, cost_history))

# Plot the cost histories
plt.figure(figsize=(10, 6))

for momentum, _, _, cost_history in results:
    label = "Standard GD" if momentum == 0.0 else f"Momentum = {momentum}"
    plt.plot(cost_history, linewidth=2, label=label)

plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration for Different Momentum Values')
plt.grid(True)
plt.legend()
plt.show()

# Create a contour plot with optimization paths for different momentum values
plt.figure(figsize=(10, 8))
contour = plt.contour(Theta_0, Theta_1, J, 30, cmap='viridis')
plt.colorbar(label='Cost (MSE)')

for momentum, _, theta_history, _ in results:
    label = "Standard GD" if momentum == 0.0 else f"Momentum = {momentum}"
    plt.plot(theta_history[:, 0, 0], theta_history[:, 1, 0], 'o-', markersize=3, label=label)
    plt.plot(theta_history[-1, 0, 0], theta_history[-1, 1, 0], 'o', markersize=6)

plt.plot(theta_history[0, 0, 0], theta_history[0, 1, 0], 'ko', markersize=6, label='Initial Parameters')

plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.title('Optimization Paths for Different Momentum Values')
plt.grid(True)
plt.legend()
plt.show()
```

## Gradient Descent with Momentum on Different Functions

Let's visualize how gradient descent with momentum behaves on different types of functions:

### Convex Function

```python
# Define a convex function
def convex_function(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_convex(x, y):
    return np.array([2*x, 2*y])

# Implement gradient descent with momentum for 2D functions
def gd_momentum_2d(start_x, start_y, grad_func, learning_rate=0.1, momentum=0.9, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    velocity_x, velocity_y = 0, 0
    
    for _ in range(n_iterations):
        grad = grad_func(x, y)
        
        # Update velocity
        velocity_x = momentum * velocity_x - learning_rate * grad[0]
        velocity_y = momentum * velocity_y - learning_rate * grad[1]
        
        # Update parameters
        x = x + velocity_x
        y = y + velocity_y
        
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = convex_function(X, Y)

# Run gradient descent with and without momentum
path_std = gd_momentum_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.0, n_iterations=50)
path_momentum = gd_momentum_2d(4, 4, grad_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_std[:, 0], path_std[:, 1], 'b-o', markersize=3, label='Standard GD')
plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'r-o', markersize=3, label='GD with Momentum')

plt.plot(path_std[0, 0], path_std[0, 1], 'ko', markersize=6, label='Initial Point')
plt.plot(path_std[-1, 0], path_std[-1, 1], 'bo', markersize=6, label='Standard GD Final')
plt.plot(path_momentum[-1, 0], path_momentum[-1, 1], 'ro', markersize=6, label='Momentum Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent with and without Momentum on a Convex Function')
plt.grid(True)
plt.legend()
plt.show()
```

### Function with a Ravine

```python
# Define a function with a ravine
def ravine_function(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_ravine(x, y):
    return np.array([2*x, 200*y])

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = ravine_function(X, Y)

# Run gradient descent with and without momentum
path_std = gd_momentum_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.0, n_iterations=50)
path_momentum = gd_momentum_2d(1.5, 0.15, grad_ravine, learning_rate=0.01, momentum=0.9, n_iterations=50)

# Create a contour plot with the optimization paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

plt.plot(path_std[:, 0], path_std[:, 1], 'b-o', markersize=3, label='Standard GD')
plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'r-o', markersize=3, label='GD with Momentum')

plt.plot(path_std[0, 0], path_std[0, 1], 'ko', markersize=6, label='Initial Point')
plt.plot(path_std[-1, 0], path_std[-1, 1], 'bo', markersize=6, label='Standard GD Final')
plt.plot(path_momentum[-1, 0], path_momentum[-1, 1], 'ro', markersize=6, label='Momentum Final')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent with and without Momentum on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

### Non-convex Function

```python
# Define a non-convex function
def non_convex_function(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_non_convex(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = non_convex_function(X, Y)

# Run gradient descent with and without momentum from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths_std = []
paths_momentum = []

for start_x, start_y in start_points:
    path_std = gd_momentum_2d(start_x, start_y, grad_non_convex, learning_rate=0.1, momentum=0.0, n_iterations=50)
    path_momentum = gd_momentum_2d(start_x, start_y, grad_non_convex, learning_rate=0.1, momentum=0.9, n_iterations=50)
    paths_std.append(path_std)
    paths_momentum.append(path_momentum)

# Create a 2x1 grid of contour plots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Standard GD
ax = axes[0]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_std):
    ax.plot(path[:, 0], path[:, 1], 'b-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'ko', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Standard Gradient Descent')
ax.grid(True)
ax.legend()

# GD with Momentum
ax = axes[1]
contour = ax.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='f(x, y) value')

for i, path in enumerate(paths_momentum):
    ax.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    ax.plot(path[0, 0], path[0, 1], 'ko', markersize=6)  # Starting point
    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=6)  # Ending point

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Gradient Descent with Momentum')
ax.grid(True)
ax.legend()

plt.tight_layout()
plt.show()
```

## Advantages and Disadvantages of Momentum

### Advantages

1. **Faster Convergence**: Momentum accelerates convergence, especially in regions where the gradient is small.
2. **Reduced Oscillation**: Momentum helps dampen oscillations in ravines, leading to a more direct path to the minimum.
3. **Escaping Local Minima**: The accumulated velocity can help the algorithm escape shallow local minima.
4. **Handling Noisy Gradients**: Momentum smooths out the updates, making the algorithm more robust to noisy gradients.

### Disadvantages

1. **Additional Hyperparameter**: Momentum introduces an additional hyperparameter ($\gamma$) that needs to be tuned.
2. **Potential Overshooting**: With high momentum, the algorithm might overshoot the minimum and take longer to converge.
3. **Memory Requirements**: Momentum requires storing the velocity vector, which increases memory usage (though usually negligible).

## Practical Tips for Using Momentum

1. **Momentum Value**: A common default value for the momentum parameter is 0.9. For fine-tuning, values between 0.5 and 0.99 are typically used.
2. **Learning Rate**: When using momentum, you might need to adjust the learning rate. Often, a smaller learning rate works better with momentum.
3. **Scheduling**: Consider using a momentum schedule, where the momentum parameter increases over time (e.g., from 0.5 to 0.99).
4. **Initialization**: Proper weight initialization is still important, even with momentum.
5. **Combining with Other Techniques**: Momentum can be combined with other optimization techniques, such as learning rate schedules or adaptive methods.

## Summary

In this section, we've explored gradient descent with momentum and visualized its behavior on various functions:

1. **Basic Concept**: Momentum introduces a velocity term that accumulates the gradient of past steps, helping accelerate convergence and reduce oscillation.
2. **Mathematical Formulation**: The update rule involves a velocity vector that is updated based on the current gradient and a momentum parameter.
3. **Visualization Techniques**: We've visualized the cost function, regression line, optimization path in parameter space, and the effect of different momentum values.
4. **Comparison with Standard GD**: We've seen how momentum helps achieve faster convergence and reduces oscillation, especially in ravines.
5. **Behavior on Different Functions**: We've seen how momentum behaves on convex functions, non-convex functions, and functions with ravines.
6. **Advantages and Disadvantages**: We've discussed the pros and cons of using momentum, including faster convergence, reduced oscillation, and the need for an additional hyperparameter.

Gradient descent with momentum is a powerful extension of the standard gradient descent algorithm that addresses some of its key limitations. In the next section, we'll explore Nesterov Accelerated Gradient, which further improves upon momentum by using a "look-ahead" approach.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).
4. Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks, 12(1), 145-151.
