# 2.2.5.3.1 Basic Gradient Descent Visualization

## Introduction

Visualizing gradient descent helps us understand how optimization algorithms navigate the loss landscape to find minima. In this section, we'll explore various techniques for visualizing gradient descent on simple functions, which will provide intuition for how these algorithms behave in more complex scenarios.

## Visualizing Gradient Descent in 1D

Let's start with the simplest case: gradient descent on a one-dimensional function. This allows us to clearly see how the algorithm updates the parameter at each step.

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Define a simple 1D function
def f(x):
    return x**2 - 4*x + 4  # (x - 2)^2

# Define the gradient of the function
def df_dx(x):
    return 2*x - 4  # 2(x - 2)

# Implement gradient descent
def gradient_descent(start_x, learning_rate=0.1, n_iterations=20):
    path = [start_x]
    x = start_x
    
    for _ in range(n_iterations):
        grad = df_dx(x)
        x = x - learning_rate * grad
        path.append(x)
    
    return np.array(path)

# Create a range of x values
x_range = np.linspace(-2, 6, 1000)
y_range = f(x_range)

# Run gradient descent from different starting points
start_points = [-1, 0, 4, 5]
paths = [gradient_descent(x0, learning_rate=0.1, n_iterations=20) for x0 in start_points]

# Plot the function and gradient descent paths
plt.figure(figsize=(12, 8))
plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = x² - 4x + 4')

for i, path in enumerate(paths):
    # Plot the path
    x_path = path
    y_path = f(path)
    plt.plot(x_path, y_path, 'r-o', markersize=6, label=f'Path from x₀ = {start_points[i]}' if i == 0 else "")
    
    # Mark the starting point
    plt.plot(x_path[0], y_path[0], 'go', markersize=8)
    
    # Mark the ending point
    plt.plot(x_path[-1], y_path[-1], 'bo', markersize=8)

# Mark the minimum
plt.plot(2, 0, 'ko', markersize=10, label='Minimum: (2, 0)')

plt.grid(True)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent on f(x) = x² - 4x + 4')
plt.legend()
plt.show()
```

### Animating Gradient Descent in 1D

We can create an animation to better visualize how gradient descent progresses over time:

```python
# Create an animation of gradient descent
def create_gd_animation(start_x, learning_rate=0.1, n_iterations=20):
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Plot the function
    x_range = np.linspace(-2, 6, 1000)
    y_range = f(x_range)
    ax.plot(x_range, y_range, 'b-', linewidth=2)
    
    # Mark the minimum
    ax.plot(2, 0, 'ko', markersize=10)
    
    # Initialize the point
    point, = ax.plot([], [], 'ro', markersize=8)
    
    # Initialize the path
    path, = ax.plot([], [], 'r-', linewidth=2)
    
    # Set up the plot
    ax.grid(True)
    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
    ax.set_xlabel('x')
    ax.set_ylabel('f(x)')
    ax.set_title(f'Gradient Descent on f(x) = x² - 4x + 4 (Learning Rate: {learning_rate})')
    
    # Run gradient descent
    gd_path = gradient_descent(start_x, learning_rate, n_iterations)
    
    # Animation initialization function
    def init():
        point.set_data([], [])
        path.set_data([], [])
        return point, path
    
    # Animation update function
    def update(frame):
        x = gd_path[frame]
        y = f(x)
        point.set_data([x], [y])
        
        path.set_data(gd_path[:frame+1], f(gd_path[:frame+1]))
        
        # Add text annotation for current position
        ax.set_title(f'Gradient Descent on f(x) = x² - 4x + 4 (Step: {frame}, x: {x:.4f}, f(x): {y:.4f})')
        
        return point, path
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(n_iterations+1),
                        init_func=init, blit=True, interval=500)
    
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_gd_animation(start_x=5, learning_rate=0.1, n_iterations=20)
# To save the animation: ani.save('gradient_descent_1d.gif', writer='pillow', fps=2)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing Gradient Descent in 2D

Now let's visualize gradient descent on a two-dimensional function, which is more representative of real optimization problems:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define a simple 2D function
def f(x, y):
    return x**2 + y**2  # Bowl-shaped function (paraboloid)

# Define the gradient of the function
def grad_f(x, y):
    return np.array([2*x, 2*y])

# Implement gradient descent
def gradient_descent_2d(start_x, start_y, learning_rate=0.1, n_iterations=20):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        grad = grad_f(x, y)
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Run gradient descent from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4)]
paths = [gradient_descent_2d(x0, y0, learning_rate=0.1, n_iterations=20) for x0, y0 in start_points]

# Create a combined 3D surface and contour plot
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('3D Surface with Gradient Descent Paths')

# Plot the paths on the 3D surface
for path in paths:
    xs, ys = path[:, 0], path[:, 1]
    zs = f(xs, ys)
    ax1.plot(xs, ys, zs, 'r-o', markersize=3)
    ax1.plot([xs[0]], [ys[0]], [zs[0]], 'go', markersize=6)  # Starting point
    ax1.plot([xs[-1]], [ys[-1]], [zs[-1]], 'bo', markersize=6)  # Ending point

# Contour plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot with Gradient Descent Paths')
ax2.grid(True)

# Plot the paths on the contour plot
for path in paths:
    ax2.plot(path[:, 0], path[:, 1], 'r-o', markersize=3)
    ax2.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    ax2.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.suptitle('Gradient Descent on f(x, y) = x² + y²', fontsize=16)
plt.tight_layout()
plt.show()
```

### Visualizing Gradient Vectors

We can also visualize the gradient vectors at each point to better understand the direction of steepest descent:

```python
# Create a contour plot with gradient vectors
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

# Create a grid of points for gradient vectors
x_grid = np.linspace(-5, 5, 10)
y_grid = np.linspace(-5, 5, 10)
X_grid, Y_grid = np.meshgrid(x_grid, y_grid)
U = np.zeros_like(X_grid)
V = np.zeros_like(Y_grid)

# Compute gradients at each grid point
for i in range(len(x_grid)):
    for j in range(len(y_grid)):
        grad = grad_f(X_grid[j, i], Y_grid[j, i])
        U[j, i] = -grad[0]  # Negative gradient for descent direction
        V[j, i] = -grad[1]

# Normalize the gradient vectors for better visualization
norm = np.sqrt(U**2 + V**2)
U_norm = U / (norm + 1e-10)  # Add small constant to avoid division by zero
V_norm = V / (norm + 1e-10)

# Plot the gradient vectors
plt.quiver(X_grid, Y_grid, U_norm, V_norm, color='r', scale=30)

# Plot the gradient descent paths
for path in paths:
    plt.plot(path[:, 0], path[:, 1], 'b-o', markersize=3)
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'mo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Contour Plot with Gradient Vectors and Descent Paths')
plt.grid(True)
plt.axis('equal')
plt.show()
```

### Animating Gradient Descent in 2D

We can create an animation to visualize how gradient descent progresses in 2D:

```python
# Create an animation of gradient descent in 2D
def create_gd_animation_2d(start_x, start_y, learning_rate=0.1, n_iterations=20):
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create a grid of x and y values
    x = np.linspace(-5, 5, 100)
    y = np.linspace(-5, 5, 100)
    X, Y = np.meshgrid(x, y)
    Z = f(X, Y)
    
    # Create a contour plot
    contour = ax.contour(X, Y, Z, 20, cmap='viridis')
    plt.colorbar(contour, ax=ax, label='f(x, y) value')
    
    # Initialize the point
    point, = ax.plot([], [], 'ro', markersize=8)
    
    # Initialize the path
    path, = ax.plot([], [], 'r-', linewidth=2)
    
    # Set up the plot
    ax.grid(True)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Gradient Descent on f(x, y) = x² + y² (Learning Rate: {learning_rate})')
    ax.set_xlim(-5, 5)
    ax.set_ylim(-5, 5)
    
    # Run gradient descent
    gd_path = gradient_descent_2d(start_x, start_y, learning_rate, n_iterations)
    
    # Animation initialization function
    def init():
        point.set_data([], [])
        path.set_data([], [])
        return point, path
    
    # Animation update function
    def update(frame):
        x, y = gd_path[frame]
        point.set_data([x], [y])
        
        path.set_data(gd_path[:frame+1, 0], gd_path[:frame+1, 1])
        
        # Add text annotation for current position
        ax.set_title(f'Gradient Descent on f(x, y) = x² + y² (Step: {frame}, x: {x:.4f}, y: {y:.4f}, f(x,y): {f(x, y):.4f})')
        
        return point, path
    
    # Create the animation
    ani = FuncAnimation(fig, update, frames=range(n_iterations+1),
                        init_func=init, blit=True, interval=500)
    
    plt.close()  # Prevent duplicate display in Jupyter
    return ani

# Create and display the animation
ani = create_gd_animation_2d(start_x=4, start_y=4, learning_rate=0.1, n_iterations=20)
# To save the animation: ani.save('gradient_descent_2d.gif', writer='pillow', fps=2)

# Display the animation in a Jupyter notebook
from IPython.display import HTML
HTML(ani.to_jshtml())
```

## Visualizing Gradient Descent on Different Functions

Let's visualize gradient descent on different types of functions to understand how the algorithm behaves in different scenarios:

### Gradient Descent on a Function with Multiple Local Minima

```python
# Define a function with multiple local minima
def f_multi_minima(x, y):
    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

# Define the gradient of the function
def grad_f_multi_minima(x, y):
    df_dx = np.cos(x) * np.cos(y) + 0.2 * x
    df_dy = -np.sin(x) * np.sin(y) + 0.2 * y
    return np.array([df_dx, df_dy])

# Implement gradient descent
def gradient_descent_multi_minima(start_x, start_y, learning_rate=0.1, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        grad = grad_f_multi_minima(x, y)
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f_multi_minima(X, Y)

# Run gradient descent from different starting points
start_points = [(-4, -4), (-4, 4), (4, -4), (4, 4), (0, 0)]
paths = [gradient_descent_multi_minima(x0, y0, learning_rate=0.1, n_iterations=50) for x0, y0 in start_points]

# Create a contour plot with gradient descent paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent on a Function with Multiple Local Minima')
plt.grid(True)
plt.legend()
plt.show()
```

### Gradient Descent on a Function with a Ravine

```python
# Define a function with a ravine
def f_ravine(x, y):
    return x**2 + 100*y**2

# Define the gradient of the function
def grad_f_ravine(x, y):
    return np.array([2*x, 200*y])

# Implement gradient descent
def gradient_descent_ravine(start_x, start_y, learning_rate=0.01, n_iterations=50):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        grad = grad_f_ravine(x, y)
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-0.2, 0.2, 100)
X, Y = np.meshgrid(x, y)
Z = f_ravine(X, Y)

# Run gradient descent from different starting points
start_points = [(-1.5, 0.15), (1.5, -0.15), (0, 0.1)]
paths = [gradient_descent_ravine(x0, y0, learning_rate=0.01, n_iterations=50) for x0, y0 in start_points]

# Create a contour plot with gradient descent paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent on a Function with a Ravine')
plt.grid(True)
plt.legend()
plt.show()
```

### Gradient Descent on the Rosenbrock Function

The Rosenbrock function is a classic test function for optimization algorithms:

```python
# Define the Rosenbrock function
def rosenbrock(x, y, a=1, b=100):
    return (a - x)**2 + b * (y - x**2)**2

# Define the gradient of the Rosenbrock function
def grad_rosenbrock(x, y, a=1, b=100):
    df_dx = -2*(a - x) - 4*b*x*(y - x**2)
    df_dy = 2*b*(y - x**2)
    return np.array([df_dx, df_dy])

# Implement gradient descent
def gradient_descent_rosenbrock(start_x, start_y, learning_rate=0.0001, n_iterations=1000):
    path = [(start_x, start_y)]
    x, y = start_x, start_y
    
    for _ in range(n_iterations):
        grad = grad_rosenbrock(x, y)
        x = x - learning_rate * grad[0]
        y = y - learning_rate * grad[1]
        path.append((x, y))
    
    return np.array(path)

# Create a grid of x and y values
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

# Run gradient descent from different starting points
start_points = [(-1, 1), (0, 0), (1.5, 1.5)]
paths = [gradient_descent_rosenbrock(x0, y0, learning_rate=0.0001, n_iterations=1000) for x0, y0 in start_points]

# Create a contour plot with gradient descent paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, np.log(Z + 1), 50, cmap='viridis')  # Log scale for better visualization
plt.colorbar(label='log(f(x, y) + 1) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, label=f'Path {i+1}' if i == 0 else "")
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

# Mark the global minimum
plt.plot(1, 1, 'ko', markersize=8, label='Global Minimum (1, 1)')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent on the Rosenbrock Function')
plt.grid(True)
plt.legend()
plt.show()
```

## Visualizing the Effect of Learning Rate

The learning rate is a crucial hyperparameter in gradient descent. Let's visualize how different learning rates affect the optimization process:

```python
# Define a simple 2D function
def f(x, y):
    return x**2 + y**2

# Define the gradient of the function
def grad_f(x, y):
    return np.array([2*x, 2*y])

# Implement gradient descent with different learning rates
def gradient_descent_multi_lr(start_x, start_y, learning_rates, n_iterations=20):
    paths = []
    
    for lr in learning_rates:
        path = [(start_x, start_y)]
        x, y = start_x, start_y
        
        for _ in range(n_iterations):
            grad = grad_f(x, y)
            x = x - lr * grad[0]
            y = y - lr * grad[1]
            path.append((x, y))
        
        paths.append(np.array(path))
    
    return paths

# Create a grid of x and y values
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Run gradient descent with different learning rates
learning_rates = [0.01, 0.05, 0.1, 0.5]
paths = gradient_descent_multi_lr(4, 4, learning_rates, n_iterations=20)

# Create a contour plot with gradient descent paths
plt.figure(figsize=(10, 8))
contour = plt.contour(X, Y, Z, 20, cmap='viridis')
plt.colorbar(label='f(x, y) value')

for i, path in enumerate(paths):
    plt.plot(path[:, 0], path[:, 1], 'o-', markersize=3, label=f'Learning Rate: {learning_rates[i]}')
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=6)  # Starting point
    plt.plot(path[-1, 0], path[-1, 1], 'bo', markersize=6)  # Ending point

plt.xlabel('x')
plt.ylabel('y')
plt.title('Effect of Learning Rate on Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()
```

## Summary

In this section, we've explored various techniques for visualizing gradient descent:

1. **1D Visualization**: Plotting the function and the path of gradient descent in one dimension
2. **2D Visualization**: Using contour plots and 3D surface plots to visualize gradient descent in two dimensions
3. **Gradient Vectors**: Visualizing the direction of steepest descent at each point
4. **Animations**: Creating dynamic visualizations of the optimization process
5. **Different Functions**: Exploring how gradient descent behaves on functions with different characteristics (multiple minima, ravines, etc.)
6. **Learning Rate Effects**: Visualizing how the learning rate affects the optimization path

These visualizations provide valuable intuition for understanding how gradient descent works and why certain modifications (like momentum or adaptive learning rates) can improve its performance. In the next sections, we'll explore these modifications in more detail.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
4. Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of a function. The Computer Journal, 3(3), 175-184.
