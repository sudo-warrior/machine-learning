# 2.2.4.2 Gradient Descent Variants

## Introduction

In the previous section, we introduced the basic gradient descent algorithm. While effective, standard gradient descent has several limitations, particularly when dealing with large datasets or complex optimization landscapes. To address these challenges, researchers have developed several variants of gradient descent that improve convergence speed, stability, and generalization performance.

In this section, we'll explore the most important variants of gradient descent used in modern machine learning:

1. Batch, Stochastic, and Mini-batch Gradient Descent
2. Gradient Descent with Momentum
3. Nesterov Accelerated Gradient
4. Adaptive Learning Rate Methods (AdaGrad, RMSProp, Adam)

## Batch, Stochastic, and Mini-batch Gradient Descent

### Batch Gradient Descent

Batch Gradient Descent (BGD), also known as Vanilla Gradient Descent, computes the gradient of the cost function with respect to the parameters using the entire training dataset. The update rule is:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t)$$

where $J(\theta)$ is the cost function computed over the entire dataset.

**Advantages:**
- Stable convergence
- Guaranteed to converge to the global minimum for convex error surfaces
- Efficient for small datasets

**Disadvantages:**
- Very slow for large datasets
- Requires the entire dataset to fit in memory
- Can get stuck in local minima for non-convex functions
- Cannot handle online learning (learning from new data on-the-fly)

### Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) computes the gradient using a single randomly selected training example at each iteration:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t; x^{(i)}, y^{(i)})$$

where $(x^{(i)}, y^{(i)})$ is a randomly selected training example.

**Advantages:**
- Much faster than batch gradient descent
- Can handle very large datasets
- Can escape local minima due to the noise in the updates
- Enables online learning

**Disadvantages:**
- High variance in parameter updates
- May not converge to the exact minimum, but will oscillate around it
- Requires careful tuning of the learning rate

### Mini-batch Gradient Descent

Mini-batch Gradient Descent is a compromise between batch and stochastic gradient descent. It computes the gradient using a small random subset (mini-batch) of the training data:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t; X^{(i:i+n)}, Y^{(i:i+n)})$$

where $(X^{(i:i+n)}, Y^{(i:i+n)})$ is a mini-batch of $n$ training examples.

**Advantages:**
- More efficient than batch gradient descent for large datasets
- More stable than stochastic gradient descent
- Can leverage optimized matrix operations for the mini-batch
- Enables parallel processing

**Disadvantages:**
- Still requires tuning of the learning rate and batch size
- May still oscillate around the minimum

Let's implement and compare these three variants:

```python
import numpy as np
import matplotlib.pyplot as plt
import time

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(1000, 1)
y = 4 + 3 * X + np.random.randn(1000, 1)

# Add bias term to X
X_b = np.c_[np.ones((len(X), 1)), X]

# Batch Gradient Descent
def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    m = len(X)
    theta = np.random.randn(2, 1)
    cost_history = []
    
    start_time = time.time()
    
    for iteration in range(n_iterations):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        theta = theta - learning_rate * gradients
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    end_time = time.time()
    
    return theta, cost_history, end_time - start_time

# Stochastic Gradient Descent
def stochastic_gradient_descent(X, y, learning_rate=0.01, n_iterations=50):
    m = len(X)
    theta = np.random.randn(2, 1)
    cost_history = []
    
    start_time = time.time()
    
    for iteration in range(n_iterations):
        for i in range(m):
            random_index = np.random.randint(m)
            xi = X[random_index:random_index+1]
            yi = y[random_index:random_index+1]
            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
            theta = theta - learning_rate * gradients
            
            # Compute cost (using the entire dataset for comparison)
            if i % 100 == 0:
                predictions = X.dot(theta)
                cost = np.mean((predictions - y) ** 2)
                cost_history.append(cost)
    
    end_time = time.time()
    
    return theta, cost_history, end_time - start_time

# Mini-batch Gradient Descent
def mini_batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=50, batch_size=32):
    m = len(X)
    theta = np.random.randn(2, 1)
    cost_history = []
    
    start_time = time.time()
    
    for iteration in range(n_iterations):
        # Shuffle the data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # Process mini-batches
        for i in range(0, m, batch_size):
            xi = X_shuffled[i:i+batch_size]
            yi = y_shuffled[i:i+batch_size]
            gradients = 2/len(xi) * xi.T.dot(xi.dot(theta) - yi)
            theta = theta - learning_rate * gradients
            
            # Compute cost (using the entire dataset for comparison)
            if i % 100 == 0:
                predictions = X.dot(theta)
                cost = np.mean((predictions - y) ** 2)
                cost_history.append(cost)
    
    end_time = time.time()
    
    return theta, cost_history, end_time - start_time

# Run the algorithms
theta_bgd, cost_history_bgd, time_bgd = batch_gradient_descent(X_b, y, learning_rate=0.1, n_iterations=100)
theta_sgd, cost_history_sgd, time_sgd = stochastic_gradient_descent(X_b, y, learning_rate=0.001, n_iterations=10)
theta_mbgd, cost_history_mbgd, time_mbgd = mini_batch_gradient_descent(X_b, y, learning_rate=0.01, n_iterations=10, batch_size=32)

# Print results
print("Batch Gradient Descent:")
print(f"Final parameters: theta = {theta_bgd.flatten()}")
print(f"Final cost: {cost_history_bgd[-1]}")
print(f"Time taken: {time_bgd:.4f} seconds")

print("\nStochastic Gradient Descent:")
print(f"Final parameters: theta = {theta_sgd.flatten()}")
print(f"Final cost: {cost_history_sgd[-1]}")
print(f"Time taken: {time_sgd:.4f} seconds")

print("\nMini-batch Gradient Descent:")
print(f"Final parameters: theta = {theta_mbgd.flatten()}")
print(f"Final cost: {cost_history_mbgd[-1]}")
print(f"Time taken: {time_mbgd:.4f} seconds")

# Plot the cost histories
plt.figure(figsize=(12, 6))
plt.plot(cost_history_bgd, 'b-', linewidth=2, label='Batch GD')
plt.plot(range(0, len(cost_history_sgd)), cost_history_sgd, 'r-', linewidth=2, label='Stochastic GD')
plt.plot(range(0, len(cost_history_mbgd)), cost_history_mbgd, 'g-', linewidth=2, label='Mini-batch GD')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)
plt.show()

# Plot the regression lines
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.3)

# Plot the regression lines
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]

plt.plot(X_new, X_new_b.dot(theta_bgd), 'b-', linewidth=2, label='Batch GD')
plt.plot(X_new, X_new_b.dot(theta_sgd), 'r-', linewidth=2, label='Stochastic GD')
plt.plot(X_new, X_new_b.dot(theta_mbgd), 'g-', linewidth=2, label='Mini-batch GD')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression using Different Gradient Descent Variants')
plt.legend()
plt.grid(True)
plt.show()
```

## Gradient Descent with Momentum

One of the challenges with standard gradient descent is that it can oscillate in ravines (areas where the surface curves much more steeply in one dimension than in another). Gradient Descent with Momentum addresses this by adding a momentum term to the update rule:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

where:
- $v_t$ is the velocity vector (initially $v_0 = 0$)
- $\gamma$ is the momentum parameter (typically around 0.9)
- $\alpha$ is the learning rate

The momentum term $\gamma v_{t-1}$ helps accelerate the optimization in the relevant direction and dampen oscillations. It's like a ball rolling down a hill, accumulating momentum as it goes.

**Advantages:**
- Faster convergence
- Reduced oscillation in ravines
- Can escape local minima with enough momentum

**Disadvantages:**
- Introduces an additional hyperparameter (momentum)
- May overshoot the minimum if momentum is too high

Let's implement gradient descent with momentum:

```python
def gradient_descent_with_momentum(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(X)
    theta = np.random.randn(2, 1)
    velocity = np.zeros_like(theta)
    cost_history = []
    
    for iteration in range(n_iterations):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        
        # Update velocity
        velocity = momentum * velocity - learning_rate * gradients
        
        # Update parameters
        theta = theta + velocity
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    return theta, cost_history

# Run gradient descent with momentum
theta_momentum, cost_history_momentum = gradient_descent_with_momentum(X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history_bgd, 'b-', linewidth=2, label='Standard GD')
plt.plot(cost_history_momentum, 'r-', linewidth=2, label='GD with Momentum')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)
plt.show()
```

## Nesterov Accelerated Gradient

Nesterov Accelerated Gradient (NAG) is a variation of gradient descent with momentum that provides a smarter way to use the momentum term. Instead of computing the gradient at the current position, NAG first takes a step in the direction of the accumulated gradient (the momentum), and then computes the gradient at this "lookahead" position:

$$v_t = \gamma v_{t-1} + \alpha \nabla_\theta J(\theta_t - \gamma v_{t-1})$$
$$\theta_{t+1} = \theta_t - v_t$$

This "look-ahead" gradient gives NAG the ability to slow down before the gradient changes direction, resulting in better convergence.

**Advantages:**
- Even faster convergence than standard momentum
- Better ability to navigate around curves
- More responsive to changes in the gradient

**Disadvantages:**
- Slightly more complex to implement
- Still requires tuning of hyperparameters

Let's implement Nesterov Accelerated Gradient:

```python
def nesterov_accelerated_gradient(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):
    m = len(X)
    theta = np.random.randn(2, 1)
    velocity = np.zeros_like(theta)
    cost_history = []
    
    for iteration in range(n_iterations):
        # Look-ahead position
        theta_lookahead = theta - momentum * velocity
        
        # Compute gradient at the look-ahead position
        gradients = 2/m * X.T.dot(X.dot(theta_lookahead) - y)
        
        # Update velocity
        velocity = momentum * velocity + learning_rate * gradients
        
        # Update parameters
        theta = theta - velocity
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    return theta, cost_history

# Run Nesterov Accelerated Gradient
theta_nag, cost_history_nag = nesterov_accelerated_gradient(X_b, y, learning_rate=0.01, momentum=0.9, n_iterations=100)

# Plot the cost history
plt.figure(figsize=(10, 6))
plt.plot(cost_history_bgd, 'b-', linewidth=2, label='Standard GD')
plt.plot(cost_history_momentum, 'r-', linewidth=2, label='GD with Momentum')
plt.plot(cost_history_nag, 'g-', linewidth=2, label='Nesterov Accelerated Gradient')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)
plt.show()
```

## Adaptive Learning Rate Methods

One of the challenges with the previous methods is that they use a fixed learning rate for all parameters. Adaptive learning rate methods adjust the learning rate for each parameter based on the history of gradients.

### AdaGrad (Adaptive Gradient Algorithm)

AdaGrad adapts the learning rate for each parameter by scaling it inversely proportional to the square root of the sum of all the historical squared values of the gradient:

$$G_{t,ii} = G_{t-1,ii} + (\nabla_\theta J(\theta_t))_i^2$$
$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \epsilon}} (\nabla_\theta J(\theta_t))_i$$

where:
- $G_t$ is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients with respect to $\theta_i$ up to time step $t$
- $\epsilon$ is a small constant to avoid division by zero (typically 1e-8)

**Advantages:**
- Eliminates the need to manually tune the learning rate
- Works well for sparse features
- Different learning rate for each parameter

**Disadvantages:**
- Accumulates the squared gradients over time, which can cause the learning rate to become very small and training to stall

### RMSProp (Root Mean Square Propagation)

RMSProp addresses the diminishing learning rates in AdaGrad by using an exponentially weighted moving average of the squared gradients:

$$G_{t,ii} = \beta G_{t-1,ii} + (1 - \beta)(\nabla_\theta J(\theta_t))_i^2$$
$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \epsilon}} (\nabla_\theta J(\theta_t))_i$$

where $\beta$ is the decay rate (typically 0.9).

**Advantages:**
- Prevents the learning rate from becoming too small
- Works well for non-stationary objectives
- Different learning rate for each parameter

**Disadvantages:**
- Still requires manual tuning of the global learning rate

### Adam (Adaptive Moment Estimation)

Adam combines the ideas of momentum and RMSProp. It keeps track of both the first moment (mean) and the second moment (uncentered variance) of the gradients:

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta_t)$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta J(\theta_t))^2$$

To correct the bias towards zero during the initial time steps, Adam computes bias-corrected moments:

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

The parameter update is then:

$$\theta_{t+1} = \theta_t - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

where:
- $\beta_1$ is the exponential decay rate for the first moment (typically 0.9)
- $\beta_2$ is the exponential decay rate for the second moment (typically 0.999)
- $\epsilon$ is a small constant to avoid division by zero (typically 1e-8)

**Advantages:**
- Combines the benefits of AdaGrad and RMSProp
- Bias correction helps during the initial time steps
- Works well for a wide range of problems
- Different learning rate for each parameter

**Disadvantages:**
- More complex to implement
- Requires more memory

Let's implement these adaptive learning rate methods:

```python
def adagrad(X, y, learning_rate=0.01, n_iterations=1000, epsilon=1e-8):
    m = len(X)
    theta = np.random.randn(2, 1)
    G = np.zeros_like(theta)
    cost_history = []
    
    for iteration in range(n_iterations):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        
        # Update accumulated squared gradients
        G += gradients**2
        
        # Update parameters
        theta = theta - learning_rate * gradients / (np.sqrt(G) + epsilon)
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    return theta, cost_history

def rmsprop(X, y, learning_rate=0.01, beta=0.9, n_iterations=1000, epsilon=1e-8):
    m = len(X)
    theta = np.random.randn(2, 1)
    G = np.zeros_like(theta)
    cost_history = []
    
    for iteration in range(n_iterations):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        
        # Update accumulated squared gradients with decay
        G = beta * G + (1 - beta) * gradients**2
        
        # Update parameters
        theta = theta - learning_rate * gradients / (np.sqrt(G) + epsilon)
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    return theta, cost_history

def adam(X, y, learning_rate=0.01, beta1=0.9, beta2=0.999, n_iterations=1000, epsilon=1e-8):
    m = len(X)
    theta = np.random.randn(2, 1)
    M = np.zeros_like(theta)  # First moment
    V = np.zeros_like(theta)  # Second moment
    cost_history = []
    
    for iteration in range(1, n_iterations + 1):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        
        # Update biased first moment estimate
        M = beta1 * M + (1 - beta1) * gradients
        
        # Update biased second moment estimate
        V = beta2 * V + (1 - beta2) * gradients**2
        
        # Compute bias-corrected first moment estimate
        M_corrected = M / (1 - beta1**iteration)
        
        # Compute bias-corrected second moment estimate
        V_corrected = V / (1 - beta2**iteration)
        
        # Update parameters
        theta = theta - learning_rate * M_corrected / (np.sqrt(V_corrected) + epsilon)
        
        # Compute cost
        predictions = X.dot(theta)
        cost = np.mean((predictions - y) ** 2)
        cost_history.append(cost)
        
    return theta, cost_history

# Run the adaptive learning rate methods
theta_adagrad, cost_history_adagrad = adagrad(X_b, y, learning_rate=0.5, n_iterations=100)
theta_rmsprop, cost_history_rmsprop = rmsprop(X_b, y, learning_rate=0.1, n_iterations=100)
theta_adam, cost_history_adam = adam(X_b, y, learning_rate=0.1, n_iterations=100)

# Plot the cost histories
plt.figure(figsize=(12, 6))
plt.plot(cost_history_bgd, 'b-', linewidth=2, label='Standard GD')
plt.plot(cost_history_adagrad, 'r-', linewidth=2, label='AdaGrad')
plt.plot(cost_history_rmsprop, 'g-', linewidth=2, label='RMSProp')
plt.plot(cost_history_adam, 'y-', linewidth=2, label='Adam')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)
plt.show()
```

## Comparison of Gradient Descent Variants

Let's compare all the gradient descent variants we've discussed:

```python
# Plot the cost histories for all methods
plt.figure(figsize=(15, 8))
plt.plot(cost_history_bgd, 'b-', linewidth=2, label='Batch GD')
plt.plot(range(0, len(cost_history_sgd)), cost_history_sgd, 'c-', linewidth=2, label='Stochastic GD')
plt.plot(range(0, len(cost_history_mbgd)), cost_history_mbgd, 'm-', linewidth=2, label='Mini-batch GD')
plt.plot(cost_history_momentum, 'r-', linewidth=2, label='GD with Momentum')
plt.plot(cost_history_nag, 'g-', linewidth=2, label='Nesterov Accelerated Gradient')
plt.plot(cost_history_adagrad, 'y-', linewidth=2, label='AdaGrad')
plt.plot(cost_history_rmsprop, 'k-', linewidth=2, label='RMSProp')
plt.plot(cost_history_adam, 'orange', linewidth=2, label='Adam')
plt.xlabel('Iteration')
plt.ylabel('Cost (MSE)')
plt.title('Comparison of Gradient Descent Variants')
plt.legend()
plt.grid(True)
plt.show()
```

## Choosing the Right Gradient Descent Variant

The choice of gradient descent variant depends on the specific problem and dataset:

1. **Batch Gradient Descent**: Use for small datasets and when you need a stable convergence.

2. **Stochastic Gradient Descent**: Use for very large datasets and when you need online learning.

3. **Mini-batch Gradient Descent**: The most common choice for most deep learning applications, providing a good balance between speed and stability.

4. **Gradient Descent with Momentum**: Use when the loss function has high curvature or ravines.

5. **Nesterov Accelerated Gradient**: A generally better choice than standard momentum, especially for problems with high curvature.

6. **AdaGrad**: Good for sparse data and when you want different learning rates for different parameters.

7. **RMSProp**: Good for non-stationary objectives and when AdaGrad's learning rate becomes too small.

8. **Adam**: The most popular choice for deep learning, combining the benefits of momentum and adaptive learning rates.

In practice, Adam is often the default choice for deep learning, while mini-batch gradient descent with momentum or Nesterov acceleration is common for simpler problems.

## Summary

In this section, we've explored various gradient descent variants that address the limitations of standard gradient descent:

1. **Batch, Stochastic, and Mini-batch Gradient Descent**: Different ways to compute the gradient, trading off between computation time and update stability.

2. **Gradient Descent with Momentum**: Adds a momentum term to accelerate convergence and reduce oscillation.

3. **Nesterov Accelerated Gradient**: A smarter version of momentum that looks ahead to compute the gradient.

4. **Adaptive Learning Rate Methods**: Algorithms like AdaGrad, RMSProp, and Adam that adapt the learning rate for each parameter based on the history of gradients.

Each variant has its strengths and weaknesses, and the choice depends on the specific problem and dataset. In practice, mini-batch gradient descent with Adam is often a good default choice for deep learning problems.

In the next section, we'll explore how to visualize the optimization process and understand the convergence properties of these algorithms.

## References

1. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
3. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.
4. Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.
5. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence O(1/k^2). Doklady ANSSSR, 269, 543-547.
