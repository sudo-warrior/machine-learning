# 2.2.1 Derivatives and Gradients

## Derivatives: Measuring Rate of Change

A derivative measures how a function changes as its input changes. Geometrically, it represents the slope of the tangent line to the function at a specific point.

### Definition of a Derivative

The derivative of a function f(x) with respect to x, denoted as f'(x) or df/dx, is defined as:

$$f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$

This is the limit of the difference quotient as h approaches zero.

### Common Derivative Rules

Here are some basic derivative rules that are frequently used in machine learning:

1. **Constant Rule**: If f(x) = c, then f'(x) = 0
2. **Power Rule**: If f(x) = x^n, then f'(x) = n·x^(n-1)
3. **Sum Rule**: If f(x) = g(x) + h(x), then f'(x) = g'(x) + h'(x)
4. **Product Rule**: If f(x) = g(x)·h(x), then f'(x) = g'(x)·h(x) + g(x)·h'(x)
5. **Chain Rule**: If f(x) = g(h(x)), then f'(x) = g'(h(x))·h'(x)

### Derivatives of Common Functions in Machine Learning

| Function | Derivative |
|----------|------------|
| f(x) = c | f'(x) = 0 |
| f(x) = x | f'(x) = 1 |
| f(x) = x^n | f'(x) = n·x^(n-1) |
| f(x) = e^x | f'(x) = e^x |
| f(x) = ln(x) | f'(x) = 1/x |
| f(x) = sin(x) | f'(x) = cos(x) |
| f(x) = cos(x) | f'(x) = -sin(x) |
| f(x) = sigmoid(x) = 1/(1+e^(-x)) | f'(x) = sigmoid(x)·(1-sigmoid(x)) |
| f(x) = tanh(x) | f'(x) = 1 - tanh^2(x) |
| f(x) = ReLU(x) = max(0,x) | f'(x) = 1 if x > 0, 0 if x < 0, undefined if x = 0 |

### Computing Derivatives in Python

We can compute derivatives numerically or symbolically in Python:

```python
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp
from scipy.misc import derivative

# Define a function
def f(x):
    return x**3 - 2*x**2 + 3*x - 1

# Compute derivative symbolically using SymPy
x_sym = sp.Symbol('x')
f_sym = x_sym**3 - 2*x_sym**2 + 3*x_sym - 1
f_prime_sym = sp.diff(f_sym, x_sym)
print(f"Symbolic derivative: {f_prime_sym}")

# Convert symbolic expression to a Python function
f_prime = sp.lambdify(x_sym, f_prime_sym, 'numpy')

# Compute derivative numerically using SciPy
def numerical_derivative(x):
    return derivative(f, x, dx=1e-6)

# Plot the function and its derivative
x = np.linspace(-2, 3, 1000)
y = f(x)
y_prime_symbolic = f_prime(x)
y_prime_numerical = [numerical_derivative(xi) for xi in x]

plt.figure(figsize=(12, 6))
plt.plot(x, y, 'b-', label='f(x) = x³ - 2x² + 3x - 1')
plt.plot(x, y_prime_symbolic, 'r-', label="f'(x) = 3x² - 4x + 3")
plt.plot(x, y_prime_numerical, 'g--', label="f'(x) numerical")
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True, alpha=0.3)
plt.legend()
plt.title('Function and its Derivative')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```

## Gradients: Derivatives in Multiple Dimensions

When dealing with functions of multiple variables, which is common in machine learning, we use gradients instead of simple derivatives.

### Definition of a Gradient

The gradient of a function f(x₁, x₂, ..., xₙ) is a vector of partial derivatives:

$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

The gradient points in the direction of steepest ascent of the function.

### Geometric Interpretation

- The gradient vector at a point points in the direction of steepest increase of the function
- The magnitude of the gradient represents the rate of increase in that direction
- The negative gradient points in the direction of steepest decrease

### Computing Gradients in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import sympy as sp

# Define a function of two variables
def f(x, y):
    return x**2 + 2*y**2

# Compute gradient symbolically
x_sym, y_sym = sp.symbols('x y')
f_sym = x_sym**2 + 2*y_sym**2
grad_f = [sp.diff(f_sym, var) for var in (x_sym, y_sym)]
print(f"Symbolic gradient: [{grad_f[0]}, {grad_f[1]}]")

# Convert to Python functions
grad_f_x = sp.lambdify((x_sym, y_sym), grad_f[0], 'numpy')
grad_f_y = sp.lambdify((x_sym, y_sym), grad_f[1], 'numpy')

# Create a grid of points
x = np.linspace(-2, 2, 20)
y = np.linspace(-2, 2, 20)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Compute gradient at each point
U = grad_f_x(X, Y)
V = grad_f_y(X, Y)

# Normalize for better visualization
norm = np.sqrt(U**2 + V**2)
U_norm = U / (norm + 1e-10)  # Add small constant to avoid division by zero
V_norm = V / (norm + 1e-10)

# Plot the function and its gradient
fig = plt.figure(figsize=(15, 10))

# 3D surface plot
ax1 = fig.add_subplot(121, projection='3d')
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('Function f(x, y) = x² + 2y²')

# Contour plot with gradient vectors
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, 20, cmap='viridis')
quiver = ax2.quiver(X[::2, ::2], Y[::2, ::2], U_norm[::2, ::2], V_norm[::2, ::2], 
                    color='r', scale=30)
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Contour Plot with Gradient Vectors')
plt.colorbar(contour, ax=ax2, label='f(x, y) value')

plt.tight_layout()
plt.show()
```

## Directional Derivatives

A directional derivative measures the rate of change of a function in a specific direction.

### Definition

The directional derivative of a function f at point p in the direction of a unit vector u is:

$$\nabla_u f(p) = \nabla f(p) \cdot u$$

where ∇f(p) is the gradient of f at p, and · denotes the dot product.

### Computing Directional Derivatives

```python
import numpy as np

# Define a function
def f(x, y):
    return x**2 + 2*y**2

# Define its gradient
def grad_f(x, y):
    return np.array([2*x, 4*y])

# Compute directional derivative at point (1, 1) in direction [1, 1]
point = np.array([1, 1])
direction = np.array([1, 1])


# Normalize the direction vector
direction = direction / np.linalg.norm(direction)

# Compute the gradient at the point
gradient = grad_f(point[0], point[1])

# Compute the directional derivative
directional_derivative = np.dot(gradient, direction)

print(f"Point: {point}")
print(f"Direction (normalized): {direction}")
print(f"Gradient at point: {gradient}")
print(f"Directional derivative: {directional_derivative}")
```

## Applications in Machine Learning

### 1. Finding Critical Points

Critical points (where the gradient is zero) can be minima, maxima, or saddle points. In machine learning, we're often looking for minima of loss functions.

```python
import numpy as np
import sympy as sp

# Define a simple loss function
x, y = sp.symbols('x y')
f = x**2 + y**2 - 2*x*y + 2*x - 4*y + 3

# Compute the gradient
grad_f = [sp.diff(f, var) for var in (x, y)]
print(f"Gradient: [{grad_f[0]}, {grad_f[1]}]")

# Find critical points by setting gradient to zero
critical_points = sp.solve(grad_f, (x, y))
print(f"Critical points: {critical_points}")

# Determine the nature of critical points using the Hessian matrix
hessian = [[sp.diff(grad_f[i], var) for var in (x, y)] for i in range(2)]
print("Hessian matrix:")
for row in hessian:
    print(row)

# Evaluate the Hessian at the critical point
x_val, y_val = critical_points[0]
hessian_at_cp = [[float(h.subs({x: x_val, y: y_val})) for h in row] for row in hessian]
print(f"Hessian at critical point {critical_points[0]}:")
for row in hessian_at_cp:
    print(row)

# Compute eigenvalues to determine if it's a minimum, maximum, or saddle point
eigenvalues = np.linalg.eigvals(hessian_at_cp)
print(f"Eigenvalues of Hessian: {eigenvalues}")

if all(eigenvalues > 0):
    print("The critical point is a minimum")
elif all(eigenvalues < 0):
    print("The critical point is a maximum")
else:
    print("The critical point is a saddle point")
```

### 2. Gradient in Linear Regression

In linear regression, we minimize the mean squared error (MSE) loss function:

$$L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - (\mathbf{w} \cdot \mathbf{x}_i + b))^2$$

The gradients with respect to weights w and bias b are:

$$\nabla_{\mathbf{w}} L = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (\mathbf{w} \cdot \mathbf{x}_i + b)) \mathbf{x}_i$$

$$\frac{\partial L}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (\mathbf{w} \cdot \mathbf{x}_i + b))$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression

# Generate synthetic data
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = X.flatten()

# Compute the MSE loss for a range of weight and bias values
w_range = np.linspace(-3, 3, 100)
b_range = np.linspace(-3, 3, 100)
W, B = np.meshgrid(w_range, b_range)
Z = np.zeros_like(W)

for i in range(len(w_range)):
    for j in range(len(b_range)):
        w, b = w_range[i], b_range[j]
        predictions = w * X + b
        Z[j, i] = np.mean((y - predictions) ** 2)  # MSE loss

# Compute the gradient of the loss function
def compute_gradients(w, b, X, y):
    n = len(X)
    predictions = w * X + b
    residuals = y - predictions
    dL_dw = -2 * np.mean(residuals * X)
    dL_db = -2 * np.mean(residuals)
    return dL_dw, dL_db

# Compute gradients at grid points for visualization
U = np.zeros_like(W)
V = np.zeros_like(B)

for i in range(len(w_range)):
    for j in range(len(b_range)):
        w, b = w_range[i], b_range[j]
        dL_dw, dL_db = compute_gradients(w, b, X, y)
        U[j, i] = -dL_dw  # Negative gradient for descent direction
        V[j, i] = -dL_db

# Normalize for better visualization
norm = np.sqrt(U**2 + V**2)
U_norm = U / (norm + 1e-10)
V_norm = V / (norm + 1e-10)

# Plot the loss landscape and gradient field
plt.figure(figsize=(12, 10))
contour = plt.contour(W, B, Z, 50, cmap='viridis')
plt.colorbar(label='MSE Loss')
quiver = plt.quiver(W[::5, ::5], B[::5, ::5], U_norm[::5, ::5], V_norm[::5, ::5], 
                   color='r', scale=30)
plt.xlabel('Weight (w)')
plt.ylabel('Bias (b)')
plt.title('MSE Loss Landscape with Gradient Descent Directions')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Implement gradient descent to find the minimum
def gradient_descent(X, y, learning_rate=0.01, n_iterations=100):
    w = 0  # Initialize weight
    b = 0  # Initialize bias
    
    # Store history for visualization
    w_history = [w]
    b_history = [b]
    loss_history = [np.mean((y - (w * X + b)) ** 2)]
    
    for i in range(n_iterations):
        dL_dw, dL_db = compute_gradients(w, b, X, y)
        
        # Update parameters
        w = w - learning_rate * dL_dw
        b = b - learning_rate * dL_db
        
        # Store history
        w_history.append(w)
        b_history.append(b)
        loss_history.append(np.mean((y - (w * X + b)) ** 2))
    
    return w, b, w_history, b_history, loss_history

# Run gradient descent
final_w, final_b, w_history, b_history, loss_history = gradient_descent(X, y, learning_rate=0.01, n_iterations=100)

# Plot the loss landscape with gradient descent path
plt.figure(figsize=(12, 10))
contour = plt.contour(W, B, Z, 50, cmap='viridis')
plt.colorbar(label='MSE Loss')
plt.plot(w_history, b_history, 'o-', color='red', markersize=3, linewidth=1, label='Gradient Descent Path')
plt.xlabel('Weight (w)')
plt.ylabel('Bias (b)')
plt.title('Gradient Descent Path on MSE Loss Landscape')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history)
plt.xlabel('Iteration')
plt.ylabel('MSE Loss')
plt.title('Loss vs. Iteration')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the final regression line
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.7, label='Data Points')
plt.plot(X, final_w * X + final_b, 'r-', linewidth=2, label=f'Fitted Line: y = {final_w:.4f}x + {final_b:.4f}')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression using Gradient Descent')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

### 3. Gradient in Logistic Regression

In logistic regression, we minimize the binary cross-entropy loss:

$$L(\mathbf{w}, b) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\sigma(\mathbf{w} \cdot \mathbf{x}_i + b)) + (1 - y_i) \log(1 - \sigma(\mathbf{w} \cdot \mathbf{x}_i + b))]$$

where σ is the sigmoid function: $\sigma(z) = \frac{1}{1 + e^{-z}}$

The gradients are:

$$\nabla_{\mathbf{w}} L = \frac{1}{n} \sum_{i=1}^{n} (\sigma(\mathbf{w} \cdot \mathbf{x}_i + b) - y_i) \mathbf{x}_i$$

$$\frac{\partial L}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (\sigma(\mathbf{w} \cdot \mathbf{x}_i + b) - y_i)$$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Generate synthetic binary classification data
X, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, 
                          n_clusters_per_class=1, random_state=42)
X = X.flatten()

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -30, 30)))  # Clip to avoid overflow

# Binary cross-entropy loss
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15  # Small constant to avoid log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions to avoid log(0)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Compute loss for a range of weight and bias values
w_range = np.linspace(-5, 5, 100)
b_range = np.linspace(-5, 5, 100)
W, B = np.meshgrid(w_range, b_range)
Z = np.zeros_like(W)

for i in range(len(w_range)):
    for j in range(len(b_range)):
        w, b = w_range[i], b_range[j]
        z = w * X + b
        predictions = sigmoid(z)
        Z[j, i] = binary_cross_entropy(y, predictions)

# Compute the gradient of the loss function
def compute_gradients(w, b, X, y):
    n = len(X)
    z = w * X + b
    predictions = sigmoid(z)
    dL_dw = np.mean((predictions - y) * X)
    dL_db = np.mean(predictions - y)
    return dL_dw, dL_db

# Implement gradient descent
def gradient_descent(X, y, learning_rate=0.1, n_iterations=100):
    w = 0  # Initialize weight
    b = 0  # Initialize bias
    
    # Store history for visualization
    w_history = [w]
    b_history = [b]
    loss_history = []
    
    # Compute initial loss
    z = w * X + b
    predictions = sigmoid(z)
    loss_history.append(binary_cross_entropy(y, predictions))
    
    for i in range(n_iterations):
        dL_dw, dL_db = compute_gradients(w, b, X, y)
        
        # Update parameters
        w = w - learning_rate * dL_dw
        b = b - learning_rate * dL_db
        
        # Store history
        w_history.append(w)
        b_history.append(b)
        
        # Compute loss
        z = w * X + b
        predictions = sigmoid(z)
        loss_history.append(binary_cross_entropy(y, predictions))
    
    return w, b, w_history, b_history, loss_history

# Run gradient descent
final_w, final_b, w_history, b_history, loss_history = gradient_descent(X, y, learning_rate=0.5, n_iterations=100)

# Plot the loss landscape with gradient descent path
plt.figure(figsize=(12, 10))
contour = plt.contour(W, B, Z, 50, cmap='viridis')
plt.colorbar(label='Binary Cross-Entropy Loss')
plt.plot(w_history, b_history, 'o-', color='red', markersize=3, linewidth=1, label='Gradient Descent Path')
plt.xlabel('Weight (w)')
plt.ylabel('Bias (b)')
plt.title('Gradient Descent Path on Binary Cross-Entropy Loss Landscape')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history)
plt.xlabel('Iteration')
plt.ylabel('Binary Cross-Entropy Loss')
plt.title('Loss vs. Iteration')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the final decision boundary
plt.figure(figsize=(10, 6))
plt.scatter(X, y, c=y, cmap='coolwarm', alpha=0.7, edgecolors='k', linewidths=0.5)

# Create a range of X values for the decision boundary
X_range = np.linspace(min(X), max(X), 1000)
# Compute the corresponding z values
z = final_w * X_range + final_b
# Apply sigmoid to get probabilities
probs = sigmoid(z)

plt.plot(X_range, probs, 'g-', linewidth=2, label='Probability Curve')
plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Boundary (p=0.5)')
plt.xlabel('X')
plt.ylabel('Probability / Class')
plt.title('Logistic Regression using Gradient Descent')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

## Summary

In this section, we've covered:

1. **Derivatives**: The fundamental concept of rate of change
2. **Common Derivative Rules**: Power rule, sum rule, product rule, chain rule
3. **Derivatives of Common Functions**: Including sigmoid, tanh, and ReLU used in neural networks
4. **Gradients**: Derivatives in multiple dimensions
5. **Directional Derivatives**: Rate of change in a specific direction
6. **Applications in Machine Learning**: Finding critical points, gradient descent in linear and logistic regression

These concepts form the foundation for understanding how machine learning models learn from data through optimization. In the next section, we'll explore partial derivatives in more detail, which are essential for understanding how to optimize functions of multiple variables.

## References

1. Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Stewart, J. (2015). Calculus: Early Transcendentals (8th ed.). Cengage Learning.
4. Ng, A. (2018). Machine Learning Yearning. deeplearning.ai.
