# 2.1 Linear Algebra for Machine Learning

## Introduction

Linear algebra is the branch of mathematics that deals with vector spaces and linear mappings between these spaces. It forms the mathematical foundation for much of machine learning, particularly for:

- Representing and manipulating data
- Transforming data into new spaces
- Reducing dimensionality of data
- Solving systems of linear equations
- Decomposing complex structures into simpler components

In this section, we'll cover the essential linear algebra concepts needed for machine learning, with practical examples using NumPy.

## 2.1.1 Scalars, Vectors, Matrices, and Tensors

### Scalars

A scalar is a single number, either real or integer. In Python, we represent scalars as regular numbers:

```python
# Examples of scalars
x = 5           # Integer scalar
y = 3.14        # Real scalar
learning_rate = 0.01  # A typical scalar parameter in ML
```

### Vectors

A vector is a one-dimensional array of numbers. In machine learning, vectors often represent:
- Features of a single data point
- Weights in a model
- Gradients during optimization

Mathematically, an n-dimensional vector x is represented as:

$$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$$

In NumPy, we represent vectors as 1D arrays:

```python
import numpy as np

# Creating vectors
feature_vector = np.array([1.2, 3.4, 5.6, 7.8])  # A feature vector
weight_vector = np.array([0.1, 0.2, 0.3, 0.4])   # A weight vector

# Vector properties
print(f"Vector shape: {feature_vector.shape}")
print(f"Vector dimension: {feature_vector.ndim}")
print(f"Vector size: {feature_vector.size}")
```

### Matrices

A matrix is a two-dimensional array of numbers arranged in rows and columns. In machine learning, matrices often represent:
- Multiple data points (each row is a data point, each column is a feature)
- Linear transformations
- Weights in neural network layers

Mathematically, an m×n matrix A is represented as:

$$\mathbf{A} = \begin{bmatrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}$$

In NumPy, we represent matrices as 2D arrays:

```python
# Creating matrices
data_matrix = np.array([
    [1.2, 2.3, 3.4],  # First data point
    [4.5, 5.6, 6.7],  # Second data point
    [7.8, 8.9, 9.0]   # Third data point
])

# Matrix properties
print(f"Matrix shape: {data_matrix.shape}")  # (3, 3) - 3 rows, 3 columns
print(f"Matrix dimension: {data_matrix.ndim}")  # 2 - two-dimensional
print(f"Matrix size: {data_matrix.size}")  # 9 - total number of elements

# Accessing elements
print(f"First row: {data_matrix[0]}")
print(f"First column: {data_matrix[:, 0]}")
print(f"Element at position (1,2): {data_matrix[1, 2]}")  # Row 1, Column 2
```

### Tensors

A tensor is a generalization of vectors and matrices to higher dimensions. In machine learning, tensors often represent:
- Batches of images (4D: batch size, height, width, channels)
- Time series data with multiple features (3D: batch size, time steps, features)
- Weights in convolutional layers

In NumPy, we represent tensors as n-dimensional arrays:

```python
# Creating a 3D tensor (batch of 2 matrices)
tensor_3d = np.array([
    [  # First matrix
        [1, 2, 3],
        [4, 5, 6]
    ],
    [  # Second matrix
        [7, 8, 9],
        [10, 11, 12]
    ]
])

print(f"Tensor shape: {tensor_3d.shape}")  # (2, 2, 3)
print(f"Tensor dimension: {tensor_3d.ndim}")  # 3

# Creating a 4D tensor (batch of images)
# Shape: (batch_size, height, width, channels)
batch_size, height, width, channels = 2, 3, 3, 3
image_batch = np.random.rand(batch_size, height, width, channels)
print(f"Image batch shape: {image_batch.shape}")  # (2, 3, 3, 3)
```

## 2.1.2 Vector Operations

### Vector Addition and Subtraction

Vector addition and subtraction are performed element-wise:

$$\mathbf{c} = \mathbf{a} + \mathbf{b} \implies c_i = a_i + b_i$$
$$\mathbf{d} = \mathbf{a} - \mathbf{b} \implies d_i = a_i - b_i$$

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# Vector addition
c = a + b  # [5, 7, 9]
print(f"a + b = {c}")

# Vector subtraction
d = a - b  # [-3, -3, -3]
print(f"a - b = {d}")
```

### Scalar Multiplication

Multiplying a vector by a scalar scales each element:

$$\mathbf{c} = \alpha \mathbf{a} \implies c_i = \alpha a_i$$

```python
a = np.array([1, 2, 3])
alpha = 2.5

# Scalar multiplication
c = alpha * a  # [2.5, 5.0, 7.5]
print(f"{alpha} * a = {c}")
```

### Dot Product

The dot product of two vectors is the sum of the products of their corresponding elements:

$$\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$$

The dot product is a measure of similarity between vectors, especially when they are normalized.

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# Dot product
dot_product = np.dot(a, b)  # 1*4 + 2*5 + 3*6 = 32
print(f"a · b = {dot_product}")

# Alternative syntax
dot_product_alt = a @ b  # Python 3.5+ syntax
print(f"a @ b = {dot_product_alt}")
```

### Vector Norm (Magnitude)

The norm or magnitude of a vector is a measure of its length. The most common is the L2 norm (Euclidean norm):

$$\|\mathbf{a}\|_2 = \sqrt{\sum_{i=1}^{n} a_i^2} = \sqrt{a_1^2 + a_2^2 + \cdots + a_n^2}$$

```python
a = np.array([3, 4])

# L2 norm (Euclidean)
l2_norm = np.linalg.norm(a)  # sqrt(3^2 + 4^2) = sqrt(25) = 5
print(f"||a||_2 = {l2_norm}")

# L1 norm (Manhattan distance)
l1_norm = np.linalg.norm(a, ord=1)  # |3| + |4| = 7
print(f"||a||_1 = {l1_norm}")

# Max norm (infinity norm)
max_norm = np.linalg.norm(a, ord=np.inf)  # max(|3|, |4|) = 4
print(f"||a||_∞ = {max_norm}")
```

### Unit Vectors

A unit vector has a norm of 1. We can normalize a vector by dividing it by its norm:

$$\hat{\mathbf{a}} = \frac{\mathbf{a}}{\|\mathbf{a}\|_2}$$

```python
a = np.array([3, 4])

# Normalize to unit vector
a_unit = a / np.linalg.norm(a)  # [0.6, 0.8]
print(f"Unit vector â = {a_unit}")
print(f"Norm of unit vector: {np.linalg.norm(a_unit)}")  # Should be 1.0
```

### Vector Projection

The projection of vector a onto vector b is:

$$\text{proj}_{\mathbf{b}} \mathbf{a} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{b}\|_2^2} \mathbf{b}$$

```python
a = np.array([3, 4])
b = np.array([5, 0])  # Unit vector along x-axis

# Projection of a onto b
proj_scalar = np.dot(a, b) / np.dot(b, b)  # (3*5 + 4*0) / (5^2 + 0^2) = 15/25 = 0.6
proj_vector = proj_scalar * b  # 0.6 * [5, 0] = [3, 0]

print(f"Projection of a onto b: {proj_vector}")
```

## 2.1.3 Matrix Operations

### Matrix Addition and Subtraction

Matrix addition and subtraction are performed element-wise, and require matrices of the same shape:

$$\mathbf{C} = \mathbf{A} + \mathbf{B} \implies c_{ij} = a_{ij} + b_{ij}$$
$$\mathbf{D} = \mathbf{A} - \mathbf{B} \implies d_{ij} = a_{ij} - b_{ij}$$

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix addition
C = A + B
print("A + B =")
print(C)

# Matrix subtraction
D = A - B
print("A - B =")
print(D)
```

### Scalar Multiplication

Multiplying a matrix by a scalar scales each element:

$$\mathbf{C} = \alpha \mathbf{A} \implies c_{ij} = \alpha a_{ij}$$

```python
A = np.array([[1, 2], [3, 4]])
alpha = 2.5

# Scalar multiplication
C = alpha * A
print(f"{alpha} * A =")
print(C)
```

### Matrix Multiplication

Matrix multiplication (or matrix product) is a fundamental operation that combines two matrices to produce a third matrix:

$$\mathbf{C} = \mathbf{A} \mathbf{B} \implies c_{ij} = \sum_{k=1}^{p} a_{ik} b_{kj}$$

For this operation to be defined, the number of columns in A must equal the number of rows in B.

```python
A = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3 matrix
B = np.array([[7, 8], [9, 10], [11, 12]])  # 3x2 matrix

# Matrix multiplication
C = np.matmul(A, B)  # or A @ B in Python 3.5+
print("A × B =")
print(C)

# Element-wise multiplication (Hadamard product)
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A * B  # Element-wise multiplication
print("A * B (element-wise) =")
print(C)
```

### Matrix Transpose

The transpose of a matrix flips it over its diagonal, switching rows and columns:

$$\mathbf{A}^T_{ij} = \mathbf{A}_{ji}$$

```python
A = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3 matrix

# Matrix transpose
A_T = A.T  # 3x2 matrix
print("A =")
print(A)
print("A^T =")
print(A_T)
```

### Matrix Inverse

The inverse of a square matrix A, denoted A⁻¹, is a matrix such that:

$$\mathbf{A} \mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A} = \mathbf{I}$$

where I is the identity matrix. Not all matrices have inverses (only square matrices with full rank).

```python
A = np.array([[4, 7], [2, 6]])

# Matrix inverse
try:
    A_inv = np.linalg.inv(A)
    print("A^-1 =")
    print(A_inv)
    
    # Verify: A × A^-1 = I
    I = np.matmul(A, A_inv)
    print("A × A^-1 =")
    print(np.round(I, decimals=10))  # Round to handle floating-point errors
except np.linalg.LinAlgError:
    print("Matrix is not invertible")
```

### Matrix Determinant

The determinant is a scalar value that can be computed from a square matrix and has important geometric and algebraic interpretations:

```python
A = np.array([[4, 7], [2, 6]])

# Matrix determinant
det_A = np.linalg.det(A)
print(f"det(A) = {det_A}")  # 4*6 - 7*2 = 24 - 14 = 10
```

## 2.1.4 Linear Transformations

A linear transformation is a function between vector spaces that preserves vector addition and scalar multiplication. Matrices represent linear transformations.

When we multiply a matrix A by a vector x, we're applying a linear transformation to x:

$$\mathbf{y} = \mathbf{A} \mathbf{x}$$

### Geometric Interpretation

Different matrices produce different types of transformations:
- Rotation
- Scaling
- Shearing
- Reflection
- Projection

Let's visualize some common transformations:

```python
import numpy as np
import matplotlib.pyplot as plt

def plot_transformation(A, title):
    # Create a grid of points
    x = np.linspace(-5, 5, 10)
    y = np.linspace(-5, 5, 10)
    X, Y = np.meshgrid(x, y)
    points = np.vstack([X.flatten(), Y.flatten()])
    
    # Apply transformation
    transformed = A @ points
    
    # Plot original and transformed grid
    plt.figure(figsize=(12, 6))
    
    # Original grid
    plt.subplot(1, 2, 1)
    plt.scatter(points[0], points[1], c='blue', alpha=0.5)
    plt.grid(True)
    plt.axis('equal')
    plt.title('Original Grid')
    
    # Transformed grid
    plt.subplot(1, 2, 2)
    plt.scatter(transformed[0], transformed[1], c='red', alpha=0.5)
    plt.grid(True)
    plt.axis('equal')
    plt.title(f'After {title}')
    
    plt.tight_layout()
    plt.show()

# Identity transformation
I = np.eye(2)
plot_transformation(I, "Identity Transformation")

# Scaling
S = np.array([[2, 0], [0, 0.5]])
plot_transformation(S, "Scaling")

# Rotation (45 degrees)
theta = np.pi/4  # 45 degrees
R = np.array([[np.cos(theta), -np.sin(theta)], 
               [np.sin(theta), np.cos(theta)]])
plot_transformation(R, "Rotation (45°)")

# Shearing
H = np.array([[1, 1], [0, 1]])
plot_transformation(H, "Horizontal Shear")

# Reflection (across y-axis)
F = np.array([[-1, 0], [0, 1]])
plot_transformation(F, "Reflection across y-axis")
```

### Applications in Machine Learning

Linear transformations are fundamental in machine learning:

1. **Feature Scaling**: Normalizing or standardizing features
2. **Dimensionality Reduction**: PCA transforms data to a new coordinate system
3. **Neural Networks**: Each layer applies a linear transformation followed by a non-linear activation
4. **Data Augmentation**: Applying transformations to images (rotation, scaling, etc.)

## 2.1.5 Eigenvalues and Eigenvectors

An eigenvector of a square matrix A is a non-zero vector v such that when A is multiplied by v, the result is a scalar multiple of v:

$$\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$$

where λ is the eigenvalue corresponding to eigenvector v.

### Computing Eigenvalues and Eigenvectors

```python
A = np.array([[4, 2], [1, 3]])

# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Matrix A:")
print(A)
print("\nEigenvalues:")
print(eigenvalues)
print("\nEigenvectors (by column):")
print(eigenvectors)

# Verify Av = λv for the first eigenvector
v1 = eigenvectors[:, 0]
lambda1 = eigenvalues[0]
Av1 = A @ v1
lambda_v1 = lambda1 * v1

print("\nVerification for first eigenvector:")
print(f"Av₁ = {Av1}")
print(f"λ₁v₁ = {lambda_v1}")
```

### Significance in Machine Learning

Eigenvalues and eigenvectors are crucial in:

1. **Principal Component Analysis (PCA)**: The eigenvectors of the covariance matrix define the principal components
2. **Spectral Clustering**: Using eigenvectors of the graph Laplacian
3. **Google's PageRank**: The page rankings are given by the eigenvector of the web graph
4. **Facial Recognition**: Eigenfaces method

Let's implement a simple PCA example:

```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the data
X_std = StandardScaler().fit_transform(X)

# Compute the covariance matrix
cov_matrix = np.cov(X_std.T)

# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort eigenvectors by decreasing eigenvalues
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

# Project data onto the first two principal components
PC1 = X_std @ eigenvectors[:, 0]
PC2 = X_std @ eigenvectors[:, 1]

# Plot the data in the PC space
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(PC1[y == i], PC2[y == i], label=target_name)

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Explained variance
explained_variance = eigenvalues / np.sum(eigenvalues)
print("Explained variance ratio:")
print(explained_variance)
print(f"First two components explain {100 * np.sum(explained_variance[:2]):.2f}% of variance")
```

## 2.1.6 Norms

Norms measure the "size" or "length" of vectors, matrices, and tensors. They're used extensively in regularization and optimization.

### Vector Norms

We've already seen the L1, L2, and max norms for vectors. Here's a more general definition:

The Lp norm of a vector x is:

$$\|\mathbf{x}\|_p = \left( \sum_{i=1}^{n} |x_i|^p \right)^{1/p}$$

```python
x = np.array([3, 4, 5])

# L1 norm (sum of absolute values)
l1_norm = np.linalg.norm(x, ord=1)  # |3| + |4| + |5| = 12
print(f"||x||₁ = {l1_norm}")

# L2 norm (Euclidean)
l2_norm = np.linalg.norm(x)  # sqrt(3² + 4² + 5²) = sqrt(50) ≈ 7.07
print(f"||x||₂ = {l2_norm}")

# Max norm (largest absolute value)
max_norm = np.linalg.norm(x, ord=np.inf)  # max(|3|, |4|, |5|) = 5
print(f"||x||₍ₘₐₓ₎ = {max_norm}")
```

### Matrix Norms

Matrix norms measure the "size" of a matrix:

```python
A = np.array([[1, 2], [3, 4]])

# Frobenius norm (Euclidean norm of all elements)
frob_norm = np.linalg.norm(A, 'fro')  # sqrt(1² + 2² + 3² + 4²) = sqrt(30) ≈ 5.48
print(f"||A||₍F₎ = {frob_norm}")

# Nuclear norm (sum of singular values)
nuclear_norm = np.linalg.norm(A, 'nuc')
print(f"||A||₍*₎ = {nuclear_norm}")

# Spectral norm (largest singular value)
spectral_norm = np.linalg.norm(A, 2)
print(f"||A||₍₂₎ = {spectral_norm}")
```

### Applications in Machine Learning

Norms are used in:

1. **Regularization**: L1 and L2 regularization (Ridge, Lasso)
2. **Gradient Clipping**: Preventing exploding gradients
3. **Error Measurement**: RMSE uses the L2 norm
4. **Convergence Criteria**: Stopping when the norm of the gradient is small

Example of L2 regularization (Ridge regression):

```python
from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models with different regularization strengths
alphas = [0, 0.1, 1.0, 10.0]
for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    
    # Predict and evaluate
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    
    # Calculate the L2 norm of the weights
    weight_norm = np.linalg.norm(model.coef_)
    
    print(f"Alpha = {alpha}:")
    print(f"  MSE on test set: {mse:.4f}")
    print(f"  L2 norm of weights: {weight_norm:.4f}")
```

## 2.1.7 Singular Value Decomposition (SVD)

SVD is a matrix factorization technique that decomposes a matrix A into three matrices:

$$\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$$

where:
- U is an m×m orthogonal matrix
- Σ is an m×n diagonal matrix with non-negative real numbers (singular values)
- V^T is the transpose of an n×n orthogonal matrix

```python
A = np.array([[1, 2], [3, 4], [5, 6]])

# Compute SVD
U, sigma, VT = np.linalg.svd(A)

print("Matrix A:")
print(A)
print("\nU:")
print(U)
print("\nSingular values:")
print(sigma)
print("\nV^T:")
print(VT)

# Reconstruct the original matrix
# Create full Sigma matrix (diagonal matrix with singular values)
Sigma = np.zeros((A.shape[0], A.shape[1]))
np.fill_diagonal(Sigma, sigma)

# Reconstruct A = U * Sigma * V^T
A_reconstructed = U @ Sigma @ VT
print("\nReconstructed A:")
print(A_reconstructed)
```

### Applications in Machine Learning

SVD is used in:

1. **Principal Component Analysis**: Closely related to SVD
2. **Image Compression**: Keeping only the largest singular values
3. **Recommendation Systems**: Matrix factorization for collaborative filtering
4. **Natural Language Processing**: Latent Semantic Analysis (LSA)

Let's implement a simple image compression example:

```python
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from urllib.request import urlopen
from io import BytesIO

# Load a sample image (you can replace with any URL or local file)
url = "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Felis_silvestris_silvestris_small_gradual_decrease_of_quality.png/200px-Felis_silvestris_silvestris_small_gradual_decrease_of_quality.png"
response = urlopen(url)
img = Image.open(BytesIO(response.read())).convert('L')  # Convert to grayscale
img_array = np.array(img)

# Perform SVD
U, sigma, VT = np.linalg.svd(img_array, full_matrices=False)

# Plot the singular values
plt.figure(figsize=(10, 4))
plt.semilogy(sigma)
plt.title('Singular Values')
plt.xlabel('Index')
plt.ylabel('Singular Value (log scale)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Reconstruct the image with different numbers of singular values
k_values = [5, 20, 50]
fig, axes = plt.subplots(1, len(k_values) + 1, figsize=(15, 5))

# Original image
axes[0].imshow(img_array, cmap='gray')
axes[0].set_title('Original')
axes[0].axis('off')

# Compressed images
for i, k in enumerate(k_values):
    # Reconstruct with k singular values
    img_compressed = U[:, :k] @ np.diag(sigma[:k]) @ VT[:k, :]
    
    # Calculate compression ratio
    original_size = img_array.size
    compressed_size = k * (U.shape[0] + VT.shape[1] + 1)
    compression_ratio = original_size / compressed_size
    
    # Display
    axes[i+1].imshow(img_compressed, cmap='gray')
    axes[i+1].set_title(f'k={k}\nCompression: {compression_ratio:.1f}x')
    axes[i+1].axis('off')

plt.tight_layout()
plt.show()
```

## Summary

In this section, we've covered the essential linear algebra concepts for machine learning:

1. **Scalars, Vectors, Matrices, and Tensors**: The basic building blocks for representing data and models
2. **Vector Operations**: Addition, subtraction, dot product, norms
3. **Matrix Operations**: Addition, multiplication, transpose, inverse
4. **Linear Transformations**: How matrices transform vectors geometrically
5. **Eigenvalues and Eigenvectors**: Special vectors that maintain their direction under transformation
6. **Norms**: Measures of size for vectors and matrices
7. **Singular Value Decomposition**: A powerful matrix factorization technique

These concepts form the mathematical foundation for many machine learning algorithms and techniques. In the next section, we'll explore calculus concepts that are essential for optimization in machine learning.

## References

1. Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press.
4. NumPy Documentation: https://numpy.org/doc/stable/reference/routines.linalg.html
