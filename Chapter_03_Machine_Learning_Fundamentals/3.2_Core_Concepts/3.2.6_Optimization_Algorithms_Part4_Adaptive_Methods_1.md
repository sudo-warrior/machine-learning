# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (1)

## Adaptive Learning Rate Methods

While momentum methods help accelerate convergence, they still use a global learning rate that applies equally to all parameters. Adaptive learning rate methods adjust the learning rate for each parameter individually, which can significantly improve performance, especially for problems with sparse gradients or ill-conditioned loss landscapes.

### AdaGrad

**AdaGrad** (Adaptive Gradient) adapts the learning rate for each parameter based on the historical gradients:

$$g_{t,i} = \nabla_{\theta_i} J(\theta_t)$$
$$v_{t,i} = v_{t-1,i} + g_{t,i}^2$$
$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{v_{t,i} + \epsilon}} g_{t,i}$$

where:
- $g_{t,i}$ is the gradient of parameter $i$ at iteration $t$
- $v_{t,i}$ is the sum of squared gradients for parameter $i$ up to iteration $t$
- $\alpha$ is the base learning rate
- $\epsilon$ is a small constant to avoid division by zero

AdaGrad effectively gives frequently updated parameters smaller learning rates and infrequently updated parameters larger learning rates, which is particularly useful for sparse data.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Add bias term to X
X_b = np.c_[np.ones(X.shape[0]), X]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Implement AdaGrad
def adagrad(X, y, learning_rate, num_iterations, epsilon=1e-8):
    """
    Perform AdaGrad optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Base learning rate
    - num_iterations: Number of iterations
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    v = np.zeros(n)  # Accumulated squared gradients
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update accumulated squared gradients
        v += gradient**2
        
        # Update parameters
        theta -= learning_rate * gradient / (np.sqrt(v) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run standard gradient descent and AdaGrad for comparison
learning_rate_gd = 0.1
learning_rate_adagrad = 0.5  # AdaGrad often works better with a larger initial learning rate
num_iterations = 100

# Standard Gradient Descent
def gradient_descent(X, y, learning_rate, num_iterations):
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        theta = theta - learning_rate * gradient
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run the algorithms
theta_gd, cost_gd = gradient_descent(X_b, y, learning_rate_gd, num_iterations)
theta_adagrad, cost_adagrad = adagrad(X_b, y, learning_rate_adagrad, num_iterations)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(cost_gd)), cost_gd, linewidth=2, label=f'GD (lr={learning_rate_gd})')
plt.plot(range(len(cost_adagrad)), cost_adagrad, linewidth=2, label=f'AdaGrad (lr={learning_rate_adagrad})')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_gd)), cost_gd, linewidth=2, label=f'GD (lr={learning_rate_gd})')
plt.semilogy(range(len(cost_adagrad)), cost_adagrad, linewidth=2, label=f'AdaGrad (lr={learning_rate_adagrad})')
plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter trajectories
plt.subplot(2, 2, 3)
plt.plot(theta_gd[:, 0], theta_gd[:, 1], 'o-', linewidth=2, markersize=4, label='GD')
plt.plot(theta_adagrad[:, 0], theta_adagrad[:, 1], 'o-', linewidth=2, markersize=4, label='AdaGrad')
plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

# Plot effective learning rates
plt.subplot(2, 2, 4)
# Calculate effective learning rates for AdaGrad
gradients = np.array([linear_regression_gradient(theta, X_b, y) for theta in theta_adagrad])
v_values = np.cumsum(gradients**2, axis=0)
effective_lr = learning_rate_adagrad / np.sqrt(v_values + 1e-8)

plt.semilogy(range(len(effective_lr)), effective_lr[:, 0], linewidth=2, label='θ₀ Effective LR')
plt.semilogy(range(len(effective_lr)), effective_lr[:, 1], linewidth=2, label='θ₁ Effective LR')
plt.axhline(y=learning_rate_gd, color='k', linestyle='--', label=f'GD LR = {learning_rate_gd}')
plt.xlabel('Iteration')
plt.ylabel('Effective Learning Rate (log scale)')
plt.title('AdaGrad Effective Learning Rates')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### RMSProp

**RMSProp** (Root Mean Square Propagation) addresses a limitation of AdaGrad: the accumulated squared gradients in the denominator can grow very large, causing the learning rate to become infinitesimally small and the algorithm to stop learning. RMSProp uses an exponentially weighted moving average instead:

$$g_{t,i} = \nabla_{\theta_i} J(\theta_t)$$
$$v_{t,i} = \beta v_{t-1,i} + (1 - \beta) g_{t,i}^2$$
$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{v_{t,i} + \epsilon}} g_{t,i}$$

where:
- $\beta$ is the decay rate (typically 0.9)

This prevents the learning rate from decreasing too rapidly, allowing RMSProp to continue making progress even after many iterations.

```python
# Implement RMSProp
def rmsprop(X, y, learning_rate, num_iterations, beta=0.9, epsilon=1e-8):
    """
    Perform RMSProp optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Base learning rate
    - num_iterations: Number of iterations
    - beta: Decay rate for moving average
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    v = np.zeros(n)  # Exponentially weighted average of squared gradients
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update exponentially weighted average
        v = beta * v + (1 - beta) * gradient**2
        
        # Update parameters
        theta -= learning_rate * gradient / (np.sqrt(v) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run AdaGrad and RMSProp for comparison
learning_rate = 0.1
num_iterations = 100
beta = 0.9

# Run the algorithms
theta_adagrad, cost_adagrad = adagrad(X_b, y, learning_rate, num_iterations)
theta_rmsprop, cost_rmsprop = rmsprop(X_b, y, learning_rate, num_iterations, beta)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(cost_adagrad)), cost_adagrad, linewidth=2, label='AdaGrad')
plt.plot(range(len(cost_rmsprop)), cost_rmsprop, linewidth=2, label=f'RMSProp (β={beta})')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_adagrad)), cost_adagrad, linewidth=2, label='AdaGrad')
plt.semilogy(range(len(cost_rmsprop)), cost_rmsprop, linewidth=2, label=f'RMSProp (β={beta})')
plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter trajectories
plt.subplot(2, 2, 3)
plt.plot(theta_adagrad[:, 0], theta_adagrad[:, 1], 'o-', linewidth=2, markersize=4, label='AdaGrad')
plt.plot(theta_rmsprop[:, 0], theta_rmsprop[:, 1], 'o-', linewidth=2, markersize=4, label='RMSProp')
plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

# Plot effective learning rates
plt.subplot(2, 2, 4)
# Calculate effective learning rates for AdaGrad
gradients_adagrad = np.array([linear_regression_gradient(theta, X_b, y) for theta in theta_adagrad])
v_values_adagrad = np.cumsum(gradients_adagrad**2, axis=0)
effective_lr_adagrad = learning_rate / np.sqrt(v_values_adagrad + 1e-8)

# Calculate effective learning rates for RMSProp
gradients_rmsprop = np.array([linear_regression_gradient(theta, X_b, y) for theta in theta_rmsprop])
v_values_rmsprop = np.zeros((len(gradients_rmsprop), 2))
v = np.zeros(2)
for i, grad in enumerate(gradients_rmsprop):
    v = beta * v + (1 - beta) * grad**2
    v_values_rmsprop[i] = v
effective_lr_rmsprop = learning_rate / np.sqrt(v_values_rmsprop + 1e-8)

plt.semilogy(range(len(effective_lr_adagrad)), effective_lr_adagrad[:, 1], linewidth=2, label='AdaGrad θ₁')
plt.semilogy(range(len(effective_lr_rmsprop)), effective_lr_rmsprop[:, 1], linewidth=2, label='RMSProp θ₁')
plt.xlabel('Iteration')
plt.ylabel('Effective Learning Rate (log scale)')
plt.title('Effective Learning Rates for θ₁')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Comparison on Challenging Functions

Let's compare the performance of standard gradient descent, AdaGrad, and RMSProp on more challenging functions:

```python
# Define challenging functions
def rosenbrock_function(theta):
    """
    Rosenbrock function: f(x,y) = (1-x)² + 100(y-x²)²
    Global minimum at (1,1)
    """
    return (1 - theta[0])**2 + 100 * (theta[1] - theta[0]**2)**2

def rosenbrock_gradient(theta):
    """Gradient of the Rosenbrock function."""
    dx = -2 * (1 - theta[0]) - 400 * theta[0] * (theta[1] - theta[0]**2)
    dy = 200 * (theta[1] - theta[0]**2)
    return np.array([dx, dy])

def beale_function(theta):
    """
    Beale function: f(x,y) = (1.5-x+xy)² + (2.25-x+xy²)² + (2.625-x+xy³)²
    Global minimum at (3, 0.5)
    """
    x, y = theta
    term1 = (1.5 - x + x*y)**2
    term2 = (2.25 - x + x*y**2)**2
    term3 = (2.625 - x + x*y**3)**2
    return term1 + term2 + term3

def beale_gradient(theta):
    """Gradient of the Beale function."""
    x, y = theta
    
    # Partial derivative with respect to x
    dx = 2*(1.5 - x + x*y)*(-1 + y) + \
         2*(2.25 - x + x*y**2)*(-1 + y**2) + \
         2*(2.625 - x + x*y**3)*(-1 + y**3)
    
    # Partial derivative with respect to y
    dy = 2*(1.5 - x + x*y)*(x) + \
         2*(2.25 - x + x*y**2)*(2*x*y) + \
         2*(2.625 - x + x*y**3)*(3*x*y**2)
    
    return np.array([dx, dy])

# Implement generic optimizer function
def optimize(func, grad_func, start_point, optimizer, **kwargs):
    """
    Perform optimization using the specified optimizer.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - optimizer: Optimization algorithm ('gd', 'adagrad', or 'rmsprop')
    - **kwargs: Additional parameters for the optimizer
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    learning_rate = kwargs.get('learning_rate', 0.01)
    num_iterations = kwargs.get('num_iterations', 1000)
    epsilon = kwargs.get('epsilon', 1e-8)
    
    if optimizer == 'gd':
        # Standard Gradient Descent
        for i in range(num_iterations):
            gradient = grad_func(theta)
            theta = theta - learning_rate * gradient
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    elif optimizer == 'adagrad':
        # AdaGrad
        v = np.zeros_like(theta)
        
        for i in range(num_iterations):
            gradient = grad_func(theta)
            v += gradient**2
            theta = theta - learning_rate * gradient / (np.sqrt(v) + epsilon)
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    elif optimizer == 'rmsprop':
        # RMSProp
        beta = kwargs.get('beta', 0.9)
        v = np.zeros_like(theta)
        
        for i in range(num_iterations):
            gradient = grad_func(theta)
            v = beta * v + (1 - beta) * gradient**2
            theta = theta - learning_rate * gradient / (np.sqrt(v) + epsilon)
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Run optimizers on the Rosenbrock function
start_point_rosenbrock = np.array([-1.0, 1.0])
num_iterations = 5000
learning_rate_gd = 0.0001
learning_rate_adaptive = 0.01

results_rosenbrock = []
results_rosenbrock.append(('GD', *optimize(rosenbrock_function, rosenbrock_gradient, 
                                         start_point_rosenbrock, 'gd', 
                                         learning_rate=learning_rate_gd, 
                                         num_iterations=num_iterations)))
results_rosenbrock.append(('AdaGrad', *optimize(rosenbrock_function, rosenbrock_gradient, 
                                              start_point_rosenbrock, 'adagrad', 
                                              learning_rate=learning_rate_adaptive, 
                                              num_iterations=num_iterations)))
results_rosenbrock.append(('RMSProp', *optimize(rosenbrock_function, rosenbrock_gradient, 
                                              start_point_rosenbrock, 'rmsprop', 
                                              learning_rate=learning_rate_adaptive, 
                                              num_iterations=num_iterations)))

# Visualize the results for Rosenbrock function
plt.figure(figsize=(15, 10))

# Create a grid for contour plot
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(len(x)):
    for j in range(len(y)):
        Z[j, i] = rosenbrock_function(np.array([X[j, i], Y[j, i]]))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 10), cmap='viridis')
plt.colorbar(contour)

for name, theta_history, cost_history in results_rosenbrock:
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'o-', linewidth=1, markersize=2, label=name)

plt.scatter(1, 1, color='red', s=100, label='Global Minimum')
plt.scatter(start_point_rosenbrock[0], start_point_rosenbrock[1], color='green', s=100, label='Start')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Trajectories on Rosenbrock Function')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for name, theta_history, cost_history in results_rosenbrock:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([1, 1])  # Global minimum of Rosenbrock function
for name, theta_history, cost_history in results_rosenbrock:
    distances = np.sqrt(np.sum((theta_history - minimum)**2, axis=1))
    plt.semilogy(range(len(distances)), distances, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot final distance vs. optimizer
plt.subplot(2, 2, 4)
names = [name for name, _, _ in results_rosenbrock]
final_distances = [np.sqrt(np.sum((theta_history[-1] - minimum)**2)) 
                  for _, theta_history, _ in results_rosenbrock]
final_costs = [cost_history[-1] for _, _, cost_history in results_rosenbrock]

plt.bar(names, final_distances, color='skyblue')
plt.xlabel('Optimizer')
plt.ylabel('Final Distance to Minimum')
plt.title('Final Distance vs. Optimizer')
plt.grid(True, axis='y')

# Add annotations
for i, distance in enumerate(final_distances):
    plt.text(i, distance/2, f'{distance:.6f}', ha='center', va='center')

plt.tight_layout()
plt.show()
```

## Summary

Adaptive learning rate methods adjust the learning rate for each parameter individually, which can significantly improve optimization performance:

1. **AdaGrad**:
   - Adapts the learning rate based on the historical gradients
   - Gives smaller learning rates to frequently updated parameters
   - Works well for sparse data
   - May stop learning if accumulated squared gradients become too large

2. **RMSProp**:
   - Uses an exponentially weighted moving average of squared gradients
   - Prevents the learning rate from decreasing too rapidly
   - Works well for non-stationary objectives
   - Popular for training deep neural networks

These methods address the limitations of standard gradient descent and momentum methods by providing parameter-specific learning rates. In the next part, we'll explore Adam, which combines the benefits of momentum and adaptive learning rates, and other advanced optimization techniques.

## References

1. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.
2. Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 26-31.
3. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
