# 3.2.5 Model Evaluation - Part 5: Practical Considerations (4)

## Model Monitoring and Evaluation in Production

Evaluating models doesn't end with deployment. Continuous monitoring and evaluation in production are essential to ensure models maintain their performance over time.

### Concept Drift Detection

**Concept drift** occurs when the statistical properties of the target variable change over time, causing model performance to degrade.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Simulate data with concept drift
np.random.seed(42)

# Generate initial data (before drift)
n_samples_initial = 1000
X_initial = np.random.randn(n_samples_initial, 2)
y_initial = (X_initial[:, 0] + X_initial[:, 1] > 0).astype(int)

# Generate data after drift
n_samples_drift = 1000
X_drift = np.random.randn(n_samples_drift, 2)
# Concept drift: decision boundary rotates
y_drift = (X_drift[:, 0] - X_drift[:, 1] > 0).astype(int)

# Split data into training and monitoring periods
X_train = X_initial[:800]
y_train = y_initial[:800]

X_monitor = np.vstack([X_initial[800:], X_drift])
y_monitor = np.hstack([y_initial[800:], y_drift])

# Train a model on the initial data
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Simulate monitoring over time
window_size = 100
n_windows = len(X_monitor) // window_size

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': []
}

for i in range(n_windows):
    start_idx = i * window_size
    end_idx = (i + 1) * window_size
    
    X_window = X_monitor[start_idx:end_idx]
    y_window = y_monitor[start_idx:end_idx]
    
    y_pred = model.predict(X_window)
    
    metrics['accuracy'].append(accuracy_score(y_window, y_pred))
    metrics['precision'].append(precision_score(y_window, y_pred, zero_division=0))
    metrics['recall'].append(recall_score(y_window, y_pred, zero_division=0))
    metrics['f1'].append(f1_score(y_window, y_pred, zero_division=0))

# Visualize the metrics over time
plt.figure(figsize=(12, 6))

for i, (metric_name, metric_values) in enumerate(metrics.items()):
    plt.plot(range(n_windows), metric_values, 'o-', linewidth=2, label=metric_name.capitalize())

plt.axvline(x=2, color='r', linestyle='--', label='Concept Drift Begins')
plt.title('Model Performance Over Time')
plt.xlabel('Time Window')
plt.ylabel('Metric Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(1, 0.5, "Before Drift", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(7, 0.5, "After Drift", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()

# Visualize the data and decision boundary
plt.figure(figsize=(15, 5))

# Plot initial data
plt.subplot(1, 3, 1)
plt.scatter(X_initial[:, 0], X_initial[:, 1], c=y_initial, cmap='coolwarm', alpha=0.7)
plt.title('Initial Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Plot data after drift
plt.subplot(1, 3, 2)
plt.scatter(X_drift[:, 0], X_drift[:, 1], c=y_drift, cmap='coolwarm', alpha=0.7)
plt.title('Data After Drift')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Plot decision boundary
plt.subplot(1, 3, 3)
x_min, x_max = -3, 3
y_min, y_max = -3, 3
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X_initial[:, 0], X_initial[:, 1], c=y_initial, cmap='coolwarm', alpha=0.3, label='Initial Data')
plt.scatter(X_drift[:, 0], X_drift[:, 1], c=y_drift, cmap='coolwarm', alpha=0.7, marker='x', label='Drift Data')
plt.title('Model Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Statistical Process Control

**Statistical Process Control (SPC)** uses control charts to monitor model performance and detect anomalies:

```python
def calculate_control_limits(data, n_sigma=3):
    """Calculate control limits for a control chart."""
    mean = np.mean(data)
    std = np.std(data)
    ucl = mean + n_sigma * std  # Upper Control Limit
    lcl = mean - n_sigma * std  # Lower Control Limit
    return mean, lcl, ucl

# Calculate control limits for accuracy
initial_accuracy = metrics['accuracy'][:2]  # Use data before drift
mean_accuracy, lcl_accuracy, ucl_accuracy = calculate_control_limits(initial_accuracy)

# Visualize control chart
plt.figure(figsize=(12, 6))
plt.plot(range(n_windows), metrics['accuracy'], 'bo-', linewidth=2, label='Accuracy')
plt.axhline(y=mean_accuracy, color='g', linestyle='-', label='Mean')
plt.axhline(y=ucl_accuracy, color='r', linestyle='--', label='Upper Control Limit (3σ)')
plt.axhline(y=lcl_accuracy, color='r', linestyle='--', label='Lower Control Limit (3σ)')
plt.axvline(x=2, color='k', linestyle='--', label='Concept Drift Begins')
plt.title('Accuracy Control Chart')
plt.xlabel('Time Window')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Identify out-of-control points
out_of_control = [i for i, acc in enumerate(metrics['accuracy']) if acc < lcl_accuracy or acc > ucl_accuracy]
for i in out_of_control:
    plt.plot(i, metrics['accuracy'][i], 'ro', markersize=10)
    plt.text(i, metrics['accuracy'][i] + 0.02, f"Out of control", ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Calculate CUSUM (Cumulative Sum) for drift detection
def calculate_cusum(data, target, k=0.5):
    """Calculate CUSUM (Cumulative Sum) for drift detection."""
    cusum_plus = np.zeros(len(data))
    cusum_minus = np.zeros(len(data))
    
    for i in range(len(data)):
        if i == 0:
            cusum_plus[i] = max(0, data[i] - target - k)
            cusum_minus[i] = max(0, target - data[i] - k)
        else:
            cusum_plus[i] = max(0, cusum_plus[i-1] + data[i] - target - k)
            cusum_minus[i] = max(0, cusum_minus[i-1] + target - data[i] - k)
    
    return cusum_plus, cusum_minus

# Calculate CUSUM for accuracy
cusum_plus, cusum_minus = calculate_cusum(metrics['accuracy'], mean_accuracy, k=0.05)

# Visualize CUSUM chart
plt.figure(figsize=(12, 6))
plt.plot(range(n_windows), cusum_plus, 'b-', linewidth=2, label='CUSUM+')
plt.plot(range(n_windows), cusum_minus, 'r-', linewidth=2, label='CUSUM-')
plt.axvline(x=2, color='k', linestyle='--', label='Concept Drift Begins')
plt.axhline(y=0.1, color='g', linestyle='--', label='Threshold')
plt.title('CUSUM Chart for Accuracy')
plt.xlabel('Time Window')
plt.ylabel('CUSUM Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Identify when CUSUM exceeds threshold
threshold = 0.1
cusum_alarm = [i for i, (cp, cm) in enumerate(zip(cusum_plus, cusum_minus)) if cp > threshold or cm > threshold]
if cusum_alarm:
    first_alarm = cusum_alarm[0]
    plt.axvline(x=first_alarm, color='m', linestyle='--', label=f'First Alarm at Window {first_alarm}')
    plt.legend()

plt.tight_layout()
plt.show()
```

### Performance Degradation Analysis

When model performance degrades, it's important to analyze the causes:

```python
# Simulate feature drift
np.random.seed(42)

# Generate initial data
n_samples = 1000
X_initial = np.random.randn(n_samples, 2)
y_initial = (X_initial[:, 0] + X_initial[:, 1] > 0).astype(int)

# Generate data with feature drift (distribution shift)
X_feature_drift = np.random.randn(n_samples, 2) + np.array([1.0, 0.0])  # Shift in feature 1
y_feature_drift = (X_feature_drift[:, 0] + X_feature_drift[:, 1] > 0).astype(int)

# Train a model on the initial data
model = LogisticRegression(random_state=42)
model.fit(X_initial, y_initial)

# Evaluate on initial and drifted data
y_pred_initial = model.predict(X_initial)
y_pred_drift = model.predict(X_feature_drift)

# Calculate metrics
metrics_initial = {
    'accuracy': accuracy_score(y_initial, y_pred_initial),
    'precision': precision_score(y_initial, y_pred_initial),
    'recall': recall_score(y_initial, y_pred_initial),
    'f1': f1_score(y_initial, y_pred_initial)
}

metrics_drift = {
    'accuracy': accuracy_score(y_feature_drift, y_pred_drift),
    'precision': precision_score(y_feature_drift, y_pred_drift),
    'recall': recall_score(y_feature_drift, y_pred_drift),
    'f1': f1_score(y_feature_drift, y_pred_drift)
}

# Print metrics
print("Metrics on Initial Data:")
for metric, value in metrics_initial.items():
    print(f"  {metric.capitalize()}: {value:.4f}")

print("\nMetrics on Data with Feature Drift:")
for metric, value in metrics_drift.items():
    print(f"  {metric.capitalize()}: {value:.4f}")
    print(f"  Change: {value - metrics_initial[metric]:.4f}")

# Visualize feature distributions
plt.figure(figsize=(15, 5))

# Feature 1 distribution
plt.subplot(1, 3, 1)
plt.hist(X_initial[:, 0], bins=30, alpha=0.5, label='Initial')
plt.hist(X_feature_drift[:, 0], bins=30, alpha=0.5, label='After Drift')
plt.title('Feature 1 Distribution')
plt.xlabel('Feature 1')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Feature 2 distribution
plt.subplot(1, 3, 2)
plt.hist(X_initial[:, 1], bins=30, alpha=0.5, label='Initial')
plt.hist(X_feature_drift[:, 1], bins=30, alpha=0.5, label='After Drift')
plt.title('Feature 2 Distribution')
plt.xlabel('Feature 2')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Decision boundary
plt.subplot(1, 3, 3)
x_min, x_max = min(np.min(X_initial[:, 0]), np.min(X_feature_drift[:, 0])) - 1, max(np.max(X_initial[:, 0]), np.max(X_feature_drift[:, 0])) + 1
y_min, y_max = min(np.min(X_initial[:, 1]), np.min(X_feature_drift[:, 1])) - 1, max(np.max(X_initial[:, 1]), np.max(X_feature_drift[:, 1])) + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X_initial[:, 0], X_initial[:, 1], c=y_initial, cmap='coolwarm', alpha=0.3, label='Initial Data')
plt.scatter(X_feature_drift[:, 0], X_feature_drift[:, 1], c=y_feature_drift, cmap='coolwarm', alpha=0.7, marker='x', label='Drift Data')
plt.title('Model Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Analyze error patterns
def analyze_errors(X, y_true, y_pred):
    """Analyze error patterns in predictions."""
    errors = (y_true != y_pred)
    X_errors = X[errors]
    y_true_errors = y_true[errors]
    
    # Calculate error rate by class
    class_error_rates = {}
    for c in np.unique(y_true):
        class_mask = (y_true == c)
        class_errors = errors[class_mask]
        class_error_rates[c] = np.mean(class_errors)
    
    # Calculate feature statistics for errors vs. correct predictions
    feature_stats = {}
    for i in range(X.shape[1]):
        feature_stats[i] = {
            'mean_errors': np.mean(X_errors[:, i]),
            'std_errors': np.std(X_errors[:, i]),
            'mean_correct': np.mean(X[~errors, i]),
            'std_correct': np.std(X[~errors, i])
        }
    
    return {
        'error_rate': np.mean(errors),
        'class_error_rates': class_error_rates,
        'feature_stats': feature_stats,
        'X_errors': X_errors,
        'y_true_errors': y_true_errors
    }

# Analyze errors for initial and drifted data
error_analysis_initial = analyze_errors(X_initial, y_initial, y_pred_initial)
error_analysis_drift = analyze_errors(X_feature_drift, y_feature_drift, y_pred_drift)

# Print error analysis
print("\nError Analysis for Initial Data:")
print(f"  Overall Error Rate: {error_analysis_initial['error_rate']:.4f}")
print("  Error Rate by Class:")
for c, rate in error_analysis_initial['class_error_rates'].items():
    print(f"    Class {c}: {rate:.4f}")

print("\nError Analysis for Data with Feature Drift:")
print(f"  Overall Error Rate: {error_analysis_drift['error_rate']:.4f}")
print("  Error Rate by Class:")
for c, rate in error_analysis_drift['class_error_rates'].items():
    print(f"    Class {c}: {rate:.4f}")

# Visualize error patterns
plt.figure(figsize=(15, 5))

# Errors on initial data
plt.subplot(1, 3, 1)
plt.scatter(X_initial[y_initial != y_pred_initial, 0], X_initial[y_initial != y_pred_initial, 1], 
            c='red', marker='x', label='Errors')
plt.scatter(X_initial[y_initial == y_pred_initial, 0], X_initial[y_initial == y_pred_initial, 1], 
            c='blue', alpha=0.3, label='Correct')
plt.title('Errors on Initial Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)

# Errors on drifted data
plt.subplot(1, 3, 2)
plt.scatter(X_feature_drift[y_feature_drift != y_pred_drift, 0], X_feature_drift[y_feature_drift != y_pred_drift, 1], 
            c='red', marker='x', label='Errors')
plt.scatter(X_feature_drift[y_feature_drift == y_pred_drift, 0], X_feature_drift[y_feature_drift == y_pred_drift, 1], 
            c='blue', alpha=0.3, label='Correct')
plt.title('Errors on Drifted Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)

# Feature means for errors vs. correct predictions
plt.subplot(1, 3, 3)
features = ['Feature 1', 'Feature 2']
x = np.arange(len(features))
width = 0.2

plt.bar(x - width*1.5, [error_analysis_initial['feature_stats'][i]['mean_correct'] for i in range(2)], 
        width, label='Initial - Correct', color='blue', alpha=0.7)
plt.bar(x - width*0.5, [error_analysis_initial['feature_stats'][i]['mean_errors'] for i in range(2)], 
        width, label='Initial - Errors', color='red', alpha=0.7)
plt.bar(x + width*0.5, [error_analysis_drift['feature_stats'][i]['mean_correct'] for i in range(2)], 
        width, label='Drift - Correct', color='green', alpha=0.7)
plt.bar(x + width*1.5, [error_analysis_drift['feature_stats'][i]['mean_errors'] for i in range(2)], 
        width, label='Drift - Errors', color='orange', alpha=0.7)

plt.title('Feature Means for Errors vs. Correct Predictions')
plt.xlabel('Feature')
plt.ylabel('Mean Value')
plt.xticks(x, features)
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

## Summary

Model monitoring and evaluation in production are essential to ensure models maintain their performance over time:

1. **Concept Drift Detection**:
   - Monitors changes in the statistical properties of the target variable
   - Helps identify when model performance degrades due to changing data patterns
   - Can be detected through performance metrics, statistical tests, or specialized algorithms

2. **Statistical Process Control**:
   - Uses control charts to monitor model performance
   - Helps identify anomalies and out-of-control situations
   - Techniques include Shewhart charts, CUSUM, and EWMA

3. **Performance Degradation Analysis**:
   - Investigates the causes of model performance degradation
   - Analyzes feature distributions, error patterns, and class-specific performance
   - Helps determine whether to retrain the model or make other adjustments

These techniques help ensure that models remain effective in production environments, where data distributions and relationships may change over time.

## References

1. Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014). A survey on concept drift adaptation. ACM Computing Surveys, 46(4), 1-37.
2. Klinkenberg, R., & Joachims, T. (2000). Detecting concept drift with support vector machines. In Proceedings of the 17th International Conference on Machine Learning (pp. 487-494).
3. Žliobaitė, I., Pechenizkiy, M., & Gama, J. (2016). An overview of concept drift applications. In Big Data Analysis: New Algorithms for a New Society (pp. 91-114). Springer.
4. Montgomery, D. C. (2009). Introduction to Statistical Quality Control (6th ed.). Wiley.
