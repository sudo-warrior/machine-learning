# 3.2.5 Model Evaluation - Part 2: Regression Metrics

## Regression Metrics

Regression metrics evaluate models that predict continuous values. These metrics measure the difference between predicted values and actual values in various ways.

### Mean Absolute Error (MAE)

**Mean Absolute Error (MAE)** is the average of the absolute differences between predictions and actual values:

$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

where:
- $y_i$ is the actual value
- $\hat{y}_i$ is the predicted value
- $n$ is the number of samples

**Pros**:
- Simple and intuitive
- Less sensitive to outliers than MSE
- Error is in the same units as the target variable

**Cons**:
- Doesn't penalize large errors as heavily as MSE
- Not differentiable at zero

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate MAE
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae:.4f}")

# Visualize the predictions and errors
plt.figure(figsize=(12, 6))

# Plot the data and predictions
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')

# Add error bars to show absolute errors
for i in range(len(X_test)):
    plt.plot([X_test[i], X_test[i]], [y_test[i], y_pred[i]], 'k-', alpha=0.3)

plt.title('Linear Regression Predictions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the absolute errors
plt.subplot(1, 2, 2)
abs_errors = np.abs(y_test - y_pred)
plt.bar(range(len(abs_errors)), abs_errors, color='skyblue')
plt.axhline(y=mae, color='r', linestyle='--', label=f'MAE = {mae:.4f}')
plt.title('Absolute Errors')
plt.xlabel('Test Sample')
plt.ylabel('Absolute Error')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Visualize MAE
plt.figure(figsize=(10, 6))
errors = np.linspace(-10, 10, 1000)
mae_values = np.abs(errors)

plt.plot(errors, mae_values, 'b-', linewidth=2)
plt.title('Mean Absolute Error')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)

# Add annotations
plt.text(5, 2, "MAE = |y_true - y_pred|", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(5, 1, "Linear penalty for all errors", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### Mean Squared Error (MSE)

**Mean Squared Error (MSE)** is the average of the squared differences between predictions and actual values:

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**Pros**:
- Penalizes large errors more heavily
- Differentiable everywhere
- Widely used in optimization

**Cons**:
- Sensitive to outliers
- Error is not in the same units as the target variable

```python
from sklearn.metrics import mean_squared_error

# Calculate MSE
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")

# Visualize the predictions and squared errors
plt.figure(figsize=(12, 6))

# Plot the data and predictions
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')

# Add error bars to show errors
for i in range(len(X_test)):
    plt.plot([X_test[i], X_test[i]], [y_test[i], y_pred[i]], 'k-', alpha=0.3)

plt.title('Linear Regression Predictions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the squared errors
plt.subplot(1, 2, 2)
squared_errors = (y_test - y_pred) ** 2
plt.bar(range(len(squared_errors)), squared_errors, color='salmon')
plt.axhline(y=mse, color='r', linestyle='--', label=f'MSE = {mse:.4f}')
plt.title('Squared Errors')
plt.xlabel('Test Sample')
plt.ylabel('Squared Error')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Visualize MSE
plt.figure(figsize=(10, 6))
errors = np.linspace(-10, 10, 1000)
mse_values = errors ** 2

plt.plot(errors, mse_values, 'r-', linewidth=2)
plt.title('Mean Squared Error')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)

# Add annotations
plt.text(5, 50, "MSE = (y_true - y_pred)²", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(5, 40, "Quadratic penalty for errors", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(5, 30, "Heavily penalizes large errors", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### Root Mean Squared Error (RMSE)

**Root Mean Squared Error (RMSE)** is the square root of the MSE:

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

**Pros**:
- Error is in the same units as the target variable
- Penalizes large errors more heavily than MAE
- Widely used in practice

**Cons**:
- Sensitive to outliers
- Less interpretable than MAE

```python
# Calculate RMSE
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error: {rmse:.4f}")

# Compare MAE and RMSE
plt.figure(figsize=(10, 6))
plt.bar(['MAE', 'RMSE'], [mae, rmse], color=['skyblue', 'salmon'])
plt.title('Comparison of MAE and RMSE')
plt.ylabel('Error')
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
plt.text(0, mae/2, f"{mae:.4f}", ha='center', va='center', fontweight='bold')
plt.text(1, rmse/2, f"{rmse:.4f}", ha='center', va='center', fontweight='bold')

# Add an explanation
plt.text(0.5, rmse * 1.1, "RMSE ≥ MAE (always)", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(0.5, rmse * 1.2, "RMSE penalizes large errors more heavily", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()

# Demonstrate the effect of outliers on MAE and RMSE
np.random.seed(42)
y_test_with_outlier = y_test.copy()
y_test_with_outlier[0] = y_test_with_outlier[0] + 50  # Add a large outlier

mae_with_outlier = mean_absolute_error(y_test_with_outlier, y_pred)
mse_with_outlier = mean_squared_error(y_test_with_outlier, y_pred)
rmse_with_outlier = np.sqrt(mse_with_outlier)

print(f"\nWith outlier:")
print(f"MAE: {mae_with_outlier:.4f} (Increase: {(mae_with_outlier - mae) / mae * 100:.2f}%)")
print(f"RMSE: {rmse_with_outlier:.4f} (Increase: {(rmse_with_outlier - rmse) / rmse * 100:.2f}%)")

# Visualize the effect of outliers
plt.figure(figsize=(12, 6))

# Plot the data with outlier
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test_with_outlier, color='green', alpha=0.7, label='Test Data')
plt.scatter(X_test[0], y_test_with_outlier[0], color='red', s=100, label='Outlier')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')

plt.title('Data with Outlier')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Compare metrics with and without outlier
plt.subplot(1, 2, 2)
metrics = ['MAE', 'RMSE']
values_no_outlier = [mae, rmse]
values_with_outlier = [mae_with_outlier, rmse_with_outlier]

x = np.arange(len(metrics))
width = 0.35

plt.bar(x - width/2, values_no_outlier, width, label='Without Outlier', color=['skyblue', 'salmon'])
plt.bar(x + width/2, values_with_outlier, width, label='With Outlier', color=['lightblue', 'lightsalmon'])

plt.title('Effect of Outlier on MAE and RMSE')
plt.xlabel('Metric')
plt.ylabel('Error')
plt.xticks(x, metrics)
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
for i, v in enumerate(values_no_outlier):
    plt.text(i - width/2, v/2, f"{v:.1f}", ha='center', va='center', fontweight='bold')
for i, v in enumerate(values_with_outlier):
    plt.text(i + width/2, v/2, f"{v:.1f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Mean Absolute Percentage Error (MAPE)

**Mean Absolute Percentage Error (MAPE)** is the average of the absolute percentage differences between predictions and actual values:

$$\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|$$

**Pros**:
- Scale-independent
- Easy to interpret as a percentage
- Useful for comparing models across different scales

**Cons**:
- Undefined or infinite when actual values are zero
- Puts a heavier penalty on negative errors than positive errors
- Can be skewed by small actual values

```python
# Define a function to calculate MAPE
def mean_absolute_percentage_error(y_true, y_pred):
    # Avoid division by zero
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

# Calculate MAPE
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"Mean Absolute Percentage Error: {mape:.4f}%")

# Visualize MAPE
plt.figure(figsize=(12, 6))

# Plot the data and predictions
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')

plt.title('Linear Regression Predictions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the percentage errors
plt.subplot(1, 2, 2)
percentage_errors = np.abs((y_test - y_pred) / y_test) * 100
plt.bar(range(len(percentage_errors)), percentage_errors, color='lightgreen')
plt.axhline(y=mape, color='r', linestyle='--', label=f'MAPE = {mape:.4f}%')
plt.title('Absolute Percentage Errors')
plt.xlabel('Test Sample')
plt.ylabel('Absolute Percentage Error (%)')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Demonstrate the scale-independence of MAPE
# Scale the target variable
scale_factor = 1000
y_train_scaled = y_train * scale_factor
y_test_scaled = y_test * scale_factor

# Train a model on the scaled data
model_scaled = LinearRegression()
model_scaled.fit(X_train, y_train_scaled)
y_pred_scaled = model_scaled.predict(X_test)

# Calculate metrics for the scaled data
mae_scaled = mean_absolute_error(y_test_scaled, y_pred_scaled)
rmse_scaled = np.sqrt(mean_squared_error(y_test_scaled, y_pred_scaled))
mape_scaled = mean_absolute_percentage_error(y_test_scaled, y_pred_scaled)

print(f"\nMetrics with scaled data (scale factor = {scale_factor}):")
print(f"MAE: {mae_scaled:.4f} (Increase: {(mae_scaled - mae) / mae * 100:.2f}%)")
print(f"RMSE: {rmse_scaled:.4f} (Increase: {(rmse_scaled - rmse) / rmse * 100:.2f}%)")
print(f"MAPE: {mape_scaled:.4f}% (Change: {(mape_scaled - mape) / mape * 100:.2f}%)")

# Visualize the effect of scaling
plt.figure(figsize=(10, 6))
metrics = ['MAE', 'RMSE', 'MAPE']
values_original = [mae, rmse, mape]
values_scaled = [mae_scaled / scale_factor, rmse_scaled / scale_factor, mape_scaled]  # Normalize MAE and RMSE for comparison

x = np.arange(len(metrics))
width = 0.35

plt.bar(x - width/2, values_original, width, label='Original Scale', color=['skyblue', 'salmon', 'lightgreen'])
plt.bar(x + width/2, values_scaled, width, label='Scaled (normalized)', color=['lightblue', 'lightsalmon', 'palegreen'])

plt.title('Effect of Scaling on Error Metrics')
plt.xlabel('Metric')
plt.ylabel('Error (normalized for MAE and RMSE)')
plt.xticks(x, metrics)
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
for i, v in enumerate(values_original):
    plt.text(i - width/2, v/2, f"{v:.1f}", ha='center', va='center', fontweight='bold')
for i, v in enumerate(values_scaled):
    plt.text(i + width/2, v/2, f"{v:.1f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### R-squared (Coefficient of Determination)

**R-squared (R²)** measures the proportion of variance in the dependent variable that is predictable from the independent variables:

$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

where:
- $\bar{y}$ is the mean of the actual values

**Pros**:
- Scale-independent
- Easy to interpret (0 to 1 in most cases)
- Widely used and understood

**Cons**:
- Can increase with the addition of variables, even if they don't improve the model
- Can be negative for non-linear models or when the model is worse than a horizontal line
- Not comparable across different datasets

```python
from sklearn.metrics import r2_score

# Calculate R-squared
r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2:.4f}")

# Calculate R-squared manually
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2_manual = 1 - (ss_residual / ss_total)
print(f"R-squared (manual calculation): {r2_manual:.4f}")

# Visualize R-squared
plt.figure(figsize=(12, 6))

# Plot the data and predictions
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')
plt.axhline(y=np.mean(y_test), color='blue', linestyle='--', label='Mean of y')

# Add annotations for SS_total and SS_residual
for i in range(len(X_test)):
    # Line from point to mean (SS_total)
    plt.plot([X_test[i], X_test[i]], [y_test[i], np.mean(y_test)], 'b-', alpha=0.1)
    # Line from point to prediction (SS_residual)
    plt.plot([X_test[i], X_test[i]], [y_test[i], y_pred[i]], 'r-', alpha=0.1)

plt.title(f'Linear Regression (R² = {r2:.4f})')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot R-squared interpretation
plt.subplot(1, 2, 2)
r2_values = [-0.5, 0, 0.25, 0.5, 0.75, 1.0]
interpretations = ['Very Poor', 'Baseline\n(Mean Model)', 'Poor', 'Moderate', 'Good', 'Perfect']

plt.barh(range(len(r2_values)), r2_values, color=['red', 'orange', 'yellow', 'yellowgreen', 'lightgreen', 'green'])
plt.axvline(x=r2, color='blue', linestyle='--', label=f'Current Model: {r2:.4f}')
plt.yticks(range(len(r2_values)), interpretations)
plt.xlim(-0.6, 1.1)
plt.title('R-squared Interpretation')
plt.xlabel('R-squared Value')
plt.grid(True, alpha=0.3, axis='x')
plt.legend()

# Add annotations
for i, v in enumerate(r2_values):
    plt.text(v, i, f"{v:.2f}", va='center', ha='left' if v < 0 else 'right', fontweight='bold')

plt.tight_layout()
plt.show()

# Demonstrate R-squared with different models
np.random.seed(42)
X_multi = np.random.rand(100, 3)  # 3 features
y_multi = 2 * X_multi[:, 0] + 3 * X_multi[:, 1] - 1 * X_multi[:, 2] + np.random.normal(0, 0.5, 100)

X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)

# Train models with different numbers of features
models = {
    '1 Feature': LinearRegression().fit(X_train_multi[:, 0].reshape(-1, 1), y_train_multi),
    '2 Features': LinearRegression().fit(X_train_multi[:, :2], y_train_multi),
    '3 Features': LinearRegression().fit(X_train_multi, y_train_multi)
}

# Calculate R-squared for each model
r2_values = {}
for name, model in models.items():
    if name == '1 Feature':
        y_pred_multi = model.predict(X_test_multi[:, 0].reshape(-1, 1))
    elif name == '2 Features':
        y_pred_multi = model.predict(X_test_multi[:, :2])
    else:
        y_pred_multi = model.predict(X_test_multi)
    
    r2_values[name] = r2_score(y_test_multi, y_pred_multi)
    print(f"R-squared ({name}): {r2_values[name]:.4f}")

# Visualize R-squared for different models
plt.figure(figsize=(10, 6))
plt.bar(r2_values.keys(), r2_values.values(), color=['lightblue', 'skyblue', 'steelblue'])
plt.title('R-squared for Models with Different Numbers of Features')
plt.ylabel('R-squared')
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
for i, (name, value) in enumerate(r2_values.items()):
    plt.text(i, value/2, f"{value:.4f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Adjusted R-squared

**Adjusted R-squared** modifies R-squared to account for the number of predictors in the model:

$$R^2_{adj} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}$$

where:
- $n$ is the number of samples
- $p$ is the number of predictors

**Pros**:
- Penalizes the addition of unnecessary variables
- More reliable for comparing models with different numbers of predictors
- Addresses the main weakness of R-squared

**Cons**:
- Still not comparable across different datasets
- Can still increase with the addition of variables that have even a small correlation with the target

```python
# Define a function to calculate adjusted R-squared
def adjusted_r2_score(y_true, y_pred, n_features):
    r2 = r2_score(y_true, y_pred)
    n = len(y_true)
    p = n_features
    return 1 - (1 - r2) * (n - 1) / (n - p - 1)

# Calculate adjusted R-squared for each model
adj_r2_values = {}
for name, model in models.items():
    if name == '1 Feature':
        y_pred_multi = model.predict(X_test_multi[:, 0].reshape(-1, 1))
        n_features = 1
    elif name == '2 Features':
        y_pred_multi = model.predict(X_test_multi[:, :2])
        n_features = 2
    else:
        y_pred_multi = model.predict(X_test_multi)
        n_features = 3
    
    adj_r2_values[name] = adjusted_r2_score(y_test_multi, y_pred_multi, n_features)
    print(f"Adjusted R-squared ({name}): {adj_r2_values[name]:.4f}")

# Visualize R-squared vs. adjusted R-squared
plt.figure(figsize=(12, 6))
x = np.arange(len(models))
width = 0.35

plt.bar(x - width/2, list(r2_values.values()), width, label='R-squared', color='skyblue')
plt.bar(x + width/2, list(adj_r2_values.values()), width, label='Adjusted R-squared', color='salmon')

plt.title('R-squared vs. Adjusted R-squared')
plt.xlabel('Model')
plt.ylabel('Score')
plt.xticks(x, r2_values.keys())
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
for i, v in enumerate(r2_values.values()):
    plt.text(i - width/2, v/2, f"{v:.4f}", ha='center', va='center', fontweight='bold')
for i, v in enumerate(adj_r2_values.values()):
    plt.text(i + width/2, v/2, f"{v:.4f}", ha='center', va='center', fontweight='bold')

# Add an explanation
plt.text(1.5, 0.9, "Adjusted R² penalizes additional features\nthat don't contribute significantly", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

## Summary

Regression metrics help evaluate the performance of regression models:

1. **Mean Absolute Error (MAE)**:
   - Average of absolute differences between predictions and actual values
   - Less sensitive to outliers
   - Error is in the same units as the target variable

2. **Mean Squared Error (MSE)**:
   - Average of squared differences between predictions and actual values
   - Penalizes large errors more heavily
   - Widely used in optimization

3. **Root Mean Squared Error (RMSE)**:
   - Square root of MSE
   - Error is in the same units as the target variable
   - Penalizes large errors more heavily than MAE

4. **Mean Absolute Percentage Error (MAPE)**:
   - Average of absolute percentage differences between predictions and actual values
   - Scale-independent
   - Easy to interpret as a percentage

5. **R-squared (Coefficient of Determination)**:
   - Proportion of variance explained by the model
   - Scale-independent
   - Easy to interpret (0 to 1 in most cases)

6. **Adjusted R-squared**:
   - Modified R-squared that accounts for the number of predictors
   - Penalizes the addition of unnecessary variables
   - More reliable for comparing models with different numbers of predictors

The choice of metric depends on the specific problem, the scale of the target variable, the presence of outliers, and the relative importance of different types of errors.

## References

1. Chai, T., & Draxler, R. R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)?–Arguments against avoiding RMSE in the literature. Geoscientific Model Development, 7(3), 1247-1250.
2. Willmott, C. J., & Matsuura, K. (2005). Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance. Climate Research, 30(1), 79-82.
3. Kvålseth, T. O. (1985). Cautionary note about R². The American Statistician, 39(4), 279-285.
4. de Myttenaere, A., Golden, B., Le Grand, B., & Rossi, F. (2016). Mean absolute percentage error for regression models. Neurocomputing, 192, 38-48.
