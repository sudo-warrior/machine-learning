# 3.2.4 Regularization Techniques - Part 3: Early Stopping (2)

## Theoretical Understanding of Early Stopping

Early stopping is not just a practical heuristic; it has solid theoretical foundations that explain why it works as a regularization technique.

### Bias-Variance Tradeoff Perspective

Early stopping can be understood through the lens of the bias-variance tradeoff:

- **Early in training**: The model has high bias (underfitting) but low variance
- **As training progresses**: Bias decreases while variance increases
- **Late in training**: The model has low bias but high variance (overfitting)

Early stopping aims to find the sweet spot where the sum of bias and variance is minimized, resulting in the best generalization performance.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Generate synthetic data
X, y = make_regression(n_samples=1000, n_features=20, noise=0.5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create a model
def create_model():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(20,)),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    
    model.compile(optimizer='adam', loss='mse')
    return model

# Train multiple models with different stopping points
num_epochs = 200
num_models = 10
all_train_losses = []
all_val_losses = []
all_test_losses = []

for i in range(num_models):
    model = create_model()
    
    # Train for a specific number of epochs
    history = model.fit(
        X_train, y_train,
        epochs=num_epochs,
        batch_size=32,
        validation_split=0.2,
        verbose=0
    )
    
    all_train_losses.append(history.history['loss'])
    all_val_losses.append(history.history['val_loss'])
    
    # Evaluate on test set at each epoch
    test_losses = []
    for epoch in range(num_epochs):
        # Create a new model and train it for 'epoch' epochs
        temp_model = create_model()
        temp_model.fit(
            X_train, y_train,
            epochs=epoch+1,
            batch_size=32,
            verbose=0
        )
        
        # Evaluate on test set
        test_loss = temp_model.evaluate(X_test, y_test, verbose=0)
        test_losses.append(test_loss)
    
    all_test_losses.append(test_losses)

# Calculate mean and standard deviation across models
mean_train_loss = np.mean(all_train_losses, axis=0)
mean_val_loss = np.mean(all_val_losses, axis=0)
mean_test_loss = np.mean(all_test_losses, axis=0)

std_train_loss = np.std(all_train_losses, axis=0)
std_val_loss = np.std(all_val_losses, axis=0)
std_test_loss = np.std(all_test_losses, axis=0)

# Plot the results
plt.figure(figsize=(12, 8))

# Plot mean losses
plt.subplot(2, 1, 1)
plt.plot(mean_train_loss, label='Training Loss')
plt.plot(mean_val_loss, label='Validation Loss')
plt.plot(mean_test_loss, label='Test Loss')
plt.axvline(x=np.argmin(mean_val_loss), color='r', linestyle='--', 
            label=f'Best Epoch ({np.argmin(mean_val_loss)})')
plt.title('Mean Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot bias-variance decomposition
plt.subplot(2, 1, 2)
bias_squared = np.square(mean_test_loss)  # Approximation of squared bias
variance = np.square(std_test_loss)       # Approximation of variance
total_error = bias_squared + variance     # Total error = bias² + variance

plt.plot(bias_squared, label='Bias²')
plt.plot(variance, label='Variance')
plt.plot(total_error, label='Total Error')
plt.axvline(x=np.argmin(total_error), color='r', linestyle='--', 
            label=f'Optimal Stopping Point ({np.argmin(total_error)})')
plt.title('Bias-Variance Decomposition')
plt.xlabel('Epoch')
plt.ylabel('Error Component')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Equivalence to Weight Decay

Research has shown that early stopping is mathematically equivalent to L2 regularization (weight decay) in some cases. Both techniques constrain the magnitude of the model parameters, preventing them from growing too large and causing overfitting.

The key difference is that early stopping implicitly determines the regularization strength based on the validation performance, while L2 regularization requires explicitly setting the regularization parameter.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# Generate synthetic data
X, y = make_regression(n_samples=1000, n_features=20, noise=0.5, random_state=42)

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Create models
def create_model_with_early_stopping():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(20,)),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    
    model.compile(optimizer='adam', loss='mse')
    return model

def create_model_with_l2(l2_lambda):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(20,), kernel_regularizer=l2(l2_lambda)),
        Dense(64, activation='relu', kernel_regularizer=l2(l2_lambda)),
        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),
        Dense(1)
    ])
    
    model.compile(optimizer='adam', loss='mse')
    return model

# Train model with early stopping
model_es = create_model_with_early_stopping()
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history_es = model_es.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=0
)

# Get weights from early stopping model
es_weights = []
for layer in model_es.layers:
    if len(layer.get_weights()) > 0:
        es_weights.extend(np.abs(layer.get_weights()[0]).flatten())

# Train models with different L2 regularization strengths
l2_lambdas = [0, 0.0001, 0.001, 0.01, 0.1]
histories_l2 = []
l2_weights = []

for l2_lambda in l2_lambdas:
    model_l2 = create_model_with_l2(l2_lambda)
    
    history_l2 = model_l2.fit(
        X_train, y_train,
        epochs=200,
        batch_size=32,
        validation_data=(X_val, y_val),
        verbose=0
    )
    
    histories_l2.append((l2_lambda, history_l2))
    
    # Get weights from L2 model
    weights = []
    for layer in model_l2.layers:
        if len(layer.get_weights()) > 0:
            weights.extend(np.abs(layer.get_weights()[0]).flatten())
    
    l2_weights.append((l2_lambda, weights))

# Plot validation loss
plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.plot(history_es.history['val_loss'], 'r-', linewidth=2, label='Early Stopping')
for l2_lambda, history_l2 in histories_l2:
    plt.plot(history_l2.history['val_loss'], linewidth=1, label=f'L2 (λ={l2_lambda})')

plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot weight distributions
plt.subplot(2, 2, 2)
plt.hist(es_weights, bins=50, alpha=0.5, label='Early Stopping')
for l2_lambda, weights in l2_weights:
    if l2_lambda > 0:  # Skip the model without regularization
        plt.hist(weights, bins=50, alpha=0.3, label=f'L2 (λ={l2_lambda})')

plt.title('Weight Distributions')
plt.xlabel('Absolute Weight Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot final validation loss vs. L2 lambda
plt.subplot(2, 2, 3)
l2_final_val_losses = [history_l2.history['val_loss'][-1] for _, history_l2 in histories_l2]
plt.semilogx([0.00001] + l2_lambdas[1:], l2_final_val_losses, 'o-', linewidth=2)
plt.axhline(y=min(history_es.history['val_loss']), color='r', linestyle='--', 
            label=f'Early Stopping (Best Val Loss)')

plt.title('Final Validation Loss vs. L2 Lambda')
plt.xlabel('L2 Lambda (log scale)')
plt.ylabel('Validation Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot L2 norm of weights
plt.subplot(2, 2, 4)
es_norm = np.linalg.norm(es_weights)
l2_norms = [np.linalg.norm(weights) for _, weights in l2_weights]

plt.semilogx([0.00001] + l2_lambdas[1:], l2_norms, 'o-', linewidth=2)
plt.axhline(y=es_norm, color='r', linestyle='--', 
            label=f'Early Stopping (Weight Norm)')

plt.title('L2 Norm of Weights')
plt.xlabel('L2 Lambda (log scale)')
plt.ylabel('L2 Norm')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Advanced Considerations for Early Stopping

### Learning Rate Schedules and Early Stopping

Early stopping interacts with learning rate schedules in interesting ways. A decreasing learning rate can sometimes mask the need for early stopping by slowing down learning to the point where overfitting happens very gradually.

```python
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Create models with different learning rate strategies
def create_model(lr=0.001):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(20,)),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    
    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')
    return model

# Train with constant learning rate
model_constant_lr = create_model(lr=0.001)
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True,
    verbose=0
)

history_constant_lr = model_constant_lr.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=0
)

# Train with learning rate schedule
model_lr_schedule = create_model(lr=0.001)
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.00001,
    verbose=0
)

history_lr_schedule = model_lr_schedule.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, reduce_lr],
    verbose=0
)

# Plot results
plt.figure(figsize=(12, 8))

# Plot validation loss
plt.subplot(2, 1, 1)
plt.plot(history_constant_lr.history['val_loss'], label='Constant LR')
plt.plot(history_lr_schedule.history['val_loss'], label='LR Schedule')
plt.axvline(x=len(history_constant_lr.history['val_loss'])-21, color='C0', 
            linestyle='--', label='Early Stopping (Constant LR)')
plt.axvline(x=len(history_lr_schedule.history['val_loss'])-21, color='C1', 
            linestyle='--', label='Early Stopping (LR Schedule)')
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot learning rate for the scheduled model
plt.subplot(2, 1, 2)
# Extract learning rates from the optimizer
if 'lr' in history_lr_schedule.history:
    learning_rates = history_lr_schedule.history['lr']
else:
    # If learning rates are not directly available, we can approximate them
    learning_rates = [0.001]
    for i in range(1, len(history_lr_schedule.history['val_loss'])):
        if i % 5 == 0 and history_lr_schedule.history['val_loss'][i] >= history_lr_schedule.history['val_loss'][i-5]:
            learning_rates.append(learning_rates[-1] * 0.5)
        else:
            learning_rates.append(learning_rates[-1])

plt.semilogy(learning_rates, label='Learning Rate')
plt.axvline(x=len(history_lr_schedule.history['val_loss'])-21, color='C1', 
            linestyle='--', label='Early Stopping')
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate (log scale)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Early Stopping with Multiple Metrics

Sometimes, you might want to monitor multiple metrics when deciding when to stop training. For example, in a classification problem, you might want to balance accuracy and fairness metrics.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import Callback

# Generate synthetic data with two classes and two groups
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)
# Create a sensitive attribute (group membership)
group = np.random.binomial(1, 0.5, size=1000)

# Make the data slightly biased (group 0 has more class 0, group 1 has more class 1)
for i in range(len(y)):
    if group[i] == 0 and np.random.random() < 0.2:
        y[i] = 0
    elif group[i] == 1 and np.random.random() < 0.2:
        y[i] = 1

# Split data
X_train, X_val, y_train, y_val, group_train, group_val = train_test_split(
    X, y, group, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Create a custom callback to monitor multiple metrics
class MultiMetricEarlyStopping(Callback):
    def __init__(self, metrics=['val_accuracy', 'val_fairness'], weights=[0.5, 0.5], 
                 patience=10, restore_best_weights=True):
        super(MultiMetricEarlyStopping, self).__init__()
        self.metrics = metrics
        self.weights = weights
        self.patience = patience
        self.restore_best_weights = restore_best_weights
        self.best_weights = None
        self.best_combined_metric = -np.inf
        self.wait = 0
        self.stopped_epoch = 0
        self.combined_metrics = []
        
    def on_train_begin(self, logs=None):
        self.wait = 0
        self.stopped_epoch = 0
        self.best_combined_metric = -np.inf
        self.combined_metrics = []
        
    def on_epoch_end(self, epoch, logs=None):
        # Calculate combined metric
        combined_metric = 0
        for i, metric in enumerate(self.metrics):
            if metric == 'val_fairness':
                # Calculate fairness metric (equal opportunity difference)
                y_pred = self.model.predict(X_val, verbose=0) > 0.5
                
                # Calculate true positive rates for each group
                tpr_0 = np.sum((y_pred.flatten() == 1) & (y_val == 1) & (group_val == 0)) / (np.sum((y_val == 1) & (group_val == 0)) + 1e-10)
                tpr_1 = np.sum((y_pred.flatten() == 1) & (y_val == 1) & (group_val == 1)) / (np.sum((y_val == 1) & (group_val == 1)) + 1e-10)
                
                # Equal opportunity difference (lower is better)
                eod = abs(tpr_0 - tpr_1)
                fairness = 1 - eod  # Convert to a metric where higher is better
                logs['val_fairness'] = fairness
                combined_metric += self.weights[i] * fairness
            else:
                combined_metric += self.weights[i] * logs.get(metric)
        
        logs['combined_metric'] = combined_metric
        self.combined_metrics.append(combined_metric)
        
        if combined_metric > self.best_combined_metric:
            self.best_combined_metric = combined_metric
            self.wait = 0
            if self.restore_best_weights:
                self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                if self.restore_best_weights and self.best_weights is not None:
                    self.model.set_weights(self.best_weights)
    
    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print(f'Epoch {self.stopped_epoch+1}: early stopping')

# Create and train model with multi-metric early stopping
model = Sequential([
    Dense(64, activation='relu', input_shape=(20,)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

multi_metric_es = MultiMetricEarlyStopping(
    metrics=['val_accuracy', 'val_fairness'],
    weights=[0.7, 0.3],  # Prioritize accuracy over fairness
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[multi_metric_es],
    verbose=0
)

# Plot results
plt.figure(figsize=(12, 8))

# Plot individual metrics
plt.subplot(2, 1, 1)
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['val_fairness'], label='Validation Fairness')
plt.title('Individual Metrics')
plt.xlabel('Epoch')
plt.ylabel('Metric Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot combined metric
plt.subplot(2, 1, 2)
plt.plot(history.history['combined_metric'], label='Combined Metric')
plt.axvline(x=np.argmax(history.history['combined_metric']), color='r', linestyle='--', 
            label=f'Best Epoch ({np.argmax(history.history["combined_metric"])})')
plt.title('Combined Metric (0.7 * Accuracy + 0.3 * Fairness)')
plt.xlabel('Epoch')
plt.ylabel('Metric Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Early Stopping in Transfer Learning

In transfer learning scenarios, early stopping becomes even more important as fine-tuning can quickly overfit to the target dataset:

```python
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt

# Create a simple synthetic dataset for demonstration
def create_synthetic_image_dataset(num_samples=1000, img_size=96):
    X = np.random.rand(num_samples, img_size, img_size, 3)
    y = np.random.randint(0, 2, size=num_samples)
    return X, y

X_train, y_train = create_synthetic_image_dataset(num_samples=1000)
X_val, y_val = create_synthetic_image_dataset(num_samples=200)

# Create a transfer learning model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Create data generators with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
val_generator = val_datagen.flow(X_val, y_val, batch_size=32)

# Train with early stopping
early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // 32,
    epochs=20,
    validation_data=val_generator,
    validation_steps=len(X_val) // 32,
    callbacks=[early_stopping]
)

# Plot results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Best Practices for Early Stopping

### 1. Use a Separate Validation Set

Always use a separate validation set for early stopping, not the test set. The test set should only be used for final evaluation.

### 2. Choose the Right Metric

Select a metric that aligns with your problem's goals:
- For classification: accuracy, F1-score, AUC
- For regression: MSE, MAE
- For ranking: NDCG, MAP

### 3. Set Appropriate Patience

- **Too low**: May stop too early due to random fluctuations
- **Too high**: May continue training too long, wasting resources
- **Rule of thumb**: 5-20 epochs, depending on the problem and dataset size

### 4. Restore Best Weights

Always set `restore_best_weights=True` to ensure you get the model from the best epoch, not the last one.

### 5. Combine with Other Techniques

Early stopping works well in combination with:
- Learning rate schedules
- Data augmentation
- Regularization (L1/L2)
- Batch normalization

### 6. Monitor Multiple Metrics

For complex problems, consider monitoring multiple metrics and using a weighted combination for stopping decisions.

## Advantages and Disadvantages

### Advantages

1. **Simplicity**: Easy to implement and understand
2. **Effectiveness**: Often as effective as explicit regularization methods
3. **Computational Efficiency**: Saves training time by stopping early
4. **Automatic Regularization**: Implicitly determines the optimal amount of regularization
5. **No Hyperparameter Tuning**: Unlike L1/L2 regularization, doesn't require setting a regularization strength

### Disadvantages

1. **Requires Validation Set**: Reduces the amount of data available for training
2. **Sensitive to Initialization**: Different random initializations can lead to different stopping points
3. **May Stop Too Early**: Random fluctuations can trigger early stopping prematurely
4. **Not Always Optimal**: In some cases, other regularization methods may perform better
5. **Difficult to Interpret**: The implicit regularization effect is harder to interpret than explicit methods

## Summary

Early stopping is a powerful regularization technique that prevents overfitting by stopping training when the model's performance on a validation set starts to degrade. It has solid theoretical foundations, being equivalent to L2 regularization in some cases, and works by finding the optimal point in the bias-variance tradeoff.

Key considerations for effective early stopping include:
- Using a separate validation set
- Choosing an appropriate metric and patience value
- Restoring the best weights after stopping
- Combining with other regularization techniques
- Monitoring multiple metrics for complex problems

When implemented correctly, early stopping can significantly improve model generalization while saving computational resources.

## References

1. Prechelt, L. (1998). Early Stopping - But When? In Neural Networks: Tricks of the Trade (pp. 55-69). Springer.
2. Morgan, N., & Bourlard, H. (1990). Generalization and Parameter Estimation in Feedforward Nets: Some Experiments. In Advances in Neural Information Processing Systems (pp. 630-637).
3. Yao, Y., Rosasco, L., & Caponnetto, A. (2007). On Early Stopping in Gradient Descent Learning. Constructive Approximation, 26(2), 289-315.
4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
5. Caruana, R., Lawrence, S., & Giles, C. L. (2001). Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping. In Advances in Neural Information Processing Systems (pp. 402-408).
