# 3.2.5 Model Evaluation - Part 5: Practical Considerations (1)

## Practical Considerations for Model Evaluation

Evaluating machine learning models effectively requires more than just calculating metrics. This section covers practical considerations to ensure reliable and meaningful evaluation.

### Cross-Validation Strategies

Cross-validation is essential for obtaining reliable estimates of model performance, but different strategies are appropriate for different scenarios.

#### k-Fold Cross-Validation

**k-Fold Cross-Validation** is the most common approach:

1. Split the data into k equal-sized folds
2. Train the model k times, each time using a different fold as the validation set
3. Average the performance across all k runs

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Define cross-validation strategies
k_values = [3, 5, 10]
cv_results = {}

for k in k_values:
    # Regular k-fold
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    
    # Stratified k-fold
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    
    # Calculate cross-validation scores
    model = LogisticRegression(random_state=42)
    kf_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
    skf_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
    
    cv_results[f"{k}-Fold"] = {
        'regular': kf_scores,
        'stratified': skf_scores
    }
    
    print(f"{k}-Fold Cross-Validation:")
    print(f"  Regular: {np.mean(kf_scores):.4f} ± {np.std(kf_scores):.4f}")
    print(f"  Stratified: {np.mean(skf_scores):.4f} ± {np.std(skf_scores):.4f}")

# Visualize cross-validation results
plt.figure(figsize=(12, 6))

# Plot mean and standard deviation of scores
x = np.arange(len(k_values))
width = 0.35

regular_means = [np.mean(cv_results[f"{k}-Fold"]['regular']) for k in k_values]
regular_stds = [np.std(cv_results[f"{k}-Fold"]['regular']) for k in k_values]

stratified_means = [np.mean(cv_results[f"{k}-Fold"]['stratified']) for k in k_values]
stratified_stds = [np.std(cv_results[f"{k}-Fold"]['stratified']) for k in k_values]

plt.bar(x - width/2, regular_means, width, yerr=regular_stds, label='Regular k-Fold', color='skyblue', capsize=5)
plt.bar(x + width/2, stratified_means, width, yerr=stratified_stds, label='Stratified k-Fold', color='salmon', capsize=5)

plt.xlabel('Number of Folds (k)')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Results')
plt.xticks(x, [f"{k}-Fold" for k in k_values])
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
for i, (reg_mean, strat_mean) in enumerate(zip(regular_means, stratified_means)):
    plt.text(i - width/2, reg_mean - 0.02, f"{reg_mean:.4f}", ha='center', va='top', fontweight='bold')
    plt.text(i + width/2, strat_mean - 0.02, f"{strat_mean:.4f}", ha='center', va='top', fontweight='bold')

plt.tight_layout()
plt.show()

# Visualize the trade-offs with different k values
plt.figure(figsize=(10, 6))

# Define characteristics
k_range = range(2, 21)
bias = [1/k for k in k_range]  # Higher k means lower bias
variance = [1 - 1/k for k in k_range]  # Higher k means higher variance
computation = [k for k in k_range]  # Higher k means more computation

# Normalize for plotting
bias_norm = [b / max(bias) for b in bias]
variance_norm = [v / max(variance) for v in variance]
computation_norm = [c / max(computation) for c in computation]

plt.plot(k_range, bias_norm, 'r-', linewidth=2, label='Bias')
plt.plot(k_range, variance_norm, 'g-', linewidth=2, label='Variance')
plt.plot(k_range, computation_norm, 'b-', linewidth=2, label='Computational Cost')

plt.title('Trade-offs in k-Fold Cross-Validation')
plt.xlabel('Number of Folds (k)')
plt.ylabel('Normalized Value')
plt.xticks(k_range[::2])
plt.legend()
plt.grid(True, alpha=0.3)

# Mark common k values
for k in [3, 5, 10]:
    plt.axvline(x=k, color='gray', linestyle='--', alpha=0.5)
    plt.text(k, 0.5, f"k={k}", ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

#### Stratified Cross-Validation

**Stratified Cross-Validation** ensures that each fold has the same proportion of classes as the original dataset:

- Essential for imbalanced datasets
- Reduces variance in performance estimates
- Default for classification in scikit-learn

```python
# Generate imbalanced data
np.random.seed(42)
X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                                  n_redundant=5, weights=[0.9, 0.1], random_state=42)

print(f"Class distribution: {np.bincount(y_imb)}")
print(f"Class proportions: {np.bincount(y_imb) / len(y_imb)}")

# Compare regular and stratified cross-validation on imbalanced data
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

# Calculate cross-validation scores
model = LogisticRegression(random_state=42)
kf_scores = cross_val_score(model, X_imb, y_imb, cv=kf, scoring='accuracy')
skf_scores = cross_val_score(model, X_imb, y_imb, cv=skf, scoring='accuracy')

print(f"\nImbalanced Dataset:")
print(f"  Regular {k}-Fold: {np.mean(kf_scores):.4f} ± {np.std(kf_scores):.4f}")
print(f"  Stratified {k}-Fold: {np.mean(skf_scores):.4f} ± {np.std(skf_scores):.4f}")

# Visualize class distribution in each fold
plt.figure(figsize=(15, 6))

# Regular k-fold
plt.subplot(1, 2, 1)
fold_class_counts = []
for train_idx, val_idx in kf.split(X_imb):
    y_train, y_val = y_imb[train_idx], y_imb[val_idx]
    train_counts = np.bincount(y_train) / len(y_train)
    val_counts = np.bincount(y_val) / len(y_val)
    fold_class_counts.append((train_counts, val_counts))

# Plot training set class proportions
train_props = np.array([counts[0][1] for counts in fold_class_counts])
plt.bar(np.arange(k) - 0.2, train_props, width=0.4, label='Training', color='blue', alpha=0.7)

# Plot validation set class proportions
val_props = np.array([counts[1][1] for counts in fold_class_counts])
plt.bar(np.arange(k) + 0.2, val_props, width=0.4, label='Validation', color='red', alpha=0.7)

# Plot overall class proportion
plt.axhline(y=np.bincount(y_imb)[1] / len(y_imb), color='k', linestyle='--', label='Overall')

plt.title('Class 1 Proportion in Regular k-Fold CV')
plt.xlabel('Fold')
plt.ylabel('Proportion of Class 1')
plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])
plt.legend()
plt.grid(True, alpha=0.3)

# Stratified k-fold
plt.subplot(1, 2, 2)
fold_class_counts = []
for train_idx, val_idx in skf.split(X_imb, y_imb):
    y_train, y_val = y_imb[train_idx], y_imb[val_idx]
    train_counts = np.bincount(y_train) / len(y_train)
    val_counts = np.bincount(y_val) / len(y_val)
    fold_class_counts.append((train_counts, val_counts))

# Plot training set class proportions
train_props = np.array([counts[0][1] for counts in fold_class_counts])
plt.bar(np.arange(k) - 0.2, train_props, width=0.4, label='Training', color='blue', alpha=0.7)

# Plot validation set class proportions
val_props = np.array([counts[1][1] for counts in fold_class_counts])
plt.bar(np.arange(k) + 0.2, val_props, width=0.4, label='Validation', color='red', alpha=0.7)

# Plot overall class proportion
plt.axhline(y=np.bincount(y_imb)[1] / len(y_imb), color='k', linestyle='--', label='Overall')

plt.title('Class 1 Proportion in Stratified k-Fold CV')
plt.xlabel('Fold')
plt.ylabel('Proportion of Class 1')
plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Time Series Cross-Validation

**Time Series Cross-Validation** respects the temporal order of data:

- Essential for time series data
- Prevents data leakage from future to past
- Simulates real-world forecasting scenarios

```python
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit

# Generate synthetic time series data
np.random.seed(42)
dates = pd.date_range(start='2020-01-01', periods=100, freq='D')
values = np.cumsum(np.random.randn(100)) + 100  # Random walk with drift
time_series = pd.Series(values, index=dates)

# Create features (lag variables)
X = np.zeros((len(time_series)-3, 3))
for i in range(3):
    X[:, i] = time_series.values[i:len(time_series)-3+i]
y = time_series.values[3:]
dates = dates[3:]

# Define time series cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Visualize time series splits
plt.figure(figsize=(12, 8))

# Plot the time series data
plt.subplot(2, 1, 1)
plt.plot(dates, y, 'b-', linewidth=2, label='Time Series')
plt.title('Time Series Data')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the cross-validation splits
plt.subplot(2, 1, 2)
for i, (train_idx, val_idx) in enumerate(tscv.split(X)):
    # Plot training and validation sets
    train_dates = dates[train_idx]
    val_dates = dates[val_idx]
    
    plt.plot(train_dates, y[train_idx], 'o-', color=f'C{i}', alpha=0.7, 
             label=f'Training Set {i+1}' if i == 0 else "")
    plt.plot(val_dates, y[val_idx], 'o-', color=f'C{i}', alpha=0.7, 
             linestyle='--', marker='x', markersize=8,
             label=f'Validation Set {i+1}' if i == 0 else "")

plt.title('Time Series Cross-Validation')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Compare with regular k-fold (which would cause data leakage)
plt.figure(figsize=(12, 8))

# Plot the time series data
plt.subplot(2, 1, 1)
plt.plot(dates, y, 'b-', linewidth=2, label='Time Series')
plt.title('Time Series Data')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot regular k-fold (incorrect for time series)
plt.subplot(2, 1, 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
for i, (train_idx, val_idx) in enumerate(kf.split(X)):
    # Sort indices to show chronological order
    train_idx = np.sort(train_idx)
    val_idx = np.sort(val_idx)
    
    # Plot training and validation sets
    train_dates = dates[train_idx]
    val_dates = dates[val_idx]
    
    plt.plot(train_dates, y[train_idx], 'o-', color=f'C{i}', alpha=0.3, 
             label=f'Training Set {i+1}' if i == 0 else "")
    plt.plot(val_dates, y[val_idx], 'x', color=f'C{i}', alpha=0.7, markersize=8,
             label=f'Validation Set {i+1}' if i == 0 else "")

plt.title('Regular k-Fold Cross-Validation (Incorrect for Time Series)')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotation about data leakage
plt.text(dates[len(dates)//2], np.max(y), "Data Leakage: Future data used to predict past", 
         ha='center', va='bottom', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### Model Selection

Model selection involves choosing the best model from a set of candidate models based on their performance on validation data.

#### Nested Cross-Validation

**Nested Cross-Validation** provides an unbiased estimate of the generalization error when hyperparameter tuning is involved:

1. Outer loop: Split the data into k folds
2. For each fold i:
   - Use the remaining folds for training and hyperparameter tuning using inner cross-validation
   - Evaluate the best model on fold i
3. Average the results across all outer folds

```python
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Define cross-validation strategies
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)

# Define hyperparameter grid
param_grid = {
    'C': [0.1, 1.0, 10.0],
    'gamma': [0.01, 0.1, 1.0]
}

# Perform nested cross-validation
outer_scores = []
best_params = []

for train_idx, test_idx in outer_cv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Inner loop for hyperparameter tuning
    grid_search = GridSearchCV(
        SVC(kernel='rbf', random_state=42),
        param_grid=param_grid,
        cv=inner_cv,
        scoring='accuracy'
    )
    grid_search.fit(X_train, y_train)
    
    # Get the best model and evaluate on the test set
    best_model = grid_search.best_estimator_
    test_score = best_model.score(X_test, y_test)
    
    outer_scores.append(test_score)
    best_params.append(grid_search.best_params_)
    
    print(f"Outer Fold:")
    print(f"  Best parameters: {grid_search.best_params_}")
    print(f"  Test accuracy: {test_score:.4f}")

print(f"\nNested CV accuracy: {np.mean(outer_scores):.4f} ± {np.std(outer_scores):.4f}")

# Count the frequency of each best parameter combination
param_counts = {}
for params in best_params:
    param_str = f"C={params['C']}, gamma={params['gamma']}"
    param_counts[param_str] = param_counts.get(param_str, 0) + 1

print("\nBest parameter frequencies:")
for param_str, count in param_counts.items():
    print(f"  {param_str}: {count}/{len(best_params)}")

# Visualize the nested cross-validation process
plt.figure(figsize=(12, 8))

# Create a grid for the parameter combinations
C_values = param_grid['C']
gamma_values = param_grid['gamma']
scores = np.zeros((len(C_values), len(gamma_values)))

# Fill the grid with the average inner CV scores for each parameter combination
for i, C in enumerate(C_values):
    for j, gamma in enumerate(gamma_values):
        # Train a model with these parameters on the full dataset
        model = SVC(C=C, gamma=gamma, kernel='rbf', random_state=42)
        inner_scores = cross_val_score(model, X, y, cv=inner_cv, scoring='accuracy')
        scores[i, j] = np.mean(inner_scores)

# Plot the parameter grid
plt.imshow(scores, interpolation='nearest', cmap='viridis')
plt.colorbar(label='Accuracy')
plt.xticks(np.arange(len(gamma_values)), gamma_values)
plt.yticks(np.arange(len(C_values)), C_values)
plt.xlabel('gamma')
plt.ylabel('C')

# Mark the best parameter combinations
for i, C in enumerate(C_values):
    for j, gamma in enumerate(gamma_values):
        param_str = f"C={C}, gamma={gamma}"
        count = param_counts.get(param_str, 0)
        if count > 0:
            plt.text(j, i, str(count), ha='center', va='center', 
                     color='white' if scores[i, j] < 0.85 else 'black', fontweight='bold')

plt.title('Nested Cross-Validation: Parameter Selection Frequency')
plt.tight_layout()
plt.show()

# Compare with non-nested cross-validation (which can lead to optimistic bias)
# Perform regular grid search on the full dataset
grid_search = GridSearchCV(
    SVC(kernel='rbf', random_state=42),
    param_grid=param_grid,
    cv=inner_cv,
    scoring='accuracy'
)
grid_search.fit(X, y)

# Get the best model and evaluate using cross-validation
best_model = grid_search.best_estimator_
cv_scores = cross_val_score(best_model, X, y, cv=outer_cv, scoring='accuracy')

print(f"\nNon-nested CV:")
print(f"  Best parameters: {grid_search.best_params_}")
print(f"  CV accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}")

# Compare nested and non-nested CV
plt.figure(figsize=(8, 6))
plt.bar(['Nested CV', 'Non-nested CV'], [np.mean(outer_scores), np.mean(cv_scores)], 
        yerr=[np.std(outer_scores), np.std(cv_scores)], capsize=10, color=['skyblue', 'salmon'])
plt.title('Nested vs. Non-nested Cross-Validation')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
plt.text(0, np.mean(outer_scores) - 0.02, f"{np.mean(outer_scores):.4f}", ha='center', va='top', fontweight='bold')
plt.text(1, np.mean(cv_scores) - 0.02, f"{np.mean(cv_scores):.4f}", ha='center', va='top', fontweight='bold')

# Add explanation
plt.text(0.5, 0.5, "Non-nested CV tends to be optimistic\ndue to information leakage", 
         ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

## Summary

Proper cross-validation and model selection are essential for reliable model evaluation:

1. **Cross-Validation Strategies**:
   - k-Fold Cross-Validation: Splits data into k equal-sized folds
   - Stratified Cross-Validation: Maintains class proportions in each fold
   - Time Series Cross-Validation: Respects temporal order of data

2. **Model Selection**:
   - Nested Cross-Validation: Provides unbiased estimates when hyperparameter tuning is involved
   - Helps avoid optimistic bias in performance estimates
   - Essential for comparing models with different hyperparameters

These techniques help ensure that model evaluation is reliable, unbiased, and representative of real-world performance.

## References

1. Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (pp. 1137-1143).
2. Cawley, G. C., & Talbot, N. L. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research, 11, 2079-2107.
3. Bergmeir, C., & Benítez, J. M. (2012). On the use of cross-validation for time series predictor evaluation. Information Sciences, 191, 192-213.
