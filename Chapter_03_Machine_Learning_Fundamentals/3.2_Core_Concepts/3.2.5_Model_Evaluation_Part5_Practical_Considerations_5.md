# 3.2.5 Model Evaluation - Part 5: Practical Considerations (5)

## Model Interpretability and Explainability

Model interpretability and explainability are increasingly important aspects of model evaluation, especially in high-stakes applications where understanding model decisions is crucial.

### Feature Importance

**Feature importance** measures the contribution of each feature to the model's predictions:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score

# Load breast cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a random forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy: {accuracy:.4f}")

# Get built-in feature importances
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Print feature ranking
print("\nFeature ranking (built-in):")
for i, idx in enumerate(indices[:10]):  # Print top 10 features
    print(f"{i+1}. {feature_names[idx]} ({importances[idx]:.4f})")

# Calculate permutation importance
result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
perm_importances = result.importances_mean
perm_indices = np.argsort(perm_importances)[::-1]

# Print feature ranking based on permutation importance
print("\nFeature ranking (permutation):")
for i, idx in enumerate(perm_indices[:10]):  # Print top 10 features
    print(f"{i+1}. {feature_names[idx]} ({perm_importances[idx]:.4f})")

# Visualize feature importances
plt.figure(figsize=(12, 10))

# Plot built-in feature importances
plt.subplot(2, 1, 1)
plt.bar(range(10), importances[indices[:10]], align='center')
plt.xticks(range(10), [feature_names[i] for i in indices[:10]], rotation=45, ha='right')
plt.title('Feature Importances (Built-in)')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.grid(True, alpha=0.3, axis='y')

# Plot permutation importances
plt.subplot(2, 1, 2)
plt.bar(range(10), perm_importances[perm_indices[:10]], align='center')
plt.xticks(range(10), [feature_names[i] for i in perm_indices[:10]], rotation=45, ha='right')
plt.title('Feature Importances (Permutation)')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Compare built-in and permutation importances
plt.figure(figsize=(12, 6))
plt.scatter(importances, perm_importances, alpha=0.7)
plt.xlabel('Built-in Importance')
plt.ylabel('Permutation Importance')
plt.title('Built-in vs. Permutation Importance')
plt.grid(True, alpha=0.3)

# Add feature names for top features
for i, name in enumerate(feature_names):
    if importances[i] > 0.05 or perm_importances[i] > 0.05:
        plt.annotate(name, (importances[i], perm_importances[i]), fontsize=8)

# Add diagonal line
max_val = max(np.max(importances), np.max(perm_importances))
plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5)

plt.tight_layout()
plt.show()
```

### Partial Dependence Plots

**Partial Dependence Plots (PDPs)** show the marginal effect of a feature on the predicted outcome:

```python
from sklearn.inspection import partial_dependence, plot_partial_dependence

# Select features for partial dependence plots
features = [0, 1, 2]  # First three features

# Create partial dependence plots
fig, ax = plt.subplots(figsize=(12, 4))
plot_partial_dependence(model, X_train, features, feature_names=feature_names, ax=ax)
plt.tight_layout()
plt.show()

# Create more detailed partial dependence plots for individual features
plt.figure(figsize=(15, 5))

for i, feature in enumerate(features):
    # Calculate partial dependence
    pdp_result = partial_dependence(model, X_train, [feature])
    pdp_values = pdp_result['average']
    pdp_grid = pdp_result['values'][0]
    
    # Plot partial dependence
    plt.subplot(1, 3, i+1)
    plt.plot(pdp_grid, pdp_values[0], 'b-', linewidth=2)
    plt.title(f'Partial Dependence of {feature_names[feature]}')
    plt.xlabel(feature_names[feature])
    plt.ylabel('Partial Dependence')
    plt.grid(True, alpha=0.3)
    
    # Add histogram of feature distribution
    ax2 = plt.twinx()
    ax2.hist(X_train[:, feature], bins=30, alpha=0.3, color='gray')
    ax2.set_ylabel('Frequency')
    ax2.set_ylim(bottom=0)

plt.tight_layout()
plt.show()

# Create 2D partial dependence plot
features_2d = [(features[0], features[1])]  # Pair of features
fig, ax = plt.subplots(figsize=(10, 8))
plot_partial_dependence(model, X_train, features_2d, feature_names=feature_names, ax=ax)
plt.tight_layout()
plt.show()
```

### SHAP Values

**SHAP (SHapley Additive exPlanations)** values provide a unified measure of feature importance based on game theory:

```python
import shap

# Create a SHAP explainer
explainer = shap.TreeExplainer(model)

# Calculate SHAP values for a subset of the test data
shap_values = explainer.shap_values(X_test[:100])

# Summary plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test[:100], feature_names=feature_names, show=False)
plt.title('SHAP Summary Plot')
plt.tight_layout()
plt.show()

# Dependence plot for the most important feature
most_important_feature = np.argmax(np.mean(np.abs(shap_values), axis=0))
plt.figure(figsize=(10, 6))
shap.dependence_plot(most_important_feature, shap_values, X_test[:100], feature_names=feature_names, show=False)
plt.title(f'SHAP Dependence Plot for {feature_names[most_important_feature]}')
plt.tight_layout()
plt.show()

# Force plot for a single prediction
plt.figure(figsize=(12, 3))
shap.force_plot(explainer.expected_value, shap_values[0], X_test[0], feature_names=feature_names, matplotlib=True, show=False)
plt.title('SHAP Force Plot for a Single Prediction')
plt.tight_layout()
plt.show()
```

### Local Interpretable Model-agnostic Explanations (LIME)

**LIME** explains individual predictions by approximating the model locally with an interpretable model:

```python
import lime
import lime.lime_tabular

# Create a LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train,
    feature_names=feature_names,
    class_names=['malignant', 'benign'],
    discretize_continuous=True
)

# Explain a single prediction
idx = 0  # Index of the instance to explain
exp = explainer.explain_instance(X_test[idx], model.predict_proba, num_features=10)

# Visualize the explanation
plt.figure(figsize=(10, 6))
exp.as_pyplot_figure()
plt.title(f'LIME Explanation for Instance {idx}')
plt.tight_layout()
plt.show()

# Compare explanations for different instances
plt.figure(figsize=(15, 10))

for i in range(4):
    idx = i  # Index of the instance to explain
    exp = explainer.explain_instance(X_test[idx], model.predict_proba, num_features=5)
    
    plt.subplot(2, 2, i+1)
    exp.as_pyplot_figure(ax=plt.gca())
    plt.title(f'Instance {idx}: {data.target_names[y_test[idx]]}')

plt.tight_layout()
plt.show()
```

### Global Surrogate Models

**Global surrogate models** are interpretable models trained to approximate the predictions of a black-box model:

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Train a decision tree as a surrogate model
surrogate_model = DecisionTreeClassifier(max_depth=3, random_state=42)
surrogate_model.fit(X_train, model.predict(X_train))

# Evaluate the surrogate model's fidelity
surrogate_pred = surrogate_model.predict(X_test)
original_pred = model.predict(X_test)
fidelity = accuracy_score(original_pred, surrogate_pred)

print(f"Surrogate model fidelity: {fidelity:.4f}")

# Visualize the surrogate decision tree
plt.figure(figsize=(15, 10))
plot_tree(surrogate_model, feature_names=feature_names, class_names=data.target_names, filled=True, rounded=True)
plt.title('Surrogate Decision Tree')
plt.tight_layout()
plt.show()

# Compare surrogate and original model predictions
plt.figure(figsize=(10, 6))
plt.scatter(model.predict_proba(X_test)[:, 1], surrogate_model.predict_proba(X_test)[:, 1], alpha=0.5)
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
plt.xlabel('Original Model Probability')
plt.ylabel('Surrogate Model Probability')
plt.title('Original vs. Surrogate Model Predictions')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Model-Specific Interpretability

Some models are inherently more interpretable than others:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Train interpretable models
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

decision_tree = DecisionTreeClassifier(max_depth=3, random_state=42)
decision_tree.fit(X_train, y_train)

# Evaluate models
log_reg_pred = log_reg.predict(X_test)
log_reg_acc = accuracy_score(y_test, log_reg_pred)

tree_pred = decision_tree.predict(X_test)
tree_acc = accuracy_score(y_test, tree_pred)

print(f"Logistic Regression accuracy: {log_reg_acc:.4f}")
print(f"Decision Tree accuracy: {tree_acc:.4f}")
print(f"Random Forest accuracy: {accuracy:.4f}")

# Visualize logistic regression coefficients
plt.figure(figsize=(12, 6))
coef = log_reg.coef_[0]
indices = np.argsort(np.abs(coef))[::-1]
plt.bar(range(10), coef[indices[:10]], align='center')
plt.xticks(range(10), [feature_names[i] for i in indices[:10]], rotation=45, ha='right')
plt.title('Logistic Regression Coefficients')
plt.xlabel('Feature')
plt.ylabel('Coefficient')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Visualize decision tree
plt.figure(figsize=(15, 10))
plot_tree(decision_tree, feature_names=feature_names, class_names=data.target_names, filled=True, rounded=True)
plt.title('Decision Tree')
plt.tight_layout()
plt.show()

# Compare model interpretability and performance
plt.figure(figsize=(10, 6))
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracies = [log_reg_acc, tree_acc, accuracy]
interpretability = [0.9, 0.7, 0.3]  # Subjective interpretability scores

plt.scatter(interpretability, accuracies, s=100)
for i, model in enumerate(models):
    plt.annotate(model, (interpretability[i], accuracies[i]), fontsize=12, 
                 xytext=(10, 5), textcoords='offset points')

plt.xlabel('Interpretability')
plt.ylabel('Accuracy')
plt.title('Model Interpretability vs. Performance')
plt.xlim(0, 1)
plt.ylim(0.9, 1)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## Summary

Model interpretability and explainability are crucial for understanding and trusting machine learning models:

1. **Feature Importance**:
   - Measures the contribution of each feature to the model's predictions
   - Can be calculated using built-in methods or permutation importance
   - Helps identify the most influential features

2. **Partial Dependence Plots (PDPs)**:
   - Show the marginal effect of a feature on the predicted outcome
   - Help understand how the model's predictions change as a feature varies
   - Can reveal non-linear relationships and interactions

3. **SHAP Values**:
   - Provide a unified measure of feature importance based on game theory
   - Explain individual predictions and global model behavior
   - Consistent and theoretically sound

4. **Local Interpretable Model-agnostic Explanations (LIME)**:
   - Explain individual predictions by approximating the model locally
   - Work with any black-box model
   - Provide intuitive explanations for specific instances

5. **Global Surrogate Models**:
   - Interpretable models trained to approximate black-box models
   - Provide global understanding of complex models
   - Trade off fidelity for interpretability

6. **Model-Specific Interpretability**:
   - Some models (e.g., linear models, decision trees) are inherently more interpretable
   - Consider the trade-off between interpretability and performance
   - Choose the right model for the specific application

These techniques help stakeholders understand, trust, and effectively use machine learning models, especially in high-stakes applications where model decisions need to be explained and justified.

## References

1. Molnar, C. (2020). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable.
2. Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why should I trust you?": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144).
3. Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems (pp. 4765-4774).
4. Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics, 24(1), 44-65.
