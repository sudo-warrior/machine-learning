# 3.2.4 Regularization Techniques - Part 1: Introduction and L1/L2 Regularization

## Introduction to Regularization

Regularization is a set of techniques used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, capturing noise and specific patterns that don't generalize to new data. A model that overfits will perform well on training data but poorly on unseen data.

### The Bias-Variance Tradeoff

Regularization is closely related to the bias-variance tradeoff, a fundamental concept in machine learning:

- **High Variance (Overfitting)**: The model is too complex, captures noise, and doesn't generalize well.
- **High Bias (Underfitting)**: The model is too simple and fails to capture important patterns in the data.

Regularization helps find the sweet spot between these extremes by adding constraints to the model.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.sort(np.random.uniform(0, 1, 30))
y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, 30)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create models with different polynomial degrees
degrees = [1, 3, 10, 20]
plt.figure(figsize=(14, 10))

for i, degree in enumerate(degrees):
    # Create polynomial features
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    
    # Create pipeline
    model = Pipeline([
        ("polynomial_features", polynomial_features),
        ("linear_regression", LinearRegression())
    ])
    
    # Fit model
    model.fit(X_train, y_train)
    
    # Make predictions
    X_test_sorted, y_test_sorted = zip(*sorted(zip(X_test.ravel(), y_test)))
    X_test_sorted = np.array(X_test_sorted).reshape(-1, 1)
    y_test_sorted = np.array(y_test_sorted)
    
    X_range = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_range)
    
    # Calculate errors
    train_error = mean_squared_error(y_train, model.predict(X_train))
    test_error = mean_squared_error(y_test, model.predict(X_test))
    
    # Plot results
    plt.subplot(2, 2, i+1)
    plt.scatter(X_train, y_train, color='blue', label='Training data')
    plt.scatter(X_test, y_test, color='red', label='Test data')
    plt.plot(X_range, y_pred, color='green', label='Prediction')
    plt.title(f'Polynomial Degree {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.ylim(-1.5, 1.5)
    plt.text(0.05, -1.3, f'Train MSE: {train_error:.4f}\nTest MSE: {test_error:.4f}')
    plt.legend()

plt.tight_layout()
plt.show()
```

### Why Regularization Works

Regularization works by adding a penalty term to the loss function that discourages complex models. This penalty increases as the model becomes more complex, forcing the learning algorithm to balance fitting the data well and keeping the model simple.

The regularized loss function typically takes the form:

$$L_{regularized}(\theta) = L(\theta) + \lambda R(\theta)$$

where:
- $L(\theta)$ is the original loss function
- $R(\theta)$ is the regularization term
- $\lambda$ is the regularization strength (hyperparameter)

By adjusting $\lambda$, we can control the tradeoff between fitting the training data and keeping the model simple.

## L1 and L2 Regularization

The two most common forms of regularization for linear models are L1 (Lasso) and L2 (Ridge) regularization.

### L2 Regularization (Ridge)

L2 regularization adds the sum of squared parameter values to the loss function:

$$L_{L2}(\theta) = L(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2$$

This is equivalent to placing a Gaussian prior on the parameters, encouraging them to be small but not exactly zero.

```python
from sklearn.linear_model import Ridge
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.sort(np.random.uniform(0, 1, 30))
y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, 30)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create models with different regularization strengths
alphas = [0, 0.001, 0.01, 0.1]
degree = 10  # High degree polynomial to demonstrate overfitting
plt.figure(figsize=(14, 10))

for i, alpha in enumerate(alphas):
    # Create polynomial features
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    
    # Create pipeline with Ridge regression
    model = Pipeline([
        ("polynomial_features", polynomial_features),
        ("ridge_regression", Ridge(alpha=alpha))
    ])
    
    # Fit model
    model.fit(X_train, y_train)
    
    # Make predictions
    X_range = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_range)
    
    # Calculate errors
    train_error = mean_squared_error(y_train, model.predict(X_train))
    test_error = mean_squared_error(y_test, model.predict(X_test))
    
    # Plot results
    plt.subplot(2, 2, i+1)
    plt.scatter(X_train, y_train, color='blue', label='Training data')
    plt.scatter(X_test, y_test, color='red', label='Test data')
    plt.plot(X_range, y_pred, color='green', label='Prediction')
    plt.title(f'Ridge Regression (alpha={alpha})')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.ylim(-1.5, 1.5)
    plt.text(0.05, -1.3, f'Train MSE: {train_error:.4f}\nTest MSE: {test_error:.4f}')
    plt.legend()

plt.tight_layout()
plt.show()

# Visualize the effect of L2 regularization on coefficients
plt.figure(figsize=(10, 6))
alphas = [0, 0.001, 0.01, 0.1, 1.0, 10.0]
coefs = []

for alpha in alphas:
    model = Pipeline([
        ("polynomial_features", PolynomialFeatures(degree=degree, include_bias=False)),
        ("ridge_regression", Ridge(alpha=alpha))
    ])
    model.fit(X_train, y_train)
    coefs.append(model.named_steps['ridge_regression'].coef_)

coefs = np.array(coefs)
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('alpha (log scale)')
plt.ylabel('Coefficient Value')
plt.title('Ridge Regression Coefficients vs. Regularization Strength')
plt.legend([f'Coef {i}' for i in range(coefs.shape[1])], loc='upper right')
plt.grid(True, alpha=0.3)
plt.show()
```

### L1 Regularization (Lasso)

L1 regularization adds the sum of absolute parameter values to the loss function:

$$L_{L1}(\theta) = L(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|$$

This is equivalent to placing a Laplace prior on the parameters, encouraging sparsity (many parameters become exactly zero).

```python
from sklearn.linear_model import Lasso
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.sort(np.random.uniform(0, 1, 30))
y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, 30)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create models with different regularization strengths
alphas = [0.0001, 0.001, 0.01, 0.1]
degree = 10  # High degree polynomial to demonstrate overfitting
plt.figure(figsize=(14, 10))

for i, alpha in enumerate(alphas):
    # Create polynomial features
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    
    # Create pipeline with Lasso regression
    model = Pipeline([
        ("polynomial_features", polynomial_features),
        ("lasso_regression", Lasso(alpha=alpha, max_iter=10000))
    ])
    
    # Fit model
    model.fit(X_train, y_train)
    
    # Make predictions
    X_range = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_range)
    
    # Calculate errors
    train_error = mean_squared_error(y_train, model.predict(X_train))
    test_error = mean_squared_error(y_test, model.predict(X_test))
    
    # Count non-zero coefficients
    non_zero_coefs = np.sum(model.named_steps['lasso_regression'].coef_ != 0)
    
    # Plot results
    plt.subplot(2, 2, i+1)
    plt.scatter(X_train, y_train, color='blue', label='Training data')
    plt.scatter(X_test, y_test, color='red', label='Test data')
    plt.plot(X_range, y_pred, color='green', label='Prediction')
    plt.title(f'Lasso Regression (alpha={alpha})')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.ylim(-1.5, 1.5)
    plt.text(0.05, -1.3, f'Train MSE: {train_error:.4f}\nTest MSE: {test_error:.4f}\nNon-zero coefs: {non_zero_coefs}/{degree}')
    plt.legend()

plt.tight_layout()
plt.show()

# Visualize the effect of L1 regularization on coefficients
plt.figure(figsize=(10, 6))
alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]
coefs = []

for alpha in alphas:
    model = Pipeline([
        ("polynomial_features", PolynomialFeatures(degree=degree, include_bias=False)),
        ("lasso_regression", Lasso(alpha=alpha, max_iter=10000))
    ])
    model.fit(X_train, y_train)
    coefs.append(model.named_steps['lasso_regression'].coef_)

coefs = np.array(coefs)
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('alpha (log scale)')
plt.ylabel('Coefficient Value')
plt.title('Lasso Regression Coefficients vs. Regularization Strength')
plt.legend([f'Coef {i}' for i in range(coefs.shape[1])], loc='upper right')
plt.grid(True, alpha=0.3)
plt.show()
```

### Elastic Net: Combining L1 and L2

Elastic Net combines L1 and L2 regularization to get the best of both worlds:

$$L_{ElasticNet}(\theta) = L(\theta) + \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2$$

This is often written as:

$$L_{ElasticNet}(\theta) = L(\theta) + \lambda \left( \alpha \sum_{i=1}^{n} |\theta_i| + (1 - \alpha) \sum_{i=1}^{n} \theta_i^2 \right)$$

where:
- $\lambda$ controls the overall regularization strength
- $\alpha$ controls the balance between L1 and L2 regularization

```python
from sklearn.linear_model import ElasticNet
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.sort(np.random.uniform(0, 1, 30))
y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, 30)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create models with different alpha values (L1 ratio)
l1_ratios = [0.0, 0.25, 0.5, 0.75, 1.0]
alpha = 0.01  # Fixed regularization strength
degree = 10  # High degree polynomial to demonstrate overfitting
plt.figure(figsize=(15, 12))

for i, l1_ratio in enumerate(l1_ratios):
    # Create polynomial features
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    
    # Create pipeline with ElasticNet regression
    model = Pipeline([
        ("polynomial_features", polynomial_features),
        ("elastic_net", ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000))
    ])
    
    # Fit model
    model.fit(X_train, y_train)
    
    # Make predictions
    X_range = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_range)
    
    # Calculate errors
    train_error = mean_squared_error(y_train, model.predict(X_train))
    test_error = mean_squared_error(y_test, model.predict(X_test))
    
    # Count non-zero coefficients
    non_zero_coefs = np.sum(model.named_steps['elastic_net'].coef_ != 0)
    
    # Plot results
    plt.subplot(3, 2, i+1)
    plt.scatter(X_train, y_train, color='blue', label='Training data')
    plt.scatter(X_test, y_test, color='red', label='Test data')
    plt.plot(X_range, y_pred, color='green', label='Prediction')
    
    if l1_ratio == 0.0:
        title = 'Ridge (L2 only)'
    elif l1_ratio == 1.0:
        title = 'Lasso (L1 only)'
    else:
        title = f'ElasticNet (L1 ratio={l1_ratio})'
        
    plt.title(title)
    plt.xlabel('X')
    plt.ylabel('y')
    plt.ylim(-1.5, 1.5)
    plt.text(0.05, -1.3, f'Train MSE: {train_error:.4f}\nTest MSE: {test_error:.4f}\nNon-zero coefs: {non_zero_coefs}/{degree}')
    plt.legend()

plt.tight_layout()
plt.show()

# Visualize the effect of L1 ratio on coefficients
plt.figure(figsize=(10, 6))
l1_ratios = np.linspace(0, 1, 11)
coefs = []

for l1_ratio in l1_ratios:
    model = Pipeline([
        ("polynomial_features", PolynomialFeatures(degree=degree, include_bias=False)),
        ("elastic_net", ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000))
    ])
    model.fit(X_train, y_train)
    coefs.append(model.named_steps['elastic_net'].coef_)

coefs = np.array(coefs)
plt.plot(l1_ratios, coefs)
plt.xlabel('L1 Ratio (0 = Ridge, 1 = Lasso)')
plt.ylabel('Coefficient Value')
plt.title('ElasticNet Coefficients vs. L1 Ratio')
plt.legend([f'Coef {i}' for i in range(coefs.shape[1])], loc='upper right')
plt.grid(True, alpha=0.3)
plt.show()
```

## Regularization in Practice

### Choosing the Regularization Strength

The regularization strength ($\lambda$ or `alpha` in scikit-learn) is a hyperparameter that needs to be tuned. Cross-validation is typically used to find the optimal value:

```python
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV
from sklearn.model_selection import KFold
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data with sparse coefficients
np.random.seed(42)
n_samples, n_features = 100, 20
X = np.random.randn(n_samples, n_features)
# Only 5 features are actually used
true_coef = np.zeros(n_features)
true_coef[:5] = np.array([1.5, -2, 0.5, -1, 1])
y = X @ true_coef + np.random.normal(0, 0.1, n_samples)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define cross-validation strategy
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Ridge with cross-validation
alphas_ridge = np.logspace(-6, 6, 13)
ridge_cv = RidgeCV(alphas=alphas_ridge, cv=cv)
ridge_cv.fit(X_train, y_train)
ridge_score = ridge_cv.score(X_test, y_test)
ridge_alpha = ridge_cv.alpha_

# Lasso with cross-validation
alphas_lasso = np.logspace(-6, 0, 13)
lasso_cv = LassoCV(alphas=alphas_lasso, cv=cv, max_iter=10000)
lasso_cv.fit(X_train, y_train)
lasso_score = lasso_cv.score(X_test, y_test)
lasso_alpha = lasso_cv.alpha_

# ElasticNet with cross-validation
alphas_elasticnet = np.logspace(-6, 0, 13)
l1_ratios = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]
elasticnet_cv = ElasticNetCV(alphas=alphas_elasticnet, l1_ratio=l1_ratios, cv=cv, max_iter=10000)
elasticnet_cv.fit(X_train, y_train)
elasticnet_score = elasticnet_cv.score(X_test, y_test)
elasticnet_alpha = elasticnet_cv.alpha_
elasticnet_l1_ratio = elasticnet_cv.l1_ratio_

# Print results
print(f"Ridge - Best alpha: {ridge_alpha:.6f}, R² score: {ridge_score:.4f}")
print(f"Lasso - Best alpha: {lasso_alpha:.6f}, R² score: {lasso_score:.4f}")
print(f"ElasticNet - Best alpha: {elasticnet_alpha:.6f}, Best L1 ratio: {elasticnet_l1_ratio:.2f}, R² score: {elasticnet_score:.4f}")

# Plot coefficients
plt.figure(figsize=(12, 6))
plt.plot(range(n_features), true_coef, 'o-', label='True coefficients')
plt.plot(range(n_features), ridge_cv.coef_, 'o-', label=f'Ridge (alpha={ridge_alpha:.6f})')
plt.plot(range(n_features), lasso_cv.coef_, 'o-', label=f'Lasso (alpha={lasso_alpha:.6f})')
plt.plot(range(n_features), elasticnet_cv.coef_, 'o-', label=f'ElasticNet (alpha={elasticnet_alpha:.6f}, L1 ratio={elasticnet_l1_ratio:.2f})')
plt.xlabel('Feature index')
plt.ylabel('Coefficient value')
plt.title('Comparison of Regularization Methods')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Regularization for Feature Selection

L1 regularization (Lasso) is particularly useful for feature selection because it tends to produce sparse models (many coefficients become exactly zero):

```python
from sklearn.linear_model import Lasso
from sklearn.datasets import load_diabetes
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# Load diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target
feature_names = diabetes.feature_names

# Standardize features
X = StandardScaler().fit_transform(X)

# Train Lasso with different regularization strengths
alphas = [0.01, 0.1, 1.0, 10.0]
plt.figure(figsize=(12, 8))

for i, alpha in enumerate(alphas):
    # Train Lasso model
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X, y)
    
    # Get coefficients
    coefs = lasso.coef_
    
    # Plot coefficients
    plt.subplot(2, 2, i+1)
    plt.bar(feature_names, coefs)
    plt.title(f'Lasso Coefficients (alpha={alpha})')
    plt.xticks(rotation=90)
    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    
    # Count non-zero coefficients
    non_zero = np.sum(coefs != 0)
    plt.text(0, min(coefs) - 0.1 * (max(coefs) - min(coefs)), 
             f'Non-zero coefficients: {non_zero}/{len(coefs)}')

plt.tight_layout()
plt.show()
```

### Regularization in Neural Networks

In neural networks, regularization is often applied to the weights of each layer. Both L1 and L2 regularization can be used, with L2 being more common:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l1, l2, l1_l2
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# Load breast cancer dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Standardize features
X = StandardScaler().fit_transform(X)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to create and train a model with specified regularization
def create_and_train_model(regularizer=None, name="No Regularization"):
    model = Sequential([
        Dense(32, activation='relu', kernel_regularizer=regularizer, input_shape=(X_train.shape[1],)),
        Dense(16, activation='relu', kernel_regularizer=regularizer),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
    history = model.fit(X_train, y_train,
                        epochs=100,
                        batch_size=32,
                        validation_split=0.2,
                        verbose=0)
    
    return model, history, name

# Create and train models with different regularization
models = [
    create_and_train_model(None, "No Regularization"),
    create_and_train_model(l2(0.01), "L2 Regularization"),
    create_and_train_model(l1(0.01), "L1 Regularization"),
    create_and_train_model(l1_l2(l1=0.01, l2=0.01), "L1+L2 Regularization")
]

# Plot training and validation loss
plt.figure(figsize=(12, 10))

for i, (model, history, name) in enumerate(models):
    plt.subplot(2, 2, i+1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{name} - Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Evaluate models on test set
plt.figure(figsize=(10, 6))
test_accuracies = []

for model, _, name in models:
    _, accuracy = model.evaluate(X_test, y_test, verbose=0)
    test_accuracies.append(accuracy)
    print(f"{name} - Test Accuracy: {accuracy:.4f}")

plt.bar([model[2] for model in models], test_accuracies)
plt.title('Test Accuracy Comparison')
plt.ylabel('Accuracy')
plt.ylim(0.9, 1.0)  # Adjust as needed
plt.grid(True, alpha=0.3, axis='y')
plt.show()
```

## Advantages and Disadvantages

### L2 Regularization (Ridge)

**Advantages:**
- Handles correlated features well
- Stable solution (small changes in data lead to small changes in the model)
- All features are kept in the model, but with reduced importance

**Disadvantages:**
- Does not perform feature selection (all coefficients remain non-zero)
- May not work well when there are many irrelevant features

### L1 Regularization (Lasso)

**Advantages:**
- Performs feature selection (sets some coefficients to exactly zero)
- Can handle high-dimensional data with many irrelevant features
- Produces sparse models that are easier to interpret

**Disadvantages:**
- Less stable than L2 (small changes in data can lead to different features being selected)
- May struggle with correlated features (tends to pick one and ignore the others)
- Can be computationally more intensive than L2

### Elastic Net

**Advantages:**
- Combines the benefits of L1 and L2 regularization
- Handles correlated features better than Lasso
- Still performs feature selection like Lasso

**Disadvantages:**
- Requires tuning two hyperparameters (overall strength and L1 ratio)
- Can be computationally more intensive than either L1 or L2 alone

## Summary

Regularization is a powerful technique for preventing overfitting in machine learning models. The main types of regularization for linear models are:

1. **L2 Regularization (Ridge)**: Adds the sum of squared parameter values to the loss function, encouraging all parameters to be small but non-zero.

2. **L1 Regularization (Lasso)**: Adds the sum of absolute parameter values to the loss function, encouraging sparsity (many parameters become exactly zero).

3. **Elastic Net**: Combines L1 and L2 regularization to get the benefits of both.

The choice of regularization method depends on the specific problem:
- Use L2 when all features are potentially relevant and you want to reduce the impact of correlated features.
- Use L1 when you suspect many features are irrelevant and want to perform feature selection.
- Use Elastic Net when you want a balance between feature selection and handling correlated features.

In the next part, we'll explore other regularization techniques such as dropout, early stopping, and data augmentation.

## References

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
3. Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1), 267-288.
4. Hoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55-67.
5. Zou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, 67(2), 301-320.
