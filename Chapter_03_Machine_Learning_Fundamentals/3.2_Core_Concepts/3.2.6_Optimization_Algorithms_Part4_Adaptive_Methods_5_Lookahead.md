# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (5) - Lookahead

## Lookahead Optimizer

**Lookahead** is a meta-optimizer that can be combined with any base optimizer (like SGD, Adam, or RMSProp) to improve convergence and stability. It was introduced in 2019 and has shown promising results across various deep learning tasks.

### Motivation

Optimization in deep learning often faces challenges like:
1. Sensitivity to learning rate
2. Getting stuck in local minima or saddle points
3. Noisy gradients leading to unstable training

Lookahead addresses these issues by maintaining two sets of weights and using a "slow" update strategy to improve stability.

### Algorithm

Lookahead works by maintaining two sets of weights:
1. **Fast weights**: Updated by the base optimizer (e.g., Adam)
2. **Slow weights**: Updated periodically by moving toward the fast weights

The algorithm can be summarized as follows:

1. Initialize slow weights $\phi_0 = \theta_0$
2. For each iteration $t$:
   - Update fast weights $\theta_t$ using the base optimizer for $k$ steps
   - Update slow weights: $\phi_{t+1} = \phi_t + \alpha(\theta_t - \phi_t)$
   - Reset fast weights: $\theta_{t+1} = \phi_{t+1}$

where:
- $\alpha$ is the slow weights step size (typically 0.5)
- $k$ is the number of fast weight updates before a slow weight update (typically 5-10)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add bias term to X
X_train_b = np.c_[np.ones(X_train.shape[0]), X_train]
X_test_b = np.c_[np.ones(X_test.shape[0]), X_test]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Implement Adam optimizer
def adam(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
         beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Implement Lookahead with Adam as the base optimizer
def lookahead_adam(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
                  beta1=0.9, beta2=0.999, epsilon=1e-8, alpha=0.5, k=5):
    """
    Perform Lookahead optimization with Adam as the base optimizer.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate for Adam
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment (Adam)
    - beta2: Exponential decay rate for second moment (Adam)
    - epsilon: Small constant to avoid division by zero (Adam)
    - alpha: Slow weights step size (Lookahead)
    - k: Number of fast weight updates before a slow weight update (Lookahead)
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    
    # Initialize slow weights
    phi = np.zeros(n)
    
    # Initialize fast weights
    theta = phi.copy()
    
    # Initialize first and second moment estimates for Adam
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [phi.copy()]
    train_cost_history = [linear_regression_cost(phi, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(phi, X_test, y_test))
    
    for outer_iter in range(1, num_iterations // k + 1):
        # Store the current slow weights
        phi_old = phi.copy()
        
        # Update fast weights for k steps using Adam
        for inner_iter in range(1, k + 1):
            t = (outer_iter - 1) * k + inner_iter
            
            gradient = linear_regression_gradient(theta, X, y)
            
            # Update biased first moment estimate
            m_t = beta1 * m_t + (1 - beta1) * gradient
            
            # Update biased second moment estimate
            v_t = beta2 * v_t + (1 - beta2) * gradient**2
            
            # Compute bias-corrected first moment estimate
            m_t_hat = m_t / (1 - beta1**t)
            
            # Compute bias-corrected second moment estimate
            v_t_hat = v_t / (1 - beta2**t)
            
            # Update fast weights
            theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
            
            # Record history
            theta_history.append(theta.copy())
            train_cost_history.append(linear_regression_cost(theta, X, y))
            
            if X_test is not None and y_test is not None:
                test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
        
        # Update slow weights
        phi = phi_old + alpha * (theta - phi_old)
        
        # Reset fast weights
        theta = phi.copy()
        
        # Record history after slow weights update
        theta_history.append(phi.copy())
        train_cost_history.append(linear_regression_cost(phi, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(phi, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Compare Adam and Lookahead+Adam
learning_rate = 0.01
num_iterations = 500
alpha = 0.5  # Slow weights step size
k = 5  # Number of fast weight updates before a slow weight update

# Run Adam
theta_adam, train_cost_adam, test_cost_adam = adam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Run Lookahead+Adam
theta_lookahead, train_cost_lookahead, test_cost_lookahead = lookahead_adam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations,
    alpha=alpha, k=k)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot training cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(train_cost_adam)), train_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(train_cost_lookahead)), train_cost_lookahead, linewidth=2, label=f'Lookahead+Adam (α={alpha}, k={k})')
plt.xlabel('Iteration')
plt.ylabel('Training Cost')
plt.title('Training Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot test cost over iterations
plt.subplot(2, 2, 2)
plt.plot(range(len(test_cost_adam)), test_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(test_cost_lookahead)), test_cost_lookahead, linewidth=2, label=f'Lookahead+Adam (α={alpha}, k={k})')
plt.xlabel('Iteration')
plt.ylabel('Test Cost')
plt.title('Test Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot parameter L2 norm over iterations
plt.subplot(2, 2, 3)
adam_norms = [np.linalg.norm(theta[1:]) for theta in theta_adam]  # Exclude bias term
lookahead_norms = [np.linalg.norm(theta[1:]) for theta in theta_lookahead]  # Exclude bias term

plt.plot(range(len(adam_norms)), adam_norms, linewidth=2, label='Adam')
plt.plot(range(len(lookahead_norms)), lookahead_norms, linewidth=2, label='Lookahead+Adam')
plt.xlabel('Iteration')
plt.ylabel('Parameter L2 Norm (excluding bias)')
plt.title('Parameter L2 Norm vs. Iteration')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
adam_updates = np.diff(theta_adam, axis=0)
lookahead_updates = np.diff(theta_lookahead, axis=0)

adam_update_norms = np.linalg.norm(adam_updates, axis=1)
lookahead_update_norms = np.linalg.norm(lookahead_updates, axis=1)

plt.semilogy(range(len(adam_update_norms)), adam_update_norms, linewidth=2, label='Adam')
plt.semilogy(range(len(lookahead_update_norms)), lookahead_update_norms, linewidth=2, label='Lookahead+Adam')
plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Lookahead on Non-Convex Functions

Let's see how Lookahead performs on a challenging non-convex function compared to the base optimizer:

```python
# Define a challenging non-convex function
def rastrigin_function(theta):
    """
    Rastrigin function: f(x,y) = 20 + x² + y² - 10(cos(2πx) + cos(2πy))
    Global minimum at (0,0)
    """
    A = 10
    n = len(theta)
    return A * n + np.sum(theta**2 - A * np.cos(2 * np.pi * theta))

def rastrigin_gradient(theta):
    """Gradient of the Rastrigin function."""
    A = 10
    return 2 * theta + 2 * np.pi * A * np.sin(2 * np.pi * theta)

# Implement generic Adam optimizer
def adam_optimize(func, grad_func, start_point, learning_rate=0.001, num_iterations=1000, 
                 beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    # Initialize moment estimates
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    
    for t in range(1, num_iterations + 1):
        gradient = grad_func(theta)
        
        # Update biased first moment estimate
        m = beta1 * m + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v = beta2 * v + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_hat = m / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_hat = v / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Implement Lookahead with Adam as the base optimizer
def lookahead_adam_optimize(func, grad_func, start_point, learning_rate=0.001, num_iterations=1000, 
                           beta1=0.9, beta2=0.999, epsilon=1e-8, alpha=0.5, k=5):
    """
    Perform Lookahead optimization with Adam as the base optimizer.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - learning_rate: Learning rate for Adam
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment (Adam)
    - beta2: Exponential decay rate for second moment (Adam)
    - epsilon: Small constant to avoid division by zero (Adam)
    - alpha: Slow weights step size (Lookahead)
    - k: Number of fast weight updates before a slow weight update (Lookahead)
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    # Initialize slow weights
    phi = start_point.copy()
    
    # Initialize fast weights
    theta = phi.copy()
    
    theta_history = [phi.copy()]
    cost_history = [func(phi)]
    
    # Initialize moment estimates for Adam
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    
    for outer_iter in range(1, num_iterations // k + 1):
        # Store the current slow weights
        phi_old = phi.copy()
        
        # Update fast weights for k steps using Adam
        for inner_iter in range(1, k + 1):
            t = (outer_iter - 1) * k + inner_iter
            
            gradient = grad_func(theta)
            
            # Update biased first moment estimate
            m = beta1 * m + (1 - beta1) * gradient
            
            # Update biased second moment estimate
            v = beta2 * v + (1 - beta2) * gradient**2
            
            # Compute bias-corrected first moment estimate
            m_hat = m / (1 - beta1**t)
            
            # Compute bias-corrected second moment estimate
            v_hat = v / (1 - beta2**t)
            
            # Update fast weights
            theta = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
            
            # Record history
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
        
        # Update slow weights
        phi = phi_old + alpha * (theta - phi_old)
        
        # Reset fast weights
        theta = phi.copy()
        
        # Record history after slow weights update
        theta_history.append(phi.copy())
        cost_history.append(func(phi))
    
    return np.array(theta_history), np.array(cost_history)

# Run optimizers on the Rastrigin function
start_point_rastrigin = np.array([2.0, 3.0])
num_iterations = 1000
learning_rate = 0.001
alpha = 0.5
k = 5

# Run Adam
theta_adam, cost_adam = adam_optimize(
    rastrigin_function, rastrigin_gradient, start_point_rastrigin,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Run Lookahead+Adam
theta_lookahead, cost_lookahead = lookahead_adam_optimize(
    rastrigin_function, rastrigin_gradient, start_point_rastrigin,
    learning_rate=learning_rate, num_iterations=num_iterations,
    alpha=alpha, k=k)

# Visualize the results for Rastrigin function
plt.figure(figsize=(15, 10))

# Create a grid for contour plot
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(len(x)):
    for j in range(len(y)):
        Z[j, i] = rastrigin_function(np.array([X[j, i], Y[j, i]]))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(X, Y, Z, levels=np.linspace(0, 50, 10), cmap='viridis')
plt.colorbar(contour)

plt.plot(theta_adam[:, 0], theta_adam[:, 1], 'o-', linewidth=1, markersize=2, label='Adam')
plt.plot(theta_lookahead[:, 0], theta_lookahead[:, 1], 'o-', linewidth=1, markersize=2, label=f'Lookahead+Adam (α={alpha}, k={k})')

plt.scatter(0, 0, color='red', s=100, label='Global Minimum')
plt.scatter(start_point_rastrigin[0], start_point_rastrigin[1], color='green', s=100, label='Start')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Trajectories on Rastrigin Function')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_adam)), cost_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(cost_lookahead)), cost_lookahead, linewidth=2, label='Lookahead+Adam')

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([0, 0])  # Global minimum of Rastrigin function
distances_adam = np.sqrt(np.sum((theta_adam - minimum)**2, axis=1))
distances_lookahead = np.sqrt(np.sum((theta_lookahead - minimum)**2, axis=1))

plt.semilogy(range(len(distances_adam)), distances_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(distances_lookahead)), distances_lookahead, linewidth=2, label='Lookahead+Adam')

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
updates_adam = np.diff(theta_adam, axis=0)
updates_lookahead = np.diff(theta_lookahead, axis=0)

update_norms_adam = np.linalg.norm(updates_adam, axis=1)
update_norms_lookahead = np.linalg.norm(updates_lookahead, axis=1)

plt.semilogy(range(len(update_norms_adam)), update_norms_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(update_norms_lookahead)), update_norms_lookahead, linewidth=2, label='Lookahead+Adam')

plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Effect of Lookahead Hyperparameters

Let's examine how the hyperparameters of Lookahead affect its performance:

```python
# Run Lookahead+Adam with different hyperparameter settings
alpha_values = [0.2, 0.5, 0.8]
k_values = [5, 10, 20]

results = []
for alpha in alpha_values:
    for k in k_values:
        theta_history, cost_history = lookahead_adam_optimize(
            rastrigin_function, rastrigin_gradient, start_point_rastrigin,
            learning_rate=learning_rate, num_iterations=num_iterations,
            alpha=alpha, k=k)
        results.append((f'α={alpha}, k={k}', theta_history, cost_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 1)
for name, theta_history, cost_history in results:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Effect of Hyperparameters on Lookahead+Adam')
plt.legend()
plt.grid(True)

# Plot final cost vs. hyperparameter setting
plt.subplot(2, 2, 2)
names = [name for name, _, _ in results]
final_costs = [cost_history[-1] for _, _, cost_history in results]

plt.bar(range(len(names)), final_costs, color='salmon')
plt.xlabel('Hyperparameter Setting')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Hyperparameter Setting')
plt.xticks(range(len(names)), names, rotation=45, ha='right')
plt.grid(True, axis='y')

# Add annotations
for i, cost in enumerate(final_costs):
    plt.text(i, cost/2, f'{cost:.6f}', ha='center', va='center')

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([0, 0])  # Global minimum of Rastrigin function
for name, theta_history, cost_history in results:
    distances = np.sqrt(np.sum((theta_history - minimum)**2, axis=1))
    plt.semilogy(range(len(distances)), distances, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot final distance vs. hyperparameter setting
plt.subplot(2, 2, 4)
final_distances = [np.sqrt(np.sum((theta_history[-1] - minimum)**2)) 
                  for _, theta_history, _ in results]

plt.bar(range(len(names)), final_distances, color='skyblue')
plt.xlabel('Hyperparameter Setting')
plt.ylabel('Final Distance to Minimum')
plt.title('Final Distance vs. Hyperparameter Setting')
plt.xticks(range(len(names)), names, rotation=45, ha='right')
plt.grid(True, axis='y')

# Add annotations
for i, distance in enumerate(final_distances):
    plt.text(i, distance/2, f'{distance:.6f}', ha='center', va='center')

plt.tight_layout()
plt.show()
```

## Advantages of Lookahead

1. **Improved Stability**:
   - More stable training, especially for complex models
   - Less sensitive to learning rate choice
   - Smoother convergence behavior

2. **Better Generalization**:
   - Often results in models that generalize better to unseen data
   - Reduces overfitting in some cases

3. **Faster Convergence**:
   - Can converge faster than the base optimizer for certain problems
   - Particularly effective for non-convex optimization

4. **Versatility**:
   - Can be combined with any base optimizer (SGD, Adam, RMSProp, etc.)
   - Minimal computational overhead

## When to Use Lookahead

Lookahead is particularly beneficial in the following scenarios:

1. **Deep Neural Networks**: When training complex models where stability is a concern

2. **Non-convex Optimization**: For problems with many local minima

3. **Learning Rate Sensitivity**: When the model is sensitive to the choice of learning rate

4. **Noisy Gradients**: When dealing with noisy or stochastic gradients

## Summary

Lookahead is a meta-optimizer that can be combined with any base optimizer to improve stability and convergence. It works by maintaining two sets of weights (fast and slow) and using a "slow" update strategy to improve stability.

Key points about Lookahead:
- Maintains two sets of weights: fast weights (updated by the base optimizer) and slow weights (updated periodically)
- Slow weights are updated by moving toward the fast weights: $\phi_{t+1} = \phi_t + \alpha(\theta_t - \phi_t)$
- Can be combined with any base optimizer (SGD, Adam, RMSProp, etc.)
- Improves stability and convergence with minimal computational overhead
- Typically uses $\alpha = 0.5$ and $k = 5$ as default hyperparameters

Lookahead is a simple yet effective technique that can improve the performance of existing optimizers with minimal additional complexity.

## References

1. Zhang, M., Lucas, J., Ba, J., & Hinton, G. E. (2019). Lookahead Optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems.
2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
3. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. In International Conference on Learning Representations.
4. Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., & Duncan, J. (2020). AdaBelief Optimizer: Adapting stepsizes by the belief in observed gradients. In Advances in Neural Information Processing Systems.
