# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (6) - RAdam

## RAdam Optimizer

**RAdam** (Rectified Adam) is a variant of Adam that addresses the "warm-up" issue in adaptive learning rate methods. It was introduced in 2019 and has shown improved convergence and generalization performance compared to Adam.

### Motivation

Adam and other adaptive methods can suffer from poor convergence in the early stages of training due to the lack of reliable variance estimates. This often requires a "warm-up" phase with a lower learning rate. RAdam addresses this issue by introducing a term that rectifies the variance of the adaptive learning rate.

### The Warm-up Issue

In Adam, the adaptive learning rate is computed as:

$$\frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon}$$

In the early stages of training, the second moment estimate $\hat{v}_t$ can be unreliable, leading to excessively large or small step sizes. This can cause instability or slow convergence.

### Algorithm

RAdam modifies the Adam update rule by introducing a rectification term that adjusts the adaptive learning rate based on the variance of the gradients:

$$g_t = \nabla_\theta J(\theta_{t-1})$$
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

Then, instead of directly using the adaptive learning rate, RAdam computes a rectification term:

$$\rho_{\infty} = \frac{2}{1 - \beta_2} - 1$$
$$\rho_t = \rho_{\infty} - \frac{2t\beta_2^t}{1 - \beta_2^t}$$

If $\rho_t > 4$ (indicating sufficient variance information):

$$r_t = \sqrt{\frac{(\rho_t - 4)(\rho_t - 2)\rho_{\infty}}{(\rho_{\infty} - 4)(\rho_{\infty} - 2)\rho_t}}$$
$$\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \cdot r_t$$

Otherwise (indicating insufficient variance information):

$$\theta_t = \theta_{t-1} - \alpha \hat{m}_t$$

This effectively implements an automatic warm-up schedule based on the variance of the gradients.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add bias term to X
X_train_b = np.c_[np.ones(X_train.shape[0]), X_train]
X_test_b = np.c_[np.ones(X_test.shape[0]), X_test]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Implement Adam optimizer
def adam(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
         beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Implement RAdam optimizer
def radam(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
         beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform RAdam (Rectified Adam) optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    # Compute rho_infinity
    rho_inf = 2 / (1 - beta2) - 1
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Compute rho_t
        rho_t = rho_inf - 2 * t * beta2**t / (1 - beta2**t)
        
        # Compute rectification term
        if rho_t > 4:
            # Variance is reliable, use adaptive learning rate with rectification
            r_t = np.sqrt(((rho_t - 4) * (rho_t - 2) * rho_inf) / 
                         ((rho_inf - 4) * (rho_inf - 2) * rho_t))
            
            # Update parameters with rectified adaptive learning rate
            theta = theta - learning_rate * r_t * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        else:
            # Variance is not reliable, use SGD with momentum
            theta = theta - learning_rate * m_t_hat
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Compare Adam and RAdam
learning_rate = 0.01
num_iterations = 500

# Run Adam
theta_adam, train_cost_adam, test_cost_adam = adam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Run RAdam
theta_radam, train_cost_radam, test_cost_radam = radam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot training cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(train_cost_adam)), train_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(train_cost_radam)), train_cost_radam, linewidth=2, label='RAdam')
plt.xlabel('Iteration')
plt.ylabel('Training Cost')
plt.title('Training Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot test cost over iterations
plt.subplot(2, 2, 2)
plt.plot(range(len(test_cost_adam)), test_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(test_cost_radam)), test_cost_radam, linewidth=2, label='RAdam')
plt.xlabel('Iteration')
plt.ylabel('Test Cost')
plt.title('Test Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot parameter L2 norm over iterations
plt.subplot(2, 2, 3)
adam_norms = [np.linalg.norm(theta[1:]) for theta in theta_adam]  # Exclude bias term
radam_norms = [np.linalg.norm(theta[1:]) for theta in theta_radam]  # Exclude bias term

plt.plot(range(len(adam_norms)), adam_norms, linewidth=2, label='Adam')
plt.plot(range(len(radam_norms)), radam_norms, linewidth=2, label='RAdam')
plt.xlabel('Iteration')
plt.ylabel('Parameter L2 Norm (excluding bias)')
plt.title('Parameter L2 Norm vs. Iteration')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
adam_updates = np.diff(theta_adam, axis=0)
radam_updates = np.diff(theta_radam, axis=0)

adam_update_norms = np.linalg.norm(adam_updates, axis=1)
radam_update_norms = np.linalg.norm(radam_updates, axis=1)

plt.semilogy(range(len(adam_update_norms)), adam_update_norms, linewidth=2, label='Adam')
plt.semilogy(range(len(radam_update_norms)), radam_update_norms, linewidth=2, label='RAdam')
plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### RAdam on Non-Convex Functions

Let's see how RAdam performs on a challenging non-convex function compared to Adam:

```python
# Define a challenging non-convex function
def rastrigin_function(theta):
    """
    Rastrigin function: f(x,y) = 20 + x² + y² - 10(cos(2πx) + cos(2πy))
    Global minimum at (0,0)
    """
    A = 10
    n = len(theta)
    return A * n + np.sum(theta**2 - A * np.cos(2 * np.pi * theta))

def rastrigin_gradient(theta):
    """Gradient of the Rastrigin function."""
    A = 10
    return 2 * theta + 2 * np.pi * A * np.sin(2 * np.pi * theta)

# Implement generic Adam optimizer
def adam_optimize(func, grad_func, start_point, learning_rate=0.001, num_iterations=1000, 
                 beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    # Initialize moment estimates
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    
    for t in range(1, num_iterations + 1):
        gradient = grad_func(theta)
        
        # Update biased first moment estimate
        m = beta1 * m + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v = beta2 * v + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_hat = m / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_hat = v / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Implement RAdam optimizer
def radam_optimize(func, grad_func, start_point, learning_rate=0.001, num_iterations=1000, 
                  beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform RAdam optimization.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    # Initialize moment estimates
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    
    # Compute rho_infinity
    rho_inf = 2 / (1 - beta2) - 1
    
    for t in range(1, num_iterations + 1):
        gradient = grad_func(theta)
        
        # Update biased first moment estimate
        m = beta1 * m + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v = beta2 * v + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_hat = m / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_hat = v / (1 - beta2**t)
        
        # Compute rho_t
        rho_t = rho_inf - 2 * t * beta2**t / (1 - beta2**t)
        
        # Compute rectification term
        if rho_t > 4:
            # Variance is reliable, use adaptive learning rate with rectification
            r_t = np.sqrt(((rho_t - 4) * (rho_t - 2) * rho_inf) / 
                         ((rho_inf - 4) * (rho_inf - 2) * rho_t))
            
            # Update parameters with rectified adaptive learning rate
            theta = theta - learning_rate * r_t * m_hat / (np.sqrt(v_hat) + epsilon)
        else:
            # Variance is not reliable, use SGD with momentum
            theta = theta - learning_rate * m_hat
        
        theta_history.append(theta.copy())
        cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Run optimizers on the Rastrigin function
start_point_rastrigin = np.array([2.0, 3.0])
num_iterations = 1000
learning_rate = 0.001

# Run Adam
theta_adam, cost_adam = adam_optimize(
    rastrigin_function, rastrigin_gradient, start_point_rastrigin,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Run RAdam
theta_radam, cost_radam = radam_optimize(
    rastrigin_function, rastrigin_gradient, start_point_rastrigin,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Visualize the results for Rastrigin function
plt.figure(figsize=(15, 10))

# Create a grid for contour plot
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(len(x)):
    for j in range(len(y)):
        Z[j, i] = rastrigin_function(np.array([X[j, i], Y[j, i]]))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(X, Y, Z, levels=np.linspace(0, 50, 10), cmap='viridis')
plt.colorbar(contour)

plt.plot(theta_adam[:, 0], theta_adam[:, 1], 'o-', linewidth=1, markersize=2, label='Adam')
plt.plot(theta_radam[:, 0], theta_radam[:, 1], 'o-', linewidth=1, markersize=2, label='RAdam')

plt.scatter(0, 0, color='red', s=100, label='Global Minimum')
plt.scatter(start_point_rastrigin[0], start_point_rastrigin[1], color='green', s=100, label='Start')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Trajectories on Rastrigin Function')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_adam)), cost_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(cost_radam)), cost_radam, linewidth=2, label='RAdam')

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([0, 0])  # Global minimum of Rastrigin function
distances_adam = np.sqrt(np.sum((theta_adam - minimum)**2, axis=1))
distances_radam = np.sqrt(np.sum((theta_radam - minimum)**2, axis=1))

plt.semilogy(range(len(distances_adam)), distances_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(distances_radam)), distances_radam, linewidth=2, label='RAdam')

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
updates_adam = np.diff(theta_adam, axis=0)
updates_radam = np.diff(theta_radam, axis=0)

update_norms_adam = np.linalg.norm(updates_adam, axis=1)
update_norms_radam = np.linalg.norm(updates_radam, axis=1)

plt.semilogy(range(len(update_norms_adam)), update_norms_adam, linewidth=2, label='Adam')
plt.semilogy(range(len(update_norms_radam)), update_norms_radam, linewidth=2, label='RAdam')

plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Visualizing the Rectification Term

Let's visualize how the rectification term in RAdam changes over time:

```python
# Calculate the rectification term for different iterations
iterations = np.arange(1, 1001)
beta2 = 0.999
rho_inf = 2 / (1 - beta2) - 1
rho_t = np.array([rho_inf - 2 * t * beta2**t / (1 - beta2**t) for t in iterations])

# Calculate the rectification term
r_t = np.zeros_like(rho_t)
for i, rho in enumerate(rho_t):
    if rho > 4:
        r_t[i] = np.sqrt(((rho - 4) * (rho - 2) * rho_inf) / 
                         ((rho_inf - 4) * (rho_inf - 2) * rho))
    else:
        r_t[i] = 0  # Not using adaptive learning rate

# Find the transition point
transition_point = np.argmax(r_t > 0)

# Visualize the rectification term
plt.figure(figsize=(12, 6))
plt.plot(iterations, r_t, linewidth=2)
plt.axvline(x=transition_point, color='r', linestyle='--', 
            label=f'Transition at iteration {transition_point}')
plt.xlabel('Iteration')
plt.ylabel('Rectification Term')
plt.title('RAdam Rectification Term vs. Iteration')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualize the effective learning rate
plt.figure(figsize=(12, 6))
plt.plot(iterations, np.ones_like(iterations) * learning_rate, 'k--', 
         label='Base Learning Rate')
plt.plot(iterations, np.where(r_t > 0, learning_rate * r_t, learning_rate), linewidth=2, 
         label='Effective Learning Rate')
plt.axvline(x=transition_point, color='r', linestyle='--', 
            label=f'Transition at iteration {transition_point}')
plt.xlabel('Iteration')
plt.ylabel('Effective Learning Rate')
plt.title('RAdam Effective Learning Rate vs. Iteration')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

## Advantages of RAdam

1. **Automatic Warm-up**:
   - Eliminates the need for a manual warm-up schedule
   - Automatically adjusts the learning rate based on the variance of the gradients
   - Smoother convergence in the early stages of training

2. **Improved Stability**:
   - More stable training, especially in the early stages
   - Less sensitive to learning rate choice
   - Reduces the risk of divergence

3. **Better Generalization**:
   - Often results in models that generalize better to unseen data
   - Reduces overfitting in some cases

4. **Faster Convergence**:
   - Can converge faster than Adam for certain problems
   - Particularly effective for deep neural networks

## When to Use RAdam

RAdam is particularly beneficial in the following scenarios:

1. **Deep Neural Networks**: When training complex models where stability is a concern

2. **Training from Scratch**: When training models from random initialization

3. **Learning Rate Sensitivity**: When the model is sensitive to the choice of learning rate

4. **Avoiding Manual Warm-up**: When you want to avoid the complexity of implementing a manual warm-up schedule

## Summary

RAdam (Rectified Adam) is a variant of Adam that addresses the "warm-up" issue in adaptive learning rate methods. It works by introducing a rectification term that adjusts the adaptive learning rate based on the variance of the gradients.

Key points about RAdam:
- Introduces a rectification term that adjusts the adaptive learning rate
- Automatically implements a warm-up schedule based on the variance of the gradients
- Uses SGD with momentum when the variance estimate is unreliable
- Uses adaptive learning rate with rectification when the variance estimate is reliable
- Improves stability and convergence, especially in the early stages of training

RAdam is a simple yet effective modification of Adam that can improve performance without requiring manual warm-up schedules.

## References

1. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., & Han, J. (2019). On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265.
2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
3. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. In International Conference on Learning Representations.
4. Zhang, M., Lucas, J., Ba, J., & Hinton, G. E. (2019). Lookahead Optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems.
