# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (2) - Adam

## Adam Optimizer

**Adam** (Adaptive Moment Estimation) combines the benefits of both momentum and adaptive learning rates. It maintains both a first-order moment estimate (momentum) and a second-order moment estimate (similar to RMSProp):

$$g_t = \nabla_\theta J(\theta_{t-1})$$
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad \text{(first moment estimate)}$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \quad \text{(second moment estimate)}$$

To correct the bias in the moment estimates (which are initialized to zero), Adam uses bias-corrected estimates:

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

The parameter update rule is then:

$$\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

where:
- $\beta_1$ is the exponential decay rate for the first moment (typically 0.9)
- $\beta_2$ is the exponential decay rate for the second moment (typically 0.999)
- $\epsilon$ is a small constant to avoid division by zero (typically 1e-8)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Add bias term to X
X_b = np.c_[np.ones(X.shape[0]), X]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Implement Adam optimizer
def adam(X, y, learning_rate=0.001, num_iterations=1000, beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run Adam and compare with other optimizers
def gradient_descent(X, y, learning_rate, num_iterations):
    """Standard gradient descent."""
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        theta = theta - learning_rate * gradient
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

def rmsprop(X, y, learning_rate, num_iterations, beta=0.9, epsilon=1e-8):
    """RMSProp optimizer."""
    m, n = X.shape
    theta = np.zeros(n)
    v = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        v = beta * v + (1 - beta) * gradient**2
        theta = theta - learning_rate * gradient / (np.sqrt(v) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run the optimizers
num_iterations = 100
learning_rate_gd = 0.1
learning_rate_rmsprop = 0.01
learning_rate_adam = 0.01

theta_gd, cost_gd = gradient_descent(X_b, y, learning_rate_gd, num_iterations)
theta_rmsprop, cost_rmsprop = rmsprop(X_b, y, learning_rate_rmsprop, num_iterations)
theta_adam, cost_adam = adam(X_b, y, learning_rate_adam, num_iterations)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(cost_gd)), cost_gd, linewidth=2, label=f'GD (lr={learning_rate_gd})')
plt.plot(range(len(cost_rmsprop)), cost_rmsprop, linewidth=2, label=f'RMSProp (lr={learning_rate_rmsprop})')
plt.plot(range(len(cost_adam)), cost_adam, linewidth=2, label=f'Adam (lr={learning_rate_adam})')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_gd)), cost_gd, linewidth=2, label=f'GD (lr={learning_rate_gd})')
plt.semilogy(range(len(cost_rmsprop)), cost_rmsprop, linewidth=2, label=f'RMSProp (lr={learning_rate_rmsprop})')
plt.semilogy(range(len(cost_adam)), cost_adam, linewidth=2, label=f'Adam (lr={learning_rate_adam})')
plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter trajectories
plt.subplot(2, 2, 3)
plt.plot(theta_gd[:, 0], theta_gd[:, 1], 'o-', linewidth=2, markersize=4, label='GD')
plt.plot(theta_rmsprop[:, 0], theta_rmsprop[:, 1], 'o-', linewidth=2, markersize=4, label='RMSProp')
plt.plot(theta_adam[:, 0], theta_adam[:, 1], 'o-', linewidth=2, markersize=4, label='Adam')
plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

# Plot final cost vs. optimizer
plt.subplot(2, 2, 4)
optimizers = ['GD', 'RMSProp', 'Adam']
final_costs = [cost_gd[-1], cost_rmsprop[-1], cost_adam[-1]]
plt.bar(optimizers, final_costs, color=['skyblue', 'lightgreen', 'salmon'])
plt.xlabel('Optimizer')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Optimizer')
plt.grid(True, axis='y')

# Add annotations
for i, cost in enumerate(final_costs):
    plt.text(i, cost/2, f'{cost:.6f}', ha='center', va='center')

plt.tight_layout()
plt.show()
```

### Adam on Non-Convex Functions

Let's see how Adam performs on a challenging non-convex function compared to other optimizers:

```python
# Define a challenging non-convex function
def rastrigin_function(theta):
    """
    Rastrigin function: f(x,y) = 20 + x² + y² - 10(cos(2πx) + cos(2πy))
    Global minimum at (0,0)
    """
    A = 10
    n = len(theta)
    return A * n + np.sum(theta**2 - A * np.cos(2 * np.pi * theta))

def rastrigin_gradient(theta):
    """Gradient of the Rastrigin function."""
    A = 10
    return 2 * theta + 2 * np.pi * A * np.sin(2 * np.pi * theta)

# Implement generic optimizer function
def optimize(func, grad_func, start_point, optimizer, **kwargs):
    """
    Perform optimization using the specified optimizer.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - optimizer: Optimization algorithm ('gd', 'rmsprop', or 'adam')
    - **kwargs: Additional parameters for the optimizer
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    learning_rate = kwargs.get('learning_rate', 0.01)
    num_iterations = kwargs.get('num_iterations', 1000)
    epsilon = kwargs.get('epsilon', 1e-8)
    
    if optimizer == 'gd':
        # Standard Gradient Descent
        for i in range(num_iterations):
            gradient = grad_func(theta)
            theta = theta - learning_rate * gradient
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    elif optimizer == 'rmsprop':
        # RMSProp
        beta = kwargs.get('beta', 0.9)
        v = np.zeros_like(theta)
        
        for i in range(num_iterations):
            gradient = grad_func(theta)
            v = beta * v + (1 - beta) * gradient**2
            theta = theta - learning_rate * gradient / (np.sqrt(v) + epsilon)
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    elif optimizer == 'adam':
        # Adam
        beta1 = kwargs.get('beta1', 0.9)
        beta2 = kwargs.get('beta2', 0.999)
        m = np.zeros_like(theta)
        v = np.zeros_like(theta)
        
        for t in range(1, num_iterations + 1):
            gradient = grad_func(theta)
            
            # Update biased first moment estimate
            m = beta1 * m + (1 - beta1) * gradient
            
            # Update biased second moment estimate
            v = beta2 * v + (1 - beta2) * gradient**2
            
            # Compute bias-corrected first moment estimate
            m_hat = m / (1 - beta1**t)
            
            # Compute bias-corrected second moment estimate
            v_hat = v / (1 - beta2**t)
            
            # Update parameters
            theta = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
            
            theta_history.append(theta.copy())
            cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Run optimizers on the Rastrigin function
start_point_rastrigin = np.array([2.0, 3.0])
num_iterations = 1000
learning_rate_gd = 0.001
learning_rate_rmsprop = 0.001
learning_rate_adam = 0.001

results_rastrigin = []
results_rastrigin.append(('GD', *optimize(rastrigin_function, rastrigin_gradient, 
                                         start_point_rastrigin, 'gd', 
                                         learning_rate=learning_rate_gd, 
                                         num_iterations=num_iterations)))
results_rastrigin.append(('RMSProp', *optimize(rastrigin_function, rastrigin_gradient, 
                                              start_point_rastrigin, 'rmsprop', 
                                              learning_rate=learning_rate_rmsprop, 
                                              num_iterations=num_iterations)))
results_rastrigin.append(('Adam', *optimize(rastrigin_function, rastrigin_gradient, 
                                           start_point_rastrigin, 'adam', 
                                           learning_rate=learning_rate_adam, 
                                           num_iterations=num_iterations)))

# Visualize the results for Rastrigin function
plt.figure(figsize=(15, 10))

# Create a grid for contour plot
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(len(x)):
    for j in range(len(y)):
        Z[j, i] = rastrigin_function(np.array([X[j, i], Y[j, i]]))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(X, Y, Z, levels=np.linspace(0, 50, 10), cmap='viridis')
plt.colorbar(contour)

for name, theta_history, cost_history in results_rastrigin:
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'o-', linewidth=1, markersize=2, label=name)

plt.scatter(0, 0, color='red', s=100, label='Global Minimum')
plt.scatter(start_point_rastrigin[0], start_point_rastrigin[1], color='green', s=100, label='Start')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Trajectories on Rastrigin Function')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for name, theta_history, cost_history in results_rastrigin:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([0, 0])  # Global minimum of Rastrigin function
for name, theta_history, cost_history in results_rastrigin:
    distances = np.sqrt(np.sum((theta_history - minimum)**2, axis=1))
    plt.semilogy(range(len(distances)), distances, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot final distance vs. optimizer
plt.subplot(2, 2, 4)
names = [name for name, _, _ in results_rastrigin]
final_distances = [np.sqrt(np.sum((theta_history[-1] - minimum)**2)) 
                  for _, theta_history, _ in results_rastrigin]
final_costs = [cost_history[-1] for _, _, cost_history in results_rastrigin]

plt.bar(names, final_distances, color=['skyblue', 'lightgreen', 'salmon'])
plt.xlabel('Optimizer')
plt.ylabel('Final Distance to Minimum')
plt.title('Final Distance vs. Optimizer')
plt.grid(True, axis='y')

# Add annotations
for i, distance in enumerate(final_distances):
    plt.text(i, distance/2, f'{distance:.6f}', ha='center', va='center')

plt.tight_layout()
plt.show()
```

### Effect of Hyperparameters

Let's examine how the hyperparameters of Adam affect its performance:

```python
# Run Adam with different hyperparameter settings
beta1_values = [0.9, 0.99]
beta2_values = [0.9, 0.999]
learning_rates = [0.001, 0.01]

results = []
for beta1 in beta1_values:
    for beta2 in beta2_values:
        for lr in learning_rates:
            theta_history, cost_history = adam(
                X_b, y, learning_rate=lr, num_iterations=100, beta1=beta1, beta2=beta2)
            results.append((f'β₁={beta1}, β₂={beta2}, lr={lr}', theta_history, cost_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 1)
for name, theta_history, cost_history in results:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Effect of Hyperparameters on Adam')
plt.legend()
plt.grid(True)

# Plot final cost vs. hyperparameter setting
plt.subplot(2, 2, 2)
names = [name for name, _, _ in results]
final_costs = [cost_history[-1] for _, _, cost_history in results]

plt.bar(range(len(names)), final_costs, color='salmon')
plt.xlabel('Hyperparameter Setting')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Hyperparameter Setting')
plt.xticks(range(len(names)), names, rotation=45, ha='right')
plt.grid(True, axis='y')

# Add annotations
for i, cost in enumerate(final_costs):
    plt.text(i, cost/2, f'{cost:.6f}', ha='center', va='center')

# Plot parameter trajectories
plt.subplot(2, 2, 3)
for name, theta_history, cost_history in results:
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'o-', linewidth=1, markersize=2, label=name)

plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Advantages and Disadvantages of Adam

### Advantages

1. **Combines the benefits of momentum and adaptive learning rates**:
   - Momentum helps accelerate convergence and reduce oscillations
   - Adaptive learning rates help handle different parameter scales and sparse gradients

2. **Bias correction** ensures unbiased estimates of the first and second moments, improving performance in the early iterations

3. **Works well with large datasets and high-dimensional parameter spaces**

4. **Robust to hyperparameter choices**: Default values of $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 10^{-8}$ work well for most problems

5. **Effective for non-convex optimization**: Performs well on challenging functions with multiple local minima

### Disadvantages

1. **May not converge to the exact minimum** for some convex problems

2. **Can exhibit poor generalization** in some cases, especially with large batch sizes

3. **Requires more memory** than simpler optimizers like SGD

4. **Computationally more expensive** per iteration than simpler methods

## Summary

Adam is one of the most popular optimization algorithms in deep learning due to its effectiveness and robustness. It combines the benefits of momentum (which helps accelerate convergence) and adaptive learning rates (which help handle different parameter scales).

Key features of Adam:
- Maintains both first-order (momentum) and second-order (adaptive learning rate) moment estimates
- Uses bias correction to improve performance in early iterations
- Works well with default hyperparameter values
- Effective for a wide range of problems, especially non-convex optimization

Adam is often the optimizer of choice for deep learning applications, but it's always good practice to try different optimizers and compare their performance for your specific problem.

## References

1. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Reddi, S. J., Kale, S., & Kumar, S. (2018). On the convergence of Adam and beyond. In International Conference on Learning Representations.
