# 3.2.4 Generalization - Part 2: Regularization

## Introduction to Regularization

**Regularization** refers to techniques that constrain or penalize a model's complexity to prevent overfitting and improve generalization. These techniques modify the learning process to produce models that generalize better to unseen data, even if they might perform slightly worse on the training data.

### Why Regularization Works

Regularization works by:

1. **Reducing Model Complexity**: Constraining the model's capacity to prevent it from memorizing the training data
2. **Introducing Inductive Bias**: Adding assumptions about what makes a good model
3. **Smoothing Decision Boundaries**: Creating simpler, more generalizable decision boundaries
4. **Reducing Sensitivity to Noise**: Making the model less likely to fit random fluctuations in the data

### L1 Regularization (Lasso)

**L1 regularization** adds the sum of the absolute values of the model parameters to the loss function:

$$L_{regularized}(\theta) = L(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|$$

where:
- $L(\theta)$ is the original loss function
- $\lambda$ is the regularization strength
- $\theta_i$ are the model parameters

**Key Properties**:
- Encourages sparse solutions (many parameters become exactly zero)
- Performs feature selection implicitly
- Useful when you suspect many features are irrelevant

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Generate synthetic data with many features
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=50, n_informative=10, noise=0.5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Lasso models with different regularization strengths
alphas = [0.001, 0.01, 0.1, 1.0, 10.0]
train_errors = []
test_errors = []
coefs = []

for alpha in alphas:
    # Create and train the model
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_train_scaled, y_train)
    
    # Calculate errors
    y_train_pred = lasso.predict(X_train_scaled)
    y_test_pred = lasso.predict(X_test_scaled)
    
    train_error = mean_squared_error(y_train, y_train_pred)
    test_error = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_error)
    test_errors.append(test_error)
    coefs.append(lasso.coef_)
    
    # Count non-zero coefficients
    n_nonzero = np.sum(lasso.coef_ != 0)
    
    print(f"Alpha = {alpha}:")
    print(f"  Training MSE: {train_error:.2f}")
    print(f"  Test MSE: {test_error:.2f}")
    print(f"  Non-zero coefficients: {n_nonzero}/{len(lasso.coef_)}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot errors vs. regularization strength
plt.subplot(2, 2, 1)
plt.semilogx(alphas, train_errors, 'bo-', linewidth=2, label='Training Error')
plt.semilogx(alphas, test_errors, 'ro-', linewidth=2, label='Test Error')
plt.title('Error vs. Regularization Strength (L1)')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot number of non-zero coefficients
plt.subplot(2, 2, 2)
n_nonzeros = [np.sum(c != 0) for c in coefs]
plt.semilogx(alphas, n_nonzeros, 'go-', linewidth=2)
plt.title('Feature Selection with L1 Regularization')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Number of Non-zero Coefficients')
plt.grid(True, alpha=0.3)

# Plot coefficients for different alphas
plt.subplot(2, 2, 3)
for i, alpha in enumerate(alphas):
    plt.plot(coefs[i], 'o', alpha=0.7, label=f'Alpha = {alpha}')
plt.title('Coefficient Values for Different Alphas')
plt.xlabel('Coefficient Index')
plt.ylabel('Coefficient Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot coefficient paths
plt.subplot(2, 2, 4)
coef_paths = np.array(coefs).T  # Transpose to get one row per coefficient
for i in range(coef_paths.shape[0]):
    plt.semilogx(alphas, coef_paths[i, :], '-', alpha=0.5)
plt.title('Coefficient Paths')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficient Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### L2 Regularization (Ridge)

**L2 regularization** adds the sum of squared values of the model parameters to the loss function:

$$L_{regularized}(\theta) = L(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2$$

**Key Properties**:
- Shrinks all parameters toward zero, but rarely makes them exactly zero
- Handles correlated features well
- Provides more stable solutions than L1 regularization
- Useful for dealing with multicollinearity

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Generate synthetic data with many features
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=50, n_informative=10, noise=0.5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Ridge models with different regularization strengths
alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_errors = []
test_errors = []
coefs = []

for alpha in alphas:
    # Create and train the model
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    
    # Calculate errors
    y_train_pred = ridge.predict(X_train_scaled)
    y_test_pred = ridge.predict(X_test_scaled)
    
    train_error = mean_squared_error(y_train, y_train_pred)
    test_error = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_error)
    test_errors.append(test_error)
    coefs.append(ridge.coef_)
    
    # Calculate coefficient norm
    coef_norm = np.linalg.norm(ridge.coef_)
    
    print(f"Alpha = {alpha}:")
    print(f"  Training MSE: {train_error:.2f}")
    print(f"  Test MSE: {test_error:.2f}")
    print(f"  Coefficient L2 norm: {coef_norm:.2f}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot errors vs. regularization strength
plt.subplot(2, 2, 1)
plt.semilogx(alphas, train_errors, 'bo-', linewidth=2, label='Training Error')
plt.semilogx(alphas, test_errors, 'ro-', linewidth=2, label='Test Error')
plt.title('Error vs. Regularization Strength (L2)')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot coefficient L2 norm
plt.subplot(2, 2, 2)
coef_norms = [np.linalg.norm(c) for c in coefs]
plt.semilogx(alphas, coef_norms, 'go-', linewidth=2)
plt.title('Coefficient Shrinkage with L2 Regularization')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficient L2 Norm')
plt.grid(True, alpha=0.3)

# Plot coefficients for different alphas
plt.subplot(2, 2, 3)
for i, alpha in enumerate(alphas):
    plt.plot(coefs[i], 'o', alpha=0.7, label=f'Alpha = {alpha}')
plt.title('Coefficient Values for Different Alphas')
plt.xlabel('Coefficient Index')
plt.ylabel('Coefficient Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot coefficient paths
plt.subplot(2, 2, 4)
coef_paths = np.array(coefs).T  # Transpose to get one row per coefficient
for i in range(coef_paths.shape[0]):
    plt.semilogx(alphas, coef_paths[i, :], '-', alpha=0.5)
plt.title('Coefficient Paths')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficient Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Elastic Net

**Elastic Net** combines L1 and L2 regularization:

$$L_{regularized}(\theta) = L(\theta) + \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2$$

or with a mixing parameter $\alpha$:

$$L_{regularized}(\theta) = L(\theta) + \lambda \left( \alpha \sum_{i=1}^{n} |\theta_i| + (1 - \alpha) \sum_{i=1}^{n} \theta_i^2 \right)$$

**Key Properties**:
- Combines the benefits of L1 and L2 regularization
- Can select variables like Lasso and handle correlated features like Ridge
- Particularly useful when the number of features is much larger than the number of samples

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Generate synthetic data with many features
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=50, n_informative=10, noise=0.5, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Elastic Net models with different mixing parameters
alpha = 0.1  # Fixed regularization strength
l1_ratios = [0.0, 0.2, 0.5, 0.8, 1.0]  # 0.0 = Ridge, 1.0 = Lasso
train_errors = []
test_errors = []
coefs = []
n_nonzeros = []

for l1_ratio in l1_ratios:
    # Create and train the model
    elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)
    elastic_net.fit(X_train_scaled, y_train)
    
    # Calculate errors
    y_train_pred = elastic_net.predict(X_train_scaled)
    y_test_pred = elastic_net.predict(X_test_scaled)
    
    train_error = mean_squared_error(y_train, y_train_pred)
    test_error = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_error)
    test_errors.append(test_error)
    coefs.append(elastic_net.coef_)
    n_nonzeros.append(np.sum(elastic_net.coef_ != 0))
    
    print(f"L1 Ratio = {l1_ratio} (0=Ridge, 1=Lasso):")
    print(f"  Training MSE: {train_error:.2f}")
    print(f"  Test MSE: {test_error:.2f}")
    print(f"  Non-zero coefficients: {n_nonzeros[-1]}/{len(elastic_net.coef_)}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot errors vs. L1 ratio
plt.subplot(2, 2, 1)
plt.plot(l1_ratios, train_errors, 'bo-', linewidth=2, label='Training Error')
plt.plot(l1_ratios, test_errors, 'ro-', linewidth=2, label='Test Error')
plt.title('Error vs. L1 Ratio (Elastic Net)')
plt.xlabel('L1 Ratio (0=Ridge, 1=Lasso)')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot number of non-zero coefficients
plt.subplot(2, 2, 2)
plt.plot(l1_ratios, n_nonzeros, 'go-', linewidth=2)
plt.title('Feature Selection with Elastic Net')
plt.xlabel('L1 Ratio (0=Ridge, 1=Lasso)')
plt.ylabel('Number of Non-zero Coefficients')
plt.grid(True, alpha=0.3)

# Plot coefficients for different L1 ratios
plt.subplot(2, 2, 3)
for i, l1_ratio in enumerate(l1_ratios):
    plt.plot(coefs[i], 'o', alpha=0.7, label=f'L1 Ratio = {l1_ratio}')
plt.title('Coefficient Values for Different L1 Ratios')
plt.xlabel('Coefficient Index')
plt.ylabel('Coefficient Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot coefficient paths
plt.subplot(2, 2, 4)
coef_paths = np.array(coefs).T  # Transpose to get one row per coefficient
for i in range(coef_paths.shape[0]):
    plt.plot(l1_ratios, coef_paths[i, :], '-', alpha=0.5)
plt.title('Coefficient Paths')
plt.xlabel('L1 Ratio (0=Ridge, 1=Lasso)')
plt.ylabel('Coefficient Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Early Stopping

**Early stopping** involves stopping the training process before the model converges to prevent overfitting:

1. Monitor the validation error during training
2. Stop training when the validation error starts to increase
3. Return the model parameters that achieved the lowest validation error

**Key Properties**:
- Acts as an implicit regularizer
- Particularly useful for iterative algorithms like gradient descent
- Requires a validation set to monitor performance
- Simple to implement and computationally efficient

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=200, n_features=20, n_informative=10, noise=0.5, random_state=42)

# Split data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train a neural network with early stopping
max_epochs = 1000
train_errors = []
val_errors = []
test_errors = []
best_val_error = float('inf')
best_epoch = 0
best_model = None

# Create a model that we can train manually
model = MLPRegressor(
    hidden_layer_sizes=(100,),
    activation='relu',
    solver='sgd',
    learning_rate_init=0.01,
    max_iter=1,  # Train for one epoch at a time
    warm_start=True,  # Continue training from where we left off
    random_state=42
)

for epoch in range(max_epochs):
    # Train for one epoch
    model.fit(X_train_scaled, y_train)
    
    # Calculate errors
    y_train_pred = model.predict(X_train_scaled)
    y_val_pred = model.predict(X_val_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    train_error = mean_squared_error(y_train, y_train_pred)
    val_error = mean_squared_error(y_val, y_val_pred)
    test_error = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_error)
    val_errors.append(val_error)
    test_errors.append(test_error)
    
    # Check if this is the best model so far
    if val_error < best_val_error:
        best_val_error = val_error
        best_epoch = epoch
        best_model = model
        
        # Print progress every 100 epochs or when we find a better model
        print(f"Epoch {epoch}: New best model!")
        print(f"  Training MSE: {train_error:.2f}")
        print(f"  Validation MSE: {val_error:.2f}")
        print(f"  Test MSE: {test_error:.2f}")
    
    # Early stopping condition: Stop if no improvement for 50 epochs
    if epoch - best_epoch >= 50:
        print(f"Early stopping at epoch {epoch}")
        print(f"Best epoch was {best_epoch} with validation MSE {best_val_error:.2f}")
        break

# Visualize the results
plt.figure(figsize=(12, 6))

# Plot errors vs. epochs
plt.plot(train_errors, 'b-', linewidth=2, label='Training Error')
plt.plot(val_errors, 'r-', linewidth=2, label='Validation Error')
plt.plot(test_errors, 'g-', linewidth=2, label='Test Error')
plt.axvline(x=best_epoch, color='k', linestyle='--', label=f'Best Epoch ({best_epoch})')
plt.title('Learning Curves with Early Stopping')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Dropout

**Dropout** is a regularization technique primarily used in neural networks:

1. During training, randomly set a fraction of the neurons to zero
2. During inference, use all neurons but scale their outputs
3. This prevents co-adaptation of neurons and creates an implicit ensemble

**Key Properties**:
- Simulates training an ensemble of neural networks
- Reduces overfitting by preventing complex co-adaptations
- Particularly effective for deep neural networks
- Computationally efficient compared to actual ensembles

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, n_classes=2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create models with and without dropout
def create_model(dropout_rate=0.0):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Create models
model_no_dropout = create_model(dropout_rate=0.0)
model_with_dropout = create_model(dropout_rate=0.5)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# Train models
history_no_dropout = model_no_dropout.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=0
)

history_with_dropout = model_with_dropout.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=0
)

# Evaluate models
_, acc_no_dropout = model_no_dropout.evaluate(X_test_scaled, y_test, verbose=0)
_, acc_with_dropout = model_with_dropout.evaluate(X_test_scaled, y_test, verbose=0)

print(f"Test accuracy without dropout: {acc_no_dropout:.4f}")
print(f"Test accuracy with dropout: {acc_with_dropout:.4f}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot training and validation loss
plt.subplot(2, 2, 1)
plt.plot(history_no_dropout.history['loss'], 'b-', linewidth=2, label='Training Loss (No Dropout)')
plt.plot(history_no_dropout.history['val_loss'], 'b--', linewidth=2, label='Validation Loss (No Dropout)')
plt.plot(history_with_dropout.history['loss'], 'r-', linewidth=2, label='Training Loss (With Dropout)')
plt.plot(history_with_dropout.history['val_loss'], 'r--', linewidth=2, label='Validation Loss (With Dropout)')
plt.title('Loss Curves')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot training and validation accuracy
plt.subplot(2, 2, 2)
plt.plot(history_no_dropout.history['accuracy'], 'b-', linewidth=2, label='Training Accuracy (No Dropout)')
plt.plot(history_no_dropout.history['val_accuracy'], 'b--', linewidth=2, label='Validation Accuracy (No Dropout)')
plt.plot(history_with_dropout.history['accuracy'], 'r-', linewidth=2, label='Training Accuracy (With Dropout)')
plt.plot(history_with_dropout.history['val_accuracy'], 'r--', linewidth=2, label='Validation Accuracy (With Dropout)')
plt.title('Accuracy Curves')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the gap between training and validation accuracy
plt.subplot(2, 2, 3)
gap_no_dropout = np.array(history_no_dropout.history['accuracy']) - np.array(history_no_dropout.history['val_accuracy'])
gap_with_dropout = np.array(history_with_dropout.history['accuracy']) - np.array(history_with_dropout.history['val_accuracy'])
plt.plot(gap_no_dropout, 'b-', linewidth=2, label='No Dropout')
plt.plot(gap_with_dropout, 'r-', linewidth=2, label='With Dropout')
plt.title('Gap Between Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Gap (Training - Validation)')
plt.legend()
plt.grid(True, alpha=0.3)

# Bar plot of test accuracy
plt.subplot(2, 2, 4)
plt.bar(['No Dropout', 'With Dropout'], [acc_no_dropout, acc_with_dropout], color=['blue', 'red'])
plt.title('Test Accuracy')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

## Summary

Regularization techniques are essential tools for preventing overfitting and improving generalization:

1. **L1 Regularization (Lasso)**:
   - Adds the sum of absolute values of parameters to the loss function
   - Encourages sparse solutions and performs feature selection
   - Useful when many features are irrelevant

2. **L2 Regularization (Ridge)**:
   - Adds the sum of squared values of parameters to the loss function
   - Shrinks all parameters toward zero
   - Handles correlated features well

3. **Elastic Net**:
   - Combines L1 and L2 regularization
   - Balances feature selection and parameter shrinkage
   - Particularly useful for high-dimensional data

4. **Early Stopping**:
   - Stops training when validation error starts to increase
   - Acts as an implicit regularizer
   - Simple to implement and computationally efficient

5. **Dropout**:
   - Randomly sets neurons to zero during training
   - Simulates an ensemble of neural networks
   - Prevents complex co-adaptations

These techniques provide different ways to constrain model complexity and improve generalization. The choice of regularization technique depends on the specific problem, the type of model, and the characteristics of the data.

## References

1. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.
2. Hoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55-67.
3. Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.
4. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929-1958.
