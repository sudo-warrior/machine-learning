# 3.2.5 Model Evaluation - Part 5: Practical Considerations (2)

## Handling Imbalanced Data

Imbalanced datasets, where one class is much more frequent than others, present unique challenges for model evaluation. Standard metrics can be misleading, and special techniques are needed to properly evaluate models on such data.

### The Problem with Accuracy

**Accuracy** can be misleading for imbalanced datasets because a model that always predicts the majority class will achieve high accuracy without learning anything useful:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Generate imbalanced data with different imbalance ratios
np.random.seed(42)
imbalance_ratios = [0.5, 0.2, 0.1, 0.05, 0.01]  # Proportion of minority class
datasets = []

for ratio in imbalance_ratios:
    weights = [1 - ratio, ratio]  # Weights for majority and minority classes
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                              n_redundant=5, weights=weights, random_state=42)
    datasets.append((X, y, ratio))

# Compare accuracy of a real model vs. a dummy classifier
results = []

for X, y, ratio in datasets:
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train a logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Train a dummy classifier that always predicts the majority class
    dummy = DummyClassifier(strategy='most_frequent', random_state=42)
    dummy.fit(X_train, y_train)
    y_pred_dummy = dummy.predict(X_test)
    accuracy_dummy = accuracy_score(y_test, y_pred_dummy)
    
    # Calculate class distribution
    class_counts = np.bincount(y)
    majority_class_ratio = class_counts[0] / len(y)
    
    results.append({
        'ratio': ratio,
        'accuracy': accuracy,
        'accuracy_dummy': accuracy_dummy,
        'majority_class_ratio': majority_class_ratio
    })
    
    print(f"Imbalance Ratio (minority class): {ratio}")
    print(f"  Class distribution: {class_counts}")
    print(f"  Majority class ratio: {majority_class_ratio:.4f}")
    print(f"  Logistic Regression accuracy: {accuracy:.4f}")
    print(f"  Dummy Classifier accuracy: {accuracy_dummy:.4f}")
    print(f"  Difference: {accuracy - accuracy_dummy:.4f}")

# Visualize the results
plt.figure(figsize=(12, 6))

# Plot accuracy vs. imbalance ratio
plt.subplot(1, 2, 1)
ratios = [r['ratio'] for r in results]
accuracies = [r['accuracy'] for r in results]
dummy_accuracies = [r['accuracy_dummy'] for r in results]

plt.plot(ratios, accuracies, 'bo-', linewidth=2, label='Logistic Regression')
plt.plot(ratios, dummy_accuracies, 'ro-', linewidth=2, label='Dummy Classifier')
plt.title('Accuracy vs. Imbalance Ratio')
plt.xlabel('Minority Class Ratio')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot accuracy vs. majority class ratio
plt.subplot(1, 2, 2)
majority_ratios = [r['majority_class_ratio'] for r in results]
plt.plot(majority_ratios, accuracies, 'bo-', linewidth=2, label='Logistic Regression')
plt.plot(majority_ratios, dummy_accuracies, 'ro-', linewidth=2, label='Dummy Classifier')
plt.title('Accuracy vs. Majority Class Ratio')
plt.xlabel('Majority Class Ratio')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotations
for i, r in enumerate(results):
    plt.subplot(1, 2, 1)
    plt.annotate(f"{r['accuracy']:.3f}", (r['ratio'], r['accuracy']), 
                 textcoords="offset points", xytext=(0,10), ha='center')
    plt.annotate(f"{r['accuracy_dummy']:.3f}", (r['ratio'], r['accuracy_dummy']), 
                 textcoords="offset points", xytext=(0,-15), ha='center')
    
    plt.subplot(1, 2, 2)
    plt.annotate(f"{r['accuracy']:.3f}", (r['majority_class_ratio'], r['accuracy']), 
                 textcoords="offset points", xytext=(0,10), ha='center')
    plt.annotate(f"{r['accuracy_dummy']:.3f}", (r['majority_class_ratio'], r['accuracy_dummy']), 
                 textcoords="offset points", xytext=(0,-15), ha='center')

plt.tight_layout()
plt.show()

# Examine confusion matrices for the most imbalanced case
X, y, ratio = datasets[-1]  # Most imbalanced dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train models
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
y_pred_dummy = dummy.predict(X_test)

# Calculate confusion matrices
cm = confusion_matrix(y_test, y_pred)
cm_dummy = confusion_matrix(y_test, y_pred_dummy)

# Print classification reports
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred))

print("\nDummy Classifier Classification Report:")
print(classification_report(y_test, y_pred_dummy))

# Visualize confusion matrices
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Logistic Regression Confusion Matrix')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm[i, j]), ha='center', va='center',
                color='white' if cm[i, j] > cm.max() / 2 else 'black')

plt.subplot(1, 2, 2)
plt.imshow(cm_dummy, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Dummy Classifier Confusion Matrix')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm_dummy[i, j]), ha='center', va='center',
                color='white' if cm_dummy[i, j] > cm_dummy.max() / 2 else 'black')

plt.tight_layout()
plt.show()
```

### Alternative Metrics for Imbalanced Data

Several metrics are more appropriate for imbalanced datasets:

#### Precision, Recall, and F1 Score

**Precision**, **Recall**, and **F1 Score** focus on the positive class and are less affected by class imbalance:

```python
from sklearn.metrics import precision_score, recall_score, f1_score

# Compare metrics across different imbalance ratios
metric_results = []

for X, y, ratio in datasets:
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train a logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    metric_results.append({
        'ratio': ratio,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    })
    
    print(f"Imbalance Ratio (minority class): {ratio}")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1 Score: {f1:.4f}")

# Visualize the metrics
plt.figure(figsize=(10, 6))
ratios = [r['ratio'] for r in metric_results]
accuracies = [r['accuracy'] for r in metric_results]
precisions = [r['precision'] for r in metric_results]
recalls = [r['recall'] for r in metric_results]
f1s = [r['f1'] for r in metric_results]

plt.plot(ratios, accuracies, 'bo-', linewidth=2, label='Accuracy')
plt.plot(ratios, precisions, 'go-', linewidth=2, label='Precision')
plt.plot(ratios, recalls, 'ro-', linewidth=2, label='Recall')
plt.plot(ratios, f1s, 'mo-', linewidth=2, label='F1 Score')

plt.title('Metrics vs. Imbalance Ratio')
plt.xlabel('Minority Class Ratio')
plt.ylabel('Score')
plt.xscale('log')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### ROC Curve and Precision-Recall Curve

**ROC Curve** and **Precision-Recall Curve** provide a more complete picture of model performance across different thresholds:

```python
from sklearn.metrics import roc_curve, precision_recall_curve, auc, average_precision_score

# Compare ROC and PR curves for different imbalance ratios
plt.figure(figsize=(15, 10))

for i, (X, y, ratio) in enumerate(datasets):
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train a logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    
    # Calculate PR curve
    precision, recall, _ = precision_recall_curve(y_test, y_prob)
    pr_auc = average_precision_score(y_test, y_prob)
    
    # Plot ROC curve
    plt.subplot(2, 3, i + 1)
    plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve (Minority Ratio = {ratio})')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    
    # Plot PR curve
    plt.subplot(2, 3, i + 4)
    plt.plot(recall, precision, 'r-', linewidth=2, label=f'PR curve (AP = {pr_auc:.3f})')
    plt.axhline(y=ratio, color='k', linestyle='--', label=f'Baseline (ratio = {ratio})')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve (Minority Ratio = {ratio})')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Compare ROC and PR curves for the most imbalanced case
X, y, ratio = datasets[-1]  # Most imbalanced dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train models with different thresholds
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)
y_prob = model.predict_proba(X_test)[:, 1]

# Calculate ROC and PR curves
fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
pr_auc = average_precision_score(y_test, y_prob)

# Visualize ROC and PR curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve (Minority Ratio = {ratio})')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(recall, precision, 'r-', linewidth=2, label=f'PR curve (AP = {pr_auc:.3f})')
plt.axhline(y=ratio, color='k', linestyle='--', label=f'Baseline (ratio = {ratio})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title(f'Precision-Recall Curve (Minority Ratio = {ratio})')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize the effect of threshold on different metrics
thresholds = np.linspace(0, 1, 100)
accuracies = []
precisions = []
recalls = []
f1s = []

for threshold in thresholds:
    y_pred = (y_prob >= threshold).astype(int)
    accuracies.append(accuracy_score(y_test, y_pred))
    precisions.append(precision_score(y_test, y_pred, zero_division=0))
    recalls.append(recall_score(y_test, y_pred))
    f1s.append(f1_score(y_test, y_pred, zero_division=0))

plt.figure(figsize=(10, 6))
plt.plot(thresholds, accuracies, 'b-', linewidth=2, label='Accuracy')
plt.plot(thresholds, precisions, 'g-', linewidth=2, label='Precision')
plt.plot(thresholds, recalls, 'r-', linewidth=2, label='Recall')
plt.plot(thresholds, f1s, 'm-', linewidth=2, label='F1 Score')
plt.axvline(x=0.5, color='k', linestyle='--', label='Default Threshold (0.5)')
plt.title('Effect of Threshold on Metrics')
plt.xlabel('Classification Threshold')
plt.ylabel('Score')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Threshold Adjustment

**Threshold Adjustment** involves changing the classification threshold to optimize a specific metric or balance precision and recall:

```python
from sklearn.metrics import precision_recall_curve, f1_score

# Find the optimal threshold for F1 score
X, y, ratio = datasets[-1]  # Most imbalanced dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)
y_prob = model.predict_proba(X_test)[:, 1]

# Calculate precision, recall, and thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

# Calculate F1 score for each threshold
f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]
optimal_f1 = f1_scores[optimal_idx]

print(f"Optimal threshold for F1 score: {optimal_threshold:.4f}")
print(f"Optimal F1 score: {optimal_f1:.4f}")

# Compare default and optimal thresholds
y_pred_default = (y_prob >= 0.5).astype(int)
y_pred_optimal = (y_prob >= optimal_threshold).astype(int)

print("\nDefault threshold (0.5):")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_default):.4f}")
print(f"  Precision: {precision_score(y_test, y_pred_default):.4f}")
print(f"  Recall: {recall_score(y_test, y_pred_default):.4f}")
print(f"  F1 Score: {f1_score(y_test, y_pred_default):.4f}")

print("\nOptimal threshold:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_optimal):.4f}")
print(f"  Precision: {precision_score(y_test, y_pred_optimal):.4f}")
print(f"  Recall: {recall_score(y_test, y_pred_optimal):.4f}")
print(f"  F1 Score: {f1_score(y_test, y_pred_optimal):.4f}")

# Visualize the effect of threshold on F1 score
plt.figure(figsize=(10, 6))
plt.plot(thresholds, f1_scores, 'b-', linewidth=2)
plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')
plt.axvline(x=optimal_threshold, color='g', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.4f})')
plt.title('F1 Score vs. Classification Threshold')
plt.xlabel('Classification Threshold')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize confusion matrices for default and optimal thresholds
cm_default = confusion_matrix(y_test, y_pred_default)
cm_optimal = confusion_matrix(y_test, y_pred_optimal)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(cm_default, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix (Default Threshold)')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm_default[i, j]), ha='center', va='center',
                color='white' if cm_default[i, j] > cm_default.max() / 2 else 'black')

plt.subplot(1, 2, 2)
plt.imshow(cm_optimal, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix (Optimal Threshold)')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm_optimal[i, j]), ha='center', va='center',
                color='white' if cm_optimal[i, j] > cm_optimal.max() / 2 else 'black')

plt.tight_layout()
plt.show()
```

### Cost-Sensitive Evaluation

**Cost-Sensitive Evaluation** takes into account the different costs of different types of errors:

```python
# Define a function to calculate cost-sensitive metrics
def cost_sensitive_evaluation(y_true, y_pred, fp_cost=1, fn_cost=1):
    """
    Calculate cost-sensitive metrics
    
    Parameters:
    y_true : array-like
        True labels
    y_pred : array-like
        Predicted labels
    fp_cost : float
        Cost of a false positive
    fn_cost : float
        Cost of a false negative
    
    Returns:
    dict
        Dictionary of cost-sensitive metrics
    """
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # Calculate costs
    total_cost = fp * fp_cost + fn * fn_cost
    avg_cost = total_cost / len(y_true)
    
    return {
        'total_cost': total_cost,
        'avg_cost': avg_cost,
        'fp': fp,
        'fn': fn,
        'fp_cost': fp_cost,
        'fn_cost': fn_cost
    }

# Compare cost-sensitive evaluation for different cost ratios
X, y, ratio = datasets[-1]  # Most imbalanced dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)
y_prob = model.predict_proba(X_test)[:, 1]

# Define cost ratios (FP:FN)
cost_ratios = [(1, 1), (1, 10), (1, 100), (10, 1), (100, 1)]
results = []

for fp_cost, fn_cost in cost_ratios:
    # Find optimal threshold for minimizing cost
    thresholds = np.linspace(0, 1, 100)
    costs = []
    
    for threshold in thresholds:
        y_pred = (y_prob >= threshold).astype(int)
        cost = cost_sensitive_evaluation(y_test, y_pred, fp_cost, fn_cost)['avg_cost']
        costs.append(cost)
    
    optimal_idx = np.argmin(costs)
    optimal_threshold = thresholds[optimal_idx]
    
    # Evaluate with optimal threshold
    y_pred_optimal = (y_prob >= optimal_threshold).astype(int)
    metrics = cost_sensitive_evaluation(y_test, y_pred_optimal, fp_cost, fn_cost)
    metrics['threshold'] = optimal_threshold
    metrics['accuracy'] = accuracy_score(y_test, y_pred_optimal)
    metrics['precision'] = precision_score(y_test, y_pred_optimal, zero_division=0)
    metrics['recall'] = recall_score(y_test, y_pred_optimal)
    metrics['f1'] = f1_score(y_test, y_pred_optimal, zero_division=0)
    metrics['cost_ratio'] = f"{fp_cost}:{fn_cost}"
    
    results.append(metrics)
    
    print(f"Cost Ratio (FP:FN) = {fp_cost}:{fn_cost}")
    print(f"  Optimal Threshold: {optimal_threshold:.4f}")
    print(f"  Accuracy: {metrics['accuracy']:.4f}")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  Recall: {metrics['recall']:.4f}")
    print(f"  F1 Score: {metrics['f1']:.4f}")
    print(f"  Average Cost: {metrics['avg_cost']:.4f}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot optimal thresholds
plt.subplot(2, 2, 1)
plt.bar([r['cost_ratio'] for r in results], [r['threshold'] for r in results], color='skyblue')
plt.title('Optimal Threshold by Cost Ratio')
plt.xlabel('Cost Ratio (FP:FN)')
plt.ylabel('Optimal Threshold')
plt.grid(True, alpha=0.3, axis='y')

# Plot precision and recall
plt.subplot(2, 2, 2)
x = np.arange(len(results))
width = 0.35
plt.bar(x - width/2, [r['precision'] for r in results], width, label='Precision', color='green')
plt.bar(x + width/2, [r['recall'] for r in results], width, label='Recall', color='red')
plt.title('Precision and Recall by Cost Ratio')
plt.xlabel('Cost Ratio (FP:FN)')
plt.ylabel('Score')
plt.xticks(x, [r['cost_ratio'] for r in results])
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Plot confusion matrix elements
plt.subplot(2, 2, 3)
x = np.arange(len(results))
width = 0.35
plt.bar(x - width/2, [r['fp'] for r in results], width, label='False Positives', color='orange')
plt.bar(x + width/2, [r['fn'] for r in results], width, label='False Negatives', color='purple')
plt.title('Errors by Cost Ratio')
plt.xlabel('Cost Ratio (FP:FN)')
plt.ylabel('Count')
plt.xticks(x, [r['cost_ratio'] for r in results])
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Plot average cost
plt.subplot(2, 2, 4)
plt.bar([r['cost_ratio'] for r in results], [r['avg_cost'] for r in results], color='salmon')
plt.title('Average Cost by Cost Ratio')
plt.xlabel('Cost Ratio (FP:FN)')
plt.ylabel('Average Cost')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

## Summary

Handling imbalanced data requires special consideration for model evaluation:

1. **The Problem with Accuracy**:
   - Accuracy can be misleading for imbalanced datasets
   - A model that always predicts the majority class can achieve high accuracy without learning anything useful

2. **Alternative Metrics**:
   - Precision, Recall, and F1 Score focus on the positive class
   - ROC Curve is less sensitive to imbalance but can still be misleading
   - Precision-Recall Curve is more informative for imbalanced datasets

3. **Threshold Adjustment**:
   - Changing the classification threshold can optimize specific metrics
   - The default threshold of 0.5 is often suboptimal for imbalanced data
   - Threshold can be tuned to balance precision and recall

4. **Cost-Sensitive Evaluation**:
   - Takes into account the different costs of different types of errors
   - Allows for domain-specific optimization
   - Can lead to different optimal thresholds based on the cost ratio

These techniques help ensure that model evaluation is meaningful and aligned with the specific goals and constraints of the problem, even when dealing with imbalanced data.

## References

1. He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9), 1263-1284.
2. Davis, J., & Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd International Conference on Machine Learning (pp. 233-240).
3. Saito, T., & Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PloS One, 10(3), e0118432.
4. Elkan, C. (2001). The foundations of cost-sensitive learning. In International Joint Conference on Artificial Intelligence (Vol. 17, No. 1, pp. 973-978).
