# 3.2.5 Model Evaluation - Part 5: Practical Considerations (3)

## Model Comparison and Statistical Significance

When comparing different models, it's important to determine whether observed differences in performance are statistically significant or just due to random variation. This section covers techniques for rigorous model comparison.

### Paired Statistical Tests

**Paired statistical tests** compare the performance of two models on the same data points to determine if one is significantly better than the other.

#### Paired t-test

The **paired t-test** compares the mean difference between paired observations:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from scipy import stats

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Define cross-validation strategy
cv = KFold(n_splits=10, shuffle=True, random_state=42)

# Define models
model1 = LogisticRegression(random_state=42)
model2 = RandomForestClassifier(n_estimators=100, random_state=42)

# Perform cross-validation
model1_scores = []
model2_scores = []

for train_idx, test_idx in cv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Train and evaluate model 1
    model1.fit(X_train, y_train)
    y_pred1 = model1.predict(X_test)
    score1 = accuracy_score(y_test, y_pred1)
    model1_scores.append(score1)
    
    # Train and evaluate model 2
    model2.fit(X_train, y_train)
    y_pred2 = model2.predict(X_test)
    score2 = accuracy_score(y_test, y_pred2)
    model2_scores.append(score2)

# Calculate mean scores
mean_score1 = np.mean(model1_scores)
mean_score2 = np.mean(model2_scores)

print(f"Model 1 (Logistic Regression) mean accuracy: {mean_score1:.4f}")
print(f"Model 2 (Random Forest) mean accuracy: {mean_score2:.4f}")
print(f"Difference: {mean_score2 - mean_score1:.4f}")

# Perform paired t-test
t_stat, p_value = stats.ttest_rel(model2_scores, model1_scores)
print(f"\nPaired t-test:")
print(f"  t-statistic: {t_stat:.4f}")
print(f"  p-value: {p_value:.4f}")
print(f"  Significant difference (p < 0.05): {p_value < 0.05}")

# Visualize the results
plt.figure(figsize=(12, 6))

# Plot individual fold scores
plt.subplot(1, 2, 1)
plt.plot(range(1, len(model1_scores) + 1), model1_scores, 'bo-', linewidth=2, label='Logistic Regression')
plt.plot(range(1, len(model2_scores) + 1), model2_scores, 'ro-', linewidth=2, label='Random Forest')
plt.title('Accuracy by Fold')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot score distributions
plt.subplot(1, 2, 2)
plt.boxplot([model1_scores, model2_scores], labels=['Logistic Regression', 'Random Forest'])
plt.title('Accuracy Distribution')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3, axis='y')

# Add mean values
plt.plot([1, 2], [mean_score1, mean_score2], 'rx', markersize=10, label='Mean')
plt.legend()

# Add p-value annotation
if p_value < 0.001:
    p_text = 'p < 0.001'
else:
    p_text = f'p = {p_value:.4f}'

plt.text(1.5, min(mean_score1, mean_score2) - 0.02, 
         f"Paired t-test: {p_text}\n{'Significant' if p_value < 0.05 else 'Not significant'}", 
         ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

#### Wilcoxon Signed-Rank Test

The **Wilcoxon signed-rank test** is a non-parametric alternative to the paired t-test that doesn't assume normality:

```python
from scipy.stats import wilcoxon

# Perform Wilcoxon signed-rank test
w_stat, p_value_wilcoxon = wilcoxon(model2_scores, model1_scores)
print(f"\nWilcoxon signed-rank test:")
print(f"  W-statistic: {w_stat:.4f}")
print(f"  p-value: {p_value_wilcoxon:.4f}")
print(f"  Significant difference (p < 0.05): {p_value_wilcoxon < 0.05}")

# Compare t-test and Wilcoxon test
plt.figure(figsize=(10, 6))
plt.bar(['Paired t-test', 'Wilcoxon signed-rank test'], [p_value, p_value_wilcoxon], color=['skyblue', 'salmon'])
plt.axhline(y=0.05, color='r', linestyle='--', label='Significance level (p=0.05)')
plt.title('p-values for Different Statistical Tests')
plt.ylabel('p-value')
plt.yscale('log')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
plt.text(0, p_value, f"{p_value:.4f}", ha='center', va='bottom')
plt.text(1, p_value_wilcoxon, f"{p_value_wilcoxon:.4f}", ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

### McNemar's Test

**McNemar's test** compares the disagreement between two models on individual predictions:

```python
from sklearn.metrics import confusion_matrix
from scipy.stats import chi2

# Make predictions on the entire dataset
model1.fit(X, y)
model2.fit(X, y)
y_pred1 = model1.predict(X)
y_pred2 = model2.predict(X)

# Create a contingency table of agreements/disagreements
# b: model1 correct, model2 incorrect
# c: model1 incorrect, model2 correct
b = np.sum((y_pred1 == y) & (y_pred2 != y))
c = np.sum((y_pred1 != y) & (y_pred2 == y))

# Calculate McNemar's statistic with continuity correction
mcnemar_stat = (abs(b - c) - 1)**2 / (b + c)
p_value_mcnemar = chi2.sf(mcnemar_stat, 1)  # chi-squared with 1 degree of freedom

print(f"\nMcNemar's test:")
print(f"  Model 1 correct, Model 2 incorrect (b): {b}")
print(f"  Model 1 incorrect, Model 2 correct (c): {c}")
print(f"  McNemar's statistic: {mcnemar_stat:.4f}")
print(f"  p-value: {p_value_mcnemar:.4f}")
print(f"  Significant difference (p < 0.05): {p_value_mcnemar < 0.05}")

# Create a 2x2 contingency table
contingency = np.zeros((2, 2))
contingency[0, 0] = np.sum((y_pred1 == y) & (y_pred2 == y))  # both correct
contingency[0, 1] = b  # model1 correct, model2 incorrect
contingency[1, 0] = c  # model1 incorrect, model2 correct
contingency[1, 1] = np.sum((y_pred1 != y) & (y_pred2 != y))  # both incorrect

# Visualize the contingency table
plt.figure(figsize=(8, 6))
plt.imshow(contingency, cmap='Blues')
plt.colorbar(label='Count')
plt.title("McNemar's Test Contingency Table")
plt.xticks([0, 1], ['Model 2 Correct', 'Model 2 Incorrect'])
plt.yticks([0, 1], ['Model 1 Correct', 'Model 1 Incorrect'])

# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(int(contingency[i, j])), ha='center', va='center', 
                 color='white' if contingency[i, j] > contingency.max() / 2 else 'black',
                 fontsize=12, fontweight='bold')

# Add McNemar's test result
plt.text(0.5, 2.2, f"McNemar's test: p = {p_value_mcnemar:.4f}\n{'Significant' if p_value_mcnemar < 0.05 else 'Not significant'}", 
         ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Highlight the cells used in McNemar's test
plt.plot([0.5, 1.5], [0.5, 0.5], 'r-', linewidth=2)
plt.plot([0.5, 0.5], [0.5, 1.5], 'r-', linewidth=2)
plt.text(1, 0, f"b = {b}", ha='center', va='center', color='red', fontweight='bold')
plt.text(0, 1, f"c = {c}", ha='center', va='center', color='red', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Multiple Model Comparison

When comparing multiple models, additional considerations are needed to account for multiple comparisons.

#### ANOVA and Friedman Test

**ANOVA** (parametric) and the **Friedman test** (non-parametric) compare multiple models simultaneously:

```python
from scipy.stats import f_oneway, friedmanchisquare
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Define additional models
model3 = SVC(probability=True, random_state=42)
model4 = KNeighborsClassifier(n_neighbors=5)

# Perform cross-validation for all models
model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'KNN']
all_scores = [model1_scores]  # We already have scores for models 1 and 2
model3_scores = []
model4_scores = []

for train_idx, test_idx in cv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Train and evaluate model 3
    model3.fit(X_train, y_train)
    y_pred3 = model3.predict(X_test)
    score3 = accuracy_score(y_test, y_pred3)
    model3_scores.append(score3)
    
    # Train and evaluate model 4
    model4.fit(X_train, y_train)
    y_pred4 = model4.predict(X_test)
    score4 = accuracy_score(y_test, y_pred4)
    model4_scores.append(score4)

all_scores.append(model2_scores)
all_scores.append(model3_scores)
all_scores.append(model4_scores)

# Calculate mean scores
mean_scores = [np.mean(scores) for scores in all_scores]
for i, (name, mean_score) in enumerate(zip(model_names, mean_scores)):
    print(f"Model {i+1} ({name}) mean accuracy: {mean_score:.4f}")

# Perform ANOVA
f_stat, p_value_anova = f_oneway(*all_scores)
print(f"\nANOVA:")
print(f"  F-statistic: {f_stat:.4f}")
print(f"  p-value: {p_value_anova:.4f}")
print(f"  Significant difference (p < 0.05): {p_value_anova < 0.05}")

# Perform Friedman test
chi2_stat, p_value_friedman = friedmanchisquare(*all_scores)
print(f"\nFriedman test:")
print(f"  Chi-squared statistic: {chi2_stat:.4f}")
print(f"  p-value: {p_value_friedman:.4f}")
print(f"  Significant difference (p < 0.05): {p_value_friedman < 0.05}")

# Visualize the results
plt.figure(figsize=(12, 6))

# Plot individual fold scores
plt.subplot(1, 2, 1)
for i, (name, scores) in enumerate(zip(model_names, all_scores)):
    plt.plot(range(1, len(scores) + 1), scores, 'o-', linewidth=2, label=name)
plt.title('Accuracy by Fold')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot score distributions
plt.subplot(1, 2, 2)
plt.boxplot(all_scores, labels=model_names)
plt.title('Accuracy Distribution')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3, axis='y')

# Add mean values
for i, mean_score in enumerate(mean_scores):
    plt.plot(i + 1, mean_score, 'rx', markersize=10)

# Add p-value annotations
if p_value_anova < 0.001:
    p_anova_text = 'p < 0.001'
else:
    p_anova_text = f'p = {p_value_anova:.4f}'

if p_value_friedman < 0.001:
    p_friedman_text = 'p < 0.001'
else:
    p_friedman_text = f'p = {p_value_friedman:.4f}'

plt.text(2.5, min(mean_scores) - 0.03, 
         f"ANOVA: {p_anova_text}\nFriedman: {p_friedman_text}", 
         ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

#### Post-hoc Tests

If ANOVA or the Friedman test indicates a significant difference, **post-hoc tests** determine which specific models differ:

```python
from scipy.stats import ttest_rel
from statsmodels.stats.multitest import multipletests

# Perform pairwise t-tests
p_values = []
comparisons = []

for i in range(len(all_scores)):
    for j in range(i+1, len(all_scores)):
        t_stat, p_value = ttest_rel(all_scores[i], all_scores[j])
        p_values.append(p_value)
        comparisons.append(f"{model_names[i]} vs {model_names[j]}")
        print(f"{model_names[i]} vs {model_names[j]}: p = {p_value:.4f}")

# Apply Bonferroni correction for multiple comparisons
reject, p_adjusted, _, _ = multipletests(p_values, method='bonferroni')

print("\nAfter Bonferroni correction:")
for i, (comparison, p, p_adj, rej) in enumerate(zip(comparisons, p_values, p_adjusted, reject)):
    print(f"{comparison}: original p = {p:.4f}, adjusted p = {p_adj:.4f}, significant: {rej}")

# Visualize the results
plt.figure(figsize=(12, 6))

# Plot original vs. adjusted p-values
plt.subplot(1, 2, 1)
x = np.arange(len(comparisons))
width = 0.35
plt.bar(x - width/2, p_values, width, label='Original p-values', color='skyblue')
plt.bar(x + width/2, p_adjusted, width, label='Adjusted p-values (Bonferroni)', color='salmon')
plt.axhline(y=0.05, color='r', linestyle='--', label='Significance level (p=0.05)')
plt.title('Original vs. Adjusted p-values')
plt.xlabel('Comparison')
plt.ylabel('p-value')
plt.xticks(x, comparisons, rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Create a heatmap of p-values
plt.subplot(1, 2, 2)
p_matrix = np.ones((len(model_names), len(model_names)))
for i in range(len(model_names)):
    p_matrix[i, i] = 0  # Diagonal elements (same model comparison)

k = 0
for i in range(len(model_names)):
    for j in range(i+1, len(model_names)):
        p_matrix[i, j] = p_adjusted[k]
        p_matrix[j, i] = p_adjusted[k]  # Symmetric matrix
        k += 1

plt.imshow(p_matrix, cmap='YlOrRd_r', vmin=0, vmax=0.05)
plt.colorbar(label='Adjusted p-value')
plt.title('Pairwise Comparison p-values (Bonferroni)')
plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')
plt.yticks(range(len(model_names)), model_names)

# Add text annotations
for i in range(len(model_names)):
    for j in range(len(model_names)):
        if i != j:
            color = 'white' if p_matrix[i, j] < 0.025 else 'black'
            text = f"{p_matrix[i, j]:.4f}" if p_matrix[i, j] < 1 else ""
            plt.text(j, i, text, ha='center', va='center', color=color, fontsize=8)
        else:
            plt.text(j, i, "-", ha='center', va='center', color='white')

plt.tight_layout()
plt.show()
```

### Confidence Intervals

**Confidence intervals** provide a range of plausible values for the true performance of a model:

```python
from scipy import stats

# Calculate 95% confidence intervals for each model
confidence_intervals = []

for i, (name, scores) in enumerate(zip(model_names, all_scores)):
    mean = np.mean(scores)
    std = np.std(scores, ddof=1)  # Sample standard deviation
    n = len(scores)
    
    # Calculate confidence interval using t-distribution
    t_critical = stats.t.ppf(0.975, df=n-1)  # 95% confidence interval
    margin_of_error = t_critical * std / np.sqrt(n)
    ci_lower = mean - margin_of_error
    ci_upper = mean + margin_of_error
    
    confidence_intervals.append((ci_lower, ci_upper))
    
    print(f"{name} 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")

# Visualize confidence intervals
plt.figure(figsize=(10, 6))
x = np.arange(len(model_names))
plt.errorbar(x, mean_scores, yerr=[(mean - ci[0]) for mean, ci in zip(mean_scores, confidence_intervals)], 
             fmt='o', capsize=5, capthick=2, linewidth=2, markersize=8)

plt.title('Model Performance with 95% Confidence Intervals')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(x, model_names)
plt.grid(True, alpha=0.3)

# Add mean values
for i, mean_score in enumerate(mean_scores):
    plt.text(i, mean_score - 0.02, f"{mean_score:.4f}", ha='center', va='top', fontweight='bold')

# Add confidence intervals
for i, (ci_lower, ci_upper) in enumerate(confidence_intervals):
    plt.text(i, ci_lower - 0.01, f"{ci_lower:.4f}", ha='center', va='top')
    plt.text(i, ci_upper + 0.01, f"{ci_upper:.4f}", ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

### Effect Size

**Effect size** measures the magnitude of the difference between models, which is important in addition to statistical significance:

```python
import numpy as np
from scipy import stats

# Calculate Cohen's d effect size for pairwise comparisons
effect_sizes = []
for i in range(len(all_scores)):
    for j in range(i+1, len(all_scores)):
        # Calculate Cohen's d
        mean1 = np.mean(all_scores[i])
        mean2 = np.mean(all_scores[j])
        std1 = np.std(all_scores[i], ddof=1)
        std2 = np.std(all_scores[j], ddof=1)
        
        # Pooled standard deviation
        pooled_std = np.sqrt((std1**2 + std2**2) / 2)
        
        # Cohen's d
        d = abs(mean1 - mean2) / pooled_std
        
        effect_sizes.append(d)
        print(f"{model_names[i]} vs {model_names[j]} effect size (Cohen's d): {d:.4f}")

# Interpret effect sizes
def interpret_cohens_d(d):
    if d < 0.2:
        return "Negligible"
    elif d < 0.5:
        return "Small"
    elif d < 0.8:
        return "Medium"
    else:
        return "Large"

interpretations = [interpret_cohens_d(d) for d in effect_sizes]
for i, (comparison, d, interp) in enumerate(zip(comparisons, effect_sizes, interpretations)):
    print(f"{comparison} effect size: {d:.4f} ({interp})")

# Visualize effect sizes
plt.figure(figsize=(12, 6))

# Plot effect sizes
plt.subplot(1, 2, 1)
plt.bar(comparisons, effect_sizes, color='lightgreen')
plt.title("Cohen's d Effect Sizes")
plt.xlabel('Comparison')
plt.ylabel("Cohen's d")
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3, axis='y')

# Add thresholds for effect size interpretation
plt.axhline(y=0.2, color='r', linestyle='--', label='Negligible/Small')
plt.axhline(y=0.5, color='g', linestyle='--', label='Small/Medium')
plt.axhline(y=0.8, color='b', linestyle='--', label='Medium/Large')
plt.legend()

# Add annotations
for i, d in enumerate(effect_sizes):
    plt.text(i, d + 0.05, f"{d:.2f}", ha='center', va='bottom', fontweight='bold')
    plt.text(i, d - 0.05, interpretations[i], ha='center', va='top')

# Create a heatmap of effect sizes
plt.subplot(1, 2, 2)
d_matrix = np.zeros((len(model_names), len(model_names)))

k = 0
for i in range(len(model_names)):
    for j in range(i+1, len(model_names)):
        d_matrix[i, j] = effect_sizes[k]
        d_matrix[j, i] = effect_sizes[k]  # Symmetric matrix
        k += 1

plt.imshow(d_matrix, cmap='YlGnBu', vmin=0, vmax=1.5)
plt.colorbar(label="Cohen's d")
plt.title('Pairwise Effect Sizes')
plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')
plt.yticks(range(len(model_names)), model_names)

# Add text annotations
for i in range(len(model_names)):
    for j in range(len(model_names)):
        if i != j:
            color = 'white' if d_matrix[i, j] > 0.5 else 'black'
            text = f"{d_matrix[i, j]:.2f}"
            interp = interpret_cohens_d(d_matrix[i, j])
            plt.text(j, i, f"{text}\n({interp})", ha='center', va='center', color=color, fontsize=8)
        else:
            plt.text(j, i, "-", ha='center', va='center', color='white')

plt.tight_layout()
plt.show()
```

## Summary

Rigorous model comparison requires statistical techniques to determine if observed differences are significant:

1. **Paired Statistical Tests**:
   - Paired t-test: Parametric test for comparing two models
   - Wilcoxon signed-rank test: Non-parametric alternative to the paired t-test

2. **McNemar's Test**:
   - Compares the disagreement between two models on individual predictions
   - Useful when models have similar overall accuracy but make different errors

3. **Multiple Model Comparison**:
   - ANOVA: Parametric test for comparing multiple models
   - Friedman test: Non-parametric alternative to ANOVA
   - Post-hoc tests: Determine which specific models differ
   - Bonferroni correction: Adjusts p-values for multiple comparisons

4. **Confidence Intervals**:
   - Provide a range of plausible values for the true performance
   - Help assess the precision of performance estimates

5. **Effect Size**:
   - Measures the magnitude of the difference between models
   - Important in addition to statistical significance
   - Cohen's d: Common measure of effect size for paired comparisons

These techniques help ensure that model comparisons are statistically sound and that conclusions about model superiority are well-founded.

## References

1. Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7), 1895-1923.
2. Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7, 1-30.
3. Japkowicz, N., & Shah, M. (2011). Evaluating Learning Algorithms: A Classification Perspective. Cambridge University Press.
4. Benavoli, A., Corani, G., & Mangili, F. (2016). Should we really use post-hoc tests based on mean-ranks? Journal of Machine Learning Research, 17(1), 152-161.
