# 3.2.4 Generalization - Part 4: Ensemble Methods

## Introduction to Ensemble Methods

**Ensemble methods** combine multiple models to create a more powerful model that generalizes better than any of the individual models. The key idea is that by combining models with different strengths and weaknesses, the ensemble can achieve better performance and reduce overfitting.

### Why Ensemble Methods Work

Ensemble methods work because:

1. **Error Reduction**: They reduce errors by averaging out individual model mistakes
2. **Variance Reduction**: They reduce variance by combining multiple models
3. **Exploration of Hypothesis Space**: They explore a larger space of possible models
4. **Reduced Overfitting**: They are less likely to overfit than a single complex model

### Bagging (Bootstrap Aggregating)

**Bagging** involves training multiple models on different bootstrap samples of the training data and combining their predictions:

1. Create multiple bootstrap samples by randomly sampling with replacement
2. Train a model on each bootstrap sample
3. Combine predictions by averaging (for regression) or voting (for classification)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test = X[:80], X[80:]
y_train, y_test = y[:80], y[80:]

# Train a single decision tree
tree = DecisionTreeRegressor(max_depth=5, random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
mse_tree = mean_squared_error(y_test, y_pred_tree)

# Train a bagging ensemble of decision trees
bagging = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(max_depth=5),
    n_estimators=100,
    random_state=42
)
bagging.fit(X_train, y_train)
y_pred_bagging = bagging.predict(X_test)
mse_bagging = mean_squared_error(y_test, y_pred_bagging)

print(f"Single Tree MSE: {mse_tree:.2f}")
print(f"Bagging MSE: {mse_bagging:.2f}")
print(f"Improvement: {(mse_tree - mse_bagging) / mse_tree * 100:.2f}%")

# Visualize the results
plt.figure(figsize=(12, 6))

# Sort X for smooth curve plotting
X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)

# Plot individual trees in the ensemble
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')

# Plot predictions of individual trees
for i, estimator in enumerate(bagging.estimators_):
    if i < 10:  # Plot only a few trees for clarity
        y_pred_individual = estimator.predict(X_plot)
        plt.plot(X_plot, y_pred_individual, 'r-', alpha=0.1)

# Plot ensemble prediction
y_pred_ensemble = bagging.predict(X_plot)
plt.plot(X_plot, y_pred_ensemble, 'k-', linewidth=2, label='Ensemble')

plt.title('Bagging: Individual Trees vs. Ensemble')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Compare single tree vs. bagging
plt.subplot(1, 2, 2)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')

# Plot single tree prediction
y_pred_tree_plot = tree.predict(X_plot)
plt.plot(X_plot, y_pred_tree_plot, 'r-', linewidth=2, label=f'Single Tree (MSE: {mse_tree:.2f})')

# Plot bagging prediction
plt.plot(X_plot, y_pred_ensemble, 'k-', linewidth=2, label=f'Bagging (MSE: {mse_bagging:.2f})')

plt.title('Single Tree vs. Bagging')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Random Forests

**Random Forests** are an extension of bagging that introduces additional randomness:

1. Create multiple bootstrap samples
2. For each sample, train a decision tree, but:
   - At each node, consider only a random subset of features
3. Combine predictions by averaging or voting

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.inspection import permutation_importance

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data into training and test sets
X_train, X_test = X[:800], X[800:]
y_train, y_test = y[:800], y[800:]

# Train a single decision tree
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
accuracy_tree = accuracy_score(y_test, y_pred_tree)

# Train a random forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

print(f"Single Tree Accuracy: {accuracy_tree:.4f}")
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print(f"Improvement: {(accuracy_rf - accuracy_tree) * 100:.2f} percentage points")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot confusion matrices
plt.subplot(2, 2, 1)
cm_tree = confusion_matrix(y_test, y_pred_tree)
plt.imshow(cm_tree, interpolation='nearest', cmap=plt.cm.Blues)
plt.title(f'Single Tree Confusion Matrix\nAccuracy: {accuracy_tree:.4f}')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm_tree[i, j]), ha='center', va='center',
                color='white' if cm_tree[i, j] > cm_tree.max() / 2 else 'black')

plt.subplot(2, 2, 2)
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.imshow(cm_rf, interpolation='nearest', cmap=plt.cm.Blues)
plt.title(f'Random Forest Confusion Matrix\nAccuracy: {accuracy_rf:.4f}')
plt.colorbar()
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.yticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cm_rf[i, j]), ha='center', va='center',
                color='white' if cm_rf[i, j] > cm_rf.max() / 2 else 'black')

# Plot feature importances
plt.subplot(2, 2, 3)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
plt.bar(range(20), importances[indices], color='skyblue')
plt.title('Random Forest Feature Importances')
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.grid(True, alpha=0.3, axis='y')

# Plot permutation importances
plt.subplot(2, 2, 4)
result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)
perm_importances = result.importances_mean
perm_indices = np.argsort(perm_importances)[::-1]
plt.bar(range(20), perm_importances[perm_indices], color='lightgreen')
plt.title('Random Forest Permutation Importances')
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### Boosting

**Boosting** involves training models sequentially, with each model focusing on the errors of previous models:

1. Train a base model on the original data
2. Identify examples that the model struggles with
3. Train the next model with more emphasis on these difficult examples
4. Combine models by weighted voting or averaging

#### AdaBoost

**AdaBoost** (Adaptive Boosting) adjusts the weights of examples based on classification errors:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, 
                          n_redundant=0, n_classes=2, random_state=42)

# Split data into training and test sets
X_train, X_test = X[:800], X[800:]
y_train, y_test = y[:800], y[800:]

# Train a single decision tree
tree = DecisionTreeClassifier(max_depth=1, random_state=42)  # Decision stump
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
accuracy_tree = accuracy_score(y_test, y_pred_tree)

# Train AdaBoost
n_estimators = 50
adaboost = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=n_estimators,
    random_state=42
)
adaboost.fit(X_train, y_train)
y_pred_ada = adaboost.predict(X_test)
accuracy_ada = accuracy_score(y_test, y_pred_ada)

print(f"Single Tree Accuracy: {accuracy_tree:.4f}")
print(f"AdaBoost Accuracy: {accuracy_ada:.4f}")
print(f"Improvement: {(accuracy_ada - accuracy_tree) * 100:.2f} percentage points")

# Visualize the decision boundaries
plt.figure(figsize=(15, 10))

# Create a mesh grid
h = 0.02  # Step size
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Plot decision boundary for single tree
plt.subplot(2, 2, 1)
Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', alpha=0.7)
plt.title(f'Single Decision Tree\nAccuracy: {accuracy_tree:.4f}')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Plot decision boundary for AdaBoost
plt.subplot(2, 2, 2)
Z = adaboost.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', alpha=0.7)
plt.title(f'AdaBoost Ensemble\nAccuracy: {accuracy_ada:.4f}')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Plot the evolution of the decision boundary
plt.subplot(2, 1, 2)
n_iterations = 5
for i, n_trees in enumerate(np.linspace(1, n_estimators, n_iterations, dtype=int)):
    # Create a model with a subset of trees
    subset_model = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators=n_trees,
        random_state=42
    )
    subset_model.fit(X_train, y_train)
    
    # Calculate accuracy
    y_pred_subset = subset_model.predict(X_test)
    accuracy_subset = accuracy_score(y_test, y_pred_subset)
    
    # Plot decision boundary
    plt.subplot(2, n_iterations, n_iterations + i + 1)
    Z = subset_model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', alpha=0.7, s=10)
    plt.title(f'n_trees = {n_trees}\nAcc = {accuracy_subset:.4f}')
    plt.xticks([])
    plt.yticks([])
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Gradient Boosting

**Gradient Boosting** trains each model to predict the residual errors of previous models:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test = X[:800], X[800:]
y_train, y_test = y[:800], y[800:]

# Train a single decision tree
tree = DecisionTreeRegressor(max_depth=3, random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
mse_tree = mean_squared_error(y_test, y_pred_tree)

# Train Gradient Boosting
n_estimators = 100
gb = GradientBoostingRegressor(
    max_depth=3,
    n_estimators=n_estimators,
    learning_rate=0.1,
    random_state=42
)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
mse_gb = mean_squared_error(y_test, y_pred_gb)

print(f"Single Tree MSE: {mse_tree:.2f}")
print(f"Gradient Boosting MSE: {mse_gb:.2f}")
print(f"Improvement: {(mse_tree - mse_gb) / mse_tree * 100:.2f}%")

# Visualize the results
plt.figure(figsize=(15, 10))

# Sort X for smooth curve plotting
X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)

# Plot single tree vs. gradient boosting
plt.subplot(2, 2, 1)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')

# Plot single tree prediction
y_pred_tree_plot = tree.predict(X_plot)
plt.plot(X_plot, y_pred_tree_plot, 'r-', linewidth=2, label=f'Single Tree (MSE: {mse_tree:.2f})')

# Plot gradient boosting prediction
y_pred_gb_plot = gb.predict(X_plot)
plt.plot(X_plot, y_pred_gb_plot, 'k-', linewidth=2, label=f'Gradient Boosting (MSE: {mse_gb:.2f})')

plt.title('Single Tree vs. Gradient Boosting')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the evolution of predictions with more trees
plt.subplot(2, 2, 2)
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')

# Plot predictions with different numbers of trees
n_trees_to_plot = [1, 5, 10, 50, 100]
colors = ['purple', 'orange', 'brown', 'pink', 'black']

for i, n_trees in enumerate(n_trees_to_plot):
    # Create a model with a subset of trees
    y_pred_stage = gb.predict(X_plot, n_trees - 1)
    plt.plot(X_plot, y_pred_stage, '-', color=colors[i], linewidth=2, label=f'n_trees = {n_trees}')

plt.title('Gradient Boosting: Evolution of Predictions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot training and test error vs. number of trees
plt.subplot(2, 2, 3)
train_scores = np.zeros(n_estimators)
test_scores = np.zeros(n_estimators)

for i, y_pred in enumerate(gb.staged_predict(X_train)):
    train_scores[i] = mean_squared_error(y_train, y_pred)

for i, y_pred in enumerate(gb.staged_predict(X_test)):
    test_scores[i] = mean_squared_error(y_test, y_pred)

plt.plot(np.arange(n_estimators) + 1, train_scores, 'b-', linewidth=2, label='Training Error')
plt.plot(np.arange(n_estimators) + 1, test_scores, 'r-', linewidth=2, label='Test Error')
plt.axhline(y=mse_tree, color='g', linestyle='--', label=f'Single Tree Error: {mse_tree:.2f}')
plt.title('Error vs. Number of Trees')
plt.xlabel('Number of Trees')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot feature importances
plt.subplot(2, 2, 4)
importances = gb.feature_importances_
plt.bar(range(len(importances)), importances, color='skyblue')
plt.title('Feature Importances')
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### Stacking

**Stacking** (Stacked Generalization) combines multiple models by training a meta-model on their predictions:

1. Split the training data into two parts
2. Train base models on the first part
3. Generate predictions on the second part
4. Train a meta-model using these predictions as features
5. For final predictions, use base models on the full training data and the meta-model on their outputs

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.metrics import accuracy_score, roc_curve, auc

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define base models
base_models = [
    ('lr', LogisticRegression(random_state=42)),
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('svm', SVC(probability=True, random_state=42))
]

# Define meta-model
meta_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Create stacking ensemble
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    stack_method='predict_proba'
)

# Train individual models and stacking ensemble
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Stacking': stacking
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'accuracy': accuracy,
        'probabilities': model.predict_proba(X_test)[:, 1]
    }
    print(f"{name} Accuracy: {accuracy:.4f}")

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot accuracy comparison
plt.subplot(2, 2, 1)
accuracies = [results[name]['accuracy'] for name in models.keys()]
plt.bar(models.keys(), accuracies, color='skyblue')
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

# Plot ROC curves
plt.subplot(2, 2, 2)
for name, result in results.items():
    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.title('ROC Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)

# Plot probability distributions
plt.subplot(2, 2, 3)
for name, result in results.items():
    if name in ['Logistic Regression', 'Stacking']:  # Plot only a subset for clarity
        plt.hist(result['probabilities'], bins=20, alpha=0.5, label=name)

plt.title('Probability Distributions')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot stacking architecture
plt.subplot(2, 2, 4)
plt.axis('off')
plt.text(0.5, 0.9, 'Stacking Architecture', ha='center', va='center', fontsize=14, fontweight='bold')

# Draw base models
base_names = ['Logistic\nRegression', 'Decision\nTree', 'SVM']
base_accs = [results['Logistic Regression']['accuracy'], 
             results['Decision Tree']['accuracy'], 
             results['SVM']['accuracy']]

for i, (name, acc) in enumerate(zip(base_names, base_accs)):
    plt.text(0.2 + i*0.3, 0.7, name, ha='center', va='center', 
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
    plt.text(0.2 + i*0.3, 0.6, f'Acc: {acc:.4f}', ha='center', va='center')

# Draw arrows to meta-model
for i in range(3):
    plt.annotate('', xy=(0.5, 0.4), xytext=(0.2 + i*0.3, 0.55),
                arrowprops=dict(arrowstyle='->'))

# Draw meta-model
plt.text(0.5, 0.35, 'Meta-Model\n(Random Forest)', ha='center', va='center',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
plt.text(0.5, 0.25, f'Final Acc: {results["Stacking"]["accuracy"]:.4f}', ha='center', va='center')

# Draw arrow to final prediction
plt.annotate('', xy=(0.5, 0.1), xytext=(0.5, 0.2),
            arrowprops=dict(arrowstyle='->'))
plt.text(0.5, 0.05, 'Final Prediction', ha='center', va='center')

plt.tight_layout()
plt.show()
```

## Summary

Ensemble methods combine multiple models to improve generalization:

1. **Bagging (Bootstrap Aggregating)**:
   - Trains models on bootstrap samples of the data
   - Reduces variance by averaging out individual model errors
   - Example: Random Forests

2. **Random Forests**:
   - Extension of bagging for decision trees
   - Introduces additional randomness by considering only a subset of features at each split
   - Reduces correlation between trees

3. **Boosting**:
   - Trains models sequentially, focusing on errors of previous models
   - Examples: AdaBoost, Gradient Boosting
   - Reduces bias by combining weak learners

4. **Stacking**:
   - Combines models by training a meta-model on their predictions
   - Leverages the strengths of different types of models
   - Can achieve better performance than any individual model

Ensemble methods are powerful tools for improving generalization and reducing overfitting. They work by combining multiple models with different strengths and weaknesses, resulting in a more robust and accurate overall model.

## References

1. Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140.
2. Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
3. Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119-139.
4. Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.
5. Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241-259.
