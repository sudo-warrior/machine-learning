# 3.2.6 Optimization Algorithms - Part 3: Gradient Descent Variants

## Variants of Gradient Descent

While batch gradient descent is effective, it can be computationally expensive for large datasets and may converge slowly for certain problems. Several variants of gradient descent have been developed to address these limitations.

### Stochastic Gradient Descent (SGD)

**Stochastic Gradient Descent (SGD)** updates parameters using the gradient computed from a single randomly selected training example:

$$\theta_{t+1} = \theta_t - \alpha \nabla J_i(\theta_t)$$

where $J_i(\theta_t)$ is the loss for the $i$-th example.

Advantages:
- Much faster per iteration than batch gradient descent
- Can escape local minima due to noise in updates
- Works well for online learning

Disadvantages:
- High variance in parameter updates
- May not converge to the exact minimum
- Requires a decreasing learning rate schedule for convergence

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Add bias term to X
X_b = np.c_[np.ones(X.shape[0]), X]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

def linear_regression_gradient_single(theta, x, y):
    """Gradient of squared error for a single example."""
    prediction = x @ theta
    return (prediction - y) * x

# Implement stochastic gradient descent for linear regression
def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size=1):
    """
    Perform stochastic gradient descent for linear regression.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_epochs: Number of passes through the entire dataset
    - batch_size: Number of examples per update (1 for pure SGD)
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for epoch in range(num_epochs):
        # Shuffle the data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(0, m, batch_size):
            # Get the batch
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # Compute gradient on the batch
            if batch_size == 1:
                gradient = linear_regression_gradient_single(theta, X_batch[0], y_batch[0])
            else:
                gradient = linear_regression_gradient(theta, X_batch, y_batch)
            
            # Update parameters
            theta = theta - learning_rate * gradient
            
            # Record history (less frequently for efficiency)
            if i % (m // 10) == 0:
                theta_history.append(theta.copy())
                cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run batch gradient descent and SGD for comparison
learning_rate = 0.1
num_iterations_bgd = 50
num_epochs_sgd = 10

# Batch Gradient Descent
def batch_gradient_descent(X, y, learning_rate, num_iterations):
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        theta = theta - learning_rate * gradient
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run the algorithms
theta_bgd, cost_bgd = batch_gradient_descent(X_b, y, learning_rate, num_iterations_bgd)
theta_sgd, cost_sgd = stochastic_gradient_descent(X_b, y, learning_rate, num_epochs_sgd)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations/epochs
plt.subplot(2, 2, 1)
plt.plot(range(len(cost_bgd)), cost_bgd, linewidth=2, label='Batch GD')
plt.plot(range(len(cost_sgd)), cost_sgd, linewidth=2, label='SGD')
plt.xlabel('Iteration/Update')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations/epochs (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_bgd)), cost_bgd, linewidth=2, label='Batch GD')
plt.semilogy(range(len(cost_sgd)), cost_sgd, linewidth=2, label='SGD')
plt.xlabel('Iteration/Update')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter values over iterations
plt.subplot(2, 2, 3)
plt.plot(range(len(theta_bgd)), theta_bgd[:, 0], '--', linewidth=2, label='θ₀ (Batch GD)')
plt.plot(range(len(theta_bgd)), theta_bgd[:, 1], '-', linewidth=2, label='θ₁ (Batch GD)')
plt.plot(range(len(theta_sgd)), theta_sgd[:, 0], '--', linewidth=2, label='θ₀ (SGD)')
plt.plot(range(len(theta_sgd)), theta_sgd[:, 1], '-', linewidth=2, label='θ₁ (SGD)')
plt.xlabel('Iteration/Update')
plt.ylabel('Parameter Value')
plt.title('Parameter Values vs. Iteration')
plt.legend()
plt.grid(True)

# Plot data and regression lines
plt.subplot(2, 2, 4)
plt.scatter(X, y, color='blue', alpha=0.1, label='Data')

final_theta_bgd = theta_bgd[-1]
final_theta_sgd = theta_sgd[-1]

x_range = np.linspace(X.min(), X.max(), 100)
y_pred_bgd = final_theta_bgd[0] + final_theta_bgd[1] * x_range
y_pred_sgd = final_theta_sgd[0] + final_theta_sgd[1] * x_range

plt.plot(x_range, y_pred_bgd, linewidth=2, 
         label=f'Batch GD: θ=[{final_theta_bgd[0]:.3f}, {final_theta_bgd[1]:.3f}]')
plt.plot(x_range, y_pred_sgd, linewidth=2, 
         label=f'SGD: θ=[{final_theta_sgd[0]:.3f}, {final_theta_sgd[1]:.3f}]')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Data and Regression Lines')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Mini-batch Gradient Descent

**Mini-batch Gradient Descent** updates parameters using the gradient computed from a small batch of training examples:

$$\theta_{t+1} = \theta_t - \alpha \nabla J_{B_t}(\theta_t)$$

where $J_{B_t}(\theta_t)$ is the average loss for the mini-batch $B_t$.

Advantages:
- More stable convergence than SGD
- More efficient than batch gradient descent
- Can leverage vectorized operations for speed
- Works well with GPU acceleration

Disadvantages:
- Still requires tuning of learning rate and batch size
- May still not converge to the exact minimum

```python
# Run mini-batch gradient descent with different batch sizes
batch_sizes = [1, 10, 100, 1000]  # 1 = SGD, 1000 = Batch GD (for this dataset)
num_epochs = 5
learning_rate = 0.1

results = []
for batch_size in batch_sizes:
    theta_history, cost_history = stochastic_gradient_descent(X_b, y, learning_rate, num_epochs, batch_size)
    results.append((batch_size, theta_history, cost_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations/epochs
plt.subplot(2, 2, 1)
for batch_size, theta_history, cost_history in results:
    label = 'SGD' if batch_size == 1 else f'Mini-batch (size={batch_size})'
    if batch_size == 1000:
        label = 'Batch GD'
    plt.plot(range(len(cost_history)), cost_history, linewidth=2, label=label)

plt.xlabel('Update')
plt.ylabel('Cost')
plt.title('Cost vs. Update')
plt.legend()
plt.grid(True)

# Plot cost over iterations/epochs (log scale)
plt.subplot(2, 2, 2)
for batch_size, theta_history, cost_history in results:
    label = 'SGD' if batch_size == 1 else f'Mini-batch (size={batch_size})'
    if batch_size == 1000:
        label = 'Batch GD'
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=label)

plt.xlabel('Update')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Update (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter values over iterations
plt.subplot(2, 2, 3)
for batch_size, theta_history, cost_history in results:
    label = 'SGD' if batch_size == 1 else f'Mini-batch (size={batch_size})'
    if batch_size == 1000:
        label = 'Batch GD'
    plt.plot(range(len(theta_history)), theta_history[:, 1], linewidth=2, label=label)

plt.xlabel('Update')
plt.ylabel('θ₁ Value')
plt.title('Parameter θ₁ vs. Update')
plt.legend()
plt.grid(True)

# Plot final cost vs. batch size
plt.subplot(2, 2, 4)
final_costs = [cost_history[-1] for _, _, cost_history in results]
plt.semilogx(batch_sizes, final_costs, 'o-', linewidth=2, markersize=8)
plt.xlabel('Batch Size (log scale)')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Batch Size')
plt.grid(True)

# Add annotations
for i, (batch_size, cost) in enumerate(zip(batch_sizes, final_costs)):
    plt.annotate(f'{cost:.6f}', (batch_size, cost), xytext=(0, 10), 
                 textcoords='offset points', ha='center')

plt.tight_layout()
plt.show()
```

### Learning Rate Schedules

**Learning Rate Schedules** adjust the learning rate during training to improve convergence:

1. **Time-based Decay**: $\alpha_t = \frac{\alpha_0}{1 + kt}$
2. **Step Decay**: $\alpha_t = \alpha_0 \times \gamma^{\lfloor t/s \rfloor}$
3. **Exponential Decay**: $\alpha_t = \alpha_0 \times e^{-kt}$

```python
# Implement learning rate schedules
def constant_schedule(alpha0, t):
    """Constant learning rate."""
    return alpha0

def time_based_decay(alpha0, t, k=0.01):
    """Time-based decay: α_t = α_0 / (1 + kt)"""
    return alpha0 / (1 + k * t)

def step_decay(alpha0, t, drop_rate=0.5, epochs_drop=10):
    """Step decay: α_t = α_0 * drop_rate^floor(epoch/epochs_drop)"""
    return alpha0 * drop_rate ** np.floor(t / epochs_drop)

def exponential_decay(alpha0, t, k=0.01):
    """Exponential decay: α_t = α_0 * exp(-kt)"""
    return alpha0 * np.exp(-k * t)

# Visualize learning rate schedules
plt.figure(figsize=(12, 6))

t = np.arange(0, 100)
alpha0 = 0.1

plt.plot(t, [constant_schedule(alpha0, i) for i in t], linewidth=2, label='Constant')
plt.plot(t, [time_based_decay(alpha0, i) for i in t], linewidth=2, label='Time-based Decay')
plt.plot(t, [step_decay(alpha0, i) for i in t], linewidth=2, label='Step Decay')
plt.plot(t, [exponential_decay(alpha0, i) for i in t], linewidth=2, label='Exponential Decay')

plt.xlabel('Iteration/Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedules')
plt.legend()
plt.grid(True)
plt.ylim(0, alpha0 * 1.1)

plt.tight_layout()
plt.show()

# Implement SGD with learning rate schedules
def sgd_with_schedule(X, y, alpha0, schedule_func, num_epochs, batch_size=1):
    """
    Perform SGD with a learning rate schedule.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - alpha0: Initial learning rate
    - schedule_func: Learning rate schedule function
    - num_epochs: Number of passes through the entire dataset
    - batch_size: Number of examples per update
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    - lr_history: History of learning rates
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    lr_history = [alpha0]
    
    t = 0  # Iteration counter
    
    for epoch in range(num_epochs):
        # Shuffle the data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(0, m, batch_size):
            # Get the batch
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # Compute gradient on the batch
            if batch_size == 1:
                gradient = linear_regression_gradient_single(theta, X_batch[0], y_batch[0])
            else:
                gradient = linear_regression_gradient(theta, X_batch, y_batch)
            
            # Get current learning rate
            learning_rate = schedule_func(alpha0, t)
            
            # Update parameters
            theta = theta - learning_rate * gradient
            
            # Increment iteration counter
            t += 1
            
            # Record history (less frequently for efficiency)
            if t % 100 == 0:
                theta_history.append(theta.copy())
                cost_history.append(linear_regression_cost(theta, X, y))
                lr_history.append(learning_rate)
    
    return np.array(theta_history), np.array(cost_history), np.array(lr_history)

# Run SGD with different learning rate schedules
alpha0 = 0.1
num_epochs = 10
batch_size = 32

schedules = [
    ("Constant", lambda a0, t: constant_schedule(a0, t)),
    ("Time-based Decay", lambda a0, t: time_based_decay(a0, t, k=0.001)),
    ("Step Decay", lambda a0, t: step_decay(a0, t, drop_rate=0.5, epochs_drop=500)),
    ("Exponential Decay", lambda a0, t: exponential_decay(a0, t, k=0.001))
]

results = []
for name, schedule_func in schedules:
    theta_history, cost_history, lr_history = sgd_with_schedule(
        X_b, y, alpha0, schedule_func, num_epochs, batch_size)
    results.append((name, theta_history, cost_history, lr_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
for name, theta_history, cost_history, lr_history in results:
    plt.plot(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Update (x100)')
plt.ylabel('Cost')
plt.title('Cost vs. Update')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for name, theta_history, cost_history, lr_history in results:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Update (x100)')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Update (Log Scale)')
plt.legend()
plt.grid(True)

# Plot learning rate over iterations
plt.subplot(2, 2, 3)
for name, theta_history, cost_history, lr_history in results:
    plt.plot(range(len(lr_history)), lr_history, linewidth=2, label=name)

plt.xlabel('Update (x100)')
plt.ylabel('Learning Rate')
plt.title('Learning Rate vs. Update')
plt.legend()
plt.grid(True)

# Plot final cost vs. schedule
plt.subplot(2, 2, 4)
names = [name for name, _, _, _ in results]
final_costs = [cost_history[-1] for _, _, cost_history, _ in results]

plt.bar(names, final_costs, color='skyblue')
plt.xlabel('Learning Rate Schedule')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Learning Rate Schedule')
plt.xticks(rotation=45)
plt.grid(True, axis='y')

# Add annotations
for i, cost in enumerate(final_costs):
    plt.text(i, cost/2, f'{cost:.6f}', ha='center', va='center')

plt.tight_layout()
plt.show()
```

### Momentum

**Momentum** adds a fraction of the previous update to the current update, helping to accelerate convergence and reduce oscillations:

$$v_t = \gamma v_{t-1} + \alpha \nabla J(\theta_{t-1})$$
$$\theta_t = \theta_{t-1} - v_t$$

where:
- $v_t$ is the velocity (momentum) at iteration $t$
- $\gamma$ is the momentum coefficient (typically 0.9)

```python
# Implement gradient descent with momentum
def gradient_descent_with_momentum(X, y, learning_rate, num_iterations, momentum=0.9):
    """
    Perform gradient descent with momentum.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - momentum: Momentum coefficient
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    velocity = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update velocity
        velocity = momentum * velocity + learning_rate * gradient
        
        # Update parameters
        theta = theta - velocity
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run gradient descent with and without momentum
learning_rate = 0.1
num_iterations = 100
momentum_values = [0, 0.5, 0.9, 0.99]

results = []
for momentum in momentum_values:
    theta_history, cost_history = gradient_descent_with_momentum(
        X_b, y, learning_rate, num_iterations, momentum)
    results.append((momentum, theta_history, cost_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
for momentum, theta_history, cost_history in results:
    label = 'No Momentum' if momentum == 0 else f'Momentum = {momentum}'
    plt.plot(range(len(cost_history)), cost_history, linewidth=2, label=label)

plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for momentum, theta_history, cost_history in results:
    label = 'No Momentum' if momentum == 0 else f'Momentum = {momentum}'
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=label)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter trajectories
plt.subplot(2, 2, 3)
for momentum, theta_history, cost_history in results:
    label = 'No Momentum' if momentum == 0 else f'Momentum = {momentum}'
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'o-', linewidth=2, markersize=4, label=label)

plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

# Plot final cost vs. momentum
plt.subplot(2, 2, 4)
final_costs = [cost_history[-1] for _, _, cost_history in results]
plt.plot(momentum_values, final_costs, 'o-', linewidth=2, markersize=8)
plt.xlabel('Momentum')
plt.ylabel('Final Cost')
plt.title('Final Cost vs. Momentum')
plt.grid(True)

# Add annotations
for i, (momentum, cost) in enumerate(zip(momentum_values, final_costs)):
    plt.annotate(f'{cost:.6f}', (momentum, cost), xytext=(0, 10), 
                 textcoords='offset points', ha='center')

plt.tight_layout()
plt.show()
```

### Nesterov Accelerated Gradient (NAG)

**Nesterov Accelerated Gradient (NAG)** is a variation of momentum that computes the gradient at the "lookahead" position:

$$v_t = \gamma v_{t-1} + \alpha \nabla J(\theta_{t-1} - \gamma v_{t-1})$$
$$\theta_t = \theta_{t-1} - v_t$$

This provides a more accurate update direction, especially when close to the minimum.

```python
# Implement Nesterov Accelerated Gradient
def nesterov_accelerated_gradient(X, y, learning_rate, num_iterations, momentum=0.9):
    """
    Perform Nesterov Accelerated Gradient.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - momentum: Momentum coefficient
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    velocity = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        # Look ahead
        theta_lookahead = theta - momentum * velocity
        
        # Compute gradient at the lookahead position
        gradient = linear_regression_gradient(theta_lookahead, X, y)
        
        # Update velocity
        velocity = momentum * velocity + learning_rate * gradient
        
        # Update parameters
        theta = theta - velocity
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run standard momentum and Nesterov momentum for comparison
learning_rate = 0.1
num_iterations = 100
momentum = 0.9

# Standard Momentum
theta_momentum, cost_momentum = gradient_descent_with_momentum(
    X_b, y, learning_rate, num_iterations, momentum)

# Nesterov Momentum
theta_nesterov, cost_nesterov = nesterov_accelerated_gradient(
    X_b, y, learning_rate, num_iterations, momentum)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(cost_momentum)), cost_momentum, linewidth=2, label='Standard Momentum')
plt.plot(range(len(cost_nesterov)), cost_nesterov, linewidth=2, label='Nesterov Momentum')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
plt.semilogy(range(len(cost_momentum)), cost_momentum, linewidth=2, label='Standard Momentum')
plt.semilogy(range(len(cost_nesterov)), cost_nesterov, linewidth=2, label='Nesterov Momentum')
plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter trajectories
plt.subplot(2, 2, 3)
plt.plot(theta_momentum[:, 0], theta_momentum[:, 1], 'o-', linewidth=2, markersize=4, label='Standard Momentum')
plt.plot(theta_nesterov[:, 0], theta_nesterov[:, 1], 'o-', linewidth=2, markersize=4, label='Nesterov Momentum')
plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Parameter Trajectories')
plt.legend()
plt.grid(True)

# Plot difference in cost
plt.subplot(2, 2, 4)
cost_diff = cost_momentum - cost_nesterov
plt.plot(range(len(cost_diff)), cost_diff, linewidth=2)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Iteration')
plt.ylabel('Cost Difference (Momentum - Nesterov)')
plt.title('Cost Difference')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Summary

Variants of gradient descent offer improvements over the basic algorithm:

1. **Stochastic Gradient Descent (SGD)**:
   - Updates parameters using a single example at a time
   - Faster per iteration but with higher variance
   - Useful for large datasets and online learning

2. **Mini-batch Gradient Descent**:
   - Updates parameters using a small batch of examples
   - Balances efficiency and stability
   - Works well with GPU acceleration

3. **Learning Rate Schedules**:
   - Adjust the learning rate during training
   - Help improve convergence and avoid oscillations
   - Common schedules: time-based, step, and exponential decay

4. **Momentum**:
   - Adds a fraction of the previous update to the current update
   - Accelerates convergence and reduces oscillations
   - Helps navigate ravines and escape local minima

5. **Nesterov Accelerated Gradient (NAG)**:
   - Computes the gradient at the "lookahead" position
   - Provides a more accurate update direction
   - Improves convergence, especially near the minimum

These variants address different challenges in optimization and are widely used in practice. In the next part, we'll explore adaptive learning rate methods that further improve optimization performance.

## References

1. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
2. Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning (pp. 1139-1147).
3. Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence O(1/k^2). Doklady ANSSSR, 269, 543-547.
4. Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186).
