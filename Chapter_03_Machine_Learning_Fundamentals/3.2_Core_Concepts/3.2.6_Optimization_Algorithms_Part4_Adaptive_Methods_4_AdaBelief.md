# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (4) - AdaBelief

## AdaBelief Optimizer

**AdaBelief** (Adapting stepsizes by the belief in observed gradients) is a variant of Adam that aims to improve training stability and generalization performance. It was introduced in 2020 and has shown promising results across various deep learning tasks.

### Motivation

Adam and other adaptive methods can sometimes exhibit the following issues:
1. Training instability
2. Poor generalization
3. Sensitivity to hyperparameters
4. Slow convergence in some scenarios

AdaBelief addresses these issues by modifying how the second-order moment is calculated, focusing on the "belief" in the current gradient estimate.

### Algorithm

AdaBelief follows a similar structure to Adam but with a key difference in how it updates the second-order moment:

$$g_t = \nabla_\theta J(\theta_{t-1})$$
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad \text{(first moment estimate)}$$
$$s_t = \beta_2 s_{t-1} + (1 - \beta_2) (g_t - m_t)^2 \quad \text{(second moment estimate)}$$
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{s}_t = \frac{s_t}{1 - \beta_2^t}$$
$$\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{s}_t} + \epsilon}$$

The key difference is in the second moment update: instead of using $g_t^2$ (as in Adam), AdaBelief uses $(g_t - m_t)^2$, which represents the squared difference between the current gradient and the running average of gradients.

This can be interpreted as the "belief" in the current gradient estimate: if the current gradient is close to the running average, the update will be larger; if it deviates significantly, the update will be smaller.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add bias term to X
X_train_b = np.c_[np.ones(X_train.shape[0]), X_train]
X_test_b = np.c_[np.ones(X_test.shape[0]), X_test]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Implement Adam optimizer
def adam(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
         beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform Adam optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Implement AdaBelief optimizer
def adabelief(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
             beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Perform AdaBelief optimization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    s_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate (AdaBelief modification)
        s_t = beta2 * s_t + (1 - beta2) * (gradient - m_t)**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        s_t_hat = s_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(s_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Compare Adam and AdaBelief
learning_rate = 0.01
num_iterations = 500

# Run Adam
theta_adam, train_cost_adam, test_cost_adam = adam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Run AdaBelief
theta_adabelief, train_cost_adabelief, test_cost_adabelief = adabelief(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot training cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(train_cost_adam)), train_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(train_cost_adabelief)), train_cost_adabelief, linewidth=2, label='AdaBelief')
plt.xlabel('Iteration')
plt.ylabel('Training Cost')
plt.title('Training Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot test cost over iterations
plt.subplot(2, 2, 2)
plt.plot(range(len(test_cost_adam)), test_cost_adam, linewidth=2, label='Adam')
plt.plot(range(len(test_cost_adabelief)), test_cost_adabelief, linewidth=2, label='AdaBelief')
plt.xlabel('Iteration')
plt.ylabel('Test Cost')
plt.title('Test Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot parameter L2 norm over iterations
plt.subplot(2, 2, 3)
adam_norms = [np.linalg.norm(theta[1:]) for theta in theta_adam]  # Exclude bias term
adabelief_norms = [np.linalg.norm(theta[1:]) for theta in theta_adabelief]  # Exclude bias term

plt.plot(range(len(adam_norms)), adam_norms, linewidth=2, label='Adam')
plt.plot(range(len(adabelief_norms)), adabelief_norms, linewidth=2, label='AdaBelief')
plt.xlabel('Iteration')
plt.ylabel('Parameter L2 Norm (excluding bias)')
plt.title('Parameter L2 Norm vs. Iteration')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
adam_updates = np.diff(theta_adam, axis=0)
adabelief_updates = np.diff(theta_adabelief, axis=0)

adam_update_norms = np.linalg.norm(adam_updates, axis=1)
adabelief_update_norms = np.linalg.norm(adabelief_updates, axis=1)

plt.semilogy(range(len(adam_update_norms)), adam_update_norms, linewidth=2, label='Adam')
plt.semilogy(range(len(adabelief_update_norms)), adabelief_update_norms, linewidth=2, label='AdaBelief')
plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### AdaBelief on Non-Convex Functions

Let's see how AdaBelief performs on a challenging non-convex function compared to Adam:

```python
# Define a challenging non-convex function
def rastrigin_function(theta):
    """
    Rastrigin function: f(x,y) = 20 + x² + y² - 10(cos(2πx) + cos(2πy))
    Global minimum at (0,0)
    """
    A = 10
    n = len(theta)
    return A * n + np.sum(theta**2 - A * np.cos(2 * np.pi * theta))

def rastrigin_gradient(theta):
    """Gradient of the Rastrigin function."""
    A = 10
    return 2 * theta + 2 * np.pi * A * np.sin(2 * np.pi * theta)

# Implement generic optimizer function
def optimize(func, grad_func, start_point, optimizer, **kwargs):
    """
    Perform optimization using the specified optimizer.
    
    Parameters:
    - func: Objective function
    - grad_func: Gradient function
    - start_point: Initial parameter values
    - optimizer: Optimization algorithm ('adam' or 'adabelief')
    - **kwargs: Additional parameters for the optimizer
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    theta = start_point.copy()
    theta_history = [theta.copy()]
    cost_history = [func(theta)]
    
    learning_rate = kwargs.get('learning_rate', 0.01)
    num_iterations = kwargs.get('num_iterations', 1000)
    beta1 = kwargs.get('beta1', 0.9)
    beta2 = kwargs.get('beta2', 0.999)
    epsilon = kwargs.get('epsilon', 1e-8)
    
    # Initialize moment estimates
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)  # For Adam
    s = np.zeros_like(theta)  # For AdaBelief
    
    for t in range(1, num_iterations + 1):
        gradient = grad_func(theta)
        
        # Update biased first moment estimate
        m = beta1 * m + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        if optimizer == 'adam':
            v = beta2 * v + (1 - beta2) * gradient**2
            second_moment = v
        elif optimizer == 'adabelief':
            s = beta2 * s + (1 - beta2) * (gradient - m)**2
            second_moment = s
        
        # Compute bias-corrected first moment estimate
        m_hat = m / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        second_moment_hat = second_moment / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_hat / (np.sqrt(second_moment_hat) + epsilon)
        
        theta_history.append(theta.copy())
        cost_history.append(func(theta))
    
    return np.array(theta_history), np.array(cost_history)

# Run optimizers on the Rastrigin function
start_point_rastrigin = np.array([2.0, 3.0])
num_iterations = 1000
learning_rate = 0.001

results_rastrigin = []
results_rastrigin.append(('Adam', *optimize(rastrigin_function, rastrigin_gradient, 
                                           start_point_rastrigin, 'adam', 
                                           learning_rate=learning_rate, 
                                           num_iterations=num_iterations)))
results_rastrigin.append(('AdaBelief', *optimize(rastrigin_function, rastrigin_gradient, 
                                                start_point_rastrigin, 'adabelief', 
                                                learning_rate=learning_rate, 
                                                num_iterations=num_iterations)))

# Visualize the results for Rastrigin function
plt.figure(figsize=(15, 10))

# Create a grid for contour plot
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(len(x)):
    for j in range(len(y)):
        Z[j, i] = rastrigin_function(np.array([X[j, i], Y[j, i]]))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(X, Y, Z, levels=np.linspace(0, 50, 10), cmap='viridis')
plt.colorbar(contour)

for name, theta_history, cost_history in results_rastrigin:
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'o-', linewidth=1, markersize=2, label=name)

plt.scatter(0, 0, color='red', s=100, label='Global Minimum')
plt.scatter(start_point_rastrigin[0], start_point_rastrigin[1], color='green', s=100, label='Start')
plt.xlabel('θ₀')
plt.ylabel('θ₁')
plt.title('Trajectories on Rastrigin Function')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for name, theta_history, cost_history in results_rastrigin:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot distance to minimum
plt.subplot(2, 2, 3)
minimum = np.array([0, 0])  # Global minimum of Rastrigin function
for name, theta_history, cost_history in results_rastrigin:
    distances = np.sqrt(np.sum((theta_history - minimum)**2, axis=1))
    plt.semilogy(range(len(distances)), distances, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Distance to Minimum (log scale)')
plt.title('Convergence to Minimum')
plt.legend()
plt.grid(True)

# Plot update magnitudes
plt.subplot(2, 2, 4)
for name, theta_history, cost_history in results_rastrigin:
    updates = np.diff(theta_history, axis=0)
    update_norms = np.linalg.norm(updates, axis=1)
    plt.semilogy(range(len(update_norms)), update_norms, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('Update Magnitude (log scale)')
plt.title('Update Magnitude vs. Iteration')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Advantages of AdaBelief

1. **Improved Stability**:
   - More stable training, especially for complex models
   - Less sensitive to hyperparameter choices
   - Smoother convergence behavior

2. **Better Generalization**:
   - Often results in models that generalize better to unseen data
   - Reduces overfitting compared to Adam in some cases

3. **Faster Convergence**:
   - Can converge faster than Adam for certain problems
   - Particularly effective for non-convex optimization

4. **Adaptive Step Sizes**:
   - Adapts step sizes based on the "belief" in the current gradient
   - Larger steps when gradients are consistent, smaller steps when they fluctuate

## When to Use AdaBelief

AdaBelief is particularly beneficial in the following scenarios:

1. **Deep Neural Networks**: When training complex models where stability is a concern

2. **Non-convex Optimization**: For problems with many local minima

3. **Generalization Concerns**: When the model is overfitting or generalization is a priority

4. **Noisy Gradients**: When dealing with noisy or stochastic gradients

## Summary

AdaBelief is a recent variant of Adam that modifies how the second-order moment is calculated, focusing on the "belief" in the current gradient estimate. This modification leads to improved stability, better generalization, and faster convergence in many cases.

Key points about AdaBelief:
- Uses $(g_t - m_t)^2$ instead of $g_t^2$ for the second moment update
- Adapts step sizes based on the consistency of gradients
- More stable training compared to Adam
- Often results in better generalization
- Retains the benefits of adaptive learning rates

AdaBelief is a promising optimizer that addresses some of the limitations of Adam while maintaining its advantages.

## References

1. Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., & Duncan, J. (2020). AdaBelief Optimizer: Adapting stepsizes by the belief in observed gradients. In Advances in Neural Information Processing Systems.
2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
3. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. In International Conference on Learning Representations.
4. Reddi, S. J., Kale, S., & Kumar, S. (2018). On the convergence of Adam and beyond. In International Conference on Learning Representations.
