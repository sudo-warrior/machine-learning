# 3.2.2 Features - Part 4A: Feature Transformation - Scaling

## Feature Transformation

**Feature transformation** modifies existing features to improve their properties for machine learning algorithms. These transformations can normalize distributions, reduce skewness, and make features more suitable for specific algorithms.

### Why Transform Features?

Feature transformation offers several benefits:

1. **Algorithm Requirements**: Some algorithms require features to be on a similar scale
2. **Convergence Speed**: Properly scaled features can lead to faster convergence in optimization
3. **Numerical Stability**: Avoid numerical issues with very large or small values
4. **Distribution Improvement**: Make feature distributions more suitable for modeling

### Feature Scaling

**Feature scaling** ensures that features are on a similar scale, which is important for many algorithms, especially those that use distance metrics or gradient-based optimization.

#### Min-Max Scaling

**Min-Max Scaling** (also called normalization) scales features to a fixed range, typically [0, 1]:

$$X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a few features to visualize
selected_features = ['RM', 'AGE', 'DIS', 'TAX']
X_selected = df[selected_features].values

# Apply Min-Max scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_selected)

# Create a DataFrame with scaled features
df_scaled = pd.DataFrame(X_scaled, columns=selected_features)

# Display original and scaled statistics
print("Original features statistics:")
print(df[selected_features].describe())
print("\nMin-Max scaled features statistics:")
print(df_scaled.describe())

# Visualize original vs. scaled features
plt.figure(figsize=(15, 10))

# Original features
plt.subplot(2, 2, 1)
plt.boxplot(X_selected)
plt.title('Original Features')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Scaled features
plt.subplot(2, 2, 2)
plt.boxplot(X_scaled)
plt.title('Min-Max Scaled Features')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Scaled Value')
plt.grid(True, alpha=0.3)

# Distribution of original features
plt.subplot(2, 2, 3)
for i, feature in enumerate(selected_features):
    plt.hist(X_selected[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Original Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Distribution of scaled features
plt.subplot(2, 2, 4)
for i, feature in enumerate(selected_features):
    plt.hist(X_scaled[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Min-Max Scaled Features')
plt.xlabel('Scaled Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Standardization (Z-score Normalization)

**Standardization** scales features to have zero mean and unit variance:

$$X_{scaled} = \frac{X - \mu}{\sigma}$$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a few features to visualize
selected_features = ['RM', 'AGE', 'DIS', 'TAX']
X_selected = df[selected_features].values

# Apply standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Create a DataFrame with scaled features
df_scaled = pd.DataFrame(X_scaled, columns=selected_features)

# Display original and scaled statistics
print("Original features statistics:")
print(df[selected_features].describe())
print("\nStandardized features statistics:")
print(df_scaled.describe())

# Visualize original vs. scaled features
plt.figure(figsize=(15, 10))

# Original features
plt.subplot(2, 2, 1)
plt.boxplot(X_selected)
plt.title('Original Features')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Scaled features
plt.subplot(2, 2, 2)
plt.boxplot(X_scaled)
plt.title('Standardized Features')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Scaled Value')
plt.grid(True, alpha=0.3)

# Distribution of original features
plt.subplot(2, 2, 3)
for i, feature in enumerate(selected_features):
    plt.hist(X_selected[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Original Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Distribution of scaled features
plt.subplot(2, 2, 4)
for i, feature in enumerate(selected_features):
    plt.hist(X_scaled[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Standardized Features')
plt.xlabel('Scaled Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Robust Scaling

**Robust Scaling** uses the median and interquartile range (IQR) instead of mean and standard deviation, making it robust to outliers:

$$X_{scaled} = \frac{X - \text{median}(X)}{\text{IQR}(X)}$$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a few features to visualize
selected_features = ['RM', 'AGE', 'DIS', 'TAX']
X_selected = df[selected_features].values

# Add some outliers to demonstrate robust scaling
X_with_outliers = X_selected.copy()
X_with_outliers[0, :] = X_with_outliers[0, :] * 5  # Multiply first row by 5
X_with_outliers[1, :] = X_with_outliers[1, :] * 10  # Multiply second row by 10

# Apply standard and robust scaling
standard_scaler = StandardScaler()
robust_scaler = RobustScaler()

X_standard = standard_scaler.fit_transform(X_with_outliers)
X_robust = robust_scaler.fit_transform(X_with_outliers)

# Create DataFrames with scaled features
df_standard = pd.DataFrame(X_standard, columns=selected_features)
df_robust = pd.DataFrame(X_robust, columns=selected_features)

# Display statistics
print("Data with outliers statistics:")
print(pd.DataFrame(X_with_outliers, columns=selected_features).describe())
print("\nStandard scaled statistics:")
print(df_standard.describe())
print("\nRobust scaled statistics:")
print(df_robust.describe())

# Visualize the scaling methods
plt.figure(figsize=(15, 10))

# Original data with outliers
plt.subplot(2, 2, 1)
plt.boxplot(X_with_outliers)
plt.title('Original Features with Outliers')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Standard scaling
plt.subplot(2, 2, 2)
plt.boxplot(X_standard)
plt.title('Standard Scaling')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Scaled Value')
plt.grid(True, alpha=0.3)

# Robust scaling
plt.subplot(2, 2, 3)
plt.boxplot(X_robust)
plt.title('Robust Scaling')
plt.xticks(range(1, len(selected_features) + 1), selected_features)
plt.ylabel('Scaled Value')
plt.grid(True, alpha=0.3)

# Compare scaling methods for a single feature
feature_idx = 0  # First feature (RM)
plt.subplot(2, 2, 4)
plt.hist(X_with_outliers[:, feature_idx], alpha=0.5, bins=30, label='Original')
plt.hist(X_standard[:, feature_idx], alpha=0.5, bins=30, label='Standard Scaled')
plt.hist(X_robust[:, feature_idx], alpha=0.5, bins=30, label='Robust Scaled')
plt.title(f'Scaling Comparison for {selected_features[feature_idx]}')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Max Absolute Scaling

**Max Absolute Scaling** scales features by their maximum absolute value:

$$X_{scaled} = \frac{X}{\max(|X|)}$$

This preserves the sign of the values and scales them to the range [-1, 1].

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MaxAbsScaler
from sklearn.datasets import make_classification

# Generate synthetic data with positive and negative values
np.random.seed(42)
X, _ = make_classification(n_samples=1000, n_features=4, random_state=42)
X = X - np.mean(X, axis=0)  # Center the data to have both positive and negative values

# Create a DataFrame
feature_names = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']
df = pd.DataFrame(X, columns=feature_names)

# Apply Max Absolute scaling
scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X)
df_scaled = pd.DataFrame(X_scaled, columns=feature_names)

# Display statistics
print("Original features statistics:")
print(df.describe())
print("\nMax Absolute scaled features statistics:")
print(df_scaled.describe())

# Visualize original vs. scaled features
plt.figure(figsize=(15, 10))

# Original features
plt.subplot(2, 2, 1)
plt.boxplot(X)
plt.title('Original Features')
plt.xticks(range(1, len(feature_names) + 1), feature_names)
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Scaled features
plt.subplot(2, 2, 2)
plt.boxplot(X_scaled)
plt.title('Max Absolute Scaled Features')
plt.xticks(range(1, len(feature_names) + 1), feature_names)
plt.ylabel('Scaled Value')
plt.grid(True, alpha=0.3)

# Distribution of original features
plt.subplot(2, 2, 3)
for i, feature in enumerate(feature_names):
    plt.hist(X[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Original Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# Distribution of scaled features
plt.subplot(2, 2, 4)
for i, feature in enumerate(feature_names):
    plt.hist(X_scaled[:, i], alpha=0.5, bins=30, label=feature)
plt.title('Distribution of Max Absolute Scaled Features')
plt.xlabel('Scaled Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Comparing Scaling Methods

Different scaling methods are appropriate for different situations:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a feature with outliers
feature = 'LSTAT'  # Lower status of the population
X_feature = df[feature].values.reshape(-1, 1)

# Add some outliers
X_with_outliers = X_feature.copy()
X_with_outliers[0] = X_with_outliers[0] * 5  # Multiply first value by 5
X_with_outliers[1] = X_with_outliers[1] * 10  # Multiply second value by 10

# Apply different scaling methods
scalers = {
    'Original': None,
    'Min-Max': MinMaxScaler(),
    'Standard': StandardScaler(),
    'Robust': RobustScaler(),
    'Max Absolute': MaxAbsScaler()
}

scaled_data = {}
for name, scaler in scalers.items():
    if scaler is None:
        scaled_data[name] = X_with_outliers
    else:
        scaled_data[name] = scaler.fit_transform(X_with_outliers)

# Visualize the scaling methods
plt.figure(figsize=(15, 10))

# Boxplots
plt.subplot(2, 1, 1)
boxplot_data = [scaled_data[name] for name in scalers.keys()]
plt.boxplot(boxplot_data, labels=scalers.keys())
plt.title('Comparison of Scaling Methods')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Histograms
plt.subplot(2, 1, 2)
for name, data in scaled_data.items():
    plt.hist(data, alpha=0.5, bins=30, label=name)
plt.title('Distribution After Scaling')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Create a table of statistics
stats = {}
for name, data in scaled_data.items():
    stats[name] = {
        'Min': np.min(data),
        'Max': np.max(data),
        'Mean': np.mean(data),
        'Median': np.median(data),
        'Std Dev': np.std(data)
    }

stats_df = pd.DataFrame(stats).T
print("Statistics after scaling:")
print(stats_df)
```

### When to Use Each Scaling Method

- **Min-Max Scaling**: When you need values in a specific range, like [0, 1]
- **Standardization**: When you need features centered around zero with unit variance
- **Robust Scaling**: When your data contains outliers
- **Max Absolute Scaling**: When you want to preserve the sign and scale to [-1, 1]

### Impact of Scaling on Algorithms

Different algorithms have different sensitivities to feature scaling:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42)
}

# Train and evaluate models with and without scaling
results = {}

for name, model in models.items():
    # Without scaling
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy_unscaled = accuracy_score(y_test, y_pred)
    
    # With scaling
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy_scaled = accuracy_score(y_test, y_pred)
    
    results[name] = {
        'Unscaled': accuracy_unscaled,
        'Scaled': accuracy_scaled,
        'Difference': accuracy_scaled - accuracy_unscaled
    }

# Convert results to DataFrame
results_df = pd.DataFrame(results).T
print("Impact of scaling on model accuracy:")
print(results_df)

# Visualize the results
plt.figure(figsize=(12, 6))
bar_width = 0.35
index = np.arange(len(models))

plt.bar(index, results_df['Unscaled'], bar_width, label='Unscaled', color='skyblue')
plt.bar(index + bar_width, results_df['Scaled'], bar_width, label='Scaled', color='lightgreen')

plt.title('Impact of Scaling on Model Accuracy')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(index + bar_width / 2, results_df.index, rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Visualize the difference
plt.figure(figsize=(12, 6))
plt.bar(results_df.index, results_df['Difference'], color=['green' if x > 0 else 'red' for x in results_df['Difference']])
plt.title('Accuracy Difference (Scaled - Unscaled)')
plt.xlabel('Model')
plt.ylabel('Accuracy Difference')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

## Summary

Feature scaling is an essential preprocessing step for many machine learning algorithms:

1. **Min-Max Scaling**:
   - Scales features to a fixed range, typically [0, 1]
   - Preserves the shape of the original distribution
   - Sensitive to outliers

2. **Standardization (Z-score Normalization)**:
   - Scales features to have zero mean and unit variance
   - Useful for algorithms that assume normally distributed data
   - Less sensitive to outliers than Min-Max scaling

3. **Robust Scaling**:
   - Uses median and IQR instead of mean and standard deviation
   - Robust to outliers
   - Useful for data with many outliers

4. **Max Absolute Scaling**:
   - Scales features by their maximum absolute value
   - Preserves the sign of the values
   - Scales to the range [-1, 1]

Different algorithms have different sensitivities to feature scaling:
- Distance-based algorithms (K-NN, SVM) are highly sensitive
- Tree-based algorithms (Decision Trees, Random Forests) are generally insensitive
- Gradient-based algorithms (Logistic Regression, Neural Networks) benefit from scaling

In the next part, we'll explore additional feature transformation techniques that focus on changing the distribution of features.

## References

1. Kuhn, M., & Johnson, K. (2019). Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.
2. GÃ©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd ed.). O'Reilly Media.
3. Raschka, S., & Mirjalili, V. (2019). Python Machine Learning (3rd ed.). Packt Publishing.
