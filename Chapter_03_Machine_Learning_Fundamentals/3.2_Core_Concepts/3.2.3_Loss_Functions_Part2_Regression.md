# 3.2.3 Loss Functions - Part 2: Regression Loss Functions

## Regression Loss Functions

Regression loss functions measure the difference between predicted continuous values and actual target values. They are used to train models that predict quantities like prices, temperatures, or any other numerical values.

### Mean Squared Error (MSE)

**Mean Squared Error (MSE)** is the most common loss function for regression problems:

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

where:
- $y_i$ is the true value
- $\hat{y}_i$ is the predicted value
- $n$ is the number of examples

**Properties**:
- Heavily penalizes large errors due to squaring
- Differentiable everywhere
- Sensitive to outliers
- Corresponds to maximum likelihood estimation under Gaussian noise assumption

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the MSE function
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Create a range of prediction errors
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)  # True value is 0
mse_values = mse(y_true, errors)

# Plot MSE
plt.figure(figsize=(10, 6))
plt.plot(errors, mse_values, 'b-', linewidth=2)
plt.title('Mean Squared Error')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Mean Absolute Error (MAE)

**Mean Absolute Error (MAE)** uses the absolute difference between predictions and true values:

$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

**Properties**:
- Less sensitive to outliers than MSE
- Not differentiable at zero
- Corresponds to maximum likelihood estimation under Laplace noise assumption
- Produces median predictions (compared to MSE which produces mean predictions)

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the MAE function
def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# Create a range of prediction errors
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)  # True value is 0
mae_values = mae(y_true, errors)

# Plot MAE
plt.figure(figsize=(10, 6))
plt.plot(errors, mae_values, 'r-', linewidth=2)
plt.title('Mean Absolute Error')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Huber Loss

**Huber Loss** combines the best properties of MSE and MAE:

$$\text{Huber}_{\delta}(y, \hat{y}) = 
\begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \leq \delta \\
\delta(|y - \hat{y}| - \frac{\delta}{2}) & \text{otherwise}
\end{cases}$$

where $\delta$ is a hyperparameter that controls the transition point.

**Properties**:
- Behaves like MSE for small errors
- Behaves like MAE for large errors
- Less sensitive to outliers than MSE
- Differentiable everywhere
- Requires tuning of the $\delta$ parameter

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Huber loss function
def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    is_small_error = np.abs(error) <= delta
    squared_loss = 0.5 * error ** 2
    linear_loss = delta * (np.abs(error) - 0.5 * delta)
    return np.where(is_small_error, squared_loss, linear_loss)

# Create a range of prediction errors
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)  # True value is 0

# Calculate losses for different delta values
delta_values = [0.5, 1.0, 2.0, 4.0]
huber_values = {}

for delta in delta_values:
    huber_values[delta] = huber_loss(y_true, errors, delta)

# Plot Huber loss for different delta values
plt.figure(figsize=(12, 6))
for delta, values in huber_values.items():
    plt.plot(errors, values, label=f'δ = {delta}')

plt.title('Huber Loss for Different δ Values')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Log-cosh Loss

**Log-cosh Loss** is the logarithm of the hyperbolic cosine of the prediction error:

$$\text{Log-cosh} = \frac{1}{n} \sum_{i=1}^{n} \log(\cosh(y_i - \hat{y}_i))$$

where $\cosh(x) = \frac{e^x + e^{-x}}{2}$

**Properties**:
- Behaves like MSE for small errors
- Behaves like MAE for large errors
- Twice differentiable everywhere
- Less sensitive to outliers than MSE
- Smoother than Huber loss

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Log-cosh loss function
def log_cosh_loss(y_true, y_pred):
    return np.mean(np.log(np.cosh(y_pred - y_true)))

# Create a range of prediction errors
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)  # True value is 0
log_cosh_values = log_cosh_loss(y_true, errors)

# Plot Log-cosh loss
plt.figure(figsize=(10, 6))
plt.plot(errors, log_cosh_values, 'g-', linewidth=2)
plt.title('Log-cosh Loss')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Quantile Loss

**Quantile Loss** is used for quantile regression, where the goal is to predict a specific quantile of the target distribution:

$$\text{Quantile}_{\tau}(y, \hat{y}) = 
\begin{cases}
\tau \cdot (y - \hat{y}) & \text{if } y - \hat{y} \geq 0 \\
(1 - \tau) \cdot (\hat{y} - y) & \text{if } y - \hat{y} < 0
\end{cases}$$

where $\tau$ is the target quantile (e.g., 0.5 for median).

**Properties**:
- Asymmetric loss function
- When $\tau = 0.5$, it's equivalent to MAE
- Useful for prediction intervals and uncertainty estimation
- Not differentiable at zero

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Quantile loss function
def quantile_loss(y_true, y_pred, quantile=0.5):
    error = y_true - y_pred
    return np.mean(np.maximum(quantile * error, (quantile - 1) * error))

# Create a range of prediction errors
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)  # True value is 0

# Calculate losses for different quantiles
quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]
quantile_values = {}

for q in quantiles:
    quantile_values[q] = quantile_loss(y_true, errors, q)

# Plot Quantile loss for different quantiles
plt.figure(figsize=(12, 6))
for q, values in quantile_values.items():
    plt.plot(errors, values, label=f'τ = {q}')

plt.title('Quantile Loss for Different τ Values')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

## Comparing Regression Loss Functions

Let's compare the different regression loss functions to understand their behavior:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, HuberRegressor, QuantileRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Generate synthetic data with outliers
np.random.seed(42)
X = np.sort(np.random.uniform(0, 10, 100))[:, np.newaxis]
y = 2 * X.ravel() + 1 + np.random.normal(0, 1, 100)
# Add outliers
y[75:80] = y[75:80] + 10
y[85:90] = y[85:90] - 10

# Split data into train and test
X_train, X_test = X[:80], X[80:]
y_train, y_test = y[:80], y[80:]

# Train models with different loss functions
# MSE (Linear Regression)
mse_model = LinearRegression()
mse_model.fit(X_train, y_train)
y_pred_mse = mse_model.predict(X_test)
mse_error = mean_squared_error(y_test, y_pred_mse)

# MAE (using Quantile Regressor with quantile=0.5)
mae_model = QuantileRegressor(quantile=0.5, alpha=0)
mae_model.fit(X_train, y_train)
y_pred_mae = mae_model.predict(X_test)
mae_error = mean_absolute_error(y_test, y_pred_mae)

# Huber
huber_model = HuberRegressor(epsilon=1.35)
huber_model.fit(X_train, y_train)
y_pred_huber = huber_model.predict(X_test)
huber_error = mean_squared_error(y_test, y_pred_huber)

# Quantile (q=0.1, q=0.9)
q10_model = QuantileRegressor(quantile=0.1, alpha=0)
q10_model.fit(X_train, y_train)
y_pred_q10 = q10_model.predict(X_test)

q90_model = QuantileRegressor(quantile=0.9, alpha=0)
q90_model.fit(X_train, y_train)
y_pred_q90 = q90_model.predict(X_test)

# Visualize the results
plt.figure(figsize=(12, 8))

# Plot the data
plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')

# Plot the predictions
X_plot = np.linspace(0, 10, 100)[:, np.newaxis]
plt.plot(X_plot, mse_model.predict(X_plot), 'r-', linewidth=2, label=f'MSE (Linear Regression), MSE={mse_error:.2f}')
plt.plot(X_plot, mae_model.predict(X_plot), 'g-', linewidth=2, label=f'MAE (Quantile τ=0.5), MAE={mae_error:.2f}')
plt.plot(X_plot, huber_model.predict(X_plot), 'y-', linewidth=2, label=f'Huber, MSE={huber_error:.2f}')
plt.plot(X_plot, q10_model.predict(X_plot), 'k--', linewidth=1, label='Quantile τ=0.1')
plt.plot(X_plot, q90_model.predict(X_plot), 'k--', linewidth=1, label='Quantile τ=0.9')

plt.title('Comparison of Regression Models with Different Loss Functions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Compare loss functions directly
errors = np.linspace(-10, 10, 1000)
y_true = np.zeros_like(errors)

mse_values = (errors) ** 2
mae_values = np.abs(errors)
huber_values = huber_loss(y_true, errors, delta=1.0)
log_cosh_values = np.log(np.cosh(errors))

plt.figure(figsize=(12, 6))
plt.plot(errors, mse_values, 'r-', label='MSE')
plt.plot(errors, mae_values, 'g-', label='MAE')
plt.plot(errors, huber_values, 'b-', label='Huber (δ=1.0)')
plt.plot(errors, log_cosh_values, 'y-', label='Log-cosh')

plt.title('Comparison of Regression Loss Functions')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

## When to Use Each Regression Loss Function

The choice of regression loss function depends on the specific problem and data characteristics:

### Mean Squared Error (MSE)
- **Use when**: The target distribution is approximately Gaussian
- **Use when**: Large errors should be penalized more heavily
- **Use when**: You want to predict the mean of the target distribution
- **Avoid when**: Data contains significant outliers

### Mean Absolute Error (MAE)
- **Use when**: The target distribution has heavy tails
- **Use when**: Outliers are present in the data
- **Use when**: You want to predict the median of the target distribution
- **Avoid when**: You need a smooth loss function for optimization

### Huber Loss
- **Use when**: You want a balance between MSE and MAE
- **Use when**: Data contains some outliers, but not too many
- **Use when**: You need a differentiable loss function
- **Avoid when**: You don't want to tune an additional hyperparameter (δ)

### Log-cosh Loss
- **Use when**: You want a smooth approximation of MAE
- **Use when**: You need second derivatives for optimization
- **Use when**: Data contains some outliers
- **Avoid when**: Computational efficiency is a concern

### Quantile Loss
- **Use when**: You want to predict a specific quantile (not just the median)
- **Use when**: You need prediction intervals
- **Use when**: The cost of over-prediction and under-prediction is asymmetric
- **Avoid when**: You need a differentiable loss function

## Summary

Regression loss functions measure the difference between predicted and actual continuous values:

1. **Mean Squared Error (MSE)**:
   - Squares the differences
   - Heavily penalizes large errors
   - Sensitive to outliers

2. **Mean Absolute Error (MAE)**:
   - Uses absolute differences
   - Less sensitive to outliers
   - Not differentiable at zero

3. **Huber Loss**:
   - Combines MSE and MAE
   - Behaves like MSE for small errors and MAE for large errors
   - Requires tuning of the δ parameter

4. **Log-cosh Loss**:
   - Smooth approximation of MAE
   - Twice differentiable everywhere
   - Less sensitive to outliers than MSE

5. **Quantile Loss**:
   - Asymmetric loss function
   - Used for quantile regression
   - Useful for prediction intervals

The choice of loss function should be based on the specific problem requirements, the distribution of the target variable, and the presence of outliers in the data.

## References

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
2. Koenker, R. (2005). Quantile Regression. Cambridge University Press.
3. Huber, P. J. (1964). Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics, 35(1), 73-101.
