# 3.2.5 Model Evaluation - Part 3: Clustering Metrics

## Clustering Metrics

Clustering metrics evaluate the performance of clustering algorithms, which group similar data points together. Unlike supervised learning metrics, clustering metrics often don't require ground truth labels, though some can use them when available.

### Internal Metrics

Internal metrics evaluate clustering performance using only the data and clustering results, without external labels.

#### Silhouette Coefficient

The **Silhouette Coefficient** measures how similar an object is to its own cluster compared to other clusters:

$$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$

where:
- $a(i)$ is the average distance between point $i$ and all other points in the same cluster
- $b(i)$ is the average distance between point $i$ and all points in the nearest cluster

The Silhouette Coefficient ranges from -1 to 1, where:
- 1 indicates points are well-matched to their clusters and far from neighboring clusters
- 0 indicates points are on or very close to the decision boundary between clusters
- -1 indicates points may have been assigned to the wrong cluster

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_moons
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.preprocessing import StandardScaler

# Generate synthetic data with well-defined clusters
X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# Apply K-means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
y_kmeans = kmeans.fit_predict(X_blobs)

# Calculate silhouette score
silhouette_avg = silhouette_score(X_blobs, y_kmeans)
print(f"Silhouette Score (K-means): {silhouette_avg:.4f}")

# Calculate silhouette values for each sample
silhouette_values = silhouette_samples(X_blobs, y_kmeans)

# Visualize the clusters and silhouette plot
plt.figure(figsize=(15, 6))

# Plot the clusters
plt.subplot(1, 2, 1)
plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=100)
plt.title(f'K-means Clustering (Silhouette Score: {silhouette_avg:.4f})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

# Create silhouette plot
plt.subplot(1, 2, 2)
y_lower = 10

# Sort the silhouette values by cluster
for i in range(4):
    # Get silhouette values for current cluster
    ith_cluster_silhouette_values = silhouette_values[y_kmeans == i]
    ith_cluster_silhouette_values.sort()
    
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = plt.cm.viridis(float(i) / 4)
    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,
                     facecolor=color, edgecolor=color, alpha=0.7)
    
    # Label the silhouette plots with their cluster numbers
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    
    # Compute the new y_lower for next plot
    y_lower = y_upper + 10

plt.title('Silhouette Plot')
plt.xlabel('Silhouette Coefficient')
plt.ylabel('Cluster')
plt.axvline(x=silhouette_avg, color='red', linestyle='--', label=f'Average: {silhouette_avg:.4f}')
plt.yticks([])
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Generate data with non-spherical clusters
X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42)

# Apply K-means and DBSCAN
kmeans_moons = KMeans(n_clusters=2, random_state=42)
y_kmeans_moons = kmeans_moons.fit_predict(X_moons)

dbscan = DBSCAN(eps=0.3, min_samples=5)
y_dbscan = dbscan.fit_predict(X_moons)

# Calculate silhouette scores
silhouette_kmeans = silhouette_score(X_moons, y_kmeans_moons)
silhouette_dbscan = silhouette_score(X_moons, y_dbscan)

print(f"Silhouette Score (K-means on moons): {silhouette_kmeans:.4f}")
print(f"Silhouette Score (DBSCAN on moons): {silhouette_dbscan:.4f}")

# Visualize the clusters
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_kmeans_moons, cmap='viridis', alpha=0.7)
plt.scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1], c='red', marker='X', s=100)
plt.title(f'K-means on Moons (Silhouette: {silhouette_kmeans:.4f})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_dbscan, cmap='viridis', alpha=0.7)
plt.title(f'DBSCAN on Moons (Silhouette: {silhouette_dbscan:.4f})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Davies-Bouldin Index

The **Davies-Bouldin Index** measures the average similarity between clusters, where similarity is the ratio of within-cluster distances to between-cluster distances:

$$DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left( \frac{s_i + s_j}{d_{ij}} \right)$$

where:
- $k$ is the number of clusters
- $s_i$ is the average distance between each point in cluster $i$ and the centroid of cluster $i$
- $d_{ij}$ is the distance between the centroids of clusters $i$ and $j$

Lower values indicate better clustering, with 0 being the lowest possible score.

```python
from sklearn.metrics import davies_bouldin_score

# Calculate Davies-Bouldin Index for K-means on blobs
db_kmeans = davies_bouldin_score(X_blobs, y_kmeans)
print(f"Davies-Bouldin Index (K-means on blobs): {db_kmeans:.4f}")

# Calculate Davies-Bouldin Index for K-means and DBSCAN on moons
db_kmeans_moons = davies_bouldin_score(X_moons, y_kmeans_moons)
db_dbscan = davies_bouldin_score(X_moons, y_dbscan)

print(f"Davies-Bouldin Index (K-means on moons): {db_kmeans_moons:.4f}")
print(f"Davies-Bouldin Index (DBSCAN on moons): {db_dbscan:.4f}")

# Visualize Davies-Bouldin Index for different numbers of clusters
plt.figure(figsize=(10, 6))
db_scores = []
silhouette_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_blobs)
    db_scores.append(davies_bouldin_score(X_blobs, labels))
    silhouette_scores.append(silhouette_score(X_blobs, labels))

plt.plot(k_values, db_scores, 'bo-', linewidth=2, label='Davies-Bouldin Index')
plt.plot(k_values, silhouette_scores, 'ro-', linewidth=2, label='Silhouette Score')
plt.title('Davies-Bouldin Index and Silhouette Score vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(8, 0.7, "Lower DB Index is better", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(8, 0.6, "Higher Silhouette Score is better", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Mark the optimal number of clusters
optimal_k_db = k_values[np.argmin(db_scores)]
optimal_k_silhouette = k_values[np.argmax(silhouette_scores)]

plt.axvline(x=optimal_k_db, color='blue', linestyle='--', alpha=0.5, label=f'Optimal k (DB): {optimal_k_db}')
plt.axvline(x=optimal_k_silhouette, color='red', linestyle='--', alpha=0.5, label=f'Optimal k (Silhouette): {optimal_k_silhouette}')
plt.legend()

plt.tight_layout()
plt.show()
```

#### Calinski-Harabasz Index

The **Calinski-Harabasz Index** (also known as the Variance Ratio Criterion) measures the ratio of between-cluster dispersion to within-cluster dispersion:

$$CH = \frac{SS_B}{SS_W} \times \frac{N - k}{k - 1}$$

where:
- $SS_B$ is the between-cluster dispersion
- $SS_W$ is the within-cluster dispersion
- $N$ is the number of data points
- $k$ is the number of clusters

Higher values indicate better clustering.

```python
from sklearn.metrics import calinski_harabasz_score

# Calculate Calinski-Harabasz Index for K-means on blobs
ch_kmeans = calinski_harabasz_score(X_blobs, y_kmeans)
print(f"Calinski-Harabasz Index (K-means on blobs): {ch_kmeans:.4f}")

# Calculate Calinski-Harabasz Index for K-means and DBSCAN on moons
ch_kmeans_moons = calinski_harabasz_score(X_moons, y_kmeans_moons)
ch_dbscan = calinski_harabasz_score(X_moons, y_dbscan)

print(f"Calinski-Harabasz Index (K-means on moons): {ch_kmeans_moons:.4f}")
print(f"Calinski-Harabasz Index (DBSCAN on moons): {ch_dbscan:.4f}")

# Visualize Calinski-Harabasz Index for different numbers of clusters
plt.figure(figsize=(10, 6))
ch_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_blobs)
    ch_scores.append(calinski_harabasz_score(X_blobs, labels))

plt.plot(k_values, ch_scores, 'go-', linewidth=2)
plt.title('Calinski-Harabasz Index vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Calinski-Harabasz Index')
plt.xticks(k_values)
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(8, max(ch_scores) * 0.7, "Higher values indicate better clustering", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Mark the optimal number of clusters
optimal_k_ch = k_values[np.argmax(ch_scores)]
plt.axvline(x=optimal_k_ch, color='green', linestyle='--', alpha=0.5, label=f'Optimal k: {optimal_k_ch}')
plt.legend()

plt.tight_layout()
plt.show()

# Compare all three internal metrics
plt.figure(figsize=(12, 6))

# Normalize scores for comparison
db_scores_norm = [1 - (score / max(db_scores)) for score in db_scores]  # Invert and normalize DB scores
silhouette_scores_norm = [(score - min(silhouette_scores)) / (max(silhouette_scores) - min(silhouette_scores)) for score in silhouette_scores]
ch_scores_norm = [(score - min(ch_scores)) / (max(ch_scores) - min(ch_scores)) for score in ch_scores]

plt.plot(k_values, db_scores_norm, 'bo-', linewidth=2, label='Davies-Bouldin (normalized)')
plt.plot(k_values, silhouette_scores_norm, 'ro-', linewidth=2, label='Silhouette (normalized)')
plt.plot(k_values, ch_scores_norm, 'go-', linewidth=2, label='Calinski-Harabasz (normalized)')
plt.title('Comparison of Internal Clustering Metrics')
plt.xlabel('Number of Clusters')
plt.ylabel('Normalized Score (higher is better)')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

# Mark the optimal number of clusters for each metric
plt.axvline(x=optimal_k_db, color='blue', linestyle='--', alpha=0.5)
plt.axvline(x=optimal_k_silhouette, color='red', linestyle='--', alpha=0.5)
plt.axvline(x=optimal_k_ch, color='green', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()
```

### External Metrics

External metrics evaluate clustering performance by comparing the clustering results with known ground truth labels.

#### Adjusted Rand Index (ARI)

The **Adjusted Rand Index (ARI)** measures the similarity between two clusterings, adjusted for chance:

$$ARI = \frac{RI - E[RI]}{\max(RI) - E[RI]}$$

where:
- $RI$ is the Rand Index
- $E[RI]$ is the expected Rand Index under random labeling

ARI ranges from -1 to 1, where:
- 1 indicates perfect agreement between the clusterings
- 0 indicates random labeling
- Negative values indicate agreement worse than random

```python
from sklearn.metrics import adjusted_rand_score

# Calculate ARI for K-means on blobs
ari_kmeans = adjusted_rand_score(y_blobs, y_kmeans)
print(f"Adjusted Rand Index (K-means on blobs): {ari_kmeans:.4f}")

# Calculate ARI for K-means and DBSCAN on moons
ari_kmeans_moons = adjusted_rand_score(y_moons, y_kmeans_moons)
ari_dbscan = adjusted_rand_score(y_moons, y_dbscan)

print(f"Adjusted Rand Index (K-means on moons): {ari_kmeans_moons:.4f}")
print(f"Adjusted Rand Index (DBSCAN on moons): {ari_dbscan:.4f}")

# Visualize ARI for different numbers of clusters
plt.figure(figsize=(10, 6))
ari_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_blobs)
    ari_scores.append(adjusted_rand_score(y_blobs, labels))

plt.plot(k_values, ari_scores, 'mo-', linewidth=2)
plt.title('Adjusted Rand Index vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Adjusted Rand Index')
plt.xticks(k_values)
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(8, 0.7, "Higher values indicate better agreement\nwith ground truth", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Mark the optimal number of clusters
optimal_k_ari = k_values[np.argmax(ari_scores)]
plt.axvline(x=optimal_k_ari, color='magenta', linestyle='--', alpha=0.5, label=f'Optimal k: {optimal_k_ari}')
plt.legend()

plt.tight_layout()
plt.show()
```

#### Normalized Mutual Information (NMI)

The **Normalized Mutual Information (NMI)** measures the mutual information between the cluster assignments and the ground truth labels, normalized by the entropy of each:

$$NMI(U, V) = \frac{2 \times I(U, V)}{H(U) + H(V)}$$

where:
- $I(U, V)$ is the mutual information between clusterings $U$ and $V$
- $H(U)$ and $H(V)$ are the entropies of $U$ and $V$

NMI ranges from 0 to 1, where:
- 1 indicates perfect agreement between the clusterings
- 0 indicates no mutual information between the clusterings

```python
from sklearn.metrics import normalized_mutual_info_score

# Calculate NMI for K-means on blobs
nmi_kmeans = normalized_mutual_info_score(y_blobs, y_kmeans)
print(f"Normalized Mutual Information (K-means on blobs): {nmi_kmeans:.4f}")

# Calculate NMI for K-means and DBSCAN on moons
nmi_kmeans_moons = normalized_mutual_info_score(y_moons, y_kmeans_moons)
nmi_dbscan = normalized_mutual_info_score(y_moons, y_dbscan)

print(f"Normalized Mutual Information (K-means on moons): {nmi_kmeans_moons:.4f}")
print(f"Normalized Mutual Information (DBSCAN on moons): {nmi_dbscan:.4f}")

# Visualize NMI for different numbers of clusters
plt.figure(figsize=(10, 6))
nmi_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_blobs)
    nmi_scores.append(normalized_mutual_info_score(y_blobs, labels))

plt.plot(k_values, nmi_scores, 'co-', linewidth=2)
plt.title('Normalized Mutual Information vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Normalized Mutual Information')
plt.xticks(k_values)
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(8, 0.7, "Higher values indicate better agreement\nwith ground truth", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Mark the optimal number of clusters
optimal_k_nmi = k_values[np.argmax(nmi_scores)]
plt.axvline(x=optimal_k_nmi, color='cyan', linestyle='--', alpha=0.5, label=f'Optimal k: {optimal_k_nmi}')
plt.legend()

plt.tight_layout()
plt.show()

# Compare external metrics
plt.figure(figsize=(12, 6))
plt.plot(k_values, ari_scores, 'mo-', linewidth=2, label='Adjusted Rand Index')
plt.plot(k_values, nmi_scores, 'co-', linewidth=2, label='Normalized Mutual Information')
plt.title('Comparison of External Clustering Metrics')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

# Mark the optimal number of clusters for each metric
plt.axvline(x=optimal_k_ari, color='magenta', linestyle='--', alpha=0.5)
plt.axvline(x=optimal_k_nmi, color='cyan', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()
```

#### Homogeneity, Completeness, and V-measure

These metrics evaluate different aspects of clustering quality:

- **Homogeneity**: Each cluster contains only members of a single class
- **Completeness**: All members of a given class are assigned to the same cluster
- **V-measure**: The harmonic mean of homogeneity and completeness

All three metrics range from 0 to 1, where higher values indicate better clustering.

```python
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score

# Calculate homogeneity, completeness, and V-measure for K-means on blobs
homogeneity_kmeans = homogeneity_score(y_blobs, y_kmeans)
completeness_kmeans = completeness_score(y_blobs, y_kmeans)
v_measure_kmeans = v_measure_score(y_blobs, y_kmeans)

print(f"Homogeneity (K-means on blobs): {homogeneity_kmeans:.4f}")
print(f"Completeness (K-means on blobs): {completeness_kmeans:.4f}")
print(f"V-measure (K-means on blobs): {v_measure_kmeans:.4f}")

# Calculate homogeneity, completeness, and V-measure for K-means and DBSCAN on moons
homogeneity_kmeans_moons = homogeneity_score(y_moons, y_kmeans_moons)
completeness_kmeans_moons = completeness_score(y_moons, y_kmeans_moons)
v_measure_kmeans_moons = v_measure_score(y_moons, y_kmeans_moons)

homogeneity_dbscan = homogeneity_score(y_moons, y_dbscan)
completeness_dbscan = completeness_score(y_moons, y_dbscan)
v_measure_dbscan = v_measure_score(y_moons, y_dbscan)

print(f"\nK-means on moons:")
print(f"  Homogeneity: {homogeneity_kmeans_moons:.4f}")
print(f"  Completeness: {completeness_kmeans_moons:.4f}")
print(f"  V-measure: {v_measure_kmeans_moons:.4f}")

print(f"\nDBSCAN on moons:")
print(f"  Homogeneity: {homogeneity_dbscan:.4f}")
print(f"  Completeness: {completeness_dbscan:.4f}")
print(f"  V-measure: {v_measure_dbscan:.4f}")

# Visualize homogeneity, completeness, and V-measure for different numbers of clusters
plt.figure(figsize=(12, 6))
homogeneity_scores = []
completeness_scores = []
v_measure_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_blobs)
    homogeneity_scores.append(homogeneity_score(y_blobs, labels))
    completeness_scores.append(completeness_score(y_blobs, labels))
    v_measure_scores.append(v_measure_score(y_blobs, labels))

plt.plot(k_values, homogeneity_scores, 'ro-', linewidth=2, label='Homogeneity')
plt.plot(k_values, completeness_scores, 'go-', linewidth=2, label='Completeness')
plt.plot(k_values, v_measure_scores, 'bo-', linewidth=2, label='V-measure')
plt.title('Homogeneity, Completeness, and V-measure vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(8, 0.4, "Homogeneity: Each cluster contains only members of a single class", ha='center', va='center', fontsize=8, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(8, 0.3, "Completeness: All members of a given class are in the same cluster", ha='center', va='center', fontsize=8, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(8, 0.2, "V-measure: Harmonic mean of homogeneity and completeness", ha='center', va='center', fontsize=8, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()

# Compare all external metrics
plt.figure(figsize=(12, 6))
plt.plot(k_values, ari_scores, 'mo-', linewidth=2, label='Adjusted Rand Index')
plt.plot(k_values, nmi_scores, 'co-', linewidth=2, label='Normalized Mutual Information')
plt.plot(k_values, v_measure_scores, 'bo-', linewidth=2, label='V-measure')
plt.title('Comparison of External Clustering Metrics')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

Clustering metrics help evaluate the performance of clustering algorithms:

### Internal Metrics (No Ground Truth Required)

1. **Silhouette Coefficient**:
   - Measures how similar an object is to its own cluster compared to other clusters
   - Ranges from -1 to 1, where higher values indicate better clustering
   - Useful for determining the optimal number of clusters

2. **Davies-Bouldin Index**:
   - Measures the average similarity between clusters
   - Lower values indicate better clustering
   - Sensitive to the shape of clusters

3. **Calinski-Harabasz Index**:
   - Measures the ratio of between-cluster dispersion to within-cluster dispersion
   - Higher values indicate better clustering
   - Works well for convex clusters

### External Metrics (Ground Truth Required)

1. **Adjusted Rand Index (ARI)**:
   - Measures the similarity between two clusterings, adjusted for chance
   - Ranges from -1 to 1, where higher values indicate better agreement
   - Penalizes both false positives and false negatives

2. **Normalized Mutual Information (NMI)**:
   - Measures the mutual information between clusterings, normalized by entropy
   - Ranges from 0 to 1, where higher values indicate better agreement
   - Information-theoretic approach

3. **Homogeneity, Completeness, and V-measure**:
   - Homogeneity: Each cluster contains only members of a single class
   - Completeness: All members of a given class are assigned to the same cluster
   - V-measure: Harmonic mean of homogeneity and completeness
   - All range from 0 to 1, where higher values indicate better clustering

The choice of metric depends on the specific problem, the characteristics of the data, and whether ground truth labels are available.

## References

1. Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.
2. Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, (2), 224-227.
3. Caliński, T., & Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1), 1-27.
4. Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1), 193-218.
5. Vinh, N. X., Epps, J., & Bailey, J. (2010). Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research, 11, 2837-2854.
