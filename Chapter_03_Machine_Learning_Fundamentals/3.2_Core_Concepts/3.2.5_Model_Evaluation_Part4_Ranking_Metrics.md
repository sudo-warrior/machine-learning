# 3.2.5 Model Evaluation - Part 4: Ranking Metrics

## Ranking Metrics

Ranking metrics evaluate the performance of models that rank items in order of relevance or preference. These metrics are commonly used in information retrieval, recommendation systems, and search engines.

### Mean Reciprocal Rank (MRR)

**Mean Reciprocal Rank (MRR)** is the average of the reciprocal ranks of the first relevant item for a set of queries:

$$\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}$$

where:
- $|Q|$ is the number of queries
- $\text{rank}_i$ is the rank of the first relevant item for the $i$-th query

MRR ranges from 0 to 1, where higher values indicate better ranking performance.

```python
import numpy as np
import matplotlib.pyplot as plt

# Define a function to calculate MRR
def mean_reciprocal_rank(rs):
    """
    Calculate Mean Reciprocal Rank
    
    Parameters:
    rs : list of lists
        Each list contains items that are relevant (1) or not (0)
    
    Returns:
    float
        Mean Reciprocal Rank
    """
    rs = (np.asarray(r).nonzero()[0] for r in rs)
    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])

# Example: Ranking results for 5 queries
# Each list represents the relevance (1) or non-relevance (0) of items in a ranked list
rankings = [
    [0, 0, 1, 0, 0],  # First relevant item at position 3 (index 2)
    [1, 0, 0, 0, 0],  # First relevant item at position 1 (index 0)
    [0, 1, 0, 0, 0],  # First relevant item at position 2 (index 1)
    [0, 0, 0, 1, 0],  # First relevant item at position 4 (index 3)
    [0, 0, 0, 0, 1]   # First relevant item at position 5 (index 4)
]

# Calculate MRR
mrr = mean_reciprocal_rank(rankings)
print(f"Mean Reciprocal Rank (MRR): {mrr:.4f}")

# Calculate individual reciprocal ranks
reciprocal_ranks = []
for ranking in rankings:
    relevant_indices = np.where(np.array(ranking) == 1)[0]
    if len(relevant_indices) > 0:
        first_relevant = relevant_indices[0]
        reciprocal_ranks.append(1.0 / (first_relevant + 1))
    else:
        reciprocal_ranks.append(0.0)

# Visualize MRR
plt.figure(figsize=(12, 6))

# Plot individual reciprocal ranks
plt.subplot(1, 2, 1)
plt.bar(range(1, len(reciprocal_ranks) + 1), reciprocal_ranks, color='skyblue')
plt.axhline(y=mrr, color='r', linestyle='--', label=f'MRR = {mrr:.4f}')
plt.title('Reciprocal Ranks for Each Query')
plt.xlabel('Query')
plt.ylabel('Reciprocal Rank')
plt.xticks(range(1, len(reciprocal_ranks) + 1))
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Plot the rankings
plt.subplot(1, 2, 2)
plt.imshow(rankings, cmap='Blues', aspect='auto')
plt.colorbar(label='Relevance')
plt.title('Ranking Results')
plt.xlabel('Position')
plt.ylabel('Query')
plt.xticks(range(len(rankings[0])), [f'Pos {i+1}' for i in range(len(rankings[0]))])
plt.yticks(range(len(rankings)), [f'Query {i+1}' for i in range(len(rankings))])

# Add text annotations
for i in range(len(rankings)):
    for j in range(len(rankings[i])):
        plt.text(j, i, rankings[i][j], ha='center', va='center', color='black')

plt.tight_layout()
plt.show()

# Demonstrate how MRR changes with ranking position
positions = range(1, 11)  # Positions 1 to 10
mrr_values = [1.0 / pos for pos in positions]

plt.figure(figsize=(10, 6))
plt.plot(positions, mrr_values, 'bo-', linewidth=2)
plt.title('Reciprocal Rank vs. Position of First Relevant Item')
plt.xlabel('Position of First Relevant Item')
plt.ylabel('Reciprocal Rank')
plt.xticks(positions)
plt.grid(True, alpha=0.3)

# Add annotations
for pos, mrr_val in zip(positions, mrr_values):
    plt.text(pos, mrr_val + 0.02, f"{mrr_val:.4f}", ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

### Precision at k (P@k)

**Precision at k (P@k)** is the proportion of relevant items among the top-k items:

$$\text{P@k} = \frac{\text{number of relevant items in top-k}}{k}$$

P@k ranges from 0 to 1, where higher values indicate better ranking performance.

```python
# Define a function to calculate Precision at k
def precision_at_k(r, k):
    """
    Calculate Precision at k
    
    Parameters:
    r : list
        Relevance scores (1 for relevant, 0 for not relevant)
    k : int
        Number of top items to consider
    
    Returns:
    float
        Precision at k
    """
    assert k >= 1
    r = np.asarray(r)[:k]
    return np.mean(r)

# Example: Ranking results for a single query
ranking = [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1 = relevant, 0 = not relevant

# Calculate Precision at different k values
k_values = range(1, len(ranking) + 1)
precision_values = [precision_at_k(ranking, k) for k in k_values]

# Print results
for k, p in zip(k_values, precision_values):
    print(f"Precision@{k}: {p:.4f}")

# Visualize Precision at k
plt.figure(figsize=(12, 6))

# Plot Precision at k
plt.subplot(1, 2, 1)
plt.plot(k_values, precision_values, 'go-', linewidth=2)
plt.title('Precision at k')
plt.xlabel('k')
plt.ylabel('Precision')
plt.xticks(k_values)
plt.grid(True, alpha=0.3)

# Add annotations
for k, p in zip(k_values, precision_values):
    plt.text(k, p + 0.02, f"{p:.2f}", ha='center', va='bottom')

# Visualize the ranking
plt.subplot(1, 2, 2)
colors = ['green' if r == 1 else 'red' for r in ranking]
plt.bar(range(1, len(ranking) + 1), [1] * len(ranking), color=colors)
plt.title('Ranking Results')
plt.xlabel('Position')
plt.ylabel('Item')
plt.xticks(range(1, len(ranking) + 1))
plt.yticks([])

# Add text annotations
for i, r in enumerate(ranking):
    plt.text(i + 1, 0.5, 'Relevant' if r == 1 else 'Not Relevant', 
             ha='center', va='center', rotation=90, color='white', fontweight='bold')

plt.tight_layout()
plt.show()

# Compare Precision at k for different rankings
rankings = [
    [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # All relevant items at the top
    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],  # All relevant items at the bottom
    [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],  # Alternating relevant and non-relevant items
    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]   # Alternating non-relevant and relevant items
]

ranking_names = ['Top-Heavy', 'Bottom-Heavy', 'Alternating (R first)', 'Alternating (NR first)']
k_values = range(1, 11)

plt.figure(figsize=(12, 6))

for i, (ranking, name) in enumerate(zip(rankings, ranking_names)):
    precision_values = [precision_at_k(ranking, k) for k in k_values]
    plt.plot(k_values, precision_values, 'o-', linewidth=2, label=name)

plt.title('Precision at k for Different Rankings')
plt.xlabel('k')
plt.ylabel('Precision at k')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Average Precision (AP) and Mean Average Precision (MAP)

**Average Precision (AP)** is the average of the precision values at each relevant item in the ranked list:

$$\text{AP} = \frac{\sum_{k=1}^{n} (P(k) \times \text{rel}(k))}{\text{number of relevant items}}$$

where:
- $P(k)$ is the precision at cutoff $k$
- $\text{rel}(k)$ is an indicator function that equals 1 if the item at rank $k$ is relevant, 0 otherwise
- $n$ is the number of items in the ranked list

**Mean Average Precision (MAP)** is the mean of the Average Precision scores for a set of queries:

$$\text{MAP} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \text{AP}_i$$

where:
- $|Q|$ is the number of queries
- $\text{AP}_i$ is the Average Precision for the $i$-th query

```python
# Define a function to calculate Average Precision
def average_precision(r):
    """
    Calculate Average Precision
    
    Parameters:
    r : list
        Relevance scores (1 for relevant, 0 for not relevant)
    
    Returns:
    float
        Average Precision
    """
    r = np.asarray(r)
    out = [precision_at_k(r, k + 1) for k in range(len(r)) if r[k]]
    if not out:
        return 0.
    return np.mean(out)

# Define a function to calculate Mean Average Precision
def mean_average_precision(rs):
    """
    Calculate Mean Average Precision
    
    Parameters:
    rs : list of lists
        Each list contains relevance scores (1 for relevant, 0 for not relevant)
    
    Returns:
    float
        Mean Average Precision
    """
    return np.mean([average_precision(r) for r in rs])

# Example: Ranking results for a single query
ranking = [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1 = relevant, 0 = not relevant

# Calculate Average Precision
ap = average_precision(ranking)
print(f"Average Precision (AP): {ap:.4f}")

# Calculate precision at each position
precision_values = [precision_at_k(ranking, k) for k in range(1, len(ranking) + 1)]

# Visualize Average Precision
plt.figure(figsize=(12, 6))

# Plot precision at each position
plt.subplot(1, 2, 1)
plt.plot(range(1, len(ranking) + 1), precision_values, 'bo-', linewidth=2)
plt.title(f'Precision at Each Position (AP = {ap:.4f})')
plt.xlabel('Position')
plt.ylabel('Precision')
plt.xticks(range(1, len(ranking) + 1))
plt.grid(True, alpha=0.3)

# Highlight precision at relevant positions
relevant_positions = [i + 1 for i, r in enumerate(ranking) if r == 1]
relevant_precisions = [precision_values[i - 1] for i in relevant_positions]
plt.plot(relevant_positions, relevant_precisions, 'ro', markersize=10, label='Relevant Positions')
plt.legend()

# Add annotations
for pos, prec in zip(relevant_positions, relevant_precisions):
    plt.text(pos, prec + 0.02, f"{prec:.2f}", ha='center', va='bottom')

# Visualize the ranking
plt.subplot(1, 2, 2)
colors = ['green' if r == 1 else 'red' for r in ranking]
plt.bar(range(1, len(ranking) + 1), [1] * len(ranking), color=colors)
plt.title('Ranking Results')
plt.xlabel('Position')
plt.ylabel('Item')
plt.xticks(range(1, len(ranking) + 1))
plt.yticks([])

# Add text annotations
for i, r in enumerate(ranking):
    plt.text(i + 1, 0.5, 'Relevant' if r == 1 else 'Not Relevant', 
             ha='center', va='center', rotation=90, color='white', fontweight='bold')

plt.tight_layout()
plt.show()

# Example: Ranking results for multiple queries
rankings = [
    [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],  # Query 1
    [0, 1, 0, 0, 1, 0, 1, 0, 1, 1],  # Query 2
    [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],  # Query 3
    [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]   # Query 4
]

# Calculate Average Precision for each query
ap_values = [average_precision(r) for r in rankings]
for i, ap in enumerate(ap_values):
    print(f"Average Precision for Query {i+1}: {ap:.4f}")

# Calculate Mean Average Precision
map_value = mean_average_precision(rankings)
print(f"Mean Average Precision (MAP): {map_value:.4f}")

# Visualize MAP
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(ap_values) + 1), ap_values, color='lightblue')
plt.axhline(y=map_value, color='r', linestyle='--', label=f'MAP = {map_value:.4f}')
plt.title('Average Precision for Each Query')
plt.xlabel('Query')
plt.ylabel('Average Precision')
plt.xticks(range(1, len(ap_values) + 1))
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
for i, ap in enumerate(ap_values):
    plt.text(i + 1, ap/2, f"{ap:.4f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Normalized Discounted Cumulative Gain (NDCG)

**Normalized Discounted Cumulative Gain (NDCG)** measures the ranking quality, taking into account the position of relevant items:

1. **Discounted Cumulative Gain (DCG)** is calculated as:

$$\text{DCG}_p = \sum_{i=1}^{p} \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}$$

2. **Ideal DCG (IDCG)** is the DCG value for the perfect ranking:

$$\text{IDCG}_p = \sum_{i=1}^{|REL_p|} \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}$$

3. **NDCG** is the ratio of DCG to IDCG:

$$\text{NDCG}_p = \frac{\text{DCG}_p}{\text{IDCG}_p}$$

where:
- $\text{rel}_i$ is the relevance score of the item at position $i$
- $p$ is the number of items to consider
- $|REL_p|$ is the number of relevant items up to position $p$

NDCG ranges from 0 to 1, where higher values indicate better ranking performance.

```python
# Define functions to calculate DCG and NDCG
def dcg_at_k(r, k, method=0):
    """
    Calculate Discounted Cumulative Gain
    
    Parameters:
    r : list
        Relevance scores (can be binary or graded)
    k : int
        Number of top items to consider
    method : int
        If 0, uses the standard formula
        If 1, uses an alternative formula
    
    Returns:
    float
        DCG@k
    """
    r = np.asfarray(r)[:k]
    if r.size:
        if method == 0:
            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))
        elif method == 1:
            return np.sum((2**r - 1) / np.log2(np.arange(2, r.size + 2)))
        else:
            raise ValueError("method must be 0 or 1")
    return 0.

def ndcg_at_k(r, k, method=0):
    """
    Calculate Normalized Discounted Cumulative Gain
    
    Parameters:
    r : list
        Relevance scores (can be binary or graded)
    k : int
        Number of top items to consider
    method : int
        If 0, uses the standard formula
        If 1, uses an alternative formula
    
    Returns:
    float
        NDCG@k
    """
    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)
    if not dcg_max:
        return 0.
    return dcg_at_k(r, k, method) / dcg_max

# Example: Ranking with graded relevance
# 0 = not relevant, 1 = somewhat relevant, 2 = relevant, 3 = very relevant
ranking_graded = [3, 2, 3, 0, 1, 2, 0, 1, 0, 3]

# Calculate NDCG at different k values
k_values = range(1, len(ranking_graded) + 1)
ndcg_values = [ndcg_at_k(ranking_graded, k, method=1) for k in k_values]

# Print results
for k, ndcg in zip(k_values, ndcg_values):
    print(f"NDCG@{k}: {ndcg:.4f}")

# Visualize NDCG
plt.figure(figsize=(12, 6))

# Plot NDCG at k
plt.subplot(1, 2, 1)
plt.plot(k_values, ndcg_values, 'mo-', linewidth=2)
plt.title('NDCG at k')
plt.xlabel('k')
plt.ylabel('NDCG')
plt.xticks(k_values)
plt.grid(True, alpha=0.3)

# Add annotations
for k, ndcg in zip(k_values, ndcg_values):
    plt.text(k, ndcg + 0.02, f"{ndcg:.2f}", ha='center', va='bottom')

# Visualize the ranking with graded relevance
plt.subplot(1, 2, 2)
colors = plt.cm.YlOrRd(np.array(ranking_graded) / 3.0)
plt.bar(range(1, len(ranking_graded) + 1), ranking_graded, color=colors)
plt.title('Ranking with Graded Relevance')
plt.xlabel('Position')
plt.ylabel('Relevance Score')
plt.xticks(range(1, len(ranking_graded) + 1))
plt.yticks(range(4))

plt.tight_layout()
plt.show()

# Compare NDCG for different rankings
rankings_graded = [
    [3, 3, 3, 2, 2, 2, 1, 1, 0, 0],  # Perfectly sorted by relevance
    [0, 0, 1, 1, 2, 2, 2, 3, 3, 3],  # Reverse sorted
    [3, 0, 3, 0, 2, 0, 2, 1, 1, 0],  # Mixed order
    [3, 2, 1, 0, 3, 2, 1, 0, 3, 2]   # Repeating pattern
]

ranking_names = ['Perfect', 'Reverse', 'Mixed', 'Repeating']
k_values = range(1, 11)

plt.figure(figsize=(12, 6))

for i, (ranking, name) in enumerate(zip(rankings_graded, ranking_names)):
    ndcg_values = [ndcg_at_k(ranking, k, method=1) for k in k_values]
    plt.plot(k_values, ndcg_values, 'o-', linewidth=2, label=name)

plt.title('NDCG at k for Different Rankings')
plt.xlabel('k')
plt.ylabel('NDCG at k')
plt.xticks(k_values)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize the effect of position on DCG
positions = range(1, 11)
dcg_values = [1.0 / np.log2(pos + 1) for pos in positions]

plt.figure(figsize=(10, 6))
plt.bar(positions, dcg_values, color='lightgreen')
plt.title('Position Discount in DCG')
plt.xlabel('Position')
plt.ylabel('Discount Factor (1/log₂(pos+1))')
plt.xticks(positions)
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
for pos, val in zip(positions, dcg_values):
    plt.text(pos, val/2, f"{val:.3f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

### Spearman's Rank Correlation Coefficient

**Spearman's Rank Correlation Coefficient** measures the correlation between two rankings:

$$\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}$$

where:
- $d_i$ is the difference between the ranks of corresponding values
- $n$ is the number of items

Spearman's coefficient ranges from -1 to 1, where:
- 1 indicates perfect positive correlation (identical rankings)
- 0 indicates no correlation
- -1 indicates perfect negative correlation (opposite rankings)

```python
from scipy.stats import spearmanr

# Example: Compare different rankings
true_ranking = list(range(1, 11))  # True ranking from 1 to 10
predicted_rankings = [
    list(range(1, 11)),                    # Identical to true ranking
    list(range(10, 0, -1)),                # Reverse of true ranking
    [1, 3, 2, 4, 6, 5, 7, 9, 8, 10],       # Slightly different
    [2, 1, 4, 3, 6, 5, 8, 7, 10, 9]        # Swapped adjacent pairs
]

ranking_names = ['Identical', 'Reverse', 'Slightly Different', 'Swapped Pairs']

# Calculate Spearman's rank correlation
spearman_values = []
for ranking in predicted_rankings:
    corr, _ = spearmanr(true_ranking, ranking)
    spearman_values.append(corr)
    print(f"Spearman's correlation: {corr:.4f}")

# Visualize Spearman's correlation
plt.figure(figsize=(12, 6))

# Plot Spearman's correlation
plt.subplot(1, 2, 1)
plt.bar(ranking_names, spearman_values, color=['green', 'red', 'orange', 'blue'])
plt.title("Spearman's Rank Correlation")
plt.xlabel('Ranking')
plt.ylabel('Correlation Coefficient')
plt.ylim(-1.1, 1.1)
plt.grid(True, alpha=0.3, axis='y')

# Add annotations
for i, val in enumerate(spearman_values):
    plt.text(i, val/2, f"{val:.4f}", ha='center', va='center', fontweight='bold', 
             color='white' if val < 0 else 'black')

# Visualize the rankings
plt.subplot(1, 2, 2)
x = range(1, 11)
plt.plot(x, true_ranking, 'k-', linewidth=2, label='True Ranking')

for i, (ranking, name) in enumerate(zip(predicted_rankings, ranking_names)):
    plt.plot(x, ranking, 'o-', linewidth=2, label=name)

plt.title('Comparison of Rankings')
plt.xlabel('Item')
plt.ylabel('Rank')
plt.xticks(x)
plt.yticks(range(1, 11))
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

Ranking metrics help evaluate the performance of ranking models:

1. **Mean Reciprocal Rank (MRR)**:
   - Average of the reciprocal ranks of the first relevant item
   - Focuses on the position of the first relevant item
   - Useful when only the first relevant result matters

2. **Precision at k (P@k)**:
   - Proportion of relevant items among the top-k items
   - Simple and intuitive
   - Doesn't consider the order of relevant items within the top-k

3. **Average Precision (AP) and Mean Average Precision (MAP)**:
   - AP is the average of precision values at each relevant item
   - MAP is the mean of AP scores across multiple queries
   - Takes into account both precision and recall
   - Widely used in information retrieval

4. **Normalized Discounted Cumulative Gain (NDCG)**:
   - Measures ranking quality with position-based discounting
   - Can handle graded relevance scores
   - Penalizes relevant items appearing lower in the ranking
   - Popular for evaluating search engines and recommendation systems

5. **Spearman's Rank Correlation Coefficient**:
   - Measures the correlation between two rankings
   - Ranges from -1 to 1
   - Useful for comparing different ranking algorithms

The choice of metric depends on the specific problem, the importance of item positions, whether relevance is binary or graded, and the specific goals of the ranking system.

## References

1. Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.
2. Järvelin, K., & Kekäläinen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4), 422-446.
3. Baeza-Yates, R., & Ribeiro-Neto, B. (2011). Modern Information Retrieval (2nd ed.). Addison-Wesley.
4. Chapelle, O., Metzler, D., Zhang, Y., & Grinspan, P. (2009). Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (pp. 621-630).
5. Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30(1/2), 81-93.
