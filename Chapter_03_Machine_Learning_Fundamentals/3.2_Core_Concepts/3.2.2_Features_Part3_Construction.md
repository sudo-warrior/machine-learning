# 3.2.2 Features - Part 3: Feature Construction

## Feature Construction

**Feature construction** (also called feature engineering) is the process of creating new features from existing ones to improve model performance. This is where domain knowledge and creativity can make a significant difference.

### Why Construct New Features?

Feature construction offers several benefits:

1. **Capture Domain Knowledge**: Incorporate expert knowledge about the problem
2. **Expose Relevant Information**: Transform data to highlight patterns
3. **Simplify Relationships**: Make complex relationships more learnable
4. **Address Data Limitations**: Compensate for missing or incomplete data

### Mathematical Transformations

Simple mathematical operations can create useful new features:

#### Polynomial Features

**Polynomial features** capture non-linear relationships:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# Generate synthetic data with non-linear relationship
np.random.seed(42)
X = np.sort(np.random.uniform(0, 1, 100)).reshape(-1, 1)
y = np.sin(2 * np.pi * X).ravel() + np.random.normal(0, 0.1, 100)

# Create polynomial features
poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

# Display the first few rows
feature_names = poly.get_feature_names_out(['x'])
poly_df = pd.DataFrame(X_poly, columns=feature_names)
print("Original feature:")
print(X[:5])
print("\nPolynomial features (degree=3):")
print(poly_df.head())

# Fit models with different polynomial degrees
degrees = [1, 3, 5, 10]
plt.figure(figsize=(14, 10))

for i, degree in enumerate(degrees):
    # Create and fit the model
    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    model.fit(X, y)
    
    # Predict on a fine grid
    X_test = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_test)
    
    # Plot the results
    plt.subplot(2, 2, i+1)
    plt.scatter(X, y, color='blue', alpha=0.7, label='Data')
    plt.plot(X_test, y_pred, color='red', label=f'Degree {degree}')
    plt.title(f'Polynomial Features (Degree {degree})')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Log and Power Transformations

**Log and power transformations** can normalize skewed distributions:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a skewed feature
skewed_feature = 'LSTAT'  # Lower status of the population
x = df[skewed_feature]

# Apply transformations
log_x = np.log1p(x)  # log(1+x) to handle zeros
sqrt_x = np.sqrt(x)
inv_x = 1 / (x + 1e-10)  # Add small constant to avoid division by zero

# Calculate skewness
original_skew = stats.skew(x)
log_skew = stats.skew(log_x)
sqrt_skew = stats.skew(sqrt_x)
inv_skew = stats.skew(inv_x)

print(f"Original skewness: {original_skew:.4f}")
print(f"Log transform skewness: {log_skew:.4f}")
print(f"Square root transform skewness: {sqrt_skew:.4f}")
print(f"Inverse transform skewness: {inv_skew:.4f}")

# Visualize the transformations
plt.figure(figsize=(15, 10))

# Original distribution
plt.subplot(2, 2, 1)
plt.hist(x, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original {skewed_feature} (Skewness: {original_skew:.4f})')
plt.xlabel(skewed_feature)
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Log transformation
plt.subplot(2, 2, 2)
plt.hist(log_x, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Log(1+{skewed_feature}) (Skewness: {log_skew:.4f})')
plt.xlabel(f'Log(1+{skewed_feature})')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Square root transformation
plt.subplot(2, 2, 3)
plt.hist(sqrt_x, bins=30, color='salmon', edgecolor='black')
plt.title(f'Sqrt({skewed_feature}) (Skewness: {sqrt_skew:.4f})')
plt.xlabel(f'Sqrt({skewed_feature})')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Inverse transformation
plt.subplot(2, 2, 4)
plt.hist(inv_x, bins=30, color='plum', edgecolor='black')
plt.title(f'1/{skewed_feature} (Skewness: {inv_skew:.4f})')
plt.xlabel(f'1/{skewed_feature}')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize Q-Q plots to check normality
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
stats.probplot(x, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: Original {skewed_feature}')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 2)
stats.probplot(log_x, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: Log(1+{skewed_feature})')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 3)
stats.probplot(sqrt_x, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: Sqrt({skewed_feature})')
plt.grid(True, alpha=0.3)

plt.subplot(2, 2, 4)
stats.probplot(inv_x, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: 1/{skewed_feature}')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interaction Features

**Interaction features** capture relationships between multiple features:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, 
                          n_redundant=0, n_clusters_per_class=1, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create interaction features
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_inter = poly.fit_transform(X_train)
X_test_inter = poly.transform(X_test)

# Get feature names
feature_names = poly.get_feature_names_out(['x1', 'x2'])
print("Feature names with interactions:")
print(feature_names)

# Train models with and without interaction features
model_original = LogisticRegression(random_state=42)
model_original.fit(X_train, y_train)
y_pred_original = model_original.predict(X_test)
accuracy_original = accuracy_score(y_test, y_pred_original)

model_inter = LogisticRegression(random_state=42)
model_inter.fit(X_train_inter, y_train)
y_pred_inter = model_inter.predict(X_test_inter)
accuracy_inter = accuracy_score(y_test, y_pred_inter)

print(f"Accuracy without interactions: {accuracy_original:.4f}")
print(f"Accuracy with interactions: {accuracy_inter:.4f}")

# Visualize decision boundaries
def plot_decision_boundary(X, y, model, title, ax):
    # Create a mesh grid
    h = 0.02  # Step size
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    # Make predictions on the mesh grid
    if model == model_inter:
        mesh_points = poly.transform(np.c_[xx.ravel(), yy.ravel()])
    else:
        mesh_points = np.c_[xx.ravel(), yy.ravel()]
    
    Z = model.predict(mesh_points)
    Z = Z.reshape(xx.shape)
    
    # Plot decision boundary
    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k', alpha=0.7)
    ax.set_title(title)
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.grid(True, alpha=0.3)

plt.figure(figsize=(12, 5))

ax1 = plt.subplot(1, 2, 1)
plot_decision_boundary(X_test, y_test, model_original, 
                      f'Without Interactions\nAccuracy: {accuracy_original:.4f}', ax1)

ax2 = plt.subplot(1, 2, 2)
plot_decision_boundary(X_test, y_test, model_inter, 
                      f'With Interactions\nAccuracy: {accuracy_inter:.4f}', ax2)

plt.tight_layout()
plt.show()
```

### Domain-Specific Features

**Domain-specific features** leverage knowledge about the problem domain:

#### Time-Based Features

For time series data, we can extract various time-related features:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Create a sample time series dataset
np.random.seed(42)
dates = pd.date_range(start='2020-01-01', periods=365, freq='D')
values = np.random.normal(0, 1, 365) + 10  # Base signal
values += 5 * np.sin(np.arange(365) * 2 * np.pi / 365)  # Annual seasonality
values += 2 * np.sin(np.arange(365) * 2 * np.pi / 7)    # Weekly seasonality
values += np.arange(365) * 0.05                         # Trend

# Create DataFrame
df = pd.DataFrame({'date': dates, 'value': values})
df['date'] = pd.to_datetime(df['date'])

# Extract time-based features
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['dayofweek'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)
df['dayofyear'] = df['date'].dt.dayofyear

# Create cyclical features for month and day of week
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)

# Display the first few rows
print("Time-based features:")
print(df.head())

# Visualize the time series
plt.figure(figsize=(15, 10))

plt.subplot(3, 1, 1)
plt.plot(df['date'], df['value'], 'b-')
plt.title('Original Time Series')
plt.xlabel('Date')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Visualize by day of week
plt.subplot(3, 1, 2)
day_means = df.groupby('dayofweek')['value'].mean()
plt.bar(day_means.index, day_means.values, color='skyblue')
plt.title('Average Value by Day of Week')
plt.xlabel('Day of Week (0=Monday, 6=Sunday)')
plt.ylabel('Average Value')
plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.grid(True, alpha=0.3, axis='y')

# Visualize by month
plt.subplot(3, 1, 3)
month_means = df.groupby('month')['value'].mean()
plt.bar(month_means.index, month_means.values, color='lightgreen')
plt.title('Average Value by Month')
plt.xlabel('Month')
plt.ylabel('Average Value')
plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Visualize cyclical features
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(df['month_sin'], df['month_cos'], c=df['month'], cmap='hsv')
plt.title('Cyclical Encoding of Month')
plt.xlabel('sin(month)')
plt.ylabel('cos(month)')
plt.grid(True, alpha=0.3)
plt.axis('equal')

plt.subplot(1, 2, 2)
plt.scatter(df['dayofweek_sin'], df['dayofweek_cos'], c=df['dayofweek'], cmap='hsv')
plt.title('Cyclical Encoding of Day of Week')
plt.xlabel('sin(day of week)')
plt.ylabel('cos(day of week)')
plt.grid(True, alpha=0.3)
plt.axis('equal')

plt.tight_layout()
plt.show()
```

#### Text-Based Features

For text data, we can extract various linguistic features:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

# Download NLTK resources (uncomment if needed)
# nltk.download('punkt')
# nltk.download('stopwords')

# Sample text data
texts = [
    "Machine learning is a fascinating field of artificial intelligence.",
    "Deep learning models have achieved remarkable results in computer vision and natural language processing.",
    "The quick brown fox jumps over the lazy dog.",
    "Python is one of the most popular programming languages for data science and machine learning.",
    "Feature engineering is crucial for building effective machine learning models."
]

# Create a DataFrame
df = pd.DataFrame({'text': texts})

# Basic text features
df['char_count'] = df['text'].apply(len)
df['word_count'] = df['text'].apply(lambda x: len(x.split()))
df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))
df['sentence_count'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

# Advanced text features
stop_words = set(stopwords.words('english'))

def count_stopwords(text):
    return sum(1 for word in text.lower().split() if word in stop_words)

def count_punctuation(text):
    return sum(1 for char in text if char in '.,;:!?()[]{}"-\'')

def count_uppercase(text):
    return sum(1 for char in text if char.isupper())

def lexical_diversity(text):
    words = text.lower().split()
    return len(set(words)) / len(words) if words else 0

df['stopword_count'] = df['text'].apply(count_stopwords)
df['punctuation_count'] = df['text'].apply(count_punctuation)
df['uppercase_count'] = df['text'].apply(count_uppercase)
df['lexical_diversity'] = df['text'].apply(lexical_diversity)

# Display the features
print("Text-based features:")
print(df[['char_count', 'word_count', 'avg_word_length', 'sentence_count', 
         'stopword_count', 'punctuation_count', 'uppercase_count', 'lexical_diversity']])

# Visualize the features
plt.figure(figsize=(15, 10))

# Character count vs. word count
plt.subplot(2, 2, 1)
plt.scatter(df['word_count'], df['char_count'], alpha=0.7, s=100)
for i, txt in enumerate(range(1, len(df) + 1)):
    plt.annotate(f"Text {txt}", (df['word_count'].iloc[i], df['char_count'].iloc[i]),
                xytext=(5, 5), textcoords='offset points')
plt.title('Character Count vs. Word Count')
plt.xlabel('Word Count')
plt.ylabel('Character Count')
plt.grid(True, alpha=0.3)

# Average word length vs. lexical diversity
plt.subplot(2, 2, 2)
plt.scatter(df['lexical_diversity'], df['avg_word_length'], alpha=0.7, s=100)
for i, txt in enumerate(range(1, len(df) + 1)):
    plt.annotate(f"Text {txt}", (df['lexical_diversity'].iloc[i], df['avg_word_length'].iloc[i]),
                xytext=(5, 5), textcoords='offset points')
plt.title('Average Word Length vs. Lexical Diversity')
plt.xlabel('Lexical Diversity')
plt.ylabel('Average Word Length')
plt.grid(True, alpha=0.3)

# Stopword count vs. word count
plt.subplot(2, 2, 3)
plt.scatter(df['word_count'], df['stopword_count'], alpha=0.7, s=100)
for i, txt in enumerate(range(1, len(df) + 1)):
    plt.annotate(f"Text {txt}", (df['word_count'].iloc[i], df['stopword_count'].iloc[i]),
                xytext=(5, 5), textcoords='offset points')
plt.title('Stopword Count vs. Word Count')
plt.xlabel('Word Count')
plt.ylabel('Stopword Count')
plt.grid(True, alpha=0.3)

# Punctuation count vs. sentence count
plt.subplot(2, 2, 4)
plt.scatter(df['sentence_count'], df['punctuation_count'], alpha=0.7, s=100)
for i, txt in enumerate(range(1, len(df) + 1)):
    plt.annotate(f"Text {txt}", (df['sentence_count'].iloc[i], df['punctuation_count'].iloc[i]),
                xytext=(5, 5), textcoords='offset points')
plt.title('Punctuation Count vs. Sentence Count')
plt.xlabel('Sentence Count')
plt.ylabel('Punctuation Count')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Aggregation Features

**Aggregation features** summarize information across multiple instances or time periods:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Create a sample transaction dataset
np.random.seed(42)
n_customers = 100
n_transactions = 1000

# Generate customer IDs
customer_ids = np.random.randint(1, n_customers + 1, n_transactions)

# Generate transaction dates
start_date = pd.to_datetime('2020-01-01')
end_date = pd.to_datetime('2020-12-31')
date_range = (end_date - start_date).days
transaction_dates = start_date + pd.to_timedelta(np.random.randint(0, date_range, n_transactions), unit='D')

# Generate transaction amounts
transaction_amounts = np.random.lognormal(mean=4, sigma=1, size=n_transactions)

# Create DataFrame
transactions = pd.DataFrame({
    'customer_id': customer_ids,
    'transaction_date': transaction_dates,
    'amount': transaction_amounts
})

# Sort by date
transactions = transactions.sort_values('transaction_date').reset_index(drop=True)
print("Sample transaction data:")
print(transactions.head())

# Create aggregation features
customer_features = transactions.groupby('customer_id').agg({
    'transaction_date': ['count', 'min', 'max'],
    'amount': ['sum', 'mean', 'median', 'min', 'max', 'std']
})

# Flatten the column names
customer_features.columns = ['_'.join(col).strip() for col in customer_features.columns.values]
customer_features = customer_features.reset_index()

# Add more derived features
customer_features['days_active'] = (customer_features['transaction_date_max'] - 
                                   customer_features['transaction_date_min']).dt.days
customer_features['avg_transaction_per_day'] = (customer_features['transaction_date_count'] / 
                                              (customer_features['days_active'] + 1))  # Add 1 to avoid division by zero
customer_features['amount_per_transaction'] = customer_features['amount_sum'] / customer_features['transaction_date_count']

print("\nCustomer aggregation features:")
print(customer_features.head())

# Visualize the aggregation features
plt.figure(figsize=(15, 10))

# Transaction count vs. total amount
plt.subplot(2, 2, 1)
plt.scatter(customer_features['transaction_date_count'], customer_features['amount_sum'], alpha=0.7)
plt.title('Transaction Count vs. Total Amount')
plt.xlabel('Number of Transactions')
plt.ylabel('Total Amount')
plt.grid(True, alpha=0.3)

# Average amount vs. transaction frequency
plt.subplot(2, 2, 2)
plt.scatter(customer_features['avg_transaction_per_day'], customer_features['amount_mean'], alpha=0.7)
plt.title('Transaction Frequency vs. Average Amount')
plt.xlabel('Average Transactions per Day')
plt.ylabel('Average Transaction Amount')
plt.grid(True, alpha=0.3)

# Days active vs. total amount
plt.subplot(2, 2, 3)
plt.scatter(customer_features['days_active'], customer_features['amount_sum'], alpha=0.7)
plt.title('Days Active vs. Total Amount')
plt.xlabel('Days Active')
plt.ylabel('Total Amount')
plt.grid(True, alpha=0.3)

# Transaction count vs. amount variability
plt.subplot(2, 2, 4)
plt.scatter(customer_features['transaction_date_count'], customer_features['amount_std'], alpha=0.7)
plt.title('Transaction Count vs. Amount Variability')
plt.xlabel('Number of Transactions')
plt.ylabel('Standard Deviation of Amount')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Create time-based aggregation features
transactions['year_month'] = transactions['transaction_date'].dt.to_period('M')
monthly_stats = transactions.groupby(['customer_id', 'year_month']).agg({
    'transaction_date': 'count',
    'amount': ['sum', 'mean']
})
monthly_stats.columns = ['_'.join(col).strip() for col in monthly_stats.columns.values]
monthly_stats = monthly_stats.reset_index()

# Calculate rolling statistics for a sample customer
customer_id = 42  # Example customer
customer_monthly = monthly_stats[monthly_stats['customer_id'] == customer_id].sort_values('year_month')

# Visualize monthly patterns
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.bar(customer_monthly['year_month'].astype(str), customer_monthly['transaction_date_count'], color='skyblue')
plt.title(f'Monthly Transaction Count for Customer {customer_id}')
plt.xlabel('Month')
plt.ylabel('Transaction Count')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

plt.subplot(2, 1, 2)
plt.bar(customer_monthly['year_month'].astype(str), customer_monthly['amount_sum'], color='lightgreen')
plt.title(f'Monthly Total Amount for Customer {customer_id}')
plt.xlabel('Month')
plt.ylabel('Total Amount')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

## Summary

Feature construction is a powerful technique to enhance the quality of features:

1. **Mathematical Transformations**:
   - Polynomial features capture non-linear relationships
   - Log and power transformations normalize skewed distributions

2. **Interaction Features**:
   - Capture relationships between multiple features
   - Can significantly improve model performance for certain problems

3. **Domain-Specific Features**:
   - Time-based features for temporal data
   - Text-based features for linguistic data
   - Image-based features for visual data

4. **Aggregation Features**:
   - Summarize information across multiple instances or time periods
   - Capture patterns at different levels of granularity

Feature construction is often an iterative process that requires domain knowledge, creativity, and experimentation. The right constructed features can make complex relationships more learnable and significantly improve model performance.

In the next part, we'll explore feature transformation techniques that can further enhance the quality of features.

## References

1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O'Reilly Media.
2. Kuhn, M., & Johnson, K. (2019). Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.
3. Dong, G., & Liu, H. (Eds.). (2018). Feature Engineering for Machine Learning and Data Analytics. CRC Press.
