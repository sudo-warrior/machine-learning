# 3.2.4 Generalization - Part 1: Fundamentals

## Introduction to Generalization

**Generalization** refers to a model's ability to perform well on new, unseen data that wasn't used during training. It's one of the most important concepts in machine learning, as the ultimate goal is to create models that can make accurate predictions on new data, not just memorize the training examples.

### The Learning Problem

The fundamental challenge in machine learning is to learn a function that not only performs well on the training data but also generalizes to new, unseen data:

- **Training Data**: $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ drawn from some unknown distribution $D$
- **Goal**: Learn a function $f$ that minimizes the expected error on new data from the same distribution

### Training Error vs. Generalization Error

Two key metrics help us understand generalization:

1. **Training Error**: The error of the model on the training data
   $$E_{train}(f) = \frac{1}{n} \sum_{i=1}^{n} L(f(x_i), y_i)$$

2. **Generalization Error** (or Test Error): The expected error on new, unseen data
   $$E_{gen}(f) = \mathbb{E}_{(x,y) \sim D}[L(f(x), y)]$$

The goal is to minimize the generalization error, but we can only observe and directly minimize the training error.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = X.reshape(-1, 1)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train polynomial models of different degrees
degrees = [1, 3, 5, 9, 15]
train_errors = []
test_errors = []
models = []

for degree in degrees:
    # Create and train the model
    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    model.fit(X_train, y_train)
    models.append(model)
    
    # Calculate errors
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    train_error = mean_squared_error(y_train, y_train_pred)
    test_error = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_error)
    test_errors.append(test_error)
    
    print(f"Degree {degree}:")
    print(f"  Training MSE: {train_error:.2f}")
    print(f"  Test MSE: {test_error:.2f}")

# Visualize the models and errors
plt.figure(figsize=(15, 10))

# Plot the models
for i, degree in enumerate(degrees):
    plt.subplot(2, 3, i+1)
    
    # Sort X for smooth curve plotting
    X_sorted = np.sort(X, axis=0)
    y_pred = models[i].predict(X_sorted)
    
    # Plot data and model
    plt.scatter(X_train, y_train, color='blue', alpha=0.7, label='Training Data')
    plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')
    plt.plot(X_sorted, y_pred, color='red', linewidth=2, label=f'Degree {degree}')
    
    plt.title(f'Polynomial Degree {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)

# Plot training vs. test error
plt.subplot(2, 3, 6)
plt.plot(degrees, train_errors, 'bo-', linewidth=2, label='Training Error')
plt.plot(degrees, test_errors, 'go-', linewidth=2, label='Test Error')
plt.title('Error vs. Model Complexity')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### The Bias-Variance Tradeoff

The generalization error can be decomposed into three components:

1. **Bias**: Error due to overly simplistic assumptions in the learning algorithm
2. **Variance**: Error due to excessive sensitivity to small fluctuations in the training data
3. **Irreducible Error**: Error due to noise in the data

This decomposition leads to the bias-variance tradeoff:

$$E_{gen}(f) = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

- **High Bias (Underfitting)**: Model is too simple to capture the underlying pattern
- **High Variance (Overfitting)**: Model is too complex and captures noise in the training data

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

# Function to generate data and calculate bias and variance
def bias_variance_decomposition(degree, n_datasets=100, test_size=0.3):
    # True function (unknown in practice)
    def f_true(x):
        return 3 * x**2 + 2 * x + 1
    
    # Generate multiple datasets
    np.random.seed(42)
    datasets = []
    X_test_global = np.linspace(-3, 3, 100).reshape(-1, 1)
    y_test_global = f_true(X_test_global)
    
    for i in range(n_datasets):
        # Generate data with noise
        X = np.random.uniform(-3, 3, 30).reshape(-1, 1)
        y = f_true(X) + np.random.normal(0, 3, X.shape[0])
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i)
        datasets.append((X_train, y_train))
    
    # Train models on each dataset
    predictions = np.zeros((n_datasets, len(X_test_global)))
    
    for i, (X_train, y_train) in enumerate(datasets):
        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
        model.fit(X_train, y_train)
        predictions[i] = model.predict(X_test_global).ravel()
    
    # Calculate average prediction and variance
    mean_prediction = np.mean(predictions, axis=0)
    variance = np.mean(np.var(predictions, axis=0))
    bias_squared = np.mean((mean_prediction - y_test_global.ravel())**2)
    
    return bias_squared, variance, X_test_global, y_test_global, predictions, mean_prediction

# Calculate bias and variance for different model complexities
degrees = [1, 3, 5, 9, 15]
bias_values = []
variance_values = []

for degree in degrees:
    bias_squared, variance, X_test, y_test, predictions, mean_prediction = bias_variance_decomposition(degree)
    bias_values.append(bias_squared)
    variance_values.append(variance)
    
    print(f"Degree {degree}:")
    print(f"  Bias^2: {bias_squared:.2f}")
    print(f"  Variance: {variance:.2f}")
    print(f"  Total Error: {bias_squared + variance:.2f}")

# Visualize bias-variance tradeoff
plt.figure(figsize=(15, 10))

# Plot bias and variance
plt.subplot(2, 2, 1)
plt.plot(degrees, bias_values, 'bo-', linewidth=2, label='Bias²')
plt.plot(degrees, variance_values, 'go-', linewidth=2, label='Variance')
plt.plot(degrees, np.array(bias_values) + np.array(variance_values), 'ro-', linewidth=2, label='Total Error')
plt.title('Bias-Variance Tradeoff')
plt.xlabel('Polynomial Degree')
plt.ylabel('Error')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot models for different degrees
for i, degree in enumerate([1, 5, 15]):
    bias_squared, variance, X_test, y_test, predictions, mean_prediction = bias_variance_decomposition(degree)
    
    plt.subplot(2, 2, i+2)
    
    # Plot true function
    plt.plot(X_test, y_test, 'k-', linewidth=2, label='True Function')
    
    # Plot individual model predictions (sample of 10)
    for j in range(10):
        plt.plot(X_test, predictions[j], 'b-', alpha=0.1)
    
    # Plot average prediction
    plt.plot(X_test, mean_prediction, 'r-', linewidth=2, label='Average Prediction')
    
    plt.title(f'Polynomial Degree {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Overfitting and Underfitting

**Overfitting** occurs when a model learns the training data too well, capturing noise and failing to generalize:
- Low training error, high test error
- Complex model relative to the amount of training data
- Model captures patterns that don't generalize

**Underfitting** occurs when a model is too simple to capture the underlying pattern:
- High training error, high test error
- Model lacks the capacity to represent the true relationship
- Model misses important patterns in the data

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# Generate synthetic data with a non-linear pattern
np.random.seed(42)
X = np.sort(np.random.uniform(-3, 3, 30))
y = 0.5 * X**2 + X + 2 + np.random.normal(0, 1, 30)
X = X.reshape(-1, 1)

# Create models with different complexities
underfit_model = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())
good_fit_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
overfit_model = make_pipeline(PolynomialFeatures(degree=15), LinearRegression())

# Fit the models
underfit_model.fit(X, y)
good_fit_model.fit(X, y)
overfit_model.fit(X, y)

# Generate points for plotting
X_plot = np.linspace(-3.5, 3.5, 100).reshape(-1, 1)
y_underfit = underfit_model.predict(X_plot)
y_good_fit = good_fit_model.predict(X_plot)
y_overfit = overfit_model.predict(X_plot)

# Visualize the models
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(X, y, color='blue', alpha=0.7)
plt.plot(X_plot, y_underfit, color='red', linewidth=2)
plt.title('Underfitting (Linear Model)')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.scatter(X, y, color='blue', alpha=0.7)
plt.plot(X_plot, y_good_fit, color='red', linewidth=2)
plt.title('Good Fit (Quadratic Model)')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.scatter(X, y, color='blue', alpha=0.7)
plt.plot(X_plot, y_overfit, color='red', linewidth=2)
plt.title('Overfitting (Degree 15 Polynomial)')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### The Role of Model Complexity

Model complexity plays a crucial role in generalization:

1. **Too Simple (High Bias)**:
   - Underfits the data
   - Fails to capture important patterns
   - Both training and test errors are high

2. **Too Complex (High Variance)**:
   - Overfits the data
   - Captures noise
   - Training error is low, but test error is high

3. **Optimal Complexity**:
   - Balances bias and variance
   - Captures the underlying pattern without fitting noise
   - Minimizes the generalization error

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# Generate synthetic data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = X.reshape(-1, 1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create models with different complexities
degrees = [1, 2, 3, 5, 10, 15]
models = [make_pipeline(PolynomialFeatures(degree), LinearRegression()) for degree in degrees]

# Calculate learning curves for each model
train_sizes = np.linspace(0.1, 1.0, 10)
results = {}

for i, (degree, model) in enumerate(zip(degrees, models)):
    train_sizes_abs, train_scores, test_scores = learning_curve(
        model, X, y, train_sizes=train_sizes, cv=5, scoring='neg_mean_squared_error'
    )
    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)
    results[degree] = {
        'train_sizes': train_sizes_abs,
        'train_scores': train_scores_mean,
        'test_scores': test_scores_mean
    }

# Visualize learning curves
plt.figure(figsize=(15, 10))

for i, degree in enumerate(degrees):
    plt.subplot(2, 3, i+1)
    res = results[degree]
    plt.plot(res['train_sizes'], res['train_scores'], 'o-', color='b', label='Training Error')
    plt.plot(res['train_sizes'], res['test_scores'], 'o-', color='r', label='Validation Error')
    plt.title(f'Degree {degree} Polynomial')
    plt.xlabel('Training Examples')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Plot error vs. model complexity
train_errors = []
test_errors = []

for degree, model in zip(degrees, models):
    model.fit(X_train, y_train)
    train_error = -model.score(X_train, y_train)  # Negative R^2
    test_error = -model.score(X_test, y_test)  # Negative R^2
    train_errors.append(train_error)
    test_errors.append(test_error)

plt.figure(figsize=(10, 6))
plt.plot(degrees, train_errors, 'bo-', linewidth=2, label='Training Error')
plt.plot(degrees, test_errors, 'ro-', linewidth=2, label='Test Error')
plt.title('Error vs. Model Complexity')
plt.xlabel('Polynomial Degree')
plt.ylabel('Error (-R²)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Capacity, Overfitting, and Underfitting

The relationship between model capacity, overfitting, and underfitting can be summarized as follows:

1. **Model Capacity**: The ability of a model to fit a wide range of functions
   - Higher capacity models can represent more complex functions
   - Capacity is influenced by model architecture, number of parameters, etc.

2. **Optimal Capacity**: The ideal capacity depends on:
   - Complexity of the true underlying function
   - Amount of training data available
   - Amount of noise in the data
   - Regularization techniques applied

3. **Balancing Act**: Finding the right model capacity is a key challenge in machine learning
   - Too little capacity leads to underfitting
   - Too much capacity leads to overfitting
   - The goal is to find the sweet spot that minimizes generalization error

## Summary

Generalization is a fundamental concept in machine learning:

1. **Goal of Learning**: Minimize generalization error, not just training error
2. **Bias-Variance Tradeoff**: Balance between underfitting and overfitting
3. **Model Complexity**: Choose the right complexity for the problem
4. **Overfitting vs. Underfitting**:
   - Overfitting: Model learns noise in the training data
   - Underfitting: Model is too simple to capture the underlying pattern

In the next parts, we'll explore techniques to improve generalization, including regularization, validation methods, and ensemble approaches.

## References

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
