# 3.2.6 Optimization Algorithms - Part 4: Adaptive Methods (3) - AdamW

## AdamW Optimizer

**AdamW** is a variant of Adam that implements weight decay correctly. In the original Adam optimizer, L2 regularization (weight decay) is applied to the gradients, which interacts with the adaptive learning rates in a way that can be suboptimal. AdamW decouples weight decay from the gradient update, leading to better generalization performance.

### Weight Decay vs. L2 Regularization

In standard optimization with L2 regularization, the objective function is modified to include a penalty term:

$$J(\theta) = J_0(\theta) + \frac{\lambda}{2} \|\theta\|_2^2$$

where:
- $J_0(\theta)$ is the original objective function
- $\lambda$ is the regularization parameter
- $\|\theta\|_2^2$ is the squared L2 norm of the parameters

The gradient of this modified objective is:

$$\nabla J(\theta) = \nabla J_0(\theta) + \lambda \theta$$

In Adam, this regularized gradient is used in the update rule, but the adaptive learning rates can cause the regularization to behave differently for different parameters.

### AdamW Algorithm

AdamW applies weight decay directly to the parameters, separate from the gradient update:

$$g_t = \nabla_\theta J_0(\theta_{t-1})$$
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
$$\theta_t = \theta_{t-1} - \alpha \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1} \right)$$

where:
- $\lambda$ is the weight decay parameter

This decoupling of weight decay from the gradient update leads to better generalization, especially for deep neural networks.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add bias term to X
X_train_b = np.c_[np.ones(X_train.shape[0]), X_train]
X_test_b = np.c_[np.ones(X_test.shape[0]), X_test]

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y, lambda_=0):
    """Mean squared error for linear regression with L2 regularization."""
    m = len(y)
    predictions = X @ theta
    reg_term = (lambda_ / (2 * m)) * np.sum(theta[1:]**2)  # Exclude bias term from regularization
    return (1/(2*m)) * np.sum((predictions - y)**2) + reg_term

def linear_regression_gradient(theta, X, y, lambda_=0):
    """Gradient of mean squared error for linear regression with L2 regularization."""
    m = len(y)
    predictions = X @ theta
    gradient = (1/m) * X.T @ (predictions - y)
    gradient[1:] += (lambda_ / m) * theta[1:]  # Apply regularization to all but the bias term
    return gradient

# Implement Adam optimizer
def adam(X, y, learning_rate=0.001, num_iterations=1000, beta1=0.9, beta2=0.999, epsilon=1e-8, lambda_=0):
    """
    Perform Adam optimization with L2 regularization.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    - lambda_: L2 regularization parameter
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y, lambda_)]
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y, lambda_)
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters
        theta = theta - learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y, lambda_))
    
    return np.array(theta_history), np.array(train_cost_history)

# Implement AdamW optimizer
def adamw(X, y, X_test=None, y_test=None, learning_rate=0.001, num_iterations=1000, 
          beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):
    """
    Perform AdamW optimization with decoupled weight decay.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - X_test: Test features (optional)
    - y_test: Test values (optional)
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    - beta1: Exponential decay rate for first moment
    - beta2: Exponential decay rate for second moment
    - epsilon: Small constant to avoid division by zero
    - weight_decay: Weight decay parameter
    
    Returns:
    - theta_history: History of parameter values
    - train_cost_history: History of training cost values
    - test_cost_history: History of test cost values (if X_test and y_test are provided)
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    # Initialize first and second moment estimates
    m_t = np.zeros(n)
    v_t = np.zeros(n)
    
    theta_history = [theta.copy()]
    train_cost_history = [linear_regression_cost(theta, X, y)]
    test_cost_history = []
    
    if X_test is not None and y_test is not None:
        test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    for t in range(1, num_iterations + 1):
        gradient = linear_regression_gradient(theta, X, y)  # No regularization in gradient
        
        # Update biased first moment estimate
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # Update biased second moment estimate
        v_t = beta2 * v_t + (1 - beta2) * gradient**2
        
        # Compute bias-corrected first moment estimate
        m_t_hat = m_t / (1 - beta1**t)
        
        # Compute bias-corrected second moment estimate
        v_t_hat = v_t / (1 - beta2**t)
        
        # Update parameters with decoupled weight decay
        # Apply weight decay to all but the bias term
        decay = np.zeros_like(theta)
        decay[1:] = weight_decay * theta[1:]
        
        theta = theta - learning_rate * (m_t_hat / (np.sqrt(v_t_hat) + epsilon) + decay)
        
        theta_history.append(theta.copy())
        train_cost_history.append(linear_regression_cost(theta, X, y))
        
        if X_test is not None and y_test is not None:
            test_cost_history.append(linear_regression_cost(theta, X_test, y_test))
    
    if X_test is not None and y_test is not None:
        return np.array(theta_history), np.array(train_cost_history), np.array(test_cost_history)
    else:
        return np.array(theta_history), np.array(train_cost_history)

# Compare Adam with L2 regularization and AdamW
learning_rate = 0.01
num_iterations = 500
lambda_adam = 0.01  # L2 regularization parameter for Adam
weight_decay_adamw = 0.01  # Weight decay parameter for AdamW

# Run Adam with L2 regularization
theta_adam, train_cost_adam, test_cost_adam = adam(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations, lambda_=lambda_adam)

# Run AdamW
theta_adamw, train_cost_adamw, test_cost_adamw = adamw(
    X_train_b, y_train, X_test=X_test_b, y_test=y_test,
    learning_rate=learning_rate, num_iterations=num_iterations, weight_decay=weight_decay_adamw)

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot training cost over iterations
plt.subplot(2, 2, 1)
plt.plot(range(len(train_cost_adam)), train_cost_adam, linewidth=2, 
         label=f'Adam (L2 reg={lambda_adam})')
plt.plot(range(len(train_cost_adamw)), train_cost_adamw, linewidth=2, 
         label=f'AdamW (weight decay={weight_decay_adamw})')
plt.xlabel('Iteration')
plt.ylabel('Training Cost')
plt.title('Training Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot test cost over iterations
plt.subplot(2, 2, 2)
plt.plot(range(len(test_cost_adam)), test_cost_adam, linewidth=2, 
         label=f'Adam (L2 reg={lambda_adam})')
plt.plot(range(len(test_cost_adamw)), test_cost_adamw, linewidth=2, 
         label=f'AdamW (weight decay={weight_decay_adamw})')
plt.xlabel('Iteration')
plt.ylabel('Test Cost')
plt.title('Test Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot parameter L2 norm over iterations
plt.subplot(2, 2, 3)
adam_norms = [np.linalg.norm(theta[1:]) for theta in theta_adam]  # Exclude bias term
adamw_norms = [np.linalg.norm(theta[1:]) for theta in theta_adamw]  # Exclude bias term

plt.plot(range(len(adam_norms)), adam_norms, linewidth=2, 
         label=f'Adam (L2 reg={lambda_adam})')
plt.plot(range(len(adamw_norms)), adamw_norms, linewidth=2, 
         label=f'AdamW (weight decay={weight_decay_adamw})')
plt.xlabel('Iteration')
plt.ylabel('Parameter L2 Norm (excluding bias)')
plt.title('Parameter L2 Norm vs. Iteration')
plt.legend()
plt.grid(True)

# Plot final test cost vs. regularization/weight decay
plt.subplot(2, 2, 4)
reg_values = [0.0001, 0.001, 0.01, 0.1, 1.0]
adam_test_costs = []
adamw_test_costs = []

for reg in reg_values:
    # Run Adam with L2 regularization
    _, _, test_cost = adam(
        X_train_b, y_train, X_test=X_test_b, y_test=y_test,
        learning_rate=learning_rate, num_iterations=num_iterations, lambda_=reg)
    adam_test_costs.append(test_cost[-1])
    
    # Run AdamW
    _, _, test_cost = adamw(
        X_train_b, y_train, X_test=X_test_b, y_test=y_test,
        learning_rate=learning_rate, num_iterations=num_iterations, weight_decay=reg)
    adamw_test_costs.append(test_cost[-1])

plt.semilogx(reg_values, adam_test_costs, 'o-', linewidth=2, label='Adam (L2 reg)')
plt.semilogx(reg_values, adamw_test_costs, 'o-', linewidth=2, label='AdamW (weight decay)')
plt.xlabel('Regularization/Weight Decay Parameter (log scale)')
plt.ylabel('Final Test Cost')
plt.title('Final Test Cost vs. Regularization/Weight Decay')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### AdamW vs. Adam with L2 Regularization

Let's examine the differences between AdamW and Adam with L2 regularization in more detail:

```python
# Compare the parameter updates for Adam with L2 regularization and AdamW
def compare_updates(X, y, theta, learning_rate=0.01, beta1=0.9, beta2=0.999, 
                   epsilon=1e-8, lambda_=0.01, weight_decay=0.01):
    """
    Compare parameter updates for Adam with L2 regularization and AdamW.
    
    Returns:
    - adam_update: Parameter update for Adam with L2 regularization
    - adamw_update: Parameter update for AdamW
    """
    # Initialize moment estimates
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    
    # Compute gradient with L2 regularization for Adam
    gradient_adam = linear_regression_gradient(theta, X, y, lambda_=lambda_)
    
    # Compute gradient without regularization for AdamW
    gradient_adamw = linear_regression_gradient(theta, X, y)
    
    # Update moment estimates (same for both)
    m = beta1 * m + (1 - beta1) * gradient_adam  # Using Adam gradient for simplicity
    v = beta2 * v + (1 - beta2) * gradient_adam**2
    
    # Bias correction (same for both)
    m_hat = m / (1 - beta1)  # Assuming t=1 for simplicity
    v_hat = v / (1 - beta2)
    
    # Compute updates
    adam_update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
    
    # For AdamW, add weight decay separately
    decay = np.zeros_like(theta)
    decay[1:] = weight_decay * theta[1:]  # Apply weight decay to all but the bias term
    adamw_update = learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + decay)
    
    return adam_update, adamw_update

# Generate a random parameter vector
np.random.seed(42)
theta = np.random.randn(X_train_b.shape[1])

# Compare updates
adam_update, adamw_update = compare_updates(X_train_b, y_train, theta)

# Visualize the difference in updates
plt.figure(figsize=(12, 6))

# Plot updates for each parameter
plt.subplot(1, 2, 1)
plt.bar(range(len(adam_update)), adam_update, alpha=0.5, label='Adam Update')
plt.bar(range(len(adamw_update)), adamw_update, alpha=0.5, label='AdamW Update')
plt.xlabel('Parameter Index')
plt.ylabel('Update Magnitude')
plt.title('Parameter Updates: Adam vs. AdamW')
plt.legend()
plt.grid(True, axis='y')

# Plot the difference in updates
plt.subplot(1, 2, 2)
update_diff = adamw_update - adam_update
plt.bar(range(len(update_diff)), update_diff)
plt.xlabel('Parameter Index')
plt.ylabel('Update Difference (AdamW - Adam)')
plt.title('Difference in Parameter Updates')
plt.grid(True, axis='y')

plt.tight_layout()
plt.show()

# Analyze the effect of adaptive learning rates on weight decay
def analyze_weight_decay_effect(X, y, num_iterations=100, learning_rate=0.01, 
                               beta1=0.9, beta2=0.999, epsilon=1e-8, 
                               lambda_=0.01, weight_decay=0.01):
    """
    Analyze how adaptive learning rates affect weight decay in Adam vs. AdamW.
    """
    m, n = X.shape
    theta_adam = np.ones(n)  # Initialize with ones for clearer effect
    theta_adamw = np.ones(n)
    
    # Initialize moment estimates
    m_adam = np.zeros(n)
    v_adam = np.zeros(n)
    m_adamw = np.zeros(n)
    v_adamw = np.zeros(n)
    
    # Track effective weight decay
    effective_decay_adam = []
    effective_decay_adamw = []
    
    for t in range(1, num_iterations + 1):
        # Adam with L2 regularization
        gradient_adam = linear_regression_gradient(theta_adam, X, y, lambda_=lambda_)
        m_adam = beta1 * m_adam + (1 - beta1) * gradient_adam
        v_adam = beta2 * v_adam + (1 - beta2) * gradient_adam**2
        m_adam_hat = m_adam / (1 - beta1**t)
        v_adam_hat = v_adam / (1 - beta2**t)
        
        # Calculate effective weight decay for Adam
        # The regularization term in the gradient is lambda_ * theta
        # The effective weight decay is this term divided by the adaptive learning rate
        adaptive_lr_adam = learning_rate / (np.sqrt(v_adam_hat) + epsilon)
        reg_term_adam = lambda_ * theta_adam[1:]  # Regularization term for non-bias parameters
        effective_decay_adam.append(np.mean(reg_term_adam * adaptive_lr_adam[1:]))
        
        # Update Adam parameters
        theta_adam = theta_adam - learning_rate * m_adam_hat / (np.sqrt(v_adam_hat) + epsilon)
        
        # AdamW
        gradient_adamw = linear_regression_gradient(theta_adamw, X, y)
        m_adamw = beta1 * m_adamw + (1 - beta1) * gradient_adamw
        v_adamw = beta2 * v_adamw + (1 - beta2) * gradient_adamw**2
        m_adamw_hat = m_adamw / (1 - beta1**t)
        v_adamw_hat = v_adamw / (1 - beta2**t)
        
        # Calculate effective weight decay for AdamW
        # The weight decay term is directly weight_decay * theta
        decay_adamw = np.zeros_like(theta_adamw)
        decay_adamw[1:] = weight_decay * theta_adamw[1:]
        effective_decay_adamw.append(np.mean(decay_adamw[1:] * learning_rate))
        
        # Update AdamW parameters
        theta_adamw = theta_adamw - learning_rate * (m_adamw_hat / (np.sqrt(v_adamw_hat) + epsilon) + decay_adamw)
    
    return effective_decay_adam, effective_decay_adamw

# Analyze weight decay effect
effective_decay_adam, effective_decay_adamw = analyze_weight_decay_effect(X_train_b, y_train)

# Visualize the effective weight decay
plt.figure(figsize=(10, 6))
plt.plot(range(len(effective_decay_adam)), effective_decay_adam, linewidth=2, 
         label='Adam Effective Weight Decay')
plt.plot(range(len(effective_decay_adamw)), effective_decay_adamw, linewidth=2, 
         label='AdamW Effective Weight Decay')
plt.xlabel('Iteration')
plt.ylabel('Effective Weight Decay')
plt.title('Effective Weight Decay: Adam vs. AdamW')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

## Advantages of AdamW

1. **Proper Weight Decay Implementation**:
   - Decouples weight decay from the adaptive learning rates
   - Ensures consistent regularization effect across all parameters
   - Leads to better generalization, especially for deep neural networks

2. **Improved Generalization**:
   - Often results in models that generalize better to unseen data
   - Particularly effective for large models with many parameters

3. **Consistent Behavior**:
   - Weight decay behaves more consistently across different optimizers
   - Makes hyperparameter tuning more intuitive

4. **Retains Adam's Benefits**:
   - Still combines the advantages of momentum and adaptive learning rates
   - Works well with sparse gradients and non-stationary objectives

## When to Use AdamW

AdamW is particularly beneficial in the following scenarios:

1. **Deep Neural Networks**: When training large models where regularization is important

2. **Transfer Learning**: When fine-tuning pre-trained models

3. **Generalization Concerns**: When the model is overfitting or generalization is a priority

4. **Modern Architectures**: For transformers, large language models, and other modern architectures

## Summary

AdamW is an important variant of Adam that implements weight decay correctly by decoupling it from the gradient update. This seemingly small change can lead to significant improvements in generalization performance, especially for deep neural networks.

Key points about AdamW:
- Applies weight decay directly to the parameters, separate from the gradient update
- Ensures consistent regularization effect across all parameters
- Often leads to better generalization than Adam with L2 regularization
- Retains all the benefits of Adam (momentum and adaptive learning rates)

AdamW has become the optimizer of choice for many state-of-the-art models, particularly in natural language processing and computer vision.

## References

1. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. In International Conference on Learning Representations.
2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
3. Zhang, M., Lucas, J., Ba, J., & Hinton, G. E. (2019). Lookahead Optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems.
4. Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., & Duncan, J. (2020). AdaBelief Optimizer: Adapting stepsizes by the belief in observed gradients. In Advances in Neural Information Processing Systems.
