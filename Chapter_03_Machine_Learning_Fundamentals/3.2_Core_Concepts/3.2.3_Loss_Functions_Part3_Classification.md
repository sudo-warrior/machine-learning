# 3.2.3 Loss Functions - Part 3: Classification Loss Functions

## Classification Loss Functions

Classification loss functions measure the performance of a classification model whose output is a probability value between 0 and 1. These loss functions penalize the model when it predicts the wrong class, with the penalty typically being larger when the model is more confident in its incorrect prediction.

### Binary Cross-Entropy Loss

**Binary Cross-Entropy Loss** (also called Log Loss) is used for binary classification problems:

$$\text{BCE}(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$

where:
- $y_i$ is the true label (0 or 1)
- $\hat{y}_i$ is the predicted probability of class 1
- $n$ is the number of examples

**Properties**:
- Measures the difference between two probability distributions
- Heavily penalizes confident incorrect predictions
- Provides good probability calibration
- Differentiable and convex
- Corresponds to maximum likelihood estimation for Bernoulli distribution

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Binary Cross-Entropy loss function
def binary_cross_entropy(y_true, y_pred):
    # Clip predictions to avoid log(0)
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Create a range of predicted probabilities
y_pred = np.linspace(0.001, 0.999, 1000)

# Calculate BCE for y_true = 1 and y_true = 0
bce_y1 = binary_cross_entropy(np.ones_like(y_pred), y_pred)
bce_y0 = binary_cross_entropy(np.zeros_like(y_pred), y_pred)

# Plot BCE
plt.figure(figsize=(10, 6))
plt.plot(y_pred, -np.log(y_pred), 'b-', linewidth=2, label='y_true = 1')
plt.plot(y_pred, -np.log(1 - y_pred), 'r-', linewidth=2, label='y_true = 0')
plt.title('Binary Cross-Entropy Loss')
plt.xlabel('Predicted Probability (y_pred)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(0, 1)
plt.ylim(0, 5)
plt.tight_layout()
plt.show()
```

### Categorical Cross-Entropy Loss

**Categorical Cross-Entropy Loss** is used for multi-class classification problems:

$$\text{CCE}(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{i,j} \log(\hat{y}_{i,j})$$

where:
- $y_{i,j}$ is 1 if example i belongs to class j and 0 otherwise (one-hot encoding)
- $\hat{y}_{i,j}$ is the predicted probability that example i belongs to class j
- $n$ is the number of examples
- $m$ is the number of classes

**Properties**:
- Generalization of binary cross-entropy to multiple classes
- Requires one-hot encoded labels
- Heavily penalizes confident incorrect predictions
- Differentiable and convex
- Corresponds to maximum likelihood estimation for Multinoulli distribution

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import log_loss

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a logistic regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)
model.fit(X_train, y_train)

# Get predicted probabilities
y_pred_proba = model.predict_proba(X_test)

# One-hot encode the true labels
encoder = OneHotEncoder(sparse=False)
y_test_onehot = encoder.fit_transform(y_test.reshape(-1, 1))

# Calculate categorical cross-entropy loss
cce = log_loss(y_test, y_pred_proba)
print(f"Categorical Cross-Entropy Loss: {cce:.4f}")

# Visualize predictions for a few examples
n_examples = 5
plt.figure(figsize=(12, 6))

for i in range(n_examples):
    plt.subplot(1, n_examples, i+1)
    plt.bar(range(3), y_pred_proba[i], color='skyblue')
    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    plt.title(f'True: {y_test[i]}')
    plt.xticks(range(3), iris.target_names, rotation=45)
    plt.ylim(0, 1)
    if i == 0:
        plt.ylabel('Predicted Probability')

plt.tight_layout()
plt.show()

# Visualize how CCE changes with prediction confidence
def plot_cce_confidence():
    # Create a range of prediction confidences
    confidence = np.linspace(0.01, 0.99, 100)
    
    # Calculate CCE for different true classes
    cce_values = []
    
    for conf in confidence:
        # Create predictions with varying confidence for class 0
        pred = np.array([conf, (1-conf)/2, (1-conf)/2])
        
        # Calculate CCE for true class 0
        y_true = np.array([1, 0, 0])  # One-hot encoded
        cce = -np.sum(y_true * np.log(pred))
        cce_values.append(cce)
    
    # Plot CCE vs. confidence
    plt.figure(figsize=(10, 6))
    plt.plot(confidence, cce_values, 'b-', linewidth=2)
    plt.title('Categorical Cross-Entropy vs. Prediction Confidence')
    plt.xlabel('Confidence in Correct Class')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()

plot_cce_confidence()
```

### Hinge Loss

**Hinge Loss** is used in Support Vector Machines and is defined as:

$$\text{Hinge}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)$$

where:
- $y_i$ is the true label (-1 or 1)
- $\hat{y}_i$ is the predicted score (not a probability)
- $n$ is the number of examples

**Properties**:
- Penalizes predictions with the wrong sign and those with correct sign but small margin
- Not differentiable at the hinge point
- Focuses on finding the maximum margin between classes
- Does not provide probability estimates
- Sparse, meaning many examples contribute nothing to the loss

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Hinge loss function
def hinge_loss(y_true, y_pred):
    return np.mean(np.maximum(0, 1 - y_true * y_pred))

# Create a range of predicted scores
y_pred = np.linspace(-3, 3, 1000)

# Calculate Hinge loss for y_true = 1 and y_true = -1
hinge_y1 = np.maximum(0, 1 - y_pred)
hinge_ym1 = np.maximum(0, 1 + y_pred)

# Plot Hinge loss
plt.figure(figsize=(10, 6))
plt.plot(y_pred, hinge_y1, 'b-', linewidth=2, label='y_true = 1')
plt.plot(y_pred, hinge_ym1, 'r-', linewidth=2, label='y_true = -1')
plt.title('Hinge Loss')
plt.xlabel('Predicted Score (y_pred)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Focal Loss

**Focal Loss** is designed to address class imbalance by focusing on hard examples:

$$\text{Focal}(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} (1 - \hat{y}_i)^\gamma y_i \log(\hat{y}_i) + \hat{y}_i^\gamma (1 - y_i) \log(1 - \hat{y}_i)$$

where:
- $y_i$ is the true label (0 or 1)
- $\hat{y}_i$ is the predicted probability of class 1
- $\gamma$ is the focusing parameter (typically 2)
- $n$ is the number of examples

**Properties**:
- Extension of binary cross-entropy
- Down-weights well-classified examples
- Focuses training on hard examples
- Useful for imbalanced datasets
- Requires tuning of the $\gamma$ parameter

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Focal loss function
def focal_loss(y_true, y_pred, gamma=2.0):
    # Clip predictions to avoid log(0)
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    
    # Calculate focal loss
    if y_true == 1:
        return -((1 - y_pred) ** gamma) * np.log(y_pred)
    else:
        return -(y_pred ** gamma) * np.log(1 - y_pred)

# Create a range of predicted probabilities
y_pred = np.linspace(0.001, 0.999, 1000)

# Calculate Focal loss for different gamma values
gamma_values = [0, 1, 2, 5]  # gamma=0 is equivalent to BCE
focal_y1 = {}
focal_y0 = {}

for gamma in gamma_values:
    focal_y1[gamma] = [focal_loss(1, p, gamma) for p in y_pred]
    focal_y0[gamma] = [focal_loss(0, p, gamma) for p in y_pred]

# Plot Focal loss for y_true = 1
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
for gamma, values in focal_y1.items():
    label = 'BCE' if gamma == 0 else f'Focal (γ={gamma})'
    plt.plot(y_pred, values, linewidth=2, label=label)
plt.title('Focal Loss (y_true = 1)')
plt.xlabel('Predicted Probability (y_pred)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(0, 1)
plt.ylim(0, 5)

# Plot Focal loss for y_true = 0
plt.subplot(1, 2, 2)
for gamma, values in focal_y0.items():
    label = 'BCE' if gamma == 0 else f'Focal (γ={gamma})'
    plt.plot(y_pred, values, linewidth=2, label=label)
plt.title('Focal Loss (y_true = 0)')
plt.xlabel('Predicted Probability (y_pred)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(0, 1)
plt.ylim(0, 5)

plt.tight_layout()
plt.show()
```

### Dice Loss

**Dice Loss** is based on the Dice coefficient, which measures the overlap between predicted and true segmentation:

$$\text{Dice Loss}(y, \hat{y}) = 1 - \frac{2 \sum_{i=1}^{n} y_i \hat{y}_i}{\sum_{i=1}^{n} y_i + \sum_{i=1}^{n} \hat{y}_i}$$

where:
- $y_i$ is the true label (0 or 1)
- $\hat{y}_i$ is the predicted probability
- $n$ is the number of examples

**Properties**:
- Commonly used in image segmentation
- Handles class imbalance well
- Focuses on the overlap between predictions and ground truth
- Less sensitive to class imbalance than cross-entropy
- Bounded between 0 and 1

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the Dice loss function
def dice_loss(y_true, y_pred):
    # Clip predictions to avoid division by zero
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    
    # Calculate Dice coefficient
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred)
    
    # Return Dice loss
    return 1 - (2 * intersection / union)

# Create a simple binary segmentation example
image_size = 10
y_true = np.zeros((image_size, image_size))
y_true[3:7, 3:7] = 1  # True segmentation is a square in the middle

# Create different predicted segmentations
y_pred_perfect = y_true.copy()  # Perfect prediction
y_pred_shifted = np.zeros((image_size, image_size))
y_pred_shifted[4:8, 4:8] = 1  # Shifted prediction
y_pred_smaller = np.zeros((image_size, image_size))
y_pred_smaller[4:6, 4:6] = 1  # Smaller prediction
y_pred_larger = np.zeros((image_size, image_size))
y_pred_larger[2:8, 2:8] = 1  # Larger prediction

# Calculate Dice loss for each prediction
dice_perfect = dice_loss(y_true, y_pred_perfect)
dice_shifted = dice_loss(y_true, y_pred_shifted)
dice_smaller = dice_loss(y_true, y_pred_smaller)
dice_larger = dice_loss(y_true, y_pred_larger)

print(f"Dice Loss (Perfect): {dice_perfect:.4f}")
print(f"Dice Loss (Shifted): {dice_shifted:.4f}")
print(f"Dice Loss (Smaller): {dice_smaller:.4f}")
print(f"Dice Loss (Larger): {dice_larger:.4f}")

# Visualize the segmentations
plt.figure(figsize=(15, 5))

plt.subplot(1, 5, 1)
plt.imshow(y_true, cmap='gray')
plt.title('True Segmentation')
plt.axis('off')

plt.subplot(1, 5, 2)
plt.imshow(y_pred_perfect, cmap='gray')
plt.title(f'Perfect (Loss: {dice_perfect:.4f})')
plt.axis('off')

plt.subplot(1, 5, 3)
plt.imshow(y_pred_shifted, cmap='gray')
plt.title(f'Shifted (Loss: {dice_shifted:.4f})')
plt.axis('off')

plt.subplot(1, 5, 4)
plt.imshow(y_pred_smaller, cmap='gray')
plt.title(f'Smaller (Loss: {dice_smaller:.4f})')
plt.axis('off')

plt.subplot(1, 5, 5)
plt.imshow(y_pred_larger, cmap='gray')
plt.title(f'Larger (Loss: {dice_larger:.4f})')
plt.axis('off')

plt.tight_layout()
plt.show()
```

## Comparing Classification Loss Functions

Let's compare the different classification loss functions to understand their behavior:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, log_loss, hinge_loss

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)
y_binary = 2 * y - 1  # Convert to -1/1 for hinge loss

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
_, _, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

# Train models with different loss functions
# Logistic Regression (Cross-Entropy)
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
lr_prob = lr_model.predict_proba(X_test)[:, 1]
lr_accuracy = accuracy_score(y_test, lr_pred)
lr_loss = log_loss(y_test, lr_prob)

# SVM (Hinge Loss)
svm_model = LinearSVC(random_state=42)
svm_model.fit(X_train, y_binary_train)
svm_pred = (svm_model.predict(X_test) > 0).astype(int)
svm_score = svm_model.decision_function(X_test)
svm_accuracy = accuracy_score(y_test, svm_pred)
svm_loss = hinge_loss(y_binary_test, svm_score)

# Display results
print("Logistic Regression (Cross-Entropy):")
print(f"  Accuracy: {lr_accuracy:.4f}")
print(f"  Log Loss: {lr_loss:.4f}")

print("\nSVM (Hinge Loss):")
print(f"  Accuracy: {svm_accuracy:.4f}")
print(f"  Hinge Loss: {svm_loss:.4f}")

# Compare loss functions directly
y_pred = np.linspace(-3, 3, 1000)
y_prob = 1 / (1 + np.exp(-y_pred))  # Convert scores to probabilities

# Calculate losses for y_true = 1
bce_y1 = -np.log(y_prob)
hinge_y1 = np.maximum(0, 1 - y_pred)

# Calculate losses for y_true = 0
bce_y0 = -np.log(1 - y_prob)
hinge_y0 = np.maximum(0, 1 + y_pred)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(y_pred, bce_y1, 'b-', linewidth=2, label='BCE (y=1)')
plt.plot(y_pred, hinge_y1, 'r-', linewidth=2, label='Hinge (y=1)')
plt.title('Loss Functions for y_true = 1')
plt.xlabel('Predicted Score')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(y_pred, bce_y0, 'b-', linewidth=2, label='BCE (y=0)')
plt.plot(y_pred, hinge_y0, 'r-', linewidth=2, label='Hinge (y=0)')
plt.title('Loss Functions for y_true = 0')
plt.xlabel('Predicted Score')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.show()
```

## When to Use Each Classification Loss Function

The choice of classification loss function depends on the specific problem and data characteristics:

### Binary Cross-Entropy
- **Use when**: You need well-calibrated probability estimates
- **Use when**: The classes are balanced
- **Use when**: You want to maximize likelihood
- **Avoid when**: The dataset is highly imbalanced

### Categorical Cross-Entropy
- **Use when**: You have a multi-class classification problem
- **Use when**: You need probability estimates for each class
- **Use when**: Classes are balanced
- **Avoid when**: The dataset is highly imbalanced

### Hinge Loss
- **Use when**: You only care about the classification boundary, not probabilities
- **Use when**: You want maximum margin between classes
- **Use when**: You're using Support Vector Machines
- **Avoid when**: You need probability estimates

### Focal Loss
- **Use when**: The dataset is highly imbalanced
- **Use when**: There are many easy examples and few hard examples
- **Use when**: You're working on object detection
- **Avoid when**: The dataset is balanced

### Dice Loss
- **Use when**: You're working on image segmentation
- **Use when**: The dataset is imbalanced (e.g., small objects in large images)
- **Use when**: You care about the overlap between predictions and ground truth
- **Avoid when**: You need probability estimates for each pixel

## Summary

Classification loss functions measure the performance of classification models:

1. **Binary Cross-Entropy**:
   - Used for binary classification
   - Measures the difference between predicted and true probability distributions
   - Heavily penalizes confident incorrect predictions

2. **Categorical Cross-Entropy**:
   - Used for multi-class classification
   - Generalization of binary cross-entropy to multiple classes
   - Requires one-hot encoded labels

3. **Hinge Loss**:
   - Used in Support Vector Machines
   - Focuses on finding the maximum margin between classes
   - Does not provide probability estimates

4. **Focal Loss**:
   - Addresses class imbalance
   - Down-weights well-classified examples
   - Focuses training on hard examples

5. **Dice Loss**:
   - Used in image segmentation
   - Measures the overlap between predicted and true segmentation
   - Handles class imbalance well

The choice of loss function should be based on the specific problem requirements, the distribution of classes, and whether probability estimates are needed.

## References

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (pp. 2980-2988).
3. Milletari, F., Navab, N., & Ahmadi, S. A. (2016). V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV), 2016 Fourth International Conference on (pp. 565-571).
