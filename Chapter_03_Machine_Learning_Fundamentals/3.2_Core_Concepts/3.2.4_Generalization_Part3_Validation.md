# 3.2.4 Generalization - Part 3: Validation Techniques

## Introduction to Validation

**Validation** techniques are methods for estimating how well a model will generalize to unseen data. They help us evaluate model performance, compare different models, and tune hyperparameters without using the test set.

### The Need for Validation

Validation is necessary because:

1. **Avoiding Test Set Leakage**: We need to preserve the test set for final evaluation
2. **Hyperparameter Tuning**: We need to select the best hyperparameters without overfitting to the test set
3. **Model Selection**: We need to compare different models fairly
4. **Estimating Generalization Error**: We need to estimate how well the model will perform on new data

### Train-Validation-Test Split

The most basic validation approach is to split the data into three sets:

1. **Training Set**: Used to train the model
2. **Validation Set**: Used to tune hyperparameters and select models
3. **Test Set**: Used only for final evaluation

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_curve, auc

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data into train, validation, and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

print(f"Training set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# Train models with different regularization strengths
C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_accuracies = []
val_accuracies = []
test_accuracies = []
models = []

for C in C_values:
    # Create and train the model
    model = LogisticRegression(C=C, max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    models.append(model)
    
    # Calculate accuracies
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    y_test_pred = model.predict(X_test)
    
    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_val, y_val_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    test_accuracies.append(test_acc)
    
    print(f"C = {C}:")
    print(f"  Training accuracy: {train_acc:.4f}")
    print(f"  Validation accuracy: {val_acc:.4f}")
    print(f"  Test accuracy: {test_acc:.4f}")

# Find the best model based on validation accuracy
best_idx = np.argmax(val_accuracies)
best_C = C_values[best_idx]
best_model = models[best_idx]

print(f"\nBest model (C = {best_C}):")
print(f"  Validation accuracy: {val_accuracies[best_idx]:.4f}")
print(f"  Test accuracy: {test_accuracies[best_idx]:.4f}")

# Visualize the results
plt.figure(figsize=(15, 5))

# Plot accuracies vs. regularization strength
plt.subplot(1, 2, 1)
plt.semilogx(C_values, train_accuracies, 'bo-', linewidth=2, label='Training Accuracy')
plt.semilogx(C_values, val_accuracies, 'ro-', linewidth=2, label='Validation Accuracy')
plt.semilogx(C_values, test_accuracies, 'go-', linewidth=2, label='Test Accuracy')
plt.axvline(x=best_C, color='k', linestyle='--', label=f'Best C = {best_C}')
plt.title('Accuracy vs. Regularization Strength')
plt.xlabel('C (Inverse of Regularization Strength)')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot ROC curves for the best model
plt.subplot(1, 2, 2)
y_train_prob = best_model.predict_proba(X_train)[:, 1]
y_val_prob = best_model.predict_proba(X_val)[:, 1]
y_test_prob = best_model.predict_proba(X_test)[:, 1]

fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob)
fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)

roc_auc_train = auc(fpr_train, tpr_train)
roc_auc_val = auc(fpr_val, tpr_val)
roc_auc_test = auc(fpr_test, tpr_test)

plt.plot(fpr_train, tpr_train, 'b-', linewidth=2, label=f'Training (AUC = {roc_auc_train:.3f})')
plt.plot(fpr_val, tpr_val, 'r-', linewidth=2, label=f'Validation (AUC = {roc_auc_val:.3f})')
plt.plot(fpr_test, tpr_test, 'g-', linewidth=2, label=f'Test (AUC = {roc_auc_test:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.title('ROC Curves for Best Model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Cross-Validation

**Cross-validation** involves splitting the data into multiple folds and training the model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.

#### k-Fold Cross-Validation

In **k-fold cross-validation**:

1. Split the data into k equal-sized folds
2. For each fold i:
   - Train the model on all folds except i
   - Validate the model on fold i
3. Average the validation results across all k runs

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Define cross-validation strategy
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Train models with different regularization strengths
C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
cv_scores = []
cv_std = []

for C in C_values:
    # Create the model
    model = LogisticRegression(C=C, max_iter=1000, random_state=42)
    
    # Perform cross-validation
    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
    
    cv_scores.append(np.mean(scores))
    cv_std.append(np.std(scores))
    
    print(f"C = {C}:")
    print(f"  Cross-validation accuracy: {np.mean(scores):.4f} ± {np.std(scores):.4f}")

# Find the best model based on cross-validation
best_idx = np.argmax(cv_scores)
best_C = C_values[best_idx]

print(f"\nBest model (C = {best_C}):")
print(f"  Cross-validation accuracy: {cv_scores[best_idx]:.4f} ± {cv_std[best_idx]:.4f}")

# Train the best model on the full dataset
best_model = LogisticRegression(C=best_C, max_iter=1000, random_state=42)
best_model.fit(X, y)

# Visualize the results
plt.figure(figsize=(10, 6))

# Plot cross-validation scores vs. regularization strength
plt.semilogx(C_values, cv_scores, 'bo-', linewidth=2)
plt.fill_between(C_values, 
                 np.array(cv_scores) - np.array(cv_std), 
                 np.array(cv_scores) + np.array(cv_std), 
                 alpha=0.2, color='b')
plt.axvline(x=best_C, color='k', linestyle='--', label=f'Best C = {best_C}')
plt.title('Cross-Validation Accuracy vs. Regularization Strength')
plt.xlabel('C (Inverse of Regularization Strength)')
plt.ylabel('Cross-Validation Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize the cross-validation process
plt.figure(figsize=(12, 6))

# Create a sample dataset for visualization
X_sample = X[:100]
y_sample = y[:100]

# Split into folds for visualization
kf_vis = KFold(n_splits=5, shuffle=True, random_state=42)
fold_indices = list(kf_vis.split(X_sample))

# Plot each fold
for i, (train_idx, val_idx) in enumerate(fold_indices):
    plt.subplot(2, 3, i+1)
    
    # Plot all points
    plt.scatter(X_sample[:, 0], X_sample[:, 1], c='lightgray', alpha=0.5)
    
    # Highlight training and validation sets
    plt.scatter(X_sample[train_idx, 0], X_sample[train_idx, 1], c='blue', alpha=0.7, label='Training')
    plt.scatter(X_sample[val_idx, 0], X_sample[val_idx, 1], c='red', alpha=0.7, label='Validation')
    
    plt.title(f'Fold {i+1}')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    
    if i == 0:
        plt.legend()

plt.subplot(2, 3, 6)
plt.text(0.5, 0.5, f"Final Model:\nTrained on all data\nBest C = {best_C}\nCV Accuracy = {cv_scores[best_idx]:.4f}", 
         ha='center', va='center', fontsize=12)
plt.axis('off')

plt.tight_layout()
plt.show()
```

#### Stratified k-Fold Cross-Validation

**Stratified k-fold cross-validation** ensures that each fold has the same proportion of classes as the original dataset, which is important for imbalanced datasets.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Generate imbalanced synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, weights=[0.9, 0.1], random_state=42)

print(f"Class distribution: {np.bincount(y)}")
print(f"Class proportions: {np.bincount(y) / len(y)}")

# Define cross-validation strategies
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

# Compare regular and stratified cross-validation
C = 1.0  # Fixed regularization strength
model = LogisticRegression(C=C, max_iter=1000, random_state=42)

# Regular k-fold
regular_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
print(f"Regular {k}-fold CV accuracy: {np.mean(regular_scores):.4f} ± {np.std(regular_scores):.4f}")

# Stratified k-fold
stratified_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
print(f"Stratified {k}-fold CV accuracy: {np.mean(stratified_scores):.4f} ± {np.std(stratified_scores):.4f}")

# Visualize the class distribution in each fold
plt.figure(figsize=(15, 10))

# Regular k-fold
plt.subplot(2, 1, 1)
fold_class_counts = []
for train_idx, val_idx in kf.split(X):
    y_train, y_val = y[train_idx], y[val_idx]
    train_counts = np.bincount(y_train) / len(y_train)
    val_counts = np.bincount(y_val) / len(y_val)
    fold_class_counts.append((train_counts, val_counts))

# Plot training set class proportions
train_props = np.array([counts[0][1] for counts in fold_class_counts])
plt.bar(np.arange(k) - 0.2, train_props, width=0.4, label='Training', color='blue', alpha=0.7)

# Plot validation set class proportions
val_props = np.array([counts[1][1] for counts in fold_class_counts])
plt.bar(np.arange(k) + 0.2, val_props, width=0.4, label='Validation', color='red', alpha=0.7)

# Plot overall class proportion
plt.axhline(y=np.bincount(y)[1] / len(y), color='k', linestyle='--', label='Overall')

plt.title('Class 1 Proportion in Regular k-Fold CV')
plt.xlabel('Fold')
plt.ylabel('Proportion of Class 1')
plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])
plt.legend()
plt.grid(True, alpha=0.3)

# Stratified k-fold
plt.subplot(2, 1, 2)
fold_class_counts = []
for train_idx, val_idx in skf.split(X, y):
    y_train, y_val = y[train_idx], y[val_idx]
    train_counts = np.bincount(y_train) / len(y_train)
    val_counts = np.bincount(y_val) / len(y_val)
    fold_class_counts.append((train_counts, val_counts))

# Plot training set class proportions
train_props = np.array([counts[0][1] for counts in fold_class_counts])
plt.bar(np.arange(k) - 0.2, train_props, width=0.4, label='Training', color='blue', alpha=0.7)

# Plot validation set class proportions
val_props = np.array([counts[1][1] for counts in fold_class_counts])
plt.bar(np.arange(k) + 0.2, val_props, width=0.4, label='Validation', color='red', alpha=0.7)

# Plot overall class proportion
plt.axhline(y=np.bincount(y)[1] / len(y), color='k', linestyle='--', label='Overall')

plt.title('Class 1 Proportion in Stratified k-Fold CV')
plt.xlabel('Fold')
plt.ylabel('Proportion of Class 1')
plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Leave-One-Out Cross-Validation (LOOCV)

**Leave-one-out cross-validation** is an extreme form of k-fold cross-validation where k equals the number of examples:

1. For each example i:
   - Train the model on all examples except i
   - Validate the model on example i
2. Average the validation results across all runs

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import LeaveOneOut, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate a small synthetic dataset
np.random.seed(42)
X, y = make_regression(n_samples=30, n_features=1, noise=10, random_state=42)
X = X.reshape(-1, 1)

# Define cross-validation strategy
loo = LeaveOneOut()

# Perform leave-one-out cross-validation
model = LinearRegression()
loo_predictions = np.zeros_like(y)

for train_idx, test_idx in loo.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    model.fit(X_train, y_train)
    loo_predictions[test_idx] = model.predict(X_test)

# Calculate LOOCV error
loo_mse = mean_squared_error(y, loo_predictions)
print(f"Leave-One-Out CV MSE: {loo_mse:.2f}")

# Train a model on the full dataset for comparison
full_model = LinearRegression()
full_model.fit(X, y)
full_predictions = full_model.predict(X)
full_mse = mean_squared_error(y, full_predictions)
print(f"Full model MSE: {full_mse:.2f}")

# Visualize the results
plt.figure(figsize=(12, 6))

# Sort X for smooth curve plotting
X_sorted = np.sort(X, axis=0)
y_sorted = full_model.predict(X_sorted)

# Plot the data and full model
plt.scatter(X, y, color='blue', alpha=0.7, label='Data')
plt.plot(X_sorted, y_sorted, color='red', linewidth=2, label='Full Model')

# Plot LOOCV predictions
plt.scatter(X, loo_predictions, color='green', marker='x', alpha=0.7, label='LOOCV Predictions')

# Connect each data point to its LOOCV prediction
for i in range(len(X)):
    plt.plot([X[i], X[i]], [y[i], loo_predictions[i]], 'k-', alpha=0.3)

plt.title('Leave-One-Out Cross-Validation')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Time Series Cross-Validation

For time series data, standard cross-validation can lead to data leakage because future data is used to predict past data. **Time series cross-validation** respects the temporal order of the data.

#### Time Series Split

**Time series split** is a form of cross-validation where each validation set consists of data that comes after the corresponding training set:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate synthetic time series data
np.random.seed(42)
n_samples = 100
time = np.arange(n_samples)
trend = 0.05 * time
seasonality = 10 * np.sin(2 * np.pi * time / 12)
noise = np.random.normal(0, 1, n_samples)
y = trend + seasonality + noise

# Create features (lag variables)
X = np.zeros((n_samples - 3, 3))
for i in range(3):
    X[:, i] = y[i:n_samples-3+i]
y_target = y[3:]

# Define time series cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Perform time series cross-validation
model = LinearRegression()
cv_scores = []
fold_predictions = []

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y_target[train_idx], y_target[val_idx]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    
    mse = mean_squared_error(y_val, y_pred)
    cv_scores.append(mse)
    fold_predictions.append((val_idx, y_pred))
    
    print(f"Fold {len(cv_scores)}:")
    print(f"  Training size: {len(X_train)}")
    print(f"  Validation size: {len(X_val)}")
    print(f"  MSE: {mse:.2f}")

print(f"\nAverage MSE: {np.mean(cv_scores):.2f} ± {np.std(cv_scores):.2f}")

# Visualize the time series splits
plt.figure(figsize=(15, 10))

# Plot the time series data
plt.subplot(2, 1, 1)
plt.plot(time, y, 'b-', linewidth=2, label='Time Series')
plt.title('Time Series Data')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot the cross-validation splits
plt.subplot(2, 1, 2)
for i, (train_idx, val_idx) in enumerate(tscv.split(X)):
    # Adjust indices to match the original time series
    train_idx_adjusted = train_idx + 3
    val_idx_adjusted = val_idx + 3
    
    # Plot training and validation sets
    plt.plot(train_idx_adjusted, y[train_idx_adjusted], 'o-', color='blue', alpha=0.7, 
             label=f'Training Set {i+1}' if i == 0 else "")
    plt.plot(val_idx_adjusted, y[val_idx_adjusted], 'o-', color='red', alpha=0.7, 
             label=f'Validation Set {i+1}' if i == 0 else "")
    
    # Plot predictions
    fold_idx, fold_pred = fold_predictions[i]
    plt.plot(fold_idx + 3, fold_pred, 'x-', color='green', alpha=0.7, 
             label='Predictions' if i == 0 else "")

plt.title('Time Series Cross-Validation')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Nested Cross-Validation

**Nested cross-validation** is used to estimate the generalization error when hyperparameter tuning is involved:

1. Outer loop: Split the data into k folds
2. For each fold i:
   - Use the remaining folds for training and hyperparameter tuning using inner cross-validation
   - Evaluate the best model on fold i
3. Average the results across all outer folds

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Define cross-validation strategies
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)

# Define hyperparameter grid
param_grid = {
    'C': [0.1, 1.0, 10.0],
    'gamma': [0.01, 0.1, 1.0]
}

# Perform nested cross-validation
outer_scores = []
best_params = []

for train_idx, test_idx in outer_cv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Inner loop for hyperparameter tuning
    grid_search = GridSearchCV(
        SVC(kernel='rbf', random_state=42),
        param_grid=param_grid,
        cv=inner_cv,
        scoring='accuracy'
    )
    grid_search.fit(X_train, y_train)
    
    # Get the best model and evaluate on the test set
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_score = accuracy_score(y_test, y_pred)
    
    outer_scores.append(test_score)
    best_params.append(grid_search.best_params_)
    
    print(f"Outer Fold:")
    print(f"  Best parameters: {grid_search.best_params_}")
    print(f"  Test accuracy: {test_score:.4f}")

print(f"\nNested CV accuracy: {np.mean(outer_scores):.4f} ± {np.std(outer_scores):.4f}")

# Count the frequency of each best parameter combination
param_counts = {}
for params in best_params:
    param_str = f"C={params['C']}, gamma={params['gamma']}"
    param_counts[param_str] = param_counts.get(param_str, 0) + 1

print("\nBest parameter frequencies:")
for param_str, count in param_counts.items():
    print(f"  {param_str}: {count}/{len(best_params)}")

# Visualize the nested cross-validation process
plt.figure(figsize=(12, 8))

# Create a grid for the parameter combinations
C_values = param_grid['C']
gamma_values = param_grid['gamma']
scores = np.zeros((len(C_values), len(gamma_values)))

# Fill the grid with the average inner CV scores for each parameter combination
for i, C in enumerate(C_values):
    for j, gamma in enumerate(gamma_values):
        # Train a model with these parameters on the full dataset
        model = SVC(C=C, gamma=gamma, kernel='rbf', random_state=42)
        inner_scores = cross_val_score(model, X, y, cv=inner_cv, scoring='accuracy')
        scores[i, j] = np.mean(inner_scores)

# Plot the parameter grid
plt.imshow(scores, interpolation='nearest', cmap='viridis')
plt.colorbar(label='Accuracy')
plt.xticks(np.arange(len(gamma_values)), gamma_values)
plt.yticks(np.arange(len(C_values)), C_values)
plt.xlabel('gamma')
plt.ylabel('C')

# Mark the best parameter combinations
for i, C in enumerate(C_values):
    for j, gamma in enumerate(gamma_values):
        param_str = f"C={C}, gamma={gamma}"
        count = param_counts.get(param_str, 0)
        if count > 0:
            plt.text(j, i, str(count), ha='center', va='center', 
                     color='white' if scores[i, j] < 0.85 else 'black', fontweight='bold')

plt.title('Nested Cross-Validation: Parameter Selection Frequency')
plt.tight_layout()
plt.show()
```

## Summary

Validation techniques are essential for estimating generalization performance and tuning hyperparameters:

1. **Train-Validation-Test Split**:
   - Simple approach that divides data into three sets
   - Provides a single validation estimate
   - Useful when data is abundant

2. **k-Fold Cross-Validation**:
   - Splits data into k folds and rotates the validation fold
   - Provides more robust estimates than a single split
   - Makes better use of available data

3. **Stratified k-Fold Cross-Validation**:
   - Maintains class proportions in each fold
   - Important for imbalanced datasets
   - Reduces variance in performance estimates

4. **Leave-One-Out Cross-Validation**:
   - Uses each example as a validation set once
   - Computationally intensive but makes maximum use of data
   - Useful for very small datasets

5. **Time Series Cross-Validation**:
   - Respects the temporal order of data
   - Prevents data leakage in time series problems
   - More realistic evaluation for forecasting tasks

6. **Nested Cross-Validation**:
   - Provides unbiased estimates when hyperparameter tuning is involved
   - Uses inner loops for tuning and outer loops for evaluation
   - Computationally intensive but more reliable

These validation techniques help ensure that models generalize well to new data and that hyperparameter tuning doesn't lead to overfitting.

## References

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.
2. Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (pp. 1137-1143).
3. Bergmeir, C., & Benítez, J. M. (2012). On the use of cross-validation for time series predictor evaluation. Information Sciences, 191, 192-213.
4. Cawley, G. C., & Talbot, N. L. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research, 11, 2079-2107.
