# 3.2.2 Features - Part 4C: Dimensionality Reduction

## Dimensionality Reduction

**Dimensionality reduction** techniques transform high-dimensional data into a lower-dimensional representation while preserving important information. These techniques are useful for visualization, addressing the curse of dimensionality, and improving model performance.

### Why Reduce Dimensionality?

Dimensionality reduction offers several benefits:

1. **Curse of Dimensionality**: As dimensions increase, data becomes sparse, and distance metrics become less meaningful
2. **Computational Efficiency**: Fewer dimensions mean faster training and inference
3. **Noise Reduction**: Removing less informative dimensions can reduce noise
4. **Visualization**: Reducing to 2D or 3D enables visualization of high-dimensional data
5. **Multicollinearity**: Addressing correlations between features

### Principal Component Analysis (PCA)

**PCA** finds orthogonal directions (principal components) that capture the maximum variance in the data:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Explained variance
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Visualize explained variance
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='skyblue')
plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, 'ro-')
plt.title('Explained Variance by Components')
plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.grid(True, alpha=0.3)
plt.xticks(range(1, len(explained_variance) + 1))

# Visualize first two principal components
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter, label='Species')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize feature loadings
plt.figure(figsize=(10, 6))
loadings = pca.components_.T
for i, feature in enumerate(feature_names):
    plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.05, head_length=0.05)
    plt.text(loadings[i, 0] * 1.1, loadings[i, 1] * 1.1, feature)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.2)
plt.title('PCA Feature Loadings')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.tight_layout()
plt.show()

# Reconstruct the data
X_reconstructed = pca.inverse_transform(X_pca)
reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)
print(f"Reconstruction error: {reconstruction_error:.4f}")

# Visualize original vs. reconstructed data for a sample
sample_idx = 0
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(feature_names, X_scaled[sample_idx], color='skyblue')
plt.title('Original Standardized Features')
plt.xlabel('Feature')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

plt.subplot(1, 2, 2)
plt.bar(feature_names, X_reconstructed[sample_idx], color='lightgreen')
plt.title('Reconstructed Features')
plt.xlabel('Feature')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### t-Distributed Stochastic Neighbor Embedding (t-SNE)

**t-SNE** is a non-linear dimensionality reduction technique that is particularly good at visualizing high-dimensional data:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits

# Load the digits dataset
digits = load_digits()
X = digits.data
y = digits.target

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# Visualize the results
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('t-SNE of Handwritten Digits')
plt.xlabel('t-SNE Feature 1')
plt.ylabel('t-SNE Feature 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Compare PCA and t-SNE
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('PCA of Handwritten Digits')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('t-SNE of Handwritten Digits')
plt.xlabel('t-SNE Feature 1')
plt.ylabel('t-SNE Feature 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Uniform Manifold Approximation and Projection (UMAP)

**UMAP** is another non-linear dimensionality reduction technique that preserves both local and global structure:

```python
import numpy as np
import matplotlib.pyplot as plt
import umap
from sklearn.datasets import load_digits

# Load the digits dataset
digits = load_digits()
X = digits.data
y = digits.target

# Apply UMAP
reducer = umap.UMAP(random_state=42)
X_umap = reducer.fit_transform(X)

# Visualize the results
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('UMAP of Handwritten Digits')
plt.xlabel('UMAP Feature 1')
plt.ylabel('UMAP Feature 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Compare PCA, t-SNE, and UMAP
plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('t-SNE')
plt.xlabel('t-SNE Feature 1')
plt.ylabel('t-SNE Feature 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('UMAP')
plt.xlabel('UMAP Feature 1')
plt.ylabel('UMAP Feature 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Autoencoders

**Autoencoders** are neural networks that learn to compress data into a lower-dimensional representation and then reconstruct it:

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.datasets import mnist
from sklearn.preprocessing import MinMaxScaler

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocess the data
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# Build the autoencoder
input_dim = 784  # 28x28 pixels
encoding_dim = 32  # Compressed representation

# Encoder
input_img = Input(shape=(input_dim,))
encoded = Dense(128, activation='relu')(input_img)
encoded = Dense(64, activation='relu')(encoded)
encoded = Dense(encoding_dim, activation='relu')(encoded)

# Decoder
decoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(input_dim, activation='sigmoid')(decoded)

# Autoencoder model
autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Encoder model
encoder = Model(input_img, encoded)

# Train the autoencoder
autoencoder.fit(x_train, x_train,
                epochs=10,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test),
                verbose=1)

# Encode and decode some digits
encoded_imgs = encoder.predict(x_test)
decoded_imgs = autoencoder.predict(x_test)

# Visualize the results
n = 10  # Number of digits to display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    plt.title('Original')
    
    # Reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    plt.title('Reconstructed')

plt.tight_layout()
plt.show()

# Visualize the encoded representations
plt.figure(figsize=(12, 10))
scatter = plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('2D Visualization of Encoded Representations')
plt.xlabel('Encoded Feature 1')
plt.ylabel('Encoded Feature 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Linear Discriminant Analysis (LDA)

**LDA** is a supervised dimensionality reduction technique that finds the directions that maximize the separation between classes:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Apply LDA
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)

# Visualize the results
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Species')
plt.title('LDA of Iris Dataset')
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Compare PCA and LDA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Species')
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Species')
plt.title('LDA of Iris Dataset')
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize the decision boundaries
from matplotlib.colors import ListedColormap

def plot_decision_boundaries(X, y, model, title):
    h = 0.02  # Step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']), edgecolors='k')
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.grid(True, alpha=0.3)

plt.figure(figsize=(12, 5))

# Train a classifier on LDA-transformed data
lda_classifier = LinearDiscriminantAnalysis()
lda_classifier.fit(X_lda, y)

plt.subplot(1, 2, 1)
plot_decision_boundaries(X_lda, y, lda_classifier, 'LDA Decision Boundaries')

# Train a classifier on PCA-transformed data
pca_classifier = LinearDiscriminantAnalysis()
pca_classifier.fit(X_pca, y)

plt.subplot(1, 2, 2)
plot_decision_boundaries(X_pca, y, pca_classifier, 'PCA + LDA Decision Boundaries')

plt.tight_layout()
plt.show()
```

### Feature Agglomeration

**Feature Agglomeration** groups similar features together using hierarchical clustering:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import FeatureAgglomeration
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Compute feature correlation matrix
correlation = np.corrcoef(X.T)

# Visualize the correlation matrix
plt.figure(figsize=(12, 10))
plt.imshow(correlation, cmap='coolwarm', vmin=-1, vmax=1)
plt.colorbar(label='Correlation Coefficient')
plt.title('Feature Correlation Matrix')
plt.xticks(range(len(feature_names)), feature_names, rotation=90)
plt.yticks(range(len(feature_names)), feature_names)
plt.tight_layout()
plt.show()

# Apply Feature Agglomeration
n_clusters = 5
agglo = FeatureAgglomeration(n_clusters=n_clusters)
X_reduced = agglo.fit_transform(X)

# Visualize the feature clusters
from scipy.cluster.hierarchy import dendrogram, linkage

# Compute linkage matrix
Z = linkage(correlation, 'ward')

plt.figure(figsize=(15, 8))
dendrogram(Z, labels=feature_names, leaf_rotation=90)
plt.title('Hierarchical Clustering of Features')
plt.xlabel('Features')
plt.ylabel('Distance')
plt.axhline(y=agglo.distances_[-n_clusters+1], color='r', linestyle='--', 
           label=f'Threshold for {n_clusters} clusters')
plt.legend()
plt.tight_layout()
plt.show()

# Visualize the reduced data
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(X[:10], aspect='auto', cmap='viridis')
plt.title('Original Data (First 10 Samples)')
plt.xlabel('Feature Index')
plt.ylabel('Sample Index')
plt.colorbar(label='Value')

plt.subplot(1, 2, 2)
plt.imshow(X_reduced[:10], aspect='auto', cmap='viridis')
plt.title(f'Reduced Data (n_clusters={n_clusters})')
plt.xlabel('Cluster Index')
plt.ylabel('Sample Index')
plt.colorbar(label='Value')

plt.tight_layout()
plt.show()
```

## Choosing the Right Dimensionality Reduction Technique

The choice of dimensionality reduction technique depends on several factors:

1. **Linear vs. Non-linear**: PCA and LDA are linear, while t-SNE, UMAP, and autoencoders can capture non-linear relationships
2. **Supervised vs. Unsupervised**: LDA is supervised, while PCA, t-SNE, UMAP, and autoencoders are unsupervised
3. **Interpretability**: PCA and LDA provide interpretable components, while t-SNE and UMAP focus on visualization
4. **Computational Efficiency**: PCA is fast, while t-SNE and UMAP can be computationally intensive
5. **Preservation of Structure**: t-SNE and UMAP preserve local structure, while PCA preserves global variance

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_swiss_roll
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# Generate Swiss roll dataset
n_samples = 1000
X, color = make_swiss_roll(n_samples=n_samples, random_state=42)

# Apply different dimensionality reduction techniques
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

reducer = umap.UMAP(random_state=42)
X_umap = reducer.fit_transform(X)

# Visualize the results
fig = plt.figure(figsize=(20, 15))

# Original 3D data
ax = fig.add_subplot(221, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap='viridis')
ax.set_title('Original Swiss Roll')
ax.view_init(elev=10, azim=-75)

# PCA
ax = fig.add_subplot(222)
ax.scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap='viridis')
ax.set_title('PCA')
ax.grid(True, alpha=0.3)

# t-SNE
ax = fig.add_subplot(223)
ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap='viridis')
ax.set_title('t-SNE')
ax.grid(True, alpha=0.3)

# UMAP
ax = fig.add_subplot(224)
ax.scatter(X_umap[:, 0], X_umap[:, 1], c=color, cmap='viridis')
ax.set_title('UMAP')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Create a comparison table
techniques = ['PCA', 't-SNE', 'UMAP', 'Autoencoders', 'LDA', 'Feature Agglomeration']
properties = ['Linear/Non-linear', 'Supervised/Unsupervised', 'Interpretability', 'Computational Efficiency', 'Structure Preservation']

comparison = pd.DataFrame({
    'PCA': ['Linear', 'Unsupervised', 'High', 'High', 'Global variance'],
    't-SNE': ['Non-linear', 'Unsupervised', 'Low', 'Low', 'Local structure'],
    'UMAP': ['Non-linear', 'Unsupervised', 'Medium', 'Medium', 'Both local and global'],
    'Autoencoders': ['Non-linear', 'Unsupervised', 'Low', 'Medium', 'Depends on architecture'],
    'LDA': ['Linear', 'Supervised', 'High', 'High', 'Class separation'],
    'Feature Agglomeration': ['Depends on linkage', 'Unsupervised', 'Medium', 'Medium', 'Feature similarity']
}, index=properties)

print("Comparison of Dimensionality Reduction Techniques:")
print(comparison)
```

## Summary

Dimensionality reduction techniques transform high-dimensional data into a lower-dimensional representation:

1. **Principal Component Analysis (PCA)**:
   - Linear technique that maximizes variance
   - Provides interpretable components
   - Fast and widely used

2. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**:
   - Non-linear technique for visualization
   - Preserves local structure
   - Computationally intensive

3. **Uniform Manifold Approximation and Projection (UMAP)**:
   - Non-linear technique similar to t-SNE
   - Preserves both local and global structure
   - Faster than t-SNE

4. **Autoencoders**:
   - Neural network-based approach
   - Can capture complex non-linear relationships
   - Flexible architecture

5. **Linear Discriminant Analysis (LDA)**:
   - Supervised technique that maximizes class separation
   - Useful for classification problems
   - Provides interpretable components

6. **Feature Agglomeration**:
   - Groups similar features using hierarchical clustering
   - Useful for highly correlated features
   - Interpretable feature grouping

These techniques help address the curse of dimensionality, improve computational efficiency, reduce noise, enable visualization, and enhance model performance.

## References

1. Jolliffe, I. T. (2002). Principal Component Analysis (2nd ed.). Springer.
2. Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov), 2579-2605.
3. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv preprint arXiv:1802.03426.
4. Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.
