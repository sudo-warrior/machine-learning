# 3.2.5 Model Evaluation - Part 1: Classification Metrics

## Introduction to Model Evaluation

**Model evaluation** is the process of assessing how well a machine learning model performs. It helps us understand the model's strengths and weaknesses, compare different models, and determine if a model is ready for deployment.

Proper evaluation is crucial because:

1. **Performance Estimation**: It provides an estimate of how well the model will perform on new, unseen data
2. **Model Comparison**: It allows us to compare different models objectively
3. **Hyperparameter Tuning**: It guides the selection of optimal hyperparameters
4. **Business Value Assessment**: It helps translate model performance into business value

## Classification Metrics

Classification metrics evaluate models that predict categorical outcomes. These metrics are derived from the **confusion matrix**, which tabulates the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).

### Confusion Matrix

The **confusion matrix** is a table that describes the performance of a classification model:

|                    | Predicted Positive | Predicted Negative |
|--------------------|--------------------|--------------------|
| **Actual Positive** | True Positive (TP) | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN) |

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Generate synthetic data
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=5, random_state=42)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix')
plt.grid(False)
plt.show()

# Explain the confusion matrix
tn, fp, fn, tp = cm.ravel()
print(f"True Positives (TP): {tp}")
print(f"True Negatives (TN): {tn}")
print(f"False Positives (FP): {fp}")
print(f"False Negatives (FN): {fn}")
```

### Accuracy

**Accuracy** is the proportion of correct predictions among the total number of predictions:

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

**Pros**:
- Simple and intuitive
- Works well for balanced datasets

**Cons**:
- Misleading for imbalanced datasets
- Doesn't distinguish between different types of errors

```python
from sklearn.metrics import accuracy_score

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Calculate accuracy from confusion matrix
accuracy_from_cm = (tp + tn) / (tp + tn + fp + fn)
print(f"Accuracy (from confusion matrix): {accuracy_from_cm:.4f}")

# Demonstrate accuracy on imbalanced data
np.random.seed(42)
X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                                  n_redundant=5, weights=[0.9, 0.1], random_state=42)

X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imb, y_imb, test_size=0.3, random_state=42)

# Train a model on imbalanced data
model_imb = LogisticRegression(random_state=42)
model_imb.fit(X_train_imb, y_train_imb)
y_pred_imb = model_imb.predict(X_test_imb)

# Calculate accuracy on imbalanced data
accuracy_imb = accuracy_score(y_test_imb, y_pred_imb)
print(f"\nImbalanced dataset:")
print(f"Class distribution: {np.bincount(y_imb)}")
print(f"Accuracy: {accuracy_imb:.4f}")

# Calculate accuracy of a dummy classifier that always predicts the majority class
dummy_pred = np.zeros_like(y_test_imb)  # Always predict class 0 (majority)
accuracy_dummy = accuracy_score(y_test_imb, dummy_pred)
print(f"Dummy classifier accuracy: {accuracy_dummy:.4f}")
```

### Precision

**Precision** is the proportion of true positive predictions among all positive predictions:

$$\text{Precision} = \frac{TP}{TP + FP}$$

**Pros**:
- Important when false positives are costly
- Works well for imbalanced datasets

**Cons**:
- Doesn't consider false negatives
- Can be artificially high if the model rarely predicts the positive class

```python
from sklearn.metrics import precision_score

# Calculate precision
precision = precision_score(y_test, y_pred)
print(f"Precision: {precision:.4f}")

# Calculate precision from confusion matrix
precision_from_cm = tp / (tp + fp) if (tp + fp) > 0 else 0
print(f"Precision (from confusion matrix): {precision_from_cm:.4f}")

# Calculate precision on imbalanced data
precision_imb = precision_score(y_test_imb, y_pred_imb)
print(f"\nPrecision on imbalanced data: {precision_imb:.4f}")

# Visualize precision
plt.figure(figsize=(10, 6))
plt.bar(['Precision'], [precision], color='skyblue')
plt.title('Precision')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bar
plt.text(0, precision/2, f"{precision:.4f}", ha='center', va='center', fontweight='bold')

# Add an illustration of precision
plt.text(0.5, 0.8, "Precision = TP / (TP + FP)", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(0.5, 0.7, "Precision focuses on the accuracy of positive predictions", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### Recall (Sensitivity)

**Recall** (also called sensitivity or true positive rate) is the proportion of true positive predictions among all actual positives:

$$\text{Recall} = \frac{TP}{TP + FN}$$

**Pros**:
- Important when false negatives are costly
- Works well for imbalanced datasets

**Cons**:
- Doesn't consider false positives
- Can be artificially high if the model always predicts the positive class

```python
from sklearn.metrics import recall_score

# Calculate recall
recall = recall_score(y_test, y_pred)
print(f"Recall: {recall:.4f}")

# Calculate recall from confusion matrix
recall_from_cm = tp / (tp + fn) if (tp + fn) > 0 else 0
print(f"Recall (from confusion matrix): {recall_from_cm:.4f}")

# Calculate recall on imbalanced data
recall_imb = recall_score(y_test_imb, y_pred_imb)
print(f"\nRecall on imbalanced data: {recall_imb:.4f}")

# Visualize recall
plt.figure(figsize=(10, 6))
plt.bar(['Recall'], [recall], color='lightgreen')
plt.title('Recall')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bar
plt.text(0, recall/2, f"{recall:.4f}", ha='center', va='center', fontweight='bold')

# Add an illustration of recall
plt.text(0.5, 0.8, "Recall = TP / (TP + FN)", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(0.5, 0.7, "Recall focuses on finding all positive instances", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### Specificity

**Specificity** (also called true negative rate) is the proportion of true negative predictions among all actual negatives:

$$\text{Specificity} = \frac{TN}{TN + FP}$$

**Pros**:
- Important when false positives are costly
- Complements sensitivity/recall

**Cons**:
- Doesn't consider false negatives
- Can be artificially high if the model always predicts the negative class

```python
# Calculate specificity
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
print(f"Specificity: {specificity:.4f}")

# Calculate specificity on imbalanced data
cm_imb = confusion_matrix(y_test_imb, y_pred_imb)
tn_imb, fp_imb, fn_imb, tp_imb = cm_imb.ravel()
specificity_imb = tn_imb / (tn_imb + fp_imb) if (tn_imb + fp_imb) > 0 else 0
print(f"\nSpecificity on imbalanced data: {specificity_imb:.4f}")

# Visualize specificity
plt.figure(figsize=(10, 6))
plt.bar(['Specificity'], [specificity], color='salmon')
plt.title('Specificity')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bar
plt.text(0, specificity/2, f"{specificity:.4f}", ha='center', va='center', fontweight='bold')

# Add an illustration of specificity
plt.text(0.5, 0.8, "Specificity = TN / (TN + FP)", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(0.5, 0.7, "Specificity focuses on correctly identifying negative instances", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### F1 Score

The **F1 score** is the harmonic mean of precision and recall:

$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

**Pros**:
- Balances precision and recall
- Works well for imbalanced datasets
- Single metric that considers both false positives and false negatives

**Cons**:
- Doesn't consider true negatives
- May not be appropriate when precision and recall are not equally important

```python
from sklearn.metrics import f1_score

# Calculate F1 score
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")

# Calculate F1 score from precision and recall
f1_from_pr = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
print(f"F1 Score (from precision and recall): {f1_from_pr:.4f}")

# Calculate F1 score on imbalanced data
f1_imb = f1_score(y_test_imb, y_pred_imb)
print(f"\nF1 Score on imbalanced data: {f1_imb:.4f}")

# Visualize F1 score
plt.figure(figsize=(12, 6))
plt.bar(['Precision', 'Recall', 'F1 Score'], [precision, recall, f1], color=['skyblue', 'lightgreen', 'orange'])
plt.title('Precision, Recall, and F1 Score')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
plt.text(0, precision/2, f"{precision:.4f}", ha='center', va='center', fontweight='bold')
plt.text(1, recall/2, f"{recall:.4f}", ha='center', va='center', fontweight='bold')
plt.text(2, f1/2, f"{f1:.4f}", ha='center', va='center', fontweight='bold')

# Add an illustration of F1 score
plt.text(1, 0.9, "F1 = 2 × (Precision × Recall) / (Precision + Recall)", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.text(1, 0.8, "F1 score balances precision and recall", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### ROC Curve and AUC

The **Receiver Operating Characteristic (ROC) curve** plots the true positive rate (recall) against the false positive rate (1 - specificity) at various classification thresholds. The **Area Under the Curve (AUC)** summarizes the ROC curve into a single value.

$$\text{False Positive Rate} = \frac{FP}{TN + FP} = 1 - \text{Specificity}$$

**Pros**:
- Threshold-independent evaluation
- Works well for imbalanced datasets
- Provides a visual representation of the tradeoff between sensitivity and specificity

**Cons**:
- Can be misleading for highly imbalanced datasets
- Doesn't directly reflect the precision of the model

```python
from sklearn.metrics import roc_curve, roc_auc_score

# Get predicted probabilities
y_prob = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)
print(f"AUC: {auc:.4f}")

# Calculate ROC curve and AUC for imbalanced data
y_prob_imb = model_imb.predict_proba(X_test_imb)[:, 1]
fpr_imb, tpr_imb, thresholds_imb = roc_curve(y_test_imb, y_prob_imb)
auc_imb = roc_auc_score(y_test_imb, y_prob_imb)
print(f"\nAUC on imbalanced data: {auc_imb:.4f}")

# Visualize ROC curve
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)

# Mark the default threshold (0.5)
default_threshold_idx = np.argmin(np.abs(thresholds - 0.5))
plt.plot(fpr[default_threshold_idx], tpr[default_threshold_idx], 'ro', markersize=8, label='Default threshold (0.5)')
plt.legend(loc='lower right')

# Add annotations
plt.text(0.6, 0.2, "Better performance →", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.arrow(0.55, 0.2, -0.1, 0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')

plt.tight_layout()
plt.show()

# Visualize the effect of threshold on precision, recall, and F1 score
plt.figure(figsize=(12, 6))

# Calculate precision and recall at different thresholds
precision_values = []
recall_values = []
f1_values = []
threshold_values = np.linspace(0, 1, 100)

for threshold in threshold_values:
    y_pred_threshold = (y_prob >= threshold).astype(int)
    precision_values.append(precision_score(y_test, y_pred_threshold, zero_division=0))
    recall_values.append(recall_score(y_test, y_pred_threshold))
    f1_values.append(f1_score(y_test, y_pred_threshold, zero_division=0))

plt.plot(threshold_values, precision_values, 'b-', linewidth=2, label='Precision')
plt.plot(threshold_values, recall_values, 'g-', linewidth=2, label='Recall')
plt.plot(threshold_values, f1_values, 'r-', linewidth=2, label='F1 Score')
plt.axvline(x=0.5, color='k', linestyle='--', label='Default threshold (0.5)')
plt.xlabel('Classification Threshold')
plt.ylabel('Score')
plt.title('Effect of Threshold on Precision, Recall, and F1 Score')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Precision-Recall Curve

The **Precision-Recall curve** plots precision against recall at various classification thresholds. The **Average Precision (AP)** summarizes the precision-recall curve into a single value.

**Pros**:
- Threshold-independent evaluation
- Better than ROC for imbalanced datasets
- Focuses on the positive class

**Cons**:
- Doesn't consider true negatives
- Can be more volatile than ROC curves

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

# Calculate precision-recall curve and average precision
precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_prob)
ap = average_precision_score(y_test, y_prob)
print(f"Average Precision: {ap:.4f}")

# Calculate precision-recall curve and average precision for imbalanced data
precision_curve_imb, recall_curve_imb, thresholds_pr_imb = precision_recall_curve(y_test_imb, y_prob_imb)
ap_imb = average_precision_score(y_test_imb, y_prob_imb)
print(f"\nAverage Precision on imbalanced data: {ap_imb:.4f}")

# Visualize precision-recall curve
plt.figure(figsize=(10, 8))
plt.plot(recall_curve, precision_curve, 'b-', linewidth=2, label=f'PR curve (AP = {ap:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)

# Mark the default threshold (0.5)
# Find the closest threshold to 0.5
if len(thresholds_pr) > 0:  # Check if thresholds_pr is not empty
    default_threshold_idx = np.argmin(np.abs(np.append(thresholds_pr, 0) - 0.5))
    if default_threshold_idx < len(precision_curve):
        plt.plot(recall_curve[default_threshold_idx], precision_curve[default_threshold_idx], 'ro', markersize=8, label='Default threshold (0.5)')
        plt.legend(loc='upper right')

# Add annotations
plt.text(0.5, 0.5, "Better performance →", ha='center', va='center', fontsize=12, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.arrow(0.45, 0.5, 0.1, 0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')

# Add a reference line for a random classifier
plt.axhline(y=sum(y_test)/len(y_test), color='r', linestyle='--', label='Random classifier')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

# Compare ROC and PR curves for balanced and imbalanced data
plt.figure(figsize=(15, 6))

# ROC curves
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Balanced (AUC = {auc:.3f})')
plt.plot(fpr_imb, tpr_imb, 'r-', linewidth=2, label=f'Imbalanced (AUC = {auc_imb:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curves: Balanced vs. Imbalanced')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)

# PR curves
plt.subplot(1, 2, 2)
plt.plot(recall_curve, precision_curve, 'b-', linewidth=2, label=f'Balanced (AP = {ap:.3f})')
plt.plot(recall_curve_imb, precision_curve_imb, 'r-', linewidth=2, label=f'Imbalanced (AP = {ap_imb:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('PR Curves: Balanced vs. Imbalanced')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)

# Add reference lines for random classifiers
plt.axhline(y=sum(y_test)/len(y_test), color='b', linestyle='--', label='_nolegend_')
plt.axhline(y=sum(y_test_imb)/len(y_test_imb), color='r', linestyle='--', label='_nolegend_')

plt.tight_layout()
plt.show()
```

### Log Loss

**Log Loss** (also called cross-entropy loss) measures the performance of a classification model whose output is a probability value between 0 and 1:

$$\text{Log Loss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$

where:
- $y_i$ is the true label (0 or 1)
- $\hat{y}_i$ is the predicted probability
- $n$ is the number of samples

**Pros**:
- Penalizes confident incorrect predictions more heavily
- Provides a continuous measure of model performance
- Used as the loss function for many classification algorithms

**Cons**:
- Less intuitive than accuracy or F1 score
- Sensitive to outliers

```python
from sklearn.metrics import log_loss

# Calculate log loss
logloss = log_loss(y_test, y_prob)
print(f"Log Loss: {logloss:.4f}")

# Calculate log loss on imbalanced data
logloss_imb = log_loss(y_test_imb, y_prob_imb)
print(f"\nLog Loss on imbalanced data: {logloss_imb:.4f}")

# Visualize log loss for different predicted probabilities
plt.figure(figsize=(10, 6))

# Create a range of predicted probabilities
p = np.linspace(0.001, 0.999, 1000)

# Calculate log loss for true label = 1
log_loss_1 = -np.log(p)
# Calculate log loss for true label = 0
log_loss_0 = -np.log(1 - p)

plt.plot(p, log_loss_1, 'b-', linewidth=2, label='True label = 1')
plt.plot(p, log_loss_0, 'r-', linewidth=2, label='True label = 0')
plt.xlabel('Predicted Probability')
plt.ylabel('Log Loss')
plt.title('Log Loss for Different Predicted Probabilities')
plt.legend()
plt.grid(True, alpha=0.3)

# Add annotations
plt.text(0.8, 3, "Higher penalty for\nconfident wrong predictions", ha='center', va='center', fontsize=10, 
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.arrow(0.8, 2.5, 0.1, -1, head_width=0.1, head_length=0.2, fc='black', ec='black')

plt.tight_layout()
plt.show()
```

### Matthews Correlation Coefficient (MCC)

The **Matthews Correlation Coefficient (MCC)** is a measure of the quality of binary classifications:

$$\text{MCC} = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$$

**Pros**:
- Balanced measure even for imbalanced datasets
- Takes into account all four values in the confusion matrix
- Returns a value between -1 and 1, where 1 is perfect prediction, 0 is random prediction, and -1 is perfect inverse prediction

**Cons**:
- Less intuitive than other metrics
- Can be unstable when one of the denominator terms is small

```python
from sklearn.metrics import matthews_corrcoef

# Calculate Matthews correlation coefficient
mcc = matthews_corrcoef(y_test, y_pred)
print(f"Matthews Correlation Coefficient: {mcc:.4f}")

# Calculate MCC on imbalanced data
mcc_imb = matthews_corrcoef(y_test_imb, y_pred_imb)
print(f"\nMCC on imbalanced data: {mcc_imb:.4f}")

# Visualize MCC compared to other metrics
plt.figure(figsize=(12, 6))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC']
values = [accuracy, precision, recall, f1, mcc]
values_imb = [accuracy_imb, precision_imb, recall_imb, f1_imb, mcc_imb]

x = np.arange(len(metrics))
width = 0.35

plt.bar(x - width/2, values, width, label='Balanced Dataset', color='skyblue')
plt.bar(x + width/2, values_imb, width, label='Imbalanced Dataset', color='salmon')

plt.xlabel('Metric')
plt.ylabel('Score')
plt.title('Comparison of Metrics on Balanced and Imbalanced Datasets')
plt.xticks(x, metrics)
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# Annotate the bars
for i, v in enumerate(values):
    plt.text(i - width/2, v/2, f"{v:.2f}", ha='center', va='center', fontweight='bold')
for i, v in enumerate(values_imb):
    plt.text(i + width/2, v/2, f"{v:.2f}", ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

## Summary

Classification metrics help evaluate the performance of classification models:

1. **Confusion Matrix**:
   - Tabulates true positives, true negatives, false positives, and false negatives
   - Foundation for many classification metrics

2. **Accuracy**:
   - Proportion of correct predictions
   - Simple but can be misleading for imbalanced datasets

3. **Precision**:
   - Proportion of true positives among positive predictions
   - Important when false positives are costly

4. **Recall (Sensitivity)**:
   - Proportion of true positives among actual positives
   - Important when false negatives are costly

5. **Specificity**:
   - Proportion of true negatives among actual negatives
   - Complements sensitivity/recall

6. **F1 Score**:
   - Harmonic mean of precision and recall
   - Balances precision and recall

7. **ROC Curve and AUC**:
   - Plots true positive rate against false positive rate
   - Threshold-independent evaluation

8. **Precision-Recall Curve and AP**:
   - Plots precision against recall
   - Better than ROC for imbalanced datasets

9. **Log Loss**:
   - Measures the quality of probabilistic predictions
   - Penalizes confident incorrect predictions

10. **Matthews Correlation Coefficient (MCC)**:
    - Balanced measure that uses all values in the confusion matrix
    - Works well for imbalanced datasets

The choice of metric depends on the specific problem, the class distribution, and the relative costs of different types of errors.

## References

1. Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing & Management, 45(4), 427-437.
2. Powers, D. M. (2011). Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. Journal of Machine Learning Technologies, 2(1), 37-63.
3. Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics, 21(1), 6.
4. Davis, J., & Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd International Conference on Machine Learning (pp. 233-240).
