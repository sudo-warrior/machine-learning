# 3.2.6 Optimization Algorithms - Part 2: Basic Gradient Descent

## Gradient Descent

**Gradient Descent** is the most fundamental optimization algorithm in machine learning. It iteratively adjusts parameters in the direction of steepest descent (negative gradient) to minimize the objective function.

### Basic Algorithm

The basic gradient descent algorithm updates parameters using the following rule:

$$\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)$$

where:
- $\theta_t$ is the parameter vector at iteration $t$
- $\alpha$ is the learning rate (step size)
- $\nabla J(\theta_t)$ is the gradient of the objective function at $\theta_t$

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

# Define a simple objective function: J(θ) = θ₁² + 2θ₂²
def objective_function(theta):
    return theta[0]**2 + 2*theta[1]**2

# Define the gradient of the objective function
def gradient(theta):
    return np.array([2*theta[0], 4*theta[1]])

# Implement basic gradient descent
def gradient_descent(start_point, learning_rate, num_iterations):
    """
    Perform gradient descent optimization.
    
    Parameters:
    - start_point: Initial parameter values
    - learning_rate: Learning rate (step size)
    - num_iterations: Number of iterations
    
    Returns:
    - trajectory: List of points visited during optimization
    - values: List of objective function values
    """
    theta = start_point.copy()
    trajectory = [theta.copy()]
    values = [objective_function(theta)]
    
    for i in range(num_iterations):
        grad = gradient(theta)
        theta = theta - learning_rate * grad
        
        trajectory.append(theta.copy())
        values.append(objective_function(theta))
    
    return np.array(trajectory), np.array(values)

# Run gradient descent with different learning rates
learning_rates = [0.01, 0.1, 0.2]
start_point = np.array([1.0, 1.0])
num_iterations = 50

results = []
for lr in learning_rates:
    trajectory, values = gradient_descent(start_point, lr, num_iterations)
    results.append((lr, trajectory, values))

# Create a grid for contour plot
theta1 = np.linspace(-1.5, 1.5, 100)
theta2 = np.linspace(-1.5, 1.5, 100)
theta1_grid, theta2_grid = np.meshgrid(theta1, theta2)
J_values = theta1_grid**2 + 2*theta2_grid**2

# Visualize the optimization trajectories
plt.figure(figsize=(15, 10))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(theta1_grid, theta2_grid, J_values, 20, cmap=cm.coolwarm)
plt.colorbar(contour)

for lr, trajectory, values in results:
    plt.plot(trajectory[:, 0], trajectory[:, 1], 'o-', linewidth=2, markersize=4, 
             label=f'Learning rate = {lr}')

plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.scatter(start_point[0], start_point[1], color='green', s=100, label='Start')
plt.xlabel('θ₁')
plt.ylabel('θ₂')
plt.title('Gradient Descent Trajectories')
plt.legend()
plt.grid(True)

# Plot function values over iterations
plt.subplot(2, 2, 2)
for lr, trajectory, values in results:
    plt.plot(range(len(values)), values, linewidth=2, 
             label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('J(θ)')
plt.title('Objective Function Value')
plt.legend()
plt.grid(True)

# Plot function values over iterations (log scale)
plt.subplot(2, 2, 3)
for lr, trajectory, values in results:
    plt.semilogy(range(len(values)), values, linewidth=2, 
                label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('J(θ) (log scale)')
plt.title('Objective Function Value (Log Scale)')
plt.legend()
plt.grid(True)

# Plot distance to optimum
plt.subplot(2, 2, 4)
for lr, trajectory, values in results:
    distances = np.sqrt(np.sum(trajectory**2, axis=1))  # Distance to origin (optimum)
    plt.semilogy(range(len(distances)), distances, linewidth=2, 
                label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('Distance to Optimum (log scale)')
plt.title('Convergence to Optimum')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Learning Rate

The **learning rate** (step size) is a critical hyperparameter in gradient descent:

- **Too small**: Convergence is slow
- **Too large**: May overshoot or diverge
- **Just right**: Converges efficiently to the minimum

```python
# Run gradient descent with a wider range of learning rates
learning_rates = [0.001, 0.01, 0.1, 0.2, 0.5, 0.8]
start_point = np.array([1.0, 1.0])
num_iterations = 50

results = []
for lr in learning_rates:
    trajectory, values = gradient_descent(start_point, lr, num_iterations)
    results.append((lr, trajectory, values))

# Visualize the effect of learning rate
plt.figure(figsize=(15, 10))

# Plot contour with trajectories
plt.subplot(2, 2, 1)
contour = plt.contour(theta1_grid, theta2_grid, J_values, 20, cmap=cm.coolwarm)
plt.colorbar(contour)

for lr, trajectory, values in results:
    plt.plot(trajectory[:, 0], trajectory[:, 1], 'o-', linewidth=2, markersize=4, 
             label=f'Learning rate = {lr}')

plt.scatter(0, 0, color='red', s=100, label='Optimum')
plt.scatter(start_point[0], start_point[1], color='green', s=100, label='Start')
plt.xlabel('θ₁')
plt.ylabel('θ₂')
plt.title('Effect of Learning Rate on Trajectories')
plt.legend()
plt.grid(True)

# Plot function values over iterations
plt.subplot(2, 2, 2)
for lr, trajectory, values in results:
    plt.semilogy(range(len(values)), values, linewidth=2, 
                label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('J(θ) (log scale)')
plt.title('Effect of Learning Rate on Convergence')
plt.legend()
plt.grid(True)

# Plot distance to optimum
plt.subplot(2, 2, 3)
for lr, trajectory, values in results:
    distances = np.sqrt(np.sum(trajectory**2, axis=1))  # Distance to origin (optimum)
    plt.semilogy(range(len(distances)), distances, linewidth=2, 
                label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('Distance to Optimum (log scale)')
plt.title('Effect of Learning Rate on Convergence Speed')
plt.legend()
plt.grid(True)

# Plot final distance to optimum vs. learning rate
plt.subplot(2, 2, 4)
final_distances = []
for lr, trajectory, values in results:
    final_distances.append(np.sqrt(np.sum(trajectory[-1]**2)))

plt.semilogx(learning_rates, final_distances, 'o-', linewidth=2, markersize=8)
plt.xlabel('Learning Rate (log scale)')
plt.ylabel('Final Distance to Optimum')
plt.title('Final Distance vs. Learning Rate')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Convergence Analysis

The convergence of gradient descent depends on several factors:

1. **Learning Rate**: Must be small enough to ensure convergence but large enough for efficiency
2. **Function Characteristics**: Convexity, smoothness, and conditioning affect convergence
3. **Initial Point**: Starting point can influence convergence speed and final solution

For convex, smooth functions, gradient descent converges at a rate of $O(1/T)$ for function values and $O(1/\sqrt{T})$ for parameters, where $T$ is the number of iterations.

```python
# Define different objective functions to analyze convergence
def quadratic_function(theta):
    """Convex quadratic function: J(θ) = θ₁² + 2θ₂²"""
    return theta[0]**2 + 2*theta[1]**2

def quadratic_gradient(theta):
    return np.array([2*theta[0], 4*theta[1]])

def ill_conditioned_function(theta):
    """Ill-conditioned quadratic function: J(θ) = θ₁² + 100θ₂²"""
    return theta[0]**2 + 100*theta[1]**2

def ill_conditioned_gradient(theta):
    return np.array([2*theta[0], 200*theta[1]])

def non_convex_function(theta):
    """Non-convex function: J(θ) = θ₁⁴ - 16θ₁² + 5θ₁ + θ₂²"""
    return theta[0]**4 - 16*theta[0]**2 + 5*theta[0] + theta[1]**2

def non_convex_gradient(theta):
    return np.array([4*theta[0]**3 - 32*theta[0] + 5, 2*theta[1]])

# Run gradient descent on different functions
functions = [
    ("Convex Quadratic", quadratic_function, quadratic_gradient),
    ("Ill-conditioned Quadratic", ill_conditioned_function, ill_conditioned_gradient),
    ("Non-convex Function", non_convex_function, non_convex_gradient)
]

start_point = np.array([1.0, 1.0])
learning_rate = 0.01
num_iterations = 100

results = []
for name, func, grad in functions:
    # Run gradient descent
    theta = start_point.copy()
    trajectory = [theta.copy()]
    values = [func(theta)]
    
    for i in range(num_iterations):
        theta = theta - learning_rate * grad(theta)
        trajectory.append(theta.copy())
        values.append(func(theta))
    
    results.append((name, np.array(trajectory), np.array(values)))

# Visualize convergence for different functions
plt.figure(figsize=(15, 10))

# Plot function values over iterations
plt.subplot(2, 2, 1)
for name, trajectory, values in results:
    plt.semilogy(range(len(values)), values - np.min(values) + 1e-10, linewidth=2, 
                label=name)

plt.xlabel('Iteration')
plt.ylabel('J(θ) - J(θ*) (log scale)')
plt.title('Convergence of Function Values')
plt.legend()
plt.grid(True)

# Plot gradient norm over iterations
plt.subplot(2, 2, 2)
for name, trajectory, values in results:
    if name == "Convex Quadratic":
        grad_norms = [np.linalg.norm(quadratic_gradient(theta)) for theta in trajectory]
    elif name == "Ill-conditioned Quadratic":
        grad_norms = [np.linalg.norm(ill_conditioned_gradient(theta)) for theta in trajectory]
    else:
        grad_norms = [np.linalg.norm(non_convex_gradient(theta)) for theta in trajectory]
    
    plt.semilogy(range(len(grad_norms)), grad_norms, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('||∇J(θ)|| (log scale)')
plt.title('Convergence of Gradient Norm')
plt.legend()
plt.grid(True)

# Plot parameter change over iterations
plt.subplot(2, 2, 3)
for name, trajectory, values in results:
    param_changes = [np.linalg.norm(trajectory[i+1] - trajectory[i]) for i in range(len(trajectory)-1)]
    param_changes.insert(0, 0)  # No change for first iteration
    
    plt.semilogy(range(len(param_changes)), param_changes, linewidth=2, label=name)

plt.xlabel('Iteration')
plt.ylabel('||θₜ₊₁ - θₜ|| (log scale)')
plt.title('Parameter Change Between Iterations')
plt.legend()
plt.grid(True)

# Plot theoretical vs. actual convergence rates
plt.subplot(2, 2, 4)
iterations = np.arange(1, num_iterations + 2)
theoretical_rate_1 = 1 / iterations  # O(1/T) rate
theoretical_rate_2 = 1 / np.sqrt(iterations)  # O(1/√T) rate

plt.loglog(iterations, theoretical_rate_1, 'k--', linewidth=2, label='O(1/T) rate')
plt.loglog(iterations, theoretical_rate_2, 'k-.', linewidth=2, label='O(1/√T) rate')

for name, trajectory, values in results:
    if name == "Convex Quadratic":
        # Normalize to start at the same point for comparison
        normalized_values = (values - np.min(values)) / (values[0] - np.min(values))
        plt.loglog(iterations, normalized_values, linewidth=2, label=name)

plt.xlabel('Iteration (log scale)')
plt.ylabel('Normalized Error (log scale)')
plt.title('Theoretical vs. Actual Convergence Rates')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Batch Gradient Descent

**Batch Gradient Descent** uses the entire dataset to compute the gradient at each iteration:

$$\nabla J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \nabla J_i(\theta)$$

where:
- $m$ is the number of training examples
- $J_i(\theta)$ is the loss for the $i$-th example

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler

# Generate synthetic regression data
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X = StandardScaler().fit_transform(X)
y = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()

# Define linear regression objective function and gradient
def linear_regression_cost(theta, X, y):
    """Mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/(2*m)) * np.sum((predictions - y)**2)

def linear_regression_gradient(theta, X, y):
    """Gradient of mean squared error for linear regression."""
    m = len(y)
    predictions = X @ theta
    return (1/m) * X.T @ (predictions - y)

# Add bias term to X
X_b = np.c_[np.ones(X.shape[0]), X]

# Implement batch gradient descent for linear regression
def batch_gradient_descent(X, y, learning_rate, num_iterations):
    """
    Perform batch gradient descent for linear regression.
    
    Parameters:
    - X: Input features (with bias term)
    - y: Target values
    - learning_rate: Learning rate
    - num_iterations: Number of iterations
    
    Returns:
    - theta_history: History of parameter values
    - cost_history: History of cost values
    """
    m, n = X.shape
    theta = np.zeros(n)
    
    theta_history = [theta.copy()]
    cost_history = [linear_regression_cost(theta, X, y)]
    
    for i in range(num_iterations):
        gradient = linear_regression_gradient(theta, X, y)
        theta = theta - learning_rate * gradient
        
        theta_history.append(theta.copy())
        cost_history.append(linear_regression_cost(theta, X, y))
    
    return np.array(theta_history), np.array(cost_history)

# Run batch gradient descent with different learning rates
learning_rates = [0.01, 0.1, 0.5, 1.0]
num_iterations = 100

results = []
for lr in learning_rates:
    theta_history, cost_history = batch_gradient_descent(X_b, y, lr, num_iterations)
    results.append((lr, theta_history, cost_history))

# Visualize the results
plt.figure(figsize=(15, 10))

# Plot cost over iterations
plt.subplot(2, 2, 1)
for lr, theta_history, cost_history in results:
    plt.plot(range(len(cost_history)), cost_history, linewidth=2, 
             label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.legend()
plt.grid(True)

# Plot cost over iterations (log scale)
plt.subplot(2, 2, 2)
for lr, theta_history, cost_history in results:
    plt.semilogy(range(len(cost_history)), cost_history, linewidth=2, 
                label=f'Learning rate = {lr}')

plt.xlabel('Iteration')
plt.ylabel('Cost (log scale)')
plt.title('Cost vs. Iteration (Log Scale)')
plt.legend()
plt.grid(True)

# Plot parameter values over iterations
plt.subplot(2, 2, 3)
for lr, theta_history, cost_history in results:
    plt.plot(range(len(theta_history)), theta_history[:, 0], '--', linewidth=2, 
             label=f'θ₀ (lr={lr})')
    plt.plot(range(len(theta_history)), theta_history[:, 1], '-', linewidth=2, 
             label=f'θ₁ (lr={lr})')

plt.xlabel('Iteration')
plt.ylabel('Parameter Value')
plt.title('Parameter Values vs. Iteration')
plt.legend()
plt.grid(True)

# Plot data and regression lines
plt.subplot(2, 2, 4)
plt.scatter(X, y, color='blue', alpha=0.5, label='Data')

for lr, theta_history, cost_history in results:
    final_theta = theta_history[-1]
    x_range = np.linspace(X.min(), X.max(), 100)
    y_pred = final_theta[0] + final_theta[1] * x_range
    plt.plot(x_range, y_pred, linewidth=2, 
             label=f'lr={lr}, θ=[{final_theta[0]:.3f}, {final_theta[1]:.3f}]')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Data and Regression Lines')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

## Summary

Gradient descent is the foundational optimization algorithm in machine learning:

1. **Basic Algorithm**: Iteratively update parameters in the direction of steepest descent (negative gradient).
2. **Learning Rate**: A critical hyperparameter that controls the step size; must be carefully chosen.
3. **Convergence**: Depends on learning rate, function characteristics, and initial point.
4. **Batch Gradient Descent**: Uses the entire dataset to compute the gradient at each iteration.

Gradient descent works well for convex, smooth functions but may struggle with ill-conditioned or non-convex functions. In the next parts, we'll explore variants of gradient descent that address these limitations.

## References

1. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.
2. Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311.
3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
4. Nesterov, Y. (2004). Introductory Lectures on Convex Optimization: A Basic Course. Springer.
