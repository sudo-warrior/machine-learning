# 3.2.2 Features - Part 2: Feature Extraction and Selection

## Feature Extraction

**Feature extraction** is the process of deriving features from raw data. This is particularly important for unstructured data like text, images, audio, and video.

### Text Feature Extraction

Text data requires special processing to convert it into numerical features:

#### Bag of Words (BoW)

The **Bag of Words** model represents text as the frequency of each word:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Sample text data
documents = [
    "Machine learning is fascinating",
    "Deep learning is a subset of machine learning",
    "Natural language processing deals with text data",
    "Computer vision processes image data",
    "Machine learning and deep learning are related fields"
]

# Create a Bag of Words representation
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Convert to DataFrame for better visualization
df_bow = pd.DataFrame(X.toarray(), columns=feature_names)
print("Bag of Words representation:")
print(df_bow)

# Visualize word frequencies
plt.figure(figsize=(12, 6))
plt.imshow(X.toarray(), aspect='auto', cmap='viridis')
plt.colorbar(label='Word Frequency')
plt.title('Bag of Words: Document-Term Matrix')
plt.xlabel('Term Index')
plt.ylabel('Document Index')
plt.tight_layout()
plt.show()

# Visualize document similarities using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8, s=100)
for i, doc in enumerate(documents):
    plt.annotate(f"Doc {i+1}", (X_pca[i, 0], X_pca[i, 1]), 
                xytext=(5, 5), textcoords='offset points')
plt.title('Document Similarity (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

#### TF-IDF (Term Frequency-Inverse Document Frequency)

**TF-IDF** weights terms based on their frequency in a document and rarity across documents:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample text data (same as before)
documents = [
    "Machine learning is fascinating",
    "Deep learning is a subset of machine learning",
    "Natural language processing deals with text data",
    "Computer vision processes image data",
    "Machine learning and deep learning are related fields"
]

# Create a TF-IDF representation
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(documents)

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Convert to DataFrame for better visualization
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)
print("TF-IDF representation:")
print(df_tfidf)

# Visualize TF-IDF values
plt.figure(figsize=(12, 6))
plt.imshow(X_tfidf.toarray(), aspect='auto', cmap='viridis')
plt.colorbar(label='TF-IDF Value')
plt.title('TF-IDF: Document-Term Matrix')
plt.xlabel('Term Index')
plt.ylabel('Document Index')
plt.tight_layout()
plt.show()

# Compare BoW and TF-IDF for a specific document
doc_idx = 1  # Second document
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(feature_names, df_bow.iloc[doc_idx], color='skyblue')
plt.title(f'BoW for Document {doc_idx+1}')
plt.xlabel('Term')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.grid(True, alpha=0.3, axis='y')

plt.subplot(1, 2, 2)
plt.bar(feature_names, df_tfidf.iloc[doc_idx], color='lightgreen')
plt.title(f'TF-IDF for Document {doc_idx+1}')
plt.xlabel('Term')
plt.ylabel('TF-IDF Value')
plt.xticks(rotation=90)
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

#### Word Embeddings

**Word embeddings** represent words as dense vectors in a continuous vector space:

```python
import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.decomposition import PCA

# Sample sentences
sentences = [
    ["machine", "learning", "is", "fascinating"],
    ["deep", "learning", "is", "a", "subset", "of", "machine", "learning"],
    ["natural", "language", "processing", "deals", "with", "text", "data"],
    ["computer", "vision", "processes", "image", "data"],
    ["machine", "learning", "and", "deep", "learning", "are", "related", "fields"]
]

# Train a Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Get word vectors for visualization
words = list(model.wv.index_to_key)
word_vectors = [model.wv[word] for word in words]

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
result = pca.fit_transform(word_vectors)

# Visualize word embeddings
plt.figure(figsize=(12, 8))
plt.scatter(result[:, 0], result[:, 1], c='lightblue', alpha=0.7)
for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]))
plt.title('Word Embeddings Visualization (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Find similar words
target_word = "learning"
similar_words = model.wv.most_similar(target_word, topn=5)
print(f"Words similar to '{target_word}':")
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.4f}")
```

### Image Feature Extraction

Images can be represented using various features:

#### Raw Pixels

The simplest representation is the raw pixel values:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA

# Load the digits dataset
digits = load_digits()
X = digits.data
y = digits.target

# Display some example digits
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(digits.images[i], cmap='gray')
    plt.title(f'Digit: {y[i]}')
    plt.axis('off')
plt.tight_layout()
plt.show()

# Visualize the pixel intensity distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(X.flatten(), bins=16, color='skyblue', edgecolor='black')
plt.title('Distribution of Pixel Intensities')
plt.xlabel('Pixel Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Visualize the mean digit for each class
plt.subplot(1, 2, 2)
mean_digits = np.zeros((10, 64))
for i in range(10):
    mean_digits[i] = X[y == i].mean(axis=0)

plt.imshow(np.vstack([d.reshape(8, 8) for d in mean_digits]), 
          aspect='auto', cmap='gray')
plt.title('Mean Digit Images')
plt.xlabel('Pixel X')
plt.ylabel('Digit Class')
plt.tight_layout()
plt.show()

# Dimensionality reduction with PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualize digits in 2D space
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.title('PCA of Handwritten Digits')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

#### Histogram of Oriented Gradients (HOG)

**HOG** captures edge directions and is useful for object detection:

```python
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from skimage import exposure
from sklearn.datasets import load_digits

# Load the digits dataset
digits = load_digits()
images = digits.images

# Select a sample image
sample_idx = 0
image = images[sample_idx]

# Calculate HOG features
fd, hog_image = hog(image, orientations=8, pixels_per_cell=(2, 2),
                   cells_per_block=(1, 1), visualize=True)

# Rescale histogram for better display
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

# Display the results
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.imshow(image, cmap='gray')
plt.title('Original Image')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(hog_image_rescaled, cmap='gray')
plt.title('HOG Features')
plt.axis('off')

plt.tight_layout()
plt.show()

# Visualize HOG features for different digits
plt.figure(figsize=(15, 8))
for i in range(10):
    # Find an example of digit i
    idx = np.where(digits.target == i)[0][0]
    image = images[idx]
    
    # Calculate HOG features
    fd, hog_image = hog(image, orientations=8, pixels_per_cell=(2, 2),
                       cells_per_block=(1, 1), visualize=True)
    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))
    
    # Display original and HOG
    plt.subplot(2, 10, i+1)
    plt.imshow(image, cmap='gray')
    plt.title(f'Digit {i}')
    plt.axis('off')
    
    plt.subplot(2, 10, i+11)
    plt.imshow(hog_image_rescaled, cmap='gray')
    plt.title('HOG')
    plt.axis('off')

plt.tight_layout()
plt.show()
```

### Feature Extraction with Principal Component Analysis (PCA)

**PCA** reduces dimensionality while preserving variance:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X)

# Explained variance
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Visualize explained variance
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='skyblue')
plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, 'ro-')
plt.title('Explained Variance by Components')
plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.grid(True, alpha=0.3)
plt.xticks(range(1, len(explained_variance) + 1))

# Visualize first two principal components
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Species')
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize feature loadings
plt.figure(figsize=(10, 6))
loadings = pca.components_.T
for i, feature in enumerate(feature_names):
    plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.05, head_length=0.05)
    plt.text(loadings[i, 0] * 1.1, loadings[i, 1] * 1.1, feature)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.2)
plt.title('PCA Feature Loadings')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.tight_layout()
plt.show()
```

## Feature Selection

**Feature selection** is the process of selecting a subset of relevant features for model training.

### Why Feature Selection?

Feature selection offers several benefits:

1. **Reduced Overfitting**: Fewer features can lead to simpler models that generalize better
2. **Improved Accuracy**: Removing irrelevant features can improve model performance
3. **Reduced Training Time**: Fewer features mean faster training
4. **Enhanced Interpretability**: Models with fewer features are easier to understand

### Filter Methods

**Filter methods** select features based on their statistical properties:

#### Correlation-based Selection

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names
y = boston.target

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)
df['PRICE'] = y

# Calculate correlation with target
correlation = df.corr()['PRICE'].sort_values(ascending=False)
print("Correlation with target (PRICE):")
print(correlation)

# Visualize correlation with target
plt.figure(figsize=(12, 6))
correlation.drop('PRICE').plot(kind='bar', color='skyblue')
plt.title('Correlation with Target Variable (PRICE)')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Visualize correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Select top k features based on correlation
k = 5
top_features = correlation.drop('PRICE').abs().sort_values(ascending=False).head(k).index.tolist()
print(f"\nTop {k} features based on correlation:")
print(top_features)

# Visualize selected features
plt.figure(figsize=(15, 10))
for i, feature in enumerate(top_features):
    plt.subplot(2, 3, i+1)
    plt.scatter(df[feature], df['PRICE'], alpha=0.7)
    plt.title(f'{feature} vs PRICE')
    plt.xlabel(feature)
    plt.ylabel('PRICE')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Variance Threshold

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_selection import VarianceThreshold
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Calculate variance of each feature
variances = df.var().sort_values(ascending=False)
print("Feature variances:")
print(variances)

# Visualize feature variances
plt.figure(figsize=(12, 6))
variances.plot(kind='bar', color='lightgreen')
plt.title('Feature Variances')
plt.xlabel('Features')
plt.ylabel('Variance')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Apply variance threshold
threshold = 10  # Example threshold
selector = VarianceThreshold(threshold=threshold)
X_selected = selector.fit_transform(X)
selected_features = [feature_names[i] for i in range(len(feature_names)) if selector.get_support()[i]]

print(f"\nFeatures with variance > {threshold}:")
print(selected_features)
print(f"Reduced from {X.shape[1]} to {X_selected.shape[1]} features")

# Visualize selected vs. removed features
plt.figure(figsize=(12, 6))
selected_var = [variances[f] for f in selected_features]
removed_var = [variances[f] for f in feature_names if f not in selected_features]
selected_names = selected_features
removed_names = [f for f in feature_names if f not in selected_features]

plt.bar(range(len(selected_var)), selected_var, color='green', alpha=0.7, label='Selected')
plt.bar(range(len(selected_var), len(selected_var) + len(removed_var)), 
       removed_var, color='red', alpha=0.7, label='Removed')
plt.axhline(y=threshold, color='k', linestyle='--', label=f'Threshold = {threshold}')
plt.xticks(range(len(feature_names)), selected_names + removed_names, rotation=90)
plt.title('Feature Selection by Variance Threshold')
plt.xlabel('Features')
plt.ylabel('Variance')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

#### Statistical Tests (ANOVA F-value)

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Apply ANOVA F-value feature selection
selector = SelectKBest(f_classif, k=2)
X_selected = selector.fit_transform(X, y)
scores = selector.scores_
p_values = selector.pvalues_

# Create a DataFrame with results
results = pd.DataFrame({
    'Feature': feature_names,
    'F-Score': scores,
    'P-Value': p_values
})
results = results.sort_values('F-Score', ascending=False)
print("ANOVA F-value feature selection results:")
print(results)

# Visualize F-scores
plt.figure(figsize=(12, 6))
plt.bar(results['Feature'], results['F-Score'], color='purple', alpha=0.7)
plt.title('Feature Importance (ANOVA F-value)')
plt.xlabel('Features')
plt.ylabel('F-Score')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Visualize data with the top 2 features
top_features = results['Feature'].iloc[:2].tolist()
top_indices = [feature_names.index(f) for f in top_features]
X_top = X[:, top_indices]

plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_top[:, 0], X_top[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Species')
plt.title('Iris Dataset with Top 2 Features')
plt.xlabel(top_features[0])
plt.ylabel(top_features[1])
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Wrapper Methods

**Wrapper methods** evaluate subsets of features using a model:

#### Recursive Feature Elimination (RFE)

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

# Load the Breast Cancer dataset
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
feature_names = cancer.feature_names

# Apply RFE with Logistic Regression
model = LogisticRegression(max_iter=1000)
rfe = RFE(estimator=model, n_features_to_select=5, step=1)
X_rfe = rfe.fit_transform(X, y)

# Get selected features
selected_features = [feature_names[i] for i in range(len(feature_names)) if rfe.support_[i]]
feature_ranking = rfe.ranking_

# Create a DataFrame with results
results = pd.DataFrame({
    'Feature': feature_names,
    'Selected': rfe.support_,
    'Ranking': feature_ranking
})
results = results.sort_values('Ranking')
print("RFE feature selection results:")
print(results)

# Visualize feature ranking
plt.figure(figsize=(12, 6))
plt.bar(results['Feature'], results['Ranking'], color='teal', alpha=0.7)
plt.title('Feature Ranking (RFE)')
plt.xlabel('Features')
plt.ylabel('Ranking (lower is better)')
plt.xticks(rotation=90)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Visualize selected vs. non-selected features
plt.figure(figsize=(12, 6))
colors = ['green' if selected else 'red' for selected in rfe.support_]
plt.bar(feature_names, feature_ranking, color=colors, alpha=0.7)
plt.title('Feature Selection with RFE')
plt.xlabel('Features')
plt.ylabel('Ranking (lower is better)')
plt.xticks(rotation=90)
plt.axhline(y=1.5, color='k', linestyle='--', label='Selection Threshold')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

### Embedded Methods

**Embedded methods** perform feature selection as part of the model training process:

#### L1 Regularization (Lasso)

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
y = boston.target
feature_names = boston.feature_names

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply Lasso with different alpha values
alphas = [0.001, 0.01, 0.1, 1, 10]
coefficients = []

for alpha in alphas:
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_scaled, y)
    coefficients.append(lasso.coef_)

# Create a DataFrame with results
results = pd.DataFrame(coefficients, index=alphas, columns=feature_names)
results.index.name = 'Alpha'

# Visualize coefficients for different alpha values
plt.figure(figsize=(15, 10))
ax = plt.gca()
results.T.plot(kind='bar', ax=ax)
plt.title('Lasso Coefficients with Different Alpha Values')
plt.xlabel('Features')
plt.ylabel('Coefficient Value')
plt.grid(True, alpha=0.3, axis='y')
plt.legend(title='Alpha')
plt.tight_layout()
plt.show()

# Visualize feature selection path
plt.figure(figsize=(12, 6))
for feature, coef_values in results.T.iterrows():
    plt.plot(alphas, coef_values, 'o-', label=feature)
plt.xscale('log')
plt.title('Lasso Feature Selection Path')
plt.xlabel('Alpha (log scale)')
plt.ylabel('Coefficient Value')
plt.grid(True, alpha=0.3)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Select optimal alpha (example)
optimal_alpha = 0.1
lasso = Lasso(alpha=optimal_alpha, max_iter=10000)
lasso.fit(X_scaled, y)

# Get selected features
selected_features = [feature_names[i] for i in range(len(feature_names)) if lasso.coef_[i] != 0]
print(f"Selected features with alpha = {optimal_alpha}:")
print(selected_features)
print(f"Reduced from {X.shape[1]} to {len(selected_features)} features")
```

## Summary

Feature extraction and selection are crucial steps in the machine learning pipeline:

1. **Feature Extraction**:
   - Text: Bag of Words, TF-IDF, Word Embeddings
   - Images: Raw Pixels, HOG, CNN Features
   - Dimensionality Reduction: PCA, t-SNE

2. **Feature Selection**:
   - Filter Methods: Correlation, Variance Threshold, Statistical Tests
   - Wrapper Methods: Recursive Feature Elimination
   - Embedded Methods: L1 Regularization (Lasso)

These techniques help create more effective features and select the most relevant ones, leading to better model performance, reduced overfitting, and improved interpretability.

In the next part, we'll explore feature construction and transformation techniques to further enhance the quality of features.

## References

1. Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of Machine Learning Research, 3, 1157-1182.
2. Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
3. Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) (Vol. 1, pp. 886-893).
4. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.
