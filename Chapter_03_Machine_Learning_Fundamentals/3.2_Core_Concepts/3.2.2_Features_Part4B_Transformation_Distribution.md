# 3.2.2 Features - Part 4B: Feature Transformation - Distribution

## Distribution Transformations

Distribution transformations change the shape of a feature's distribution to make it more suitable for modeling. These transformations can help normalize skewed data, stabilize variance, and make relationships more linear.

### Log Transformation

**Log transformation** is useful for positively skewed data:

$$X_{transformed} = \log(X + c)$$

where c is a constant added to handle zeros or negative values.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a positively skewed feature
feature = 'LSTAT'  # Lower status of the population
x = df[feature].values

# Apply log transformation
log_x = np.log1p(x)  # log(1+x) to handle zeros

# Calculate skewness
original_skew = stats.skew(x)
log_skew = stats.skew(log_x)

print(f"Original skewness: {original_skew:.4f}")
print(f"Log transform skewness: {log_skew:.4f}")

# Visualize the transformation
plt.figure(figsize=(15, 5))

# Original distribution
plt.subplot(1, 3, 1)
plt.hist(x, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original {feature} (Skewness: {original_skew:.4f})')
plt.xlabel(feature)
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Log transformation
plt.subplot(1, 3, 2)
plt.hist(log_x, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Log(1+{feature}) (Skewness: {log_skew:.4f})')
plt.xlabel(f'Log(1+{feature})')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Q-Q plot
plt.subplot(1, 3, 3)
stats.probplot(log_x, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: Log(1+{feature})')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Demonstrate the effect on correlation
target = boston.target
original_corr = np.corrcoef(x, target)[0, 1]
log_corr = np.corrcoef(log_x, target)[0, 1]

print(f"Correlation with target (original): {original_corr:.4f}")
print(f"Correlation with target (log transform): {log_corr:.4f}")

# Visualize the relationship with the target
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(x, target, alpha=0.7)
plt.title(f'Original {feature} vs. Target (r = {original_corr:.4f})')
plt.xlabel(feature)
plt.ylabel('Target (MEDV)')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(log_x, target, alpha=0.7)
plt.title(f'Log(1+{feature}) vs. Target (r = {log_corr:.4f})')
plt.xlabel(f'Log(1+{feature})')
plt.ylabel('Target (MEDV)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Box-Cox Transformation

**Box-Cox transformation** is a family of power transformations that includes the log transformation as a special case:

$$X_{transformed} = \begin{cases}
\frac{X^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\
\log(X) & \text{if } \lambda = 0
\end{cases}$$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a positively skewed feature
feature = 'LSTAT'  # Lower status of the population
x = df[feature].values

# Apply Box-Cox transformation
# Box-Cox requires positive values
x_positive = x + 1e-10  # Add small constant to ensure positivity
x_boxcox, lambda_value = stats.boxcox(x_positive)

# Calculate skewness
original_skew = stats.skew(x)
boxcox_skew = stats.skew(x_boxcox)

print(f"Original skewness: {original_skew:.4f}")
print(f"Box-Cox transform skewness: {boxcox_skew:.4f}")
print(f"Optimal lambda value: {lambda_value:.4f}")

# Visualize the transformation
plt.figure(figsize=(15, 5))

# Original distribution
plt.subplot(1, 3, 1)
plt.hist(x, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original {feature} (Skewness: {original_skew:.4f})')
plt.xlabel(feature)
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Box-Cox transformation
plt.subplot(1, 3, 2)
plt.hist(x_boxcox, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Box-Cox {feature} (Skewness: {boxcox_skew:.4f}, λ = {lambda_value:.4f})')
plt.xlabel(f'Box-Cox({feature})')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Q-Q plot
plt.subplot(1, 3, 3)
stats.probplot(x_boxcox, dist="norm", plot=plt)
plt.title(f'Q-Q Plot: Box-Cox({feature})')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Compare different lambda values
lambda_values = [-2, -1, -0.5, 0, 0.5, 1, 2]
plt.figure(figsize=(15, 10))

for i, lambda_val in enumerate(lambda_values):
    plt.subplot(3, 3, i+1)
    
    if lambda_val == 0:
        x_transformed = np.log(x_positive)
    else:
        x_transformed = (x_positive ** lambda_val - 1) / lambda_val
    
    skewness = stats.skew(x_transformed)
    
    plt.hist(x_transformed, bins=30, color='lightgreen', edgecolor='black')
    plt.title(f'λ = {lambda_val} (Skewness: {skewness:.4f})')
    plt.xlabel(f'Transformed {feature}')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)

# Optimal lambda
plt.subplot(3, 3, 8)
plt.hist(x_boxcox, bins=30, color='salmon', edgecolor='black')
plt.title(f'Optimal λ = {lambda_value:.4f} (Skewness: {boxcox_skew:.4f})')
plt.xlabel(f'Box-Cox({feature})')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Skewness vs. lambda
plt.subplot(3, 3, 9)
lambda_range = np.linspace(-2, 2, 100)
skewness_values = []

for lambda_val in lambda_range:
    if lambda_val == 0:
        x_transformed = np.log(x_positive)
    else:
        x_transformed = (x_positive ** lambda_val - 1) / lambda_val
    
    skewness_values.append(abs(stats.skew(x_transformed)))

plt.plot(lambda_range, skewness_values, 'b-')
plt.axvline(x=lambda_value, color='r', linestyle='--', label=f'Optimal λ = {lambda_value:.4f}')
plt.title('Absolute Skewness vs. λ')
plt.xlabel('λ')
plt.ylabel('|Skewness|')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Yeo-Johnson Transformation

**Yeo-Johnson transformation** is similar to Box-Cox but can handle negative values:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.preprocessing import PowerTransformer
from sklearn.datasets import make_regression

# Generate synthetic data with negative values
np.random.seed(42)
X, y = make_regression(n_samples=1000, n_features=1, noise=0.5, random_state=42)
X = X.flatten()

# Add skewness by exponentiating and shifting
X_skewed = np.exp(X) - 5  # This will have both positive and negative values

# Apply Yeo-Johnson transformation
pt = PowerTransformer(method='yeo-johnson')
X_yeojohnson = pt.fit_transform(X_skewed.reshape(-1, 1)).flatten()

# Calculate skewness
original_skew = stats.skew(X_skewed)
yeojohnson_skew = stats.skew(X_yeojohnson)

print(f"Original skewness: {original_skew:.4f}")
print(f"Yeo-Johnson transform skewness: {yeojohnson_skew:.4f}")
print(f"Optimal lambda value: {pt.lambdas_[0]:.4f}")

# Visualize the transformation
plt.figure(figsize=(15, 5))

# Original distribution
plt.subplot(1, 3, 1)
plt.hist(X_skewed, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original Data (Skewness: {original_skew:.4f})')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Yeo-Johnson transformation
plt.subplot(1, 3, 2)
plt.hist(X_yeojohnson, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Yeo-Johnson Transform (Skewness: {yeojohnson_skew:.4f})')
plt.xlabel('Transformed Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Q-Q plot
plt.subplot(1, 3, 3)
stats.probplot(X_yeojohnson, dist="norm", plot=plt)
plt.title('Q-Q Plot: Yeo-Johnson Transform')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Compare Box-Cox and Yeo-Johnson
# For Box-Cox, we need to make all values positive
X_positive = X_skewed - np.min(X_skewed) + 1  # Shift to make all values positive

# Apply Box-Cox transformation
X_boxcox, lambda_boxcox = stats.boxcox(X_positive)

# Calculate skewness
boxcox_skew = stats.skew(X_boxcox)

print(f"Box-Cox transform skewness: {boxcox_skew:.4f}")
print(f"Box-Cox optimal lambda value: {lambda_boxcox:.4f}")

# Visualize the comparison
plt.figure(figsize=(15, 5))

# Original distribution
plt.subplot(1, 3, 1)
plt.hist(X_skewed, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original Data (Skewness: {original_skew:.4f})')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Box-Cox transformation
plt.subplot(1, 3, 2)
plt.hist(X_boxcox, bins=30, color='salmon', edgecolor='black')
plt.title(f'Box-Cox Transform (Skewness: {boxcox_skew:.4f})')
plt.xlabel('Transformed Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Yeo-Johnson transformation
plt.subplot(1, 3, 3)
plt.hist(X_yeojohnson, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Yeo-Johnson Transform (Skewness: {yeojohnson_skew:.4f})')
plt.xlabel('Transformed Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Quantile Transformation

**Quantile transformation** maps the original distribution to a uniform or normal distribution:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.preprocessing import QuantileTransformer
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)

# Select a skewed feature
feature = 'LSTAT'  # Lower status of the population
x = df[feature].values.reshape(-1, 1)

# Apply quantile transformations
qt_uniform = QuantileTransformer(output_distribution='uniform', random_state=42)
qt_normal = QuantileTransformer(output_distribution='normal', random_state=42)

x_uniform = qt_uniform.fit_transform(x).flatten()
x_normal = qt_normal.fit_transform(x).flatten()

# Calculate skewness
original_skew = stats.skew(x)
uniform_skew = stats.skew(x_uniform)
normal_skew = stats.skew(x_normal)

print(f"Original skewness: {original_skew:.4f}")
print(f"Uniform quantile transform skewness: {uniform_skew:.4f}")
print(f"Normal quantile transform skewness: {normal_skew:.4f}")

# Visualize the transformations
plt.figure(figsize=(15, 10))

# Original distribution
plt.subplot(2, 2, 1)
plt.hist(x, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original {feature} (Skewness: {original_skew:.4f})')
plt.xlabel(feature)
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Uniform quantile transformation
plt.subplot(2, 2, 2)
plt.hist(x_uniform, bins=30, color='lightgreen', edgecolor='black')
plt.title(f'Uniform Quantile Transform (Skewness: {uniform_skew:.4f})')
plt.xlabel('Transformed Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Normal quantile transformation
plt.subplot(2, 2, 3)
plt.hist(x_normal, bins=30, color='salmon', edgecolor='black')
plt.title(f'Normal Quantile Transform (Skewness: {normal_skew:.4f})')
plt.xlabel('Transformed Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Q-Q plot for normal quantile transform
plt.subplot(2, 2, 4)
stats.probplot(x_normal, dist="norm", plot=plt)
plt.title('Q-Q Plot: Normal Quantile Transform')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Demonstrate the effect on outliers
# Add some outliers to the data
x_with_outliers = x.copy()
x_with_outliers[0] = x_with_outliers[0] * 5  # Multiply first value by 5
x_with_outliers[1] = x_with_outliers[1] * 10  # Multiply second value by 10

# Apply quantile transformations
x_with_outliers_uniform = qt_uniform.transform(x_with_outliers).flatten()
x_with_outliers_normal = qt_normal.transform(x_with_outliers).flatten()

# Visualize the effect on outliers
plt.figure(figsize=(15, 5))

# Original data with outliers
plt.subplot(1, 3, 1)
plt.boxplot(x_with_outliers)
plt.title('Original Data with Outliers')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)

# Uniform quantile transformation
plt.subplot(1, 3, 2)
plt.boxplot(x_with_outliers_uniform)
plt.title('Uniform Quantile Transform')
plt.ylabel('Transformed Value')
plt.grid(True, alpha=0.3)

# Normal quantile transformation
plt.subplot(1, 3, 3)
plt.boxplot(x_with_outliers_normal)
plt.title('Normal Quantile Transform')
plt.ylabel('Transformed Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Binning and Discretization

**Binning** (or discretization) transforms continuous features into categorical ones:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
boston = load_boston()
X = boston.data
feature_names = boston.feature_names
y = boston.target

# Create a DataFrame
df = pd.DataFrame(X, columns=feature_names)
df['MEDV'] = y  # Add target variable

# Select a feature to discretize
feature = 'LSTAT'  # Lower status of the population
x = df[feature].values.reshape(-1, 1)

# Apply different binning strategies
n_bins = 5
discretizers = {
    'Uniform': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform'),
    'Quantile': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile'),
    'K-means': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='kmeans')
}

binned_data = {}
for name, discretizer in discretizers.items():
    binned_data[name] = discretizer.fit_transform(x).flatten()

# Visualize the binning strategies
plt.figure(figsize=(15, 10))

# Original data
plt.subplot(2, 2, 1)
plt.hist(x, bins=30, color='skyblue', edgecolor='black')
plt.title(f'Original {feature}')
plt.xlabel(feature)
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Binned data
for i, (name, data) in enumerate(binned_data.items()):
    plt.subplot(2, 2, i+2)
    plt.hist(data, bins=n_bins, color='lightgreen', edgecolor='black')
    plt.title(f'{name} Binning (n_bins={n_bins})')
    plt.xlabel(f'Binned {feature}')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualize the relationship with the target variable
plt.figure(figsize=(15, 10))

# Original data
plt.subplot(2, 2, 1)
plt.scatter(x, y, alpha=0.7)
plt.title(f'Original {feature} vs. Target')
plt.xlabel(feature)
plt.ylabel('MEDV')
plt.grid(True, alpha=0.3)

# Binned data
for i, (name, data) in enumerate(binned_data.items()):
    plt.subplot(2, 2, i+2)
    plt.scatter(data, y, alpha=0.7)
    plt.title(f'{name} Binning vs. Target')
    plt.xlabel(f'Binned {feature}')
    plt.ylabel('MEDV')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# One-hot encoding of binned data
discretizer = KBinsDiscretizer(n_bins=n_bins, encode='onehot-dense', strategy='quantile')
x_onehot = discretizer.fit_transform(x)

# Create a DataFrame with one-hot encoded features
onehot_columns = [f'{feature}_bin_{i}' for i in range(n_bins)]
df_onehot = pd.DataFrame(x_onehot, columns=onehot_columns)
df_onehot['MEDV'] = y  # Add target variable

# Display the first few rows
print("One-hot encoded binned features:")
print(df_onehot.head())

# Visualize the one-hot encoded features
plt.figure(figsize=(15, 5))
for i in range(n_bins):
    plt.subplot(1, n_bins, i+1)
    bin_col = f'{feature}_bin_{i}'
    plt.scatter(df_onehot[bin_col], df_onehot['MEDV'], alpha=0.7)
    plt.title(f'{bin_col} vs. Target')
    plt.xlabel(bin_col)
    plt.ylabel('MEDV')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

Distribution transformations can significantly improve the quality of features:

1. **Log Transformation**:
   - Reduces positive skewness
   - Stabilizes variance
   - Makes multiplicative relationships more linear

2. **Box-Cox Transformation**:
   - Family of power transformations
   - Automatically finds the optimal transformation parameter
   - Requires positive values

3. **Yeo-Johnson Transformation**:
   - Similar to Box-Cox but can handle negative values
   - Useful for data with mixed signs

4. **Quantile Transformation**:
   - Maps to uniform or normal distribution
   - Robust to outliers
   - Preserves rank ordering of values

5. **Binning and Discretization**:
   - Transforms continuous features into categorical ones
   - Can capture non-linear relationships
   - Reduces the impact of outliers and noise

These transformations can help normalize data, stabilize variance, and make relationships more linear, leading to better model performance.

## References

1. Box, G. E., & Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statistical Society: Series B (Methodological), 26(2), 211-243.
2. Yeo, I. K., & Johnson, R. A. (2000). A new family of power transformations to improve normality or symmetry. Biometrika, 87(4), 954-959.
3. Kuhn, M., & Johnson, K. (2019). Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.
